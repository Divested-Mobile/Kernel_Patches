From 74db4a59104154293244709a6bcff9ddfc2ae275 Mon Sep 17 00:00:00 2001
From: Daniel Micay <danielmicay@gmail.com>
Date: Thu, 4 May 2017 15:58:57 -0400
Subject: [PATCH] slub: Add support for verifying slab sanitization

This is an extension to the sanitization feature in PaX for when
sacricifing more performance for security is acceptable.

The initial version from Daniel Micay was relying on PAGE_SANITIZE. It
now relies on upstream's init_on_free.

Signed-off-by: Daniel Micay <danielmicay@gmail.com>
Signed-off-by: Thibaut Sautereau <thibaut.sautereau@ssi.gouv.fr>
Signed-off-by: anupritaisno1 <www.anuprita804@gmail.com>
Signed-off-by: randomhydrosol <randomhydrosol@glassrom.org>
---
 mm/slub.c                  | 36 ++++++++++++++++++++++++++++++++----
 security/Kconfig.hardening |  7 +++++++
 2 files changed, 39 insertions(+), 4 deletions(-)

diff --git a/mm/slub.c b/mm/slub.c
index cd83372126ba..3962bcabe490 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -125,6 +125,12 @@ static inline int kmem_cache_debug(struct kmem_cache *s)
 #endif
 }
 
+static inline bool has_sanitize_verify(struct kmem_cache *s)
+{
+	return IS_ENABLED(CONFIG_SLAB_SANITIZE_VERIFY) &&
+	       slab_want_init_on_free(s);
+}
+
 void *fixup_red_left(struct kmem_cache *s, void *p)
 {
 	if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE)
@@ -1455,7 +1461,7 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
 							   : 0;
 			memset((char *)object + s->inuse, 0,
 			       s->size - s->inuse - rsize);
-			if (s->ctor)
+			if (!IS_ENABLED(CONFIG_SLAB_SANITIZE_VERIFY) && s->ctor)
 				s->ctor(object);
 		}
 		/* If object's reuse doesn't have to be delayed */
@@ -1499,7 +1505,7 @@ static void *setup_object(struct kmem_cache *s, struct page *page,
 {
 	setup_object_debug(s, page, object);
 	object = kasan_init_slab_obj(s, object);
-	if (unlikely(s->ctor)) {
+	if (unlikely(s->ctor) && !has_sanitize_verify(s)) {
 		kasan_unpoison_object_data(s, object);
 		s->ctor(object);
 		kasan_poison_object_data(s, object);
@@ -2815,7 +2821,16 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 
 	maybe_wipe_obj_freeptr(s, object);
 
-	if (unlikely(slab_want_init_on_alloc(gfpflags, s)) && object)
+	if (has_sanitize_verify(s) && object) {
+		/* KASAN hasn't unpoisoned the object yet (this is done in the
+		 * post-alloc hook), so let's do it temporarily.
+		 */
+		kasan_unpoison_object_data(s, object);
+		BUG_ON(memchr_inv(object, 0, s->object_size));
+		if (s->ctor)
+			s->ctor(object);
+		kasan_poison_object_data(s, object);
+	} else if (unlikely(slab_want_init_on_alloc(gfpflags, s)) && object)
 		memset(object, 0, s->object_size);
 
 	slab_post_alloc_hook(s, gfpflags, 1, &object);
@@ -3251,7 +3266,20 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 	local_irq_enable();
 
 	/* Clear memory outside IRQ disabled fastpath loop */
-	if (unlikely(slab_want_init_on_alloc(flags, s))) {
+	if (has_sanitize_verify(s)) {
+		int j;
+
+		for (j = 0; j < i; j++) {
+			/* KASAN hasn't unpoisoned the object yet (this is done
+			 * in the post-alloc hook), so let's do it temporarily.
+			 */
+			kasan_unpoison_object_data(s, p[j]);
+			BUG_ON(memchr_inv(p[j], 0, s->object_size));
+			if (s->ctor)
+				s->ctor(p[j]);
+			kasan_poison_object_data(s, p[j]);
+		}
+	} else if (unlikely(slab_want_init_on_alloc(flags, s))) {
 		int j;
 
 		for (j = 0; j < i; j++)
diff --git a/security/Kconfig.hardening b/security/Kconfig.hardening
index 0d037ed7abec..61b5c63c941c 100644
--- a/security/Kconfig.hardening
+++ b/security/Kconfig.hardening
@@ -137,6 +137,13 @@ config PAGE_SANITIZE_VERIFY
 	  When init_on_free is enabled, verify that newly allocated pages
 	  are zeroed to detect write-after-free bugs.
 
+config SLAB_SANITIZE_VERIFY
+	default y
+	bool "Verify sanitized SLAB allocations"
+	help
+	  When init_on_free is enabled, verify that newly allocated slab
+	  objects are zeroed to detect write-after-free bugs.
+
 endmenu
 
 endmenu
