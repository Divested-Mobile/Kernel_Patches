From 38b1a598c0c7ea165b7bfe92baa6330e55e0ec6b Mon Sep 17 00:00:00 2001
From: Yasir Malik <ymalik@codeaurora.org>
Date: Thu, 3 Aug 2017 11:37:32 -0400
Subject: msm: ipa: delete unused source files in FSM

Delete source files and Makefile for ipa/.  Update parent Makefile
to no longer call ipa/ Makefile.

Change-Id: I8b48b1483b403a298d0b61b23a4cb744cfd32a2f
CRs-Fixed: 2037144, 2056307
Signed-off-by: Yasir Malik <ymalik@codeaurora.org>
---
 drivers/platform/msm/Makefile                      |    1 -
 drivers/platform/msm/ipa/Makefile                  |    7 -
 drivers/platform/msm/ipa/ipa.c                     | 4504 ------------------
 drivers/platform/msm/ipa/ipa_client.c              |  736 ---
 drivers/platform/msm/ipa/ipa_debugfs.c             | 2024 --------
 drivers/platform/msm/ipa/ipa_dma.c                 |  880 ----
 drivers/platform/msm/ipa/ipa_dp.c                  | 3307 -------------
 drivers/platform/msm/ipa/ipa_flt.c                 | 1508 ------
 drivers/platform/msm/ipa/ipa_hdr.c                 | 1423 ------
 drivers/platform/msm/ipa/ipa_hw_defs.h             |  450 --
 drivers/platform/msm/ipa/ipa_i.h                   | 1660 -------
 drivers/platform/msm/ipa/ipa_interrupts.c          |  313 --
 drivers/platform/msm/ipa/ipa_intf.c                |  642 ---
 drivers/platform/msm/ipa/ipa_mhi.c                 | 1865 --------
 drivers/platform/msm/ipa/ipa_nat.c                 |  839 ----
 drivers/platform/msm/ipa/ipa_qmi_service.c         | 1210 -----
 drivers/platform/msm/ipa/ipa_qmi_service.h         |  286 --
 drivers/platform/msm/ipa/ipa_qmi_service_v01.c     | 2366 ---------
 drivers/platform/msm/ipa/ipa_ram_mmap.h            |  560 ---
 drivers/platform/msm/ipa/ipa_reg.h                 |  317 --
 drivers/platform/msm/ipa/ipa_rm.c                  | 1079 -----
 drivers/platform/msm/ipa/ipa_rm_dependency_graph.c |  245 -
 drivers/platform/msm/ipa/ipa_rm_dependency_graph.h |   47 -
 drivers/platform/msm/ipa/ipa_rm_i.h                |  128 -
 drivers/platform/msm/ipa/ipa_rm_inactivity_timer.c |  268 --
 drivers/platform/msm/ipa/ipa_rm_peers_list.c       |  247 -
 drivers/platform/msm/ipa/ipa_rm_peers_list.h       |   53 -
 drivers/platform/msm/ipa/ipa_rm_resource.c         | 1164 -----
 drivers/platform/msm/ipa/ipa_rm_resource.h         |  162 -
 drivers/platform/msm/ipa/ipa_rt.c                  | 1489 ------
 drivers/platform/msm/ipa/ipa_uc.c                  |  888 ----
 drivers/platform/msm/ipa/ipa_uc_mhi.c              |  948 ----
 drivers/platform/msm/ipa/ipa_uc_wdi.c              | 1617 -------
 drivers/platform/msm/ipa/ipa_utils.c               | 5008 --------------------
 drivers/platform/msm/ipa/odu_bridge.c              | 1212 -----
 drivers/platform/msm/ipa/rmnet_ipa.c               | 2810 -----------
 drivers/platform/msm/ipa/rmnet_ipa_fd_ioctl.c      |  391 --
 drivers/platform/msm/ipa/teth_bridge.c             |  243 -
 38 files changed, 42897 deletions(-)
 delete mode 100644 drivers/platform/msm/ipa/Makefile
 delete mode 100644 drivers/platform/msm/ipa/ipa.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_client.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_debugfs.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_dma.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_dp.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_flt.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_hdr.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_hw_defs.h
 delete mode 100644 drivers/platform/msm/ipa/ipa_i.h
 delete mode 100644 drivers/platform/msm/ipa/ipa_interrupts.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_intf.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_mhi.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_nat.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_qmi_service.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_qmi_service.h
 delete mode 100644 drivers/platform/msm/ipa/ipa_qmi_service_v01.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_ram_mmap.h
 delete mode 100644 drivers/platform/msm/ipa/ipa_reg.h
 delete mode 100644 drivers/platform/msm/ipa/ipa_rm.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_rm_dependency_graph.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_rm_dependency_graph.h
 delete mode 100644 drivers/platform/msm/ipa/ipa_rm_i.h
 delete mode 100644 drivers/platform/msm/ipa/ipa_rm_inactivity_timer.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_rm_peers_list.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_rm_peers_list.h
 delete mode 100644 drivers/platform/msm/ipa/ipa_rm_resource.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_rm_resource.h
 delete mode 100644 drivers/platform/msm/ipa/ipa_rt.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_uc.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_uc_mhi.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_uc_wdi.c
 delete mode 100644 drivers/platform/msm/ipa/ipa_utils.c
 delete mode 100644 drivers/platform/msm/ipa/odu_bridge.c
 delete mode 100644 drivers/platform/msm/ipa/rmnet_ipa.c
 delete mode 100644 drivers/platform/msm/ipa/rmnet_ipa_fd_ioctl.c
 delete mode 100644 drivers/platform/msm/ipa/teth_bridge.c

diff --git a/drivers/platform/msm/Makefile b/drivers/platform/msm/Makefile
index 99241fa..f426af5 100644
--- a/drivers/platform/msm/Makefile
+++ b/drivers/platform/msm/Makefile
@@ -12,7 +12,6 @@ obj-$(CONFIG_MSM_MHI_UCI) += mhi_uci/
 obj-$(CONFIG_MSM_SSBI) += ssbi.o
 obj-$(CONFIG_USB_BAM) += usb_bam.o
 obj-$(CONFIG_I2C_MSM_PROF_DBG) += i2c-msm-prof-dbg.o
-obj-$(CONFIG_IPA) += ipa/
 obj-$(CONFIG_SPS) += sps/
 obj-$(CONFIG_EP_PCIE) += ep_pcie/
 obj-$(CONFIG_QPNP_POWER_ON) += qpnp-power-on.o
diff --git a/drivers/platform/msm/ipa/Makefile b/drivers/platform/msm/ipa/Makefile
deleted file mode 100644
index c1ca6b7d..00000000
--- a/drivers/platform/msm/ipa/Makefile
+++ /dev/null
@@ -1,7 +0,0 @@
-obj-$(CONFIG_IPA) += ipat.o
-ipat-y := ipa.o ipa_debugfs.o ipa_hdr.o ipa_flt.o ipa_rt.o ipa_dp.o ipa_client.o \
-	ipa_utils.o ipa_nat.o ipa_intf.o teth_bridge.o ipa_interrupts.o odu_bridge.o \
-	ipa_rm.o ipa_rm_dependency_graph.o ipa_rm_peers_list.o ipa_rm_resource.o ipa_rm_inactivity_timer.o \
-	ipa_uc.o ipa_uc_wdi.o ipa_dma.o ipa_uc_mhi.o ipa_mhi.o
-
-obj-$(CONFIG_RMNET_IPA) += rmnet_ipa.o ipa_qmi_service_v01.o ipa_qmi_service.o rmnet_ipa_fd_ioctl.o
diff --git a/drivers/platform/msm/ipa/ipa.c b/drivers/platform/msm/ipa/ipa.c
deleted file mode 100644
index 936def77..00000000
--- a/drivers/platform/msm/ipa/ipa.c
+++ /dev/null
@@ -1,4504 +0,0 @@
-/* Copyright (c) 2012-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/clk.h>
-#include <linux/compat.h>
-#include <linux/device.h>
-#include <linux/dmapool.h>
-#include <linux/fs.h>
-#include <linux/genalloc.h>
-#include <linux/init.h>
-#include <linux/kernel.h>
-#include <linux/mm.h>
-#include <linux/module.h>
-#include <linux/of.h>
-#include <linux/of_platform.h>
-#include <linux/platform_device.h>
-#include <linux/rbtree.h>
-#include <linux/uaccess.h>
-#include <linux/interrupt.h>
-#include <linux/msm-bus.h>
-#include <linux/msm-bus-board.h>
-#include <linux/netdevice.h>
-#include <linux/delay.h>
-#include <linux/qcom_iommu.h>
-#include "ipa_i.h"
-#include "ipa_rm_i.h"
-
-#define IPA_SUMMING_THRESHOLD (0x10)
-#define IPA_PIPE_MEM_START_OFST (0x0)
-#define IPA_PIPE_MEM_SIZE (0x0)
-#define IPA_MOBILE_AP_MODE(x) (x == IPA_MODE_MOBILE_AP_ETH || \
-			       x == IPA_MODE_MOBILE_AP_WAN || \
-			       x == IPA_MODE_MOBILE_AP_WLAN)
-#define IPA_CNOC_CLK_RATE (75 * 1000 * 1000UL)
-#define IPA_A5_MUX_HEADER_LENGTH (8)
-#define IPA_ROUTING_RULE_BYTE_SIZE (4)
-#define IPA_BAM_CNFG_BITS_VALv1_1 (0x7FFFE004)
-#define IPA_BAM_CNFG_BITS_VALv2_0 (0xFFFFE004)
-#define IPA_STATUS_CLEAR_OFST (0x3f28)
-#define IPA_STATUS_CLEAR_SIZE (32)
-#define SPS_IPC_LOGLEVEL 3
-
-#define IPA_AGGR_MAX_STR_LENGTH (10)
-
-#define CLEANUP_TAG_PROCESS_TIMEOUT 150
-
-#define IPA_AGGR_STR_IN_BYTES(str) \
-	(strnlen((str), IPA_AGGR_MAX_STR_LENGTH - 1) + 1)
-
-#define IPA_SPS_PROD_TIMEOUT_MSEC 100
-
-#ifdef CONFIG_COMPAT
-#define IPA_IOC_ADD_HDR32 _IOWR(IPA_IOC_MAGIC, \
-					IPA_IOCTL_ADD_HDR, \
-					compat_uptr_t)
-#define IPA_IOC_DEL_HDR32 _IOWR(IPA_IOC_MAGIC, \
-					IPA_IOCTL_DEL_HDR, \
-					compat_uptr_t)
-#define IPA_IOC_ADD_RT_RULE32 _IOWR(IPA_IOC_MAGIC, \
-					IPA_IOCTL_ADD_RT_RULE, \
-					compat_uptr_t)
-#define IPA_IOC_DEL_RT_RULE32 _IOWR(IPA_IOC_MAGIC, \
-					IPA_IOCTL_DEL_RT_RULE, \
-					compat_uptr_t)
-#define IPA_IOC_ADD_FLT_RULE32 _IOWR(IPA_IOC_MAGIC, \
-					IPA_IOCTL_ADD_FLT_RULE, \
-					compat_uptr_t)
-#define IPA_IOC_DEL_FLT_RULE32 _IOWR(IPA_IOC_MAGIC, \
-					IPA_IOCTL_DEL_FLT_RULE, \
-					compat_uptr_t)
-#define IPA_IOC_GET_RT_TBL32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_GET_RT_TBL, \
-				compat_uptr_t)
-#define IPA_IOC_COPY_HDR32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_COPY_HDR, \
-				compat_uptr_t)
-#define IPA_IOC_QUERY_INTF32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_QUERY_INTF, \
-				compat_uptr_t)
-#define IPA_IOC_QUERY_INTF_TX_PROPS32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_QUERY_INTF_TX_PROPS, \
-				compat_uptr_t)
-#define IPA_IOC_QUERY_INTF_RX_PROPS32 _IOWR(IPA_IOC_MAGIC, \
-					IPA_IOCTL_QUERY_INTF_RX_PROPS, \
-					compat_uptr_t)
-#define IPA_IOC_QUERY_INTF_EXT_PROPS32 _IOWR(IPA_IOC_MAGIC, \
-					IPA_IOCTL_QUERY_INTF_EXT_PROPS, \
-					compat_uptr_t)
-#define IPA_IOC_GET_HDR32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_GET_HDR, \
-				compat_uptr_t)
-#define IPA_IOC_ALLOC_NAT_MEM32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_ALLOC_NAT_MEM, \
-				compat_uptr_t)
-#define IPA_IOC_V4_INIT_NAT32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_V4_INIT_NAT, \
-				compat_uptr_t)
-#define IPA_IOC_NAT_DMA32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_NAT_DMA, \
-				compat_uptr_t)
-#define IPA_IOC_V4_DEL_NAT32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_V4_DEL_NAT, \
-				compat_uptr_t)
-#define IPA_IOC_GET_NAT_OFFSET32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_GET_NAT_OFFSET, \
-				compat_uptr_t)
-#define IPA_IOC_PULL_MSG32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_PULL_MSG, \
-				compat_uptr_t)
-#define IPA_IOC_RM_ADD_DEPENDENCY32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_RM_ADD_DEPENDENCY, \
-				compat_uptr_t)
-#define IPA_IOC_RM_DEL_DEPENDENCY32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_RM_DEL_DEPENDENCY, \
-				compat_uptr_t)
-#define IPA_IOC_GENERATE_FLT_EQ32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_GENERATE_FLT_EQ, \
-				compat_uptr_t)
-#define IPA_IOC_QUERY_RT_TBL_INDEX32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_QUERY_RT_TBL_INDEX, \
-				compat_uptr_t)
-#define IPA_IOC_WRITE_QMAPID32  _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_WRITE_QMAPID, \
-				compat_uptr_t)
-#define IPA_IOC_MDFY_FLT_RULE32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_MDFY_FLT_RULE, \
-				compat_uptr_t)
-#define IPA_IOC_NOTIFY_WAN_UPSTREAM_ROUTE_ADD32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_NOTIFY_WAN_UPSTREAM_ROUTE_ADD, \
-				compat_uptr_t)
-#define IPA_IOC_NOTIFY_WAN_UPSTREAM_ROUTE_DEL32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_NOTIFY_WAN_UPSTREAM_ROUTE_DEL, \
-				compat_uptr_t)
-#define IPA_IOC_NOTIFY_WAN_EMBMS_CONNECTED32 _IOWR(IPA_IOC_MAGIC, \
-					IPA_IOCTL_NOTIFY_WAN_EMBMS_CONNECTED, \
-					compat_uptr_t)
-#define IPA_IOC_ADD_HDR_PROC_CTX32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_ADD_HDR_PROC_CTX, \
-				compat_uptr_t)
-#define IPA_IOC_DEL_HDR_PROC_CTX32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_DEL_HDR_PROC_CTX, \
-				compat_uptr_t)
-#define IPA_IOC_MDFY_RT_RULE32 _IOWR(IPA_IOC_MAGIC, \
-				IPA_IOCTL_MDFY_RT_RULE, \
-				compat_uptr_t)
-
-/**
- * struct ipa_ioc_nat_alloc_mem32 - nat table memory allocation
- * properties
- * @dev_name: input parameter, the name of table
- * @size: input parameter, size of table in bytes
- * @offset: output parameter, offset into page in case of system memory
- */
-struct ipa_ioc_nat_alloc_mem32 {
-	char dev_name[IPA_RESOURCE_NAME_MAX];
-	compat_size_t size;
-	compat_off_t offset;
-};
-#endif
-
-static void ipa_start_tag_process(struct work_struct *work);
-static DECLARE_WORK(ipa_tag_work, ipa_start_tag_process);
-
-static void ipa_sps_release_resource(struct work_struct *work);
-static DECLARE_DELAYED_WORK(ipa_sps_release_resource_work,
-	ipa_sps_release_resource);
-
-static struct ipa_plat_drv_res ipa_res = {0, };
-static struct of_device_id ipa_plat_drv_match[] = {
-	{ .compatible = "qcom,ipa", },
-	{ .compatible = "qcom,ipa-smmu-ap-cb", },
-	{ .compatible = "qcom,ipa-smmu-wlan-cb", },
-	{ .compatible = "qcom,ipa-smmu-uc-cb", },
-	{}
-};
-struct msm_bus_scale_pdata *bus_scale_table;
-
-static struct clk *ipa_clk_src;
-static struct clk *ipa_clk;
-static struct clk *smmu_clk;
-static struct clk *sys_noc_ipa_axi_clk;
-static struct clk *ipa_cnoc_clk;
-static struct clk *ipa_inactivity_clk;
-
-struct ipa_context *ipa_ctx;
-static struct device *master_dev;
-struct platform_device *ipa_pdev;
-static bool smmu_present;
-static bool arm_smmu;
-static bool smmu_disable_htw;
-
-enum ipa_smmu_cb_type {
-	IPA_SMMU_CB_AP,
-	IPA_SMMU_CB_WLAN,
-	IPA_SMMU_CB_UC,
-	IPA_SMMU_CB_MAX
-
-};
-
-static struct ipa_smmu_cb_ctx smmu_cb[IPA_SMMU_CB_MAX];
-
-#if !defined(CONFIG_ARM_DMA_USE_IOMMU) && !defined(CONFIG_ARM64_DMA_USE_IOMMU)
-struct dma_iommu_mapping *
-arm_iommu_create_mapping(struct bus_type *bus, dma_addr_t base, size_t size,
-					 int order)
-{
-	return NULL;
-}
-
-void arm_iommu_release_mapping(struct dma_iommu_mapping *mapping) { }
-
-int arm_iommu_attach_device(struct device *dev,
-		struct dma_iommu_mapping *mapping)
-{
-	return 0;
-}
-
-void arm_iommu_detach_device(struct device *dev) { }
-#endif
-
-struct iommu_domain *ipa_get_smmu_domain(void)
-{
-	if (smmu_cb[IPA_SMMU_CB_AP].valid) {
-		return smmu_cb[IPA_SMMU_CB_AP].mapping->domain;
-	} else {
-		IPAERR("CB not valid\n");
-		return NULL;
-	}
-}
-EXPORT_SYMBOL(ipa_get_smmu_domain);
-
-struct iommu_domain *ipa_get_uc_smmu_domain(void)
-{
-	struct iommu_domain *domain = NULL;
-
-	if (smmu_cb[IPA_SMMU_CB_UC].valid)
-		domain = smmu_cb[IPA_SMMU_CB_UC].mapping->domain;
-	else
-		IPAERR("CB not valid\n");
-
-	return domain;
-}
-
-struct device *ipa_get_dma_dev(void)
-{
-	return ipa_ctx->pdev;
-}
-EXPORT_SYMBOL(ipa_get_dma_dev);
-
-struct ipa_smmu_cb_ctx *ipa_get_wlan_smmu_ctx(void)
-{
-	return &smmu_cb[IPA_SMMU_CB_WLAN];
-}
-
-struct ipa_smmu_cb_ctx *ipa_get_uc_smmu_ctx(void)
-{
-	return &smmu_cb[IPA_SMMU_CB_UC];
-}
-
-static int ipa_open(struct inode *inode, struct file *filp)
-{
-	struct ipa_context *ctx = NULL;
-
-	IPADBG("ENTER\n");
-	ctx = container_of(inode->i_cdev, struct ipa_context, cdev);
-	filp->private_data = ctx;
-
-	return 0;
-}
-
-/**
-* ipa_flow_control() - Enable/Disable flow control on a particular client.
-* Return codes:
-* None
-*/
-void ipa_flow_control(enum ipa_client_type ipa_client,
-		bool enable, uint32_t qmap_id)
-{
-	struct ipa_ep_cfg_ctrl ep_ctrl = {0};
-	int ep_idx;
-	struct ipa_ep_context *ep;
-
-	/* Check if tethered flow control is needed or not.*/
-	if (!ipa_ctx->tethered_flow_control) {
-		IPADBG("Apps flow control is not needed\n");
-		return;
-	}
-
-	/* Check if ep is valid. */
-	ep_idx = ipa_get_ep_mapping(ipa_client);
-	if (ep_idx == -1) {
-		IPADBG("Invalid IPA client\n");
-		return;
-	}
-
-	ep = &ipa_ctx->ep[ep_idx];
-	if (!ep->valid || (ep->client != IPA_CLIENT_USB_PROD)) {
-		IPADBG("EP not valid/Not applicable for client.\n");
-		return;
-	}
-
-	spin_lock(&ipa_ctx->disconnect_lock);
-	/* Check if the QMAP_ID matches. */
-	if (ep->cfg.meta.qmap_id != qmap_id) {
-		IPADBG("Flow control ind not for same flow: %u %u\n",
-			ep->cfg.meta.qmap_id, qmap_id);
-		spin_unlock(&ipa_ctx->disconnect_lock);
-		return;
-	}
-	if (!ep->disconnect_in_progress) {
-		if (enable) {
-			IPADBG("Enabling Flow\n");
-			ep_ctrl.ipa_ep_delay = false;
-			IPA_STATS_INC_CNT(ipa_ctx->stats.flow_enable);
-		} else {
-			IPADBG("Disabling Flow\n");
-			ep_ctrl.ipa_ep_delay = true;
-			IPA_STATS_INC_CNT(ipa_ctx->stats.flow_disable);
-		}
-		ep_ctrl.ipa_ep_suspend = false;
-		ipa_cfg_ep_ctrl(ep_idx, &ep_ctrl);
-	} else
-		IPADBG("EP disconnect is in progress\n");
-	spin_unlock(&ipa_ctx->disconnect_lock);
-}
-static void ipa_wan_msg_free_cb(void *buff, u32 len, u32 type)
-{
-	if (!buff) {
-		IPAERR("Null buffer\n");
-		return;
-	}
-
-	if (type != WAN_UPSTREAM_ROUTE_ADD &&
-	    type != WAN_UPSTREAM_ROUTE_DEL &&
-	    type != WAN_EMBMS_CONNECT) {
-		IPAERR("Wrong type given. buff %p type %d\n", buff, type);
-	}
-	kfree(buff);
-}
-
-static int ipa_send_wan_msg(unsigned long usr_param, uint8_t msg_type)
-{
-	int retval;
-	struct ipa_wan_msg *wan_msg;
-	struct ipa_msg_meta msg_meta;
-
-	wan_msg = kzalloc(sizeof(struct ipa_wan_msg), GFP_KERNEL);
-	if (!wan_msg) {
-		IPAERR("no memory\n");
-		return -ENOMEM;
-	}
-
-	if (copy_from_user((u8 *)wan_msg, (u8 *)usr_param,
-		sizeof(struct ipa_wan_msg))) {
-		kfree(wan_msg);
-		return -EFAULT;
-	}
-
-	memset(&msg_meta, 0, sizeof(struct ipa_msg_meta));
-	msg_meta.msg_type = msg_type;
-	msg_meta.msg_len = sizeof(struct ipa_wan_msg);
-	retval = ipa_send_msg(&msg_meta, wan_msg, ipa_wan_msg_free_cb);
-	if (retval) {
-		IPAERR("ipa_send_msg failed: %d\n", retval);
-		kfree(wan_msg);
-		return retval;
-	}
-
-	return 0;
-}
-
-
-static long ipa_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
-{
-	int retval = 0;
-	u32 pyld_sz;
-	u8 header[128] = { 0 };
-	u8 *param = NULL;
-	struct ipa_ioc_nat_alloc_mem nat_mem;
-	struct ipa_ioc_v4_nat_init nat_init;
-	struct ipa_ioc_v4_nat_del nat_del;
-	struct ipa_ioc_rm_dependency rm_depend;
-	size_t sz;
-	int pre_entry;
-
-	IPADBG("cmd=%x nr=%d\n", cmd, _IOC_NR(cmd));
-
-	if (_IOC_TYPE(cmd) != IPA_IOC_MAGIC)
-		return -ENOTTY;
-	if (_IOC_NR(cmd) >= IPA_IOCTL_MAX)
-		return -ENOTTY;
-
-	ipa_inc_client_enable_clks();
-
-	switch (cmd) {
-	case IPA_IOC_ALLOC_NAT_MEM:
-		if (copy_from_user((u8 *)&nat_mem, (u8 *)arg,
-					sizeof(struct ipa_ioc_nat_alloc_mem))) {
-			retval = -EFAULT;
-			break;
-		}
-		/* null terminate the string */
-		nat_mem.dev_name[IPA_RESOURCE_NAME_MAX - 1] = '\0';
-
-		if (allocate_nat_device(&nat_mem)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, (u8 *)&nat_mem,
-					sizeof(struct ipa_ioc_nat_alloc_mem))) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_V4_INIT_NAT:
-		if (copy_from_user((u8 *)&nat_init, (u8 *)arg,
-					sizeof(struct ipa_ioc_v4_nat_init))) {
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_nat_init_cmd(&nat_init)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_NAT_DMA:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_nat_dma_cmd))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_nat_dma_cmd *)header)->entries;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_nat_dma_cmd) +
-		   pre_entry * sizeof(struct ipa_ioc_nat_dma_one);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_nat_dma_cmd *)param)->entries
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_nat_dma_cmd *)param)->entries,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_nat_dma_cmd((struct ipa_ioc_nat_dma_cmd *)param)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_V4_DEL_NAT:
-		if (copy_from_user((u8 *)&nat_del, (u8 *)arg,
-					sizeof(struct ipa_ioc_v4_nat_del))) {
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_nat_del_cmd(&nat_del)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_ADD_HDR:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_add_hdr))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_add_hdr *)header)->num_hdrs;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_add_hdr) +
-		   pre_entry * sizeof(struct ipa_hdr_add);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_add_hdr *)param)->num_hdrs
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_add_hdr *)param)->num_hdrs,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_add_hdr((struct ipa_ioc_add_hdr *)param)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_DEL_HDR:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_del_hdr))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_del_hdr *)header)->num_hdls;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_del_hdr) +
-		   pre_entry * sizeof(struct ipa_hdr_del);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_del_hdr *)param)->num_hdls
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_del_hdr *)param)->num_hdls,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_del_hdr_by_user((struct ipa_ioc_del_hdr *)param,
-			true)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_ADD_RT_RULE:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_add_rt_rule))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_add_rt_rule *)header)->num_rules;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_add_rt_rule) +
-		   pre_entry * sizeof(struct ipa_rt_rule_add);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_add_rt_rule *)param)->num_rules
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_add_rt_rule *)param)->
-				num_rules,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_add_rt_rule((struct ipa_ioc_add_rt_rule *)param)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_MDFY_RT_RULE:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_mdfy_rt_rule))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_mdfy_rt_rule *)header)->num_rules;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_mdfy_rt_rule) +
-		   pre_entry * sizeof(struct ipa_rt_rule_mdfy);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_mdfy_rt_rule *)param)->num_rules
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_mdfy_rt_rule *)param)->
-				num_rules,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_mdfy_rt_rule((struct ipa_ioc_mdfy_rt_rule *)param)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_DEL_RT_RULE:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_del_rt_rule))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_del_rt_rule *)header)->num_hdls;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_del_rt_rule) +
-		   pre_entry * sizeof(struct ipa_rt_rule_del);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_del_rt_rule *)param)->num_hdls
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_del_rt_rule *)param)->num_hdls,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_del_rt_rule((struct ipa_ioc_del_rt_rule *)param)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_ADD_FLT_RULE:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_add_flt_rule))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_add_flt_rule *)header)->num_rules;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_add_flt_rule) +
-		   pre_entry * sizeof(struct ipa_flt_rule_add);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_add_flt_rule *)param)->num_rules
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_add_flt_rule *)param)->
-				num_rules,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_add_flt_rule((struct ipa_ioc_add_flt_rule *)param)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_DEL_FLT_RULE:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_del_flt_rule))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_del_flt_rule *)header)->num_hdls;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_del_flt_rule) +
-		   pre_entry * sizeof(struct ipa_flt_rule_del);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_del_flt_rule *)param)->num_hdls
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_del_flt_rule *)param)->
-				num_hdls,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_del_flt_rule((struct ipa_ioc_del_flt_rule *)param)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_MDFY_FLT_RULE:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_mdfy_flt_rule))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_mdfy_flt_rule *)header)->num_rules;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_mdfy_flt_rule) +
-		   pre_entry * sizeof(struct ipa_flt_rule_mdfy);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_mdfy_flt_rule *)param)->num_rules
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_mdfy_flt_rule *)param)->
-				num_rules,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_mdfy_flt_rule((struct ipa_ioc_mdfy_flt_rule *)param)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case IPA_IOC_COMMIT_HDR:
-		retval = ipa_commit_hdr();
-		break;
-	case IPA_IOC_RESET_HDR:
-		retval = ipa_reset_hdr();
-		break;
-	case IPA_IOC_COMMIT_RT:
-		retval = ipa_commit_rt(arg);
-		break;
-	case IPA_IOC_RESET_RT:
-		retval = ipa_reset_rt(arg);
-		break;
-	case IPA_IOC_COMMIT_FLT:
-		retval = ipa_commit_flt(arg);
-		break;
-	case IPA_IOC_RESET_FLT:
-		retval = ipa_reset_flt(arg);
-		break;
-	case IPA_IOC_GET_RT_TBL:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_get_rt_tbl))) {
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_get_rt_tbl((struct ipa_ioc_get_rt_tbl *)header)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, header,
-					sizeof(struct ipa_ioc_get_rt_tbl))) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_PUT_RT_TBL:
-		retval = ipa_put_rt_tbl(arg);
-		break;
-	case IPA_IOC_GET_HDR:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_get_hdr))) {
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_get_hdr((struct ipa_ioc_get_hdr *)header)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, header,
-					sizeof(struct ipa_ioc_get_hdr))) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_PUT_HDR:
-		retval = ipa_put_hdr(arg);
-		break;
-	case IPA_IOC_SET_FLT:
-		retval = ipa_cfg_filter(arg);
-		break;
-	case IPA_IOC_COPY_HDR:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_copy_hdr))) {
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_copy_hdr((struct ipa_ioc_copy_hdr *)header)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, header,
-					sizeof(struct ipa_ioc_copy_hdr))) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_QUERY_INTF:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_query_intf))) {
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_query_intf((struct ipa_ioc_query_intf *)header)) {
-			retval = -1;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, header,
-					sizeof(struct ipa_ioc_query_intf))) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_QUERY_INTF_TX_PROPS:
-		sz = sizeof(struct ipa_ioc_query_intf_tx_props);
-		if (copy_from_user(header, (u8 *)arg, sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (((struct ipa_ioc_query_intf_tx_props *)
-			header)->num_tx_props > IPA_NUM_PROPS_MAX) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_query_intf_tx_props *)
-			header)->num_tx_props;
-		pyld_sz = sz + pre_entry *
-			sizeof(struct ipa_ioc_tx_intf_prop);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_query_intf_tx_props *)
-			param)->num_tx_props
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_query_intf_tx_props *)
-				param)->num_tx_props, pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_query_intf_tx_props(
-				(struct ipa_ioc_query_intf_tx_props *)param)) {
-			retval = -1;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_QUERY_INTF_RX_PROPS:
-		sz = sizeof(struct ipa_ioc_query_intf_rx_props);
-		if (copy_from_user(header, (u8 *)arg, sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (((struct ipa_ioc_query_intf_rx_props *)
-			header)->num_rx_props > IPA_NUM_PROPS_MAX) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_query_intf_rx_props *)
-			header)->num_rx_props;
-		pyld_sz = sz + pre_entry *
-			sizeof(struct ipa_ioc_rx_intf_prop);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_query_intf_rx_props *)
-			param)->num_rx_props != pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_query_intf_rx_props *)
-				param)->num_rx_props, pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_query_intf_rx_props(
-				(struct ipa_ioc_query_intf_rx_props *)param)) {
-			retval = -1;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_QUERY_INTF_EXT_PROPS:
-		sz = sizeof(struct ipa_ioc_query_intf_ext_props);
-		if (copy_from_user(header, (u8 *)arg, sz)) {
-			retval = -EFAULT;
-			break;
-		}
-
-		if (((struct ipa_ioc_query_intf_ext_props *)
-				header)->num_ext_props > IPA_NUM_PROPS_MAX) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_query_intf_ext_props *)
-			header)->num_ext_props;
-		pyld_sz = sz + pre_entry *
-			sizeof(struct ipa_ioc_ext_intf_prop);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_query_intf_ext_props *)
-			param)->num_ext_props != pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_query_intf_ext_props *)
-				param)->num_ext_props, pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_query_intf_ext_props(
-				(struct ipa_ioc_query_intf_ext_props *)param)) {
-			retval = -1;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_PULL_MSG:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_msg_meta))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_msg_meta *)header)->msg_len;
-		pyld_sz = sizeof(struct ipa_msg_meta) +
-		   pre_entry;
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_msg_meta *)param)->msg_len
-			!= pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_msg_meta *)param)->msg_len,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_pull_msg((struct ipa_msg_meta *)param,
-				 (char *)param + sizeof(struct ipa_msg_meta),
-				 ((struct ipa_msg_meta *)param)->msg_len) !=
-		       ((struct ipa_msg_meta *)param)->msg_len) {
-			retval = -1;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_RM_ADD_DEPENDENCY:
-		if (copy_from_user((u8 *)&rm_depend, (u8 *)arg,
-				sizeof(struct ipa_ioc_rm_dependency))) {
-			retval = -EFAULT;
-			break;
-		}
-		retval = ipa_rm_add_dependency(rm_depend.resource_name,
-						rm_depend.depends_on_name);
-		break;
-	case IPA_IOC_RM_DEL_DEPENDENCY:
-		if (copy_from_user((u8 *)&rm_depend, (u8 *)arg,
-				sizeof(struct ipa_ioc_rm_dependency))) {
-			retval = -EFAULT;
-			break;
-		}
-		retval = ipa_rm_delete_dependency(rm_depend.resource_name,
-						rm_depend.depends_on_name);
-		break;
-	case IPA_IOC_GENERATE_FLT_EQ:
-		{
-			struct ipa_ioc_generate_flt_eq flt_eq;
-			if (copy_from_user(&flt_eq, (u8 *)arg,
-				sizeof(struct ipa_ioc_generate_flt_eq))) {
-				retval = -EFAULT;
-				break;
-			}
-			if (ipa_generate_flt_eq(flt_eq.ip, &flt_eq.attrib,
-						&flt_eq.eq_attrib)) {
-				retval = -EFAULT;
-				break;
-			}
-			if (copy_to_user((u8 *)arg, &flt_eq,
-				sizeof(struct ipa_ioc_generate_flt_eq))) {
-				retval = -EFAULT;
-				break;
-			}
-			break;
-		}
-	case IPA_IOC_QUERY_EP_MAPPING:
-		{
-			retval = ipa_get_ep_mapping(arg);
-			break;
-		}
-	case IPA_IOC_QUERY_RT_TBL_INDEX:
-		if (copy_from_user(header, (u8 *)arg,
-				sizeof(struct ipa_ioc_get_rt_tbl_indx))) {
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_query_rt_index(
-			 (struct ipa_ioc_get_rt_tbl_indx *)header)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, header,
-				sizeof(struct ipa_ioc_get_rt_tbl_indx))) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_WRITE_QMAPID:
-		if (copy_from_user(header, (u8 *)arg,
-					sizeof(struct ipa_ioc_write_qmapid))) {
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_write_qmap_id((struct ipa_ioc_write_qmapid *)header)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, header,
-					sizeof(struct ipa_ioc_write_qmapid))) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_NOTIFY_WAN_UPSTREAM_ROUTE_ADD:
-		retval = ipa_send_wan_msg(arg, WAN_UPSTREAM_ROUTE_ADD);
-		if (retval) {
-			IPAERR("ipa_send_wan_msg failed: %d\n", retval);
-			break;
-		}
-		break;
-	case IPA_IOC_NOTIFY_WAN_UPSTREAM_ROUTE_DEL:
-		retval = ipa_send_wan_msg(arg, WAN_UPSTREAM_ROUTE_DEL);
-		if (retval) {
-			IPAERR("ipa_send_wan_msg failed: %d\n", retval);
-			break;
-		}
-		break;
-	case IPA_IOC_NOTIFY_WAN_EMBMS_CONNECTED:
-		retval = ipa_send_wan_msg(arg, WAN_EMBMS_CONNECT);
-		if (retval) {
-			IPAERR("ipa_send_wan_msg failed: %d\n", retval);
-			break;
-		}
-		break;
-	case IPA_IOC_ADD_HDR_PROC_CTX:
-		if (copy_from_user(header, (u8 *)arg,
-			sizeof(struct ipa_ioc_add_hdr_proc_ctx))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_add_hdr_proc_ctx *)
-			header)->num_proc_ctxs;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_add_hdr_proc_ctx) +
-		   pre_entry * sizeof(struct ipa_hdr_proc_ctx_add);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_add_hdr_proc_ctx *)
-			param)->num_proc_ctxs != pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_add_hdr_proc_ctx *)
-				param)->num_proc_ctxs, pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_add_hdr_proc_ctx(
-			(struct ipa_ioc_add_hdr_proc_ctx *)param)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-	case IPA_IOC_DEL_HDR_PROC_CTX:
-		if (copy_from_user(header, (u8 *)arg,
-			sizeof(struct ipa_ioc_del_hdr_proc_ctx))) {
-			retval = -EFAULT;
-			break;
-		}
-		pre_entry =
-			((struct ipa_ioc_del_hdr_proc_ctx *)header)->num_hdls;
-		pyld_sz =
-		   sizeof(struct ipa_ioc_del_hdr_proc_ctx) +
-		   pre_entry * sizeof(struct ipa_hdr_proc_ctx_del);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		/* add check in case user-space module compromised */
-		if (unlikely(((struct ipa_ioc_del_hdr_proc_ctx *)
-			param)->num_hdls != pre_entry)) {
-			IPAERR("current %d pre %d\n",
-				((struct ipa_ioc_del_hdr_proc_ctx *)param)->
-				num_hdls,
-				pre_entry);
-			retval = -EFAULT;
-			break;
-		}
-		if (ipa_del_hdr_proc_ctx_by_user(
-			(struct ipa_ioc_del_hdr_proc_ctx *)param, true)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	default:        /* redundant, as cmd was checked against MAXNR */
-		ipa_dec_client_disable_clks();
-		return -ENOTTY;
-	}
-	kfree(param);
-
-	ipa_dec_client_disable_clks();
-
-	return retval;
-}
-
-/**
-* ipa_setup_dflt_rt_tables() - Setup default routing tables
-*
-* Return codes:
-* 0: success
-* -ENOMEM: failed to allocate memory
-* -EPERM: failed to add the tables
-*/
-int ipa_setup_dflt_rt_tables(void)
-{
-	struct ipa_ioc_add_rt_rule *rt_rule;
-	struct ipa_rt_rule_add *rt_rule_entry;
-
-	rt_rule =
-	   kzalloc(sizeof(struct ipa_ioc_add_rt_rule) + 1 *
-			   sizeof(struct ipa_rt_rule_add), GFP_KERNEL);
-	if (!rt_rule) {
-		IPAERR("fail to alloc mem\n");
-		return -ENOMEM;
-	}
-	/* setup a default v4 route to point to Apps */
-	rt_rule->num_rules = 1;
-	rt_rule->commit = 1;
-	rt_rule->ip = IPA_IP_v4;
-	strlcpy(rt_rule->rt_tbl_name, IPA_DFLT_RT_TBL_NAME,
-			IPA_RESOURCE_NAME_MAX);
-
-	rt_rule_entry = &rt_rule->rules[0];
-	rt_rule_entry->at_rear = 1;
-	rt_rule_entry->rule.dst = IPA_CLIENT_APPS_LAN_CONS;
-	rt_rule_entry->rule.hdr_hdl = ipa_ctx->excp_hdr_hdl;
-
-	if (ipa_add_rt_rule(rt_rule)) {
-		IPAERR("fail to add dflt v4 rule\n");
-		kfree(rt_rule);
-		return -EPERM;
-	}
-	IPADBG("dflt v4 rt rule hdl=%x\n", rt_rule_entry->rt_rule_hdl);
-	ipa_ctx->dflt_v4_rt_rule_hdl = rt_rule_entry->rt_rule_hdl;
-
-	/* setup a default v6 route to point to A5 */
-	rt_rule->ip = IPA_IP_v6;
-	if (ipa_add_rt_rule(rt_rule)) {
-		IPAERR("fail to add dflt v6 rule\n");
-		kfree(rt_rule);
-		return -EPERM;
-	}
-	IPADBG("dflt v6 rt rule hdl=%x\n", rt_rule_entry->rt_rule_hdl);
-	ipa_ctx->dflt_v6_rt_rule_hdl = rt_rule_entry->rt_rule_hdl;
-
-	/*
-	 * because these tables are the very first to be added, they will both
-	 * have the same index (0) which is essential for programming the
-	 * "route" end-point config
-	 */
-
-	kfree(rt_rule);
-
-	return 0;
-}
-
-static int ipa_setup_exception_path(void)
-{
-	struct ipa_ioc_add_hdr *hdr;
-	struct ipa_hdr_add *hdr_entry;
-	struct ipa_route route = { 0 };
-	int ret;
-
-	/* install the basic exception header */
-	hdr = kzalloc(sizeof(struct ipa_ioc_add_hdr) + 1 *
-		      sizeof(struct ipa_hdr_add), GFP_KERNEL);
-	if (!hdr) {
-		IPAERR("fail to alloc exception hdr\n");
-		return -ENOMEM;
-	}
-	hdr->num_hdrs = 1;
-	hdr->commit = 1;
-	hdr_entry = &hdr->hdr[0];
-
-	if (ipa_ctx->ipa_hw_type == IPA_HW_v1_1) {
-		strlcpy(hdr_entry->name, IPA_A5_MUX_HDR_NAME,
-				IPA_RESOURCE_NAME_MAX);
-		/* set template for the A5_MUX hdr in header addition block */
-		hdr_entry->hdr_len = IPA_A5_MUX_HEADER_LENGTH;
-	} else if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_0) {
-		strlcpy(hdr_entry->name, IPA_LAN_RX_HDR_NAME,
-				IPA_RESOURCE_NAME_MAX);
-		hdr_entry->hdr_len = IPA_LAN_RX_HEADER_LENGTH;
-	} else {
-		WARN_ON(1);
-	}
-
-	if (ipa_add_hdr(hdr)) {
-		IPAERR("fail to add exception hdr\n");
-		ret = -EPERM;
-		goto bail;
-	}
-
-	if (hdr_entry->status) {
-		IPAERR("fail to add exception hdr\n");
-		ret = -EPERM;
-		goto bail;
-	}
-
-	ipa_ctx->excp_hdr_hdl = hdr_entry->hdr_hdl;
-
-	/* set the route register to pass exception packets to Apps */
-	route.route_def_pipe = ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_CONS);
-	route.route_frag_def_pipe = ipa_get_ep_mapping(
-		IPA_CLIENT_APPS_LAN_CONS);
-	route.route_def_hdr_table = !ipa_ctx->hdr_tbl_lcl;
-
-	if (ipa_cfg_route(&route)) {
-		IPAERR("fail to add exception hdr\n");
-		ret = -EPERM;
-		goto bail;
-	}
-
-	ret = 0;
-bail:
-	kfree(hdr);
-	return ret;
-}
-
-static int ipa_init_smem_region(int memory_region_size,
-				int memory_region_offset)
-{
-	struct ipa_hw_imm_cmd_dma_shared_mem cmd;
-	struct ipa_desc desc;
-	struct ipa_mem_buffer mem;
-	int rc;
-
-	if (memory_region_size == 0)
-		return 0;
-
-	memset(&desc, 0, sizeof(desc));
-	memset(&cmd, 0, sizeof(cmd));
-	memset(&mem, 0, sizeof(mem));
-
-	mem.size = memory_region_size;
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size,
-		&mem.phys_base, GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("failed to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-
-	memset(mem.base, 0, mem.size);
-	cmd.size = mem.size;
-	cmd.system_addr = mem.phys_base;
-	cmd.local_addr = ipa_ctx->smem_restricted_bytes +
-		memory_region_offset;
-	desc.opcode = IPA_DMA_SHARED_MEM;
-	desc.pyld = &cmd;
-	desc.len = sizeof(cmd);
-	desc.type = IPA_IMM_CMD_DESC;
-
-	rc = ipa_send_cmd(1, &desc);
-	if (rc) {
-		IPAERR("failed to send immediate command (error %d)\n", rc);
-		rc = -EFAULT;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base,
-		mem.phys_base);
-
-	return rc;
-}
-
-/**
-* ipa_init_q6_smem() - Initialize Q6 general memory and
-*                      header memory regions in IPA.
-*
-* Return codes:
-* 0: success
-* -ENOMEM: failed to allocate dma memory
-* -EFAULT: failed to send IPA command to initialize the memory
-*/
-int ipa_init_q6_smem(void)
-{
-	int rc;
-
-	ipa_inc_client_enable_clks();
-
-	if (ipa_ctx->ipa_hw_type == IPA_HW_v2_0)
-		rc = ipa_init_smem_region(IPA_MEM_PART(modem_size) -
-			IPA_MEM_RAM_MODEM_NETWORK_STATS_SIZE,
-			IPA_MEM_PART(modem_ofst));
-	else
-		rc = ipa_init_smem_region(IPA_MEM_PART(modem_size),
-			IPA_MEM_PART(modem_ofst));
-
-	if (rc) {
-		IPAERR("failed to initialize Modem RAM memory\n");
-		ipa_dec_client_disable_clks();
-		return rc;
-	}
-
-	rc = ipa_init_smem_region(IPA_MEM_PART(modem_hdr_size),
-		IPA_MEM_PART(modem_hdr_ofst));
-	if (rc) {
-		IPAERR("failed to initialize Modem HDRs RAM memory\n");
-		ipa_dec_client_disable_clks();
-		return rc;
-	}
-
-	rc = ipa_init_smem_region(IPA_MEM_PART(modem_hdr_proc_ctx_size),
-		IPA_MEM_PART(modem_hdr_proc_ctx_ofst));
-	if (rc) {
-		IPAERR("failed to initialize Modem proc ctx RAM memory\n");
-		ipa_dec_client_disable_clks();
-		return rc;
-	}
-
-	rc = ipa_init_smem_region(IPA_MEM_PART(modem_comp_decomp_size),
-		IPA_MEM_PART(modem_comp_decomp_ofst));
-	if (rc) {
-		IPAERR("failed to initialize Modem Comp/Decomp RAM memory\n");
-		ipa_dec_client_disable_clks();
-		return rc;
-	}
-
-	ipa_dec_client_disable_clks();
-
-	return rc;
-}
-
-static void ipa_free_buffer(void *user1, int user2)
-{
-	kfree(user1);
-}
-
-static int ipa_q6_pipe_delay(bool zip_pipes)
-{
-	u32 reg_val = 0;
-	int client_idx;
-	int ep_idx;
-
-	/* For ZIP pipes, processing is done in AFTER_SHUTDOWN callback. */
-	for (client_idx = 0; client_idx < IPA_CLIENT_MAX; client_idx++) {
-		/* Skip the processing for non Q6 pipes. */
-		if (!IPA_CLIENT_IS_Q6_PROD(client_idx))
-			continue;
-		/* Skip the processing for NON-ZIP pipes. */
-		else if (zip_pipes && IPA_CLIENT_IS_Q6_NON_ZIP_PROD(client_idx))
-			continue;
-		/* Skip the processing for ZIP pipes. */
-		else if (!zip_pipes && IPA_CLIENT_IS_Q6_ZIP_PROD(client_idx))
-			continue;
-
-		ep_idx = ipa_get_ep_mapping(client_idx);
-		if (ep_idx == -1)
-			continue;
-
-		IPA_SETFIELD_IN_REG(reg_val, 1,
-			IPA_ENDP_INIT_CTRL_N_ENDP_DELAY_SHFT,
-			IPA_ENDP_INIT_CTRL_N_ENDP_DELAY_BMSK);
-
-		ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_CTRL_N_OFST(ep_idx), reg_val);
-	}
-
-	return 0;
-}
-
-int ipa_q6_monitor_holb_mitigation(bool enable)
-{
-	int ep_idx;
-	int client_idx;
-
-	ipa_inc_client_enable_clks();
-	for (client_idx = 0; client_idx < IPA_CLIENT_MAX; client_idx++) {
-		if (IPA_CLIENT_IS_Q6_NON_ZIP_CONS(client_idx)) {
-			ep_idx = ipa_get_ep_mapping(client_idx);
-			if (ep_idx == -1)
-				continue;
-			/* Send a command to Uc to enable/disable
-			 * holb monitoring.
-			 */
-			ipa_uc_monitor_holb(client_idx, enable);
-		}
-	}
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-
-static int ipa_q6_avoid_holb(bool zip_pipes)
-{
-	u32 reg_val;
-	int ep_idx;
-	int client_idx;
-	struct ipa_ep_cfg_ctrl avoid_holb;
-
-	memset(&avoid_holb, 0, sizeof(avoid_holb));
-	avoid_holb.ipa_ep_suspend = true;
-
-	/* For ZIP pipes, processing is done in AFTER_SHUTDOWN callback. */
-	for (client_idx = 0; client_idx < IPA_CLIENT_MAX; client_idx++) {
-		/* Skip the processing for non Q6 pipes. */
-		if (!IPA_CLIENT_IS_Q6_CONS(client_idx))
-			continue;
-		/* Skip the processing for NON-ZIP pipes. */
-		else if (zip_pipes && IPA_CLIENT_IS_Q6_NON_ZIP_CONS(client_idx))
-			continue;
-		/* Skip the processing for ZIP pipes. */
-		else if (!zip_pipes && IPA_CLIENT_IS_Q6_ZIP_CONS(client_idx))
-			continue;
-
-		ep_idx = ipa_get_ep_mapping(client_idx);
-		if (ep_idx == -1)
-			continue;
-
-		/*
-		 * ipa_cfg_ep_holb is not used here because we are
-		 * setting HOLB on Q6 pipes, and from APPS perspective
-		 * they are not valid, therefore, the above function
-		 * will fail.
-		 */
-		reg_val = 0;
-		IPA_SETFIELD_IN_REG(reg_val, 0,
-			IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_TIMER_SHFT,
-			IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_TIMER_BMSK);
-
-		ipa_write_reg(ipa_ctx->mmio,
-		IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_OFST_v2_0(ep_idx),
-			reg_val);
-
-		reg_val = 0;
-		IPA_SETFIELD_IN_REG(reg_val, 1,
-			IPA_ENDP_INIT_HOL_BLOCK_EN_N_EN_SHFT,
-			IPA_ENDP_INIT_HOL_BLOCK_EN_N_EN_BMSK);
-
-		ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HOL_BLOCK_EN_N_OFST_v2_0(ep_idx),
-			reg_val);
-
-		ipa_cfg_ep_ctrl(ep_idx, &avoid_holb);
-	}
-
-	return 0;
-}
-
-static u32 ipa_get_max_flt_rt_cmds(u32 num_pipes)
-{
-	u32 max_cmds = 0;
-
-	/* As many filter tables as there are pipes, x2 for IPv4 and IPv6 */
-	max_cmds += num_pipes * 2;
-
-	/* For each of the Modem routing tables */
-	max_cmds += (IPA_MEM_PART(v4_modem_rt_index_hi) -
-		     IPA_MEM_PART(v4_modem_rt_index_lo) + 1);
-
-	max_cmds += (IPA_MEM_PART(v6_modem_rt_index_hi) -
-		     IPA_MEM_PART(v6_modem_rt_index_lo) + 1);
-
-	return max_cmds;
-}
-
-static int ipa_q6_clean_q6_tables(void)
-{
-	struct ipa_desc *desc;
-	struct ipa_hw_imm_cmd_dma_shared_mem *cmd = NULL;
-	int pipe_idx;
-	int num_cmds = 0;
-	int index;
-	int retval;
-	struct ipa_mem_buffer mem = { 0 };
-	u32 *entry;
-	u32 max_cmds = ipa_get_max_flt_rt_cmds(ipa_ctx->ipa_num_pipes);
-
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, 4, &mem.phys_base,
-		GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("failed to alloc DMA buff of size 4\n");
-		return -ENOMEM;
-	}
-
-	mem.size = 4;
-	entry = mem.base;
-	*entry = ipa_ctx->empty_rt_tbl_mem.phys_base;
-
-	desc = kzalloc(sizeof(struct ipa_desc) * max_cmds, GFP_KERNEL);
-	if (!desc) {
-		IPAERR("failed to allocate memory\n");
-		retval = -ENOMEM;
-		goto bail_dma;
-	}
-
-	cmd = kzalloc(sizeof(struct ipa_hw_imm_cmd_dma_shared_mem) * max_cmds,
-		GFP_KERNEL);
-	if (!cmd) {
-		IPAERR("failed to allocate memory\n");
-		retval = -ENOMEM;
-		goto bail_desc;
-	}
-
-	/*
-	 * Iterating over all the pipes which are either invalid but connected
-	 * or connected but not configured by AP.
-	 */
-	for (pipe_idx = 0; pipe_idx < ipa_ctx->ipa_num_pipes; pipe_idx++) {
-		if (!ipa_ctx->ep[pipe_idx].valid ||
-		    ipa_ctx->ep[pipe_idx].skip_ep_cfg) {
-			/*
-			 * Need to point v4 and v6 fltr tables to an empty
-			 * table
-			 */
-			cmd[num_cmds].size = mem.size;
-			cmd[num_cmds].system_addr = mem.phys_base;
-			cmd[num_cmds].local_addr =
-				ipa_ctx->smem_restricted_bytes +
-				IPA_MEM_PART(v4_flt_ofst) + 8 + pipe_idx * 4;
-
-			desc[num_cmds].opcode = IPA_DMA_SHARED_MEM;
-			desc[num_cmds].pyld = &cmd[num_cmds];
-			desc[num_cmds].len = sizeof(*cmd);
-			desc[num_cmds].type = IPA_IMM_CMD_DESC;
-			num_cmds++;
-
-			cmd[num_cmds].size = mem.size;
-			cmd[num_cmds].system_addr =  mem.phys_base;
-			cmd[num_cmds].local_addr =
-				ipa_ctx->smem_restricted_bytes +
-				IPA_MEM_PART(v6_flt_ofst) + 8 + pipe_idx * 4;
-
-			desc[num_cmds].opcode = IPA_DMA_SHARED_MEM;
-			desc[num_cmds].pyld = &cmd[num_cmds];
-			desc[num_cmds].len = sizeof(*cmd);
-			desc[num_cmds].type = IPA_IMM_CMD_DESC;
-			num_cmds++;
-		}
-	}
-
-	/* Need to point v4/v6 modem routing tables to an empty table */
-	for (index = IPA_MEM_PART(v4_modem_rt_index_lo);
-		 index <= IPA_MEM_PART(v4_modem_rt_index_hi);
-		 index++) {
-		cmd[num_cmds].size = mem.size;
-		cmd[num_cmds].system_addr =  mem.phys_base;
-		cmd[num_cmds].local_addr = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(v4_rt_ofst) + index * 4;
-
-		desc[num_cmds].opcode = IPA_DMA_SHARED_MEM;
-		desc[num_cmds].pyld = &cmd[num_cmds];
-		desc[num_cmds].len = sizeof(*cmd);
-		desc[num_cmds].type = IPA_IMM_CMD_DESC;
-		num_cmds++;
-	}
-
-	for (index = IPA_MEM_PART(v6_modem_rt_index_lo);
-		 index <= IPA_MEM_PART(v6_modem_rt_index_hi);
-		 index++) {
-		cmd[num_cmds].size = mem.size;
-		cmd[num_cmds].system_addr =  mem.phys_base;
-		cmd[num_cmds].local_addr = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(v6_rt_ofst) + index * 4;
-
-		desc[num_cmds].opcode = IPA_DMA_SHARED_MEM;
-		desc[num_cmds].pyld = &cmd[num_cmds];
-		desc[num_cmds].len = sizeof(*cmd);
-		desc[num_cmds].type = IPA_IMM_CMD_DESC;
-		num_cmds++;
-	}
-
-	retval = ipa_send_cmd(num_cmds, desc);
-	if (retval) {
-		IPAERR("failed to send immediate command (error %d)\n", retval);
-		retval = -EFAULT;
-	}
-
-	kfree(cmd);
-
-bail_desc:
-	kfree(desc);
-
-bail_dma:
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-
-	return retval;
-}
-
-static void ipa_q6_disable_agg_reg(struct ipa_register_write *reg_write,
-				   int ep_idx)
-{
-	reg_write->skip_pipeline_clear = 0;
-
-	reg_write->offset = IPA_ENDP_INIT_AGGR_N_OFST_v2_0(ep_idx);
-	reg_write->value =
-		(1 & IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_BMSK) <<
-		IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_SHFT;
-	reg_write->value_mask =
-		IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_BMSK <<
-		IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_SHFT;
-
-	reg_write->value |=
-		((0 & IPA_ENDP_INIT_AGGR_N_AGGR_EN_BMSK) <<
-		IPA_ENDP_INIT_AGGR_N_AGGR_EN_SHFT);
-	reg_write->value_mask |=
-		((IPA_ENDP_INIT_AGGR_N_AGGR_EN_BMSK <<
-		IPA_ENDP_INIT_AGGR_N_AGGR_EN_SHFT));
-}
-
-static int ipa_q6_set_ex_path_dis_agg(void)
-{
-	int ep_idx;
-	int client_idx;
-	struct ipa_desc *desc;
-	int num_descs = 0;
-	int index;
-	struct ipa_register_write *reg_write;
-	int retval;
-
-	desc = kzalloc(sizeof(struct ipa_desc) * ipa_ctx->ipa_num_pipes,
-		GFP_KERNEL);
-	if (!desc) {
-		IPAERR("failed to allocate memory\n");
-		return -ENOMEM;
-	}
-
-	/* Set the exception path to AP */
-	for (client_idx = 0; client_idx < IPA_CLIENT_MAX; client_idx++) {
-		ep_idx = ipa_get_ep_mapping(client_idx);
-		if (ep_idx == -1)
-			continue;
-
-		if (ipa_ctx->ep[ep_idx].valid &&
-			ipa_ctx->ep[ep_idx].skip_ep_cfg) {
-			BUG_ON(num_descs >= ipa_ctx->ipa_num_pipes);
-			reg_write = kzalloc(sizeof(*reg_write), GFP_KERNEL);
-
-			if (!reg_write) {
-				IPAERR("failed to allocate memory\n");
-				BUG();
-			}
-			reg_write->skip_pipeline_clear = 0;
-			reg_write->offset = IPA_ENDP_STATUS_n_OFST(ep_idx);
-			reg_write->value =
-				(ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_CONS) &
-				IPA_ENDP_STATUS_n_STATUS_ENDP_BMSK) <<
-				IPA_ENDP_STATUS_n_STATUS_ENDP_SHFT;
-			reg_write->value_mask =
-				IPA_ENDP_STATUS_n_STATUS_ENDP_BMSK <<
-				IPA_ENDP_STATUS_n_STATUS_ENDP_SHFT;
-
-			desc[num_descs].opcode = IPA_REGISTER_WRITE;
-			desc[num_descs].pyld = reg_write;
-			desc[num_descs].len = sizeof(*reg_write);
-			desc[num_descs].type = IPA_IMM_CMD_DESC;
-			desc[num_descs].callback = ipa_free_buffer;
-			desc[num_descs].user1 = reg_write;
-			num_descs++;
-		}
-	}
-
-	/* Disable AGGR on IPA->Q6 pipes */
-	for (client_idx = 0; client_idx < IPA_CLIENT_MAX; client_idx++) {
-		if (IPA_CLIENT_IS_Q6_NON_ZIP_CONS(client_idx) ||
-			IPA_CLIENT_IS_Q6_ZIP_CONS(client_idx)) {
-			reg_write = kzalloc(sizeof(*reg_write), GFP_KERNEL);
-
-			if (!reg_write) {
-				IPAERR("failed to allocate memory\n");
-				BUG();
-			}
-
-			ipa_q6_disable_agg_reg(reg_write,
-					       ipa_get_ep_mapping(client_idx));
-
-			desc[num_descs].opcode = IPA_REGISTER_WRITE;
-			desc[num_descs].pyld = reg_write;
-			desc[num_descs].len = sizeof(*reg_write);
-			desc[num_descs].type = IPA_IMM_CMD_DESC;
-			desc[num_descs].callback = ipa_free_buffer;
-			desc[num_descs].user1 = reg_write;
-			num_descs++;
-		}
-	}
-
-	/* Will wait 150msecs for IPA tag process completion */
-	retval = ipa_tag_process(desc, num_descs,
-				 msecs_to_jiffies(CLEANUP_TAG_PROCESS_TIMEOUT));
-	if (retval) {
-		IPAERR("TAG process failed! (error %d)\n", retval);
-		/* For timeout error ipa_free_buffer cb will free user1 */
-		if (retval != -ETIME) {
-			for (index = 0; index < num_descs; index++)
-				kfree(desc[index].user1);
-			retval = -EINVAL;
-		}
-	}
-
-	kfree(desc);
-
-	return retval;
-}
-
-/**
-* ipa_q6_pre_shutdown_cleanup() - A cleanup for all Q6 related configuration
-*                    in IPA HW before modem shutdown. This is performed in
-*                    case of SSR.
-*
-* Return codes:
-* 0: success
-* This is a mandatory procedure, in case one of the steps fails, the
-* AP needs to restart.
-*/
-int ipa_q6_pre_shutdown_cleanup(void)
-{
-	/* If uC has notified the APPS upon a ZIP engine error,
-	 * APPS need to assert (This is a non recoverable error).
-	 */
-	if (ipa_ctx->uc_ctx.uc_zip_error)
-		BUG();
-
-	ipa_inc_client_enable_clks();
-	/*
-	 * pipe delay and holb discard for ZIP pipes are handled
-	 * in post shutdown callback.
-	 */
-	if (ipa_q6_pipe_delay(false)) {
-		IPAERR("Failed to delay Q6 pipes\n");
-		BUG();
-	}
-
-	if (ipa_q6_monitor_holb_mitigation(false)) {
-		IPAERR("Failed to disable HOLB monitroing on Q6 pipes\n");
-		BUG();
-	}
-
-	if (ipa_q6_avoid_holb(false)) {
-		IPAERR("Failed to set HOLB on Q6 pipes\n");
-		BUG();
-	}
-	if (ipa_q6_clean_q6_tables()) {
-		IPAERR("Failed to clean Q6 tables\n");
-		BUG();
-	}
-	if (ipa_q6_set_ex_path_dis_agg()) {
-		IPAERR("Failed to disable aggregation on Q6 pipes\n");
-		BUG();
-	}
-
-	/* set proxy vote before decrement */
-	ipa_proxy_clk_vote();
-	ipa_dec_client_disable_clks();
-	return 0;
-}
-
-/**
-* ipa_q6_post_shutdown_cleanup() - A cleanup for the Q6 pipes
-*                    in IPA HW after modem shutdown. This is performed
-*                    in case of SSR.
-*
-* Return codes:
-* 0: success
-* This is a mandatory procedure, in case one of the steps fails, the
-* AP needs to restart.
-*/
-int ipa_q6_post_shutdown_cleanup(void)
-{
-	int client_idx;
-	int res;
-
-	/*
-	 * pipe delay and holb discard for ZIP pipes are handled in
-	 * post shutdown.
-	 */
-	if (ipa_q6_pipe_delay(true)) {
-		IPAERR("Failed to delay Q6 ZIP pipes\n");
-		BUG();
-	}
-	if (ipa_q6_avoid_holb(true)) {
-		IPAERR("Failed to set HOLB on Q6 ZIP pipes\n");
-		BUG();
-	}
-
-	if (!ipa_ctx->uc_ctx.uc_loaded) {
-		IPAERR("uC is not loaded, won't reset Q6 pipes\n");
-		return 0;
-	}
-
-	for (client_idx = 0; client_idx < IPA_CLIENT_MAX; client_idx++)
-		if (IPA_CLIENT_IS_Q6_NON_ZIP_CONS(client_idx) ||
-			IPA_CLIENT_IS_Q6_ZIP_CONS(client_idx) ||
-			IPA_CLIENT_IS_Q6_NON_ZIP_PROD(client_idx) ||
-			IPA_CLIENT_IS_Q6_ZIP_PROD(client_idx)) {
-			res = ipa_uc_reset_pipe(client_idx);
-			if (res)
-				BUG();
-		}
-	return 0;
-}
-
-
-int _ipa_init_sram_v2(void)
-{
-	u32 *ipa_sram_mmio;
-	unsigned long phys_addr;
-	struct ipa_hw_imm_cmd_dma_shared_mem cmd = {0};
-	struct ipa_desc desc = {0};
-	struct ipa_mem_buffer mem;
-	int rc = 0;
-
-	phys_addr = ipa_ctx->ipa_wrapper_base +
-		ipa_ctx->ctrl->ipa_reg_base_ofst +
-		IPA_SRAM_DIRECT_ACCESS_N_OFST_v2_0(
-			ipa_ctx->smem_restricted_bytes / 4);
-
-	ipa_sram_mmio = ioremap(phys_addr,
-			ipa_ctx->smem_sz - ipa_ctx->smem_restricted_bytes);
-	if (!ipa_sram_mmio) {
-		IPAERR("fail to ioremap IPA SRAM\n");
-		return -ENOMEM;
-	}
-
-#define IPA_SRAM_SET(ofst, val) (ipa_sram_mmio[(ofst - 4) / 4] = val)
-
-	IPA_SRAM_SET(IPA_MEM_PART(v6_flt_ofst) - 4, IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v6_flt_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v4_rt_ofst) - 4, IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v4_rt_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v6_rt_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(modem_hdr_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(modem_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(apps_v4_flt_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(uc_info_ofst), IPA_MEM_CANARY_VAL);
-
-	iounmap(ipa_sram_mmio);
-
-	mem.size = IPA_STATUS_CLEAR_SIZE;
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-			GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-	memset(mem.base, 0, mem.size);
-
-	cmd.size = mem.size;
-	cmd.system_addr = mem.phys_base;
-	cmd.local_addr = IPA_STATUS_CLEAR_OFST;
-	desc.opcode = IPA_DMA_SHARED_MEM;
-	desc.pyld = &cmd;
-	desc.len = sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-	desc.type = IPA_IMM_CMD_DESC;
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		rc = -EFAULT;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-	return rc;
-}
-
-int _ipa_init_sram_v2_5(void)
-{
-	u32 *ipa_sram_mmio;
-	unsigned long phys_addr;
-
-	phys_addr = ipa_ctx->ipa_wrapper_base +
-			ipa_ctx->ctrl->ipa_reg_base_ofst +
-			IPA_SRAM_SW_FIRST_v2_5;
-
-	ipa_sram_mmio = ioremap(phys_addr,
-		ipa_ctx->smem_sz - ipa_ctx->smem_restricted_bytes);
-	if (!ipa_sram_mmio) {
-		IPAERR("fail to ioremap IPA SRAM\n");
-		return -ENOMEM;
-	}
-
-#define IPA_SRAM_SET(ofst, val) (ipa_sram_mmio[(ofst - 4) / 4] = val)
-
-	IPA_SRAM_SET(IPA_MEM_PART(v4_flt_ofst) - 4, IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v4_flt_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v6_flt_ofst) - 4, IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v6_flt_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v4_rt_ofst) - 4, IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v4_rt_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(v6_rt_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(modem_hdr_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(modem_hdr_proc_ctx_ofst) - 4,
-							IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(modem_hdr_proc_ctx_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(modem_ofst), IPA_MEM_CANARY_VAL);
-	IPA_SRAM_SET(IPA_MEM_PART(end_ofst), IPA_MEM_CANARY_VAL);
-
-	iounmap(ipa_sram_mmio);
-
-	return 0;
-}
-
-static inline void ipa_sram_set_canary(u32 *sram_mmio, int offset)
-{
-	/* Set 4 bytes of CANARY before the offset */
-	sram_mmio[(offset - 4) / 4] = IPA_MEM_CANARY_VAL;
-}
-
-int _ipa_init_sram_v2_6L(void)
-{
-	u32 *ipa_sram_mmio;
-	unsigned long phys_addr;
-
-	phys_addr = ipa_ctx->ipa_wrapper_base +
-		ipa_ctx->ctrl->ipa_reg_base_ofst +
-		IPA_SRAM_SW_FIRST_v2_5;
-
-	ipa_sram_mmio = ioremap(phys_addr,
-		ipa_ctx->smem_sz - ipa_ctx->smem_restricted_bytes);
-	if (!ipa_sram_mmio) {
-		IPAERR("fail to ioremap IPA SRAM\n");
-		return -ENOMEM;
-	}
-
-	/* Consult with ipa_ram_mmap.h on the location of the CANARY values */
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(v4_flt_ofst) - 4);
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(v4_flt_ofst));
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(v6_flt_ofst) - 4);
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(v6_flt_ofst));
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(v4_rt_ofst) - 4);
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(v4_rt_ofst));
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(v6_rt_ofst));
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(modem_hdr_ofst));
-	ipa_sram_set_canary(ipa_sram_mmio,
-			    IPA_MEM_PART(modem_comp_decomp_ofst) - 4);
-	ipa_sram_set_canary(ipa_sram_mmio,
-			    IPA_MEM_PART(modem_comp_decomp_ofst));
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(modem_ofst));
-	ipa_sram_set_canary(ipa_sram_mmio, IPA_MEM_PART(end_ofst));
-
-	iounmap(ipa_sram_mmio);
-
-	return 0;
-}
-
-int _ipa_init_hdr_v2(void)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer mem;
-	struct ipa_hdr_init_local cmd;
-	int rc = 0;
-
-	mem.size = IPA_MEM_PART(modem_hdr_size) + IPA_MEM_PART(apps_hdr_size);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-			GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-	memset(mem.base, 0, mem.size);
-
-	cmd.hdr_table_src_addr = mem.phys_base;
-	cmd.size_hdr_table = mem.size;
-	cmd.hdr_table_dst_addr = ipa_ctx->smem_restricted_bytes +
-		IPA_MEM_PART(modem_hdr_ofst);
-
-	desc.opcode = IPA_HDR_INIT_LOCAL;
-	desc.pyld = &cmd;
-	desc.len = sizeof(struct ipa_hdr_init_local);
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem.base, mem.phys_base, mem.size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		rc = -EFAULT;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-	return rc;
-}
-
-int _ipa_init_hdr_v2_5(void)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer mem;
-	struct ipa_hdr_init_local cmd = { 0 };
-	struct ipa_hw_imm_cmd_dma_shared_mem dma_cmd = { 0 };
-
-	mem.size = IPA_MEM_PART(modem_hdr_size) + IPA_MEM_PART(apps_hdr_size);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-		GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-	memset(mem.base, 0, mem.size);
-
-	cmd.hdr_table_src_addr = mem.phys_base;
-	cmd.size_hdr_table = mem.size;
-	cmd.hdr_table_dst_addr = ipa_ctx->smem_restricted_bytes +
-		IPA_MEM_PART(modem_hdr_ofst);
-
-	desc.opcode = IPA_HDR_INIT_LOCAL;
-	desc.pyld = &cmd;
-	desc.len = sizeof(struct ipa_hdr_init_local);
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem.base, mem.phys_base, mem.size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		dma_free_coherent(ipa_ctx->pdev,
-			mem.size, mem.base,
-			mem.phys_base);
-		return -EFAULT;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-
-	mem.size = IPA_MEM_PART(modem_hdr_proc_ctx_size) +
-		IPA_MEM_PART(apps_hdr_proc_ctx_size);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-		GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-	memset(mem.base, 0, mem.size);
-	memset(&desc, 0, sizeof(desc));
-
-	dma_cmd.system_addr = mem.phys_base;
-	dma_cmd.local_addr = ipa_ctx->smem_restricted_bytes +
-		IPA_MEM_PART(modem_hdr_proc_ctx_ofst);
-	dma_cmd.size = mem.size;
-	desc.opcode = IPA_DMA_SHARED_MEM;
-	desc.pyld = &dma_cmd;
-	desc.len = sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem.base, mem.phys_base, mem.size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		dma_free_coherent(ipa_ctx->pdev,
-			mem.size,
-			mem.base,
-			mem.phys_base);
-		return -EFAULT;
-	}
-
-	ipa_write_reg(ipa_ctx->mmio,
-		IPA_LOCAL_PKT_PROC_CNTXT_BASE_OFST,
-		dma_cmd.local_addr);
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-
-	return 0;
-}
-
-int _ipa_init_hdr_v2_6L(void)
-{
-	/* Same implementation as IPAv2 */
-	return _ipa_init_hdr_v2();
-}
-
-int _ipa_init_rt4_v2(void)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer mem;
-	struct ipa_ip_v4_routing_init v4_cmd;
-	u32 *entry;
-	int i;
-	int rc = 0;
-
-	for (i = IPA_MEM_PART(v4_modem_rt_index_lo);
-		i <= IPA_MEM_PART(v4_modem_rt_index_hi);
-		i++)
-		ipa_ctx->rt_idx_bitmap[IPA_IP_v4] |= (1 << i);
-	IPADBG("v4 rt bitmap 0x%lx\n", ipa_ctx->rt_idx_bitmap[IPA_IP_v4]);
-
-	mem.size = IPA_MEM_PART(v4_rt_size);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-			GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-
-	entry = mem.base;
-	for (i = 0; i < IPA_MEM_PART(v4_num_index); i++) {
-		*entry = ipa_ctx->empty_rt_tbl_mem.phys_base;
-		entry++;
-	}
-
-	desc.opcode = IPA_IP_V4_ROUTING_INIT;
-	v4_cmd.ipv4_rules_addr = mem.phys_base;
-	v4_cmd.size_ipv4_rules = mem.size;
-	v4_cmd.ipv4_addr = ipa_ctx->smem_restricted_bytes +
-		IPA_MEM_PART(v4_rt_ofst);
-	IPADBG("putting Routing IPv4 rules to phys 0x%x",
-				v4_cmd.ipv4_addr);
-
-	desc.pyld = &v4_cmd;
-	desc.len = sizeof(struct ipa_ip_v4_routing_init);
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem.base, mem.phys_base, mem.size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		rc = -EFAULT;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-	return rc;
-}
-
-int _ipa_init_rt6_v2(void)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer mem;
-	struct ipa_ip_v6_routing_init v6_cmd;
-	u32 *entry;
-	int i;
-	int rc = 0;
-
-	for (i = IPA_MEM_PART(v6_modem_rt_index_lo);
-		i <= IPA_MEM_PART(v6_modem_rt_index_hi);
-		i++)
-		ipa_ctx->rt_idx_bitmap[IPA_IP_v6] |= (1 << i);
-	IPADBG("v6 rt bitmap 0x%lx\n", ipa_ctx->rt_idx_bitmap[IPA_IP_v6]);
-
-	mem.size = IPA_MEM_PART(v6_rt_size);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-			GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-
-	entry = mem.base;
-	for (i = 0; i < IPA_MEM_PART(v6_num_index); i++) {
-		*entry = ipa_ctx->empty_rt_tbl_mem.phys_base;
-		entry++;
-	}
-
-	desc.opcode = IPA_IP_V6_ROUTING_INIT;
-	v6_cmd.ipv6_rules_addr = mem.phys_base;
-	v6_cmd.size_ipv6_rules = mem.size;
-	v6_cmd.ipv6_addr = ipa_ctx->smem_restricted_bytes +
-		IPA_MEM_PART(v6_rt_ofst);
-	IPADBG("putting Routing IPv6 rules to phys 0x%x",
-				v6_cmd.ipv6_addr);
-
-	desc.pyld = &v6_cmd;
-	desc.len = sizeof(struct ipa_ip_v6_routing_init);
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem.base, mem.phys_base, mem.size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		rc = -EFAULT;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-	return rc;
-}
-
-int _ipa_init_flt4_v2(void)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer mem;
-	struct ipa_ip_v4_filter_init v4_cmd;
-	u32 *entry;
-	int i;
-	int rc = 0;
-
-	mem.size = IPA_MEM_PART(v4_flt_size);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-			GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-
-	entry = mem.base;
-
-	*entry = ((0xFFFFF << 1) | 0x1);
-	entry++;
-
-	for (i = 0; i <= ipa_ctx->ipa_num_pipes; i++) {
-		*entry = ipa_ctx->empty_rt_tbl_mem.phys_base;
-		entry++;
-	}
-
-	desc.opcode = IPA_IP_V4_FILTER_INIT;
-	v4_cmd.ipv4_rules_addr = mem.phys_base;
-	v4_cmd.size_ipv4_rules = mem.size;
-	v4_cmd.ipv4_addr = ipa_ctx->smem_restricted_bytes +
-		IPA_MEM_PART(v4_flt_ofst);
-	IPADBG("putting Filtering IPv4 rules to phys 0x%x",
-				v4_cmd.ipv4_addr);
-
-	desc.pyld = &v4_cmd;
-	desc.len = sizeof(struct ipa_ip_v4_filter_init);
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem.base, mem.phys_base, mem.size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		rc = -EFAULT;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-	return rc;
-}
-
-int _ipa_init_flt6_v2(void)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer mem;
-	struct ipa_ip_v6_filter_init v6_cmd;
-	u32 *entry;
-	int i;
-	int rc = 0;
-
-	mem.size = IPA_MEM_PART(v6_flt_size);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-			GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-
-	entry = mem.base;
-
-	*entry = (0xFFFFF << 1) | 0x1;
-	entry++;
-
-	for (i = 0; i <= ipa_ctx->ipa_num_pipes; i++) {
-		*entry = ipa_ctx->empty_rt_tbl_mem.phys_base;
-		entry++;
-	}
-
-	desc.opcode = IPA_IP_V6_FILTER_INIT;
-	v6_cmd.ipv6_rules_addr = mem.phys_base;
-	v6_cmd.size_ipv6_rules = mem.size;
-	v6_cmd.ipv6_addr = ipa_ctx->smem_restricted_bytes +
-		IPA_MEM_PART(v6_flt_ofst);
-	IPADBG("putting Filtering IPv6 rules to phys 0x%x",
-				v6_cmd.ipv6_addr);
-
-	desc.pyld = &v6_cmd;
-	desc.len = sizeof(struct ipa_ip_v6_filter_init);
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem.base, mem.phys_base, mem.size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		rc = -EFAULT;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-	return rc;
-}
-
-static int ipa_setup_apps_pipes(void)
-{
-	struct ipa_sys_connect_params sys_in;
-	int result = 0;
-
-	/* CMD OUT (A5->IPA) */
-	memset(&sys_in, 0, sizeof(struct ipa_sys_connect_params));
-	sys_in.client = IPA_CLIENT_APPS_CMD_PROD;
-	sys_in.desc_fifo_sz = IPA_SYS_DESC_FIFO_SZ;
-	sys_in.ipa_ep_cfg.mode.mode = IPA_DMA;
-	sys_in.ipa_ep_cfg.mode.dst = IPA_CLIENT_APPS_LAN_CONS;
-	sys_in.skip_ep_cfg = true;
-	if (ipa_setup_sys_pipe(&sys_in, &ipa_ctx->clnt_hdl_cmd)) {
-		IPAERR(":setup sys pipe failed.\n");
-		result = -EPERM;
-		goto fail_cmd;
-	}
-	IPADBG("Apps to IPA cmd pipe is connected\n");
-
-	ipa_ctx->ctrl->ipa_init_sram();
-	IPADBG("SRAM initialized\n");
-
-	ipa_ctx->ctrl->ipa_init_hdr();
-	IPADBG("HDR initialized\n");
-
-	ipa_ctx->ctrl->ipa_init_rt4();
-	IPADBG("V4 RT initialized\n");
-
-	ipa_ctx->ctrl->ipa_init_rt6();
-	IPADBG("V6 RT initialized\n");
-
-	ipa_ctx->ctrl->ipa_init_flt4();
-	IPADBG("V4 FLT initialized\n");
-
-	ipa_ctx->ctrl->ipa_init_flt6();
-	IPADBG("V6 FLT initialized\n");
-
-	if (ipa_setup_exception_path()) {
-		IPAERR(":fail to setup excp path\n");
-		result = -EPERM;
-		goto fail_schedule_delayed_work;
-	}
-	IPADBG("Exception path was successfully set");
-
-	if (ipa_setup_dflt_rt_tables()) {
-		IPAERR(":fail to setup dflt routes\n");
-		result = -EPERM;
-		goto fail_schedule_delayed_work;
-	}
-	IPADBG("default routing was set\n");
-
-	/* LAN IN (IPA->A5) */
-	memset(&sys_in, 0, sizeof(struct ipa_sys_connect_params));
-	sys_in.client = IPA_CLIENT_APPS_LAN_CONS;
-	sys_in.desc_fifo_sz = IPA_SYS_DESC_FIFO_SZ;
-	if (ipa_ctx->ipa_hw_type == IPA_HW_v1_1) {
-		sys_in.ipa_ep_cfg.hdr.hdr_a5_mux = 1;
-		sys_in.ipa_ep_cfg.hdr.hdr_len = IPA_A5_MUX_HEADER_LENGTH;
-	} else if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_0) {
-		sys_in.notify = ipa_lan_rx_cb;
-		sys_in.priv = NULL;
-		sys_in.ipa_ep_cfg.hdr.hdr_len = IPA_LAN_RX_HEADER_LENGTH;
-		sys_in.ipa_ep_cfg.hdr_ext.hdr_little_endian = false;
-		sys_in.ipa_ep_cfg.hdr_ext.hdr_total_len_or_pad_valid = true;
-		sys_in.ipa_ep_cfg.hdr_ext.hdr_total_len_or_pad = IPA_HDR_PAD;
-		sys_in.ipa_ep_cfg.hdr_ext.hdr_payload_len_inc_padding = false;
-		sys_in.ipa_ep_cfg.hdr_ext.hdr_total_len_or_pad_offset = 0;
-		sys_in.ipa_ep_cfg.hdr_ext.hdr_pad_to_alignment = 2;
-		sys_in.ipa_ep_cfg.cfg.cs_offload_en = IPA_ENABLE_CS_OFFLOAD_DL;
-	} else {
-		WARN_ON(1);
-	}
-
-	/**
-	 * ipa_lan_rx_cb() intended to notify the source EP about packet
-	 * being received on the LAN_CONS via calling the source EP call-back.
-	 * There could be a race condition with calling this call-back. Other
-	 * thread may nullify it - e.g. on EP disconnect.
-	 * This lock intended to protect the access to the source EP call-back
-	 */
-	spin_lock_init(&ipa_ctx->disconnect_lock);
-	if (ipa_setup_sys_pipe(&sys_in, &ipa_ctx->clnt_hdl_data_in)) {
-		IPAERR(":setup sys pipe failed.\n");
-		result = -EPERM;
-		goto fail_schedule_delayed_work;
-	}
-
-	/* LAN-WAN OUT (A5->IPA) */
-	memset(&sys_in, 0, sizeof(struct ipa_sys_connect_params));
-	sys_in.client = IPA_CLIENT_APPS_LAN_WAN_PROD;
-	sys_in.desc_fifo_sz = IPA_SYS_TX_DATA_DESC_FIFO_SZ;
-	sys_in.ipa_ep_cfg.mode.mode = IPA_BASIC;
-	if (ipa_setup_sys_pipe(&sys_in, &ipa_ctx->clnt_hdl_data_out)) {
-		IPAERR(":setup sys pipe failed.\n");
-		result = -EPERM;
-		goto fail_data_out;
-	}
-
-	return 0;
-
-fail_data_out:
-	ipa_teardown_sys_pipe(ipa_ctx->clnt_hdl_data_in);
-fail_schedule_delayed_work:
-	if (ipa_ctx->dflt_v6_rt_rule_hdl)
-		__ipa_del_rt_rule(ipa_ctx->dflt_v6_rt_rule_hdl);
-	if (ipa_ctx->dflt_v4_rt_rule_hdl)
-		__ipa_del_rt_rule(ipa_ctx->dflt_v4_rt_rule_hdl);
-	if (ipa_ctx->excp_hdr_hdl)
-		__ipa_del_hdr(ipa_ctx->excp_hdr_hdl, false);
-	ipa_teardown_sys_pipe(ipa_ctx->clnt_hdl_cmd);
-fail_cmd:
-	return result;
-}
-
-static void ipa_teardown_apps_pipes(void)
-{
-	ipa_teardown_sys_pipe(ipa_ctx->clnt_hdl_data_out);
-	ipa_teardown_sys_pipe(ipa_ctx->clnt_hdl_data_in);
-	__ipa_del_rt_rule(ipa_ctx->dflt_v6_rt_rule_hdl);
-	__ipa_del_rt_rule(ipa_ctx->dflt_v4_rt_rule_hdl);
-	__ipa_del_hdr(ipa_ctx->excp_hdr_hdl, false);
-	ipa_teardown_sys_pipe(ipa_ctx->clnt_hdl_cmd);
-}
-
-#ifdef CONFIG_COMPAT
-long compat_ipa_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
-{
-	int retval = 0;
-	struct ipa_ioc_nat_alloc_mem32 nat_mem32;
-	struct ipa_ioc_nat_alloc_mem nat_mem;
-
-	switch (cmd) {
-	case IPA_IOC_ADD_HDR32:
-		cmd = IPA_IOC_ADD_HDR;
-		break;
-	case IPA_IOC_DEL_HDR32:
-		cmd = IPA_IOC_DEL_HDR;
-		break;
-	case IPA_IOC_ADD_RT_RULE32:
-		cmd = IPA_IOC_ADD_RT_RULE;
-		break;
-	case IPA_IOC_DEL_RT_RULE32:
-		cmd = IPA_IOC_DEL_RT_RULE;
-		break;
-	case IPA_IOC_ADD_FLT_RULE32:
-		cmd = IPA_IOC_ADD_FLT_RULE;
-		break;
-	case IPA_IOC_DEL_FLT_RULE32:
-		cmd = IPA_IOC_DEL_FLT_RULE;
-		break;
-	case IPA_IOC_GET_RT_TBL32:
-		cmd = IPA_IOC_GET_RT_TBL;
-		break;
-	case IPA_IOC_COPY_HDR32:
-		cmd = IPA_IOC_COPY_HDR;
-		break;
-	case IPA_IOC_QUERY_INTF32:
-		cmd = IPA_IOC_QUERY_INTF;
-		break;
-	case IPA_IOC_QUERY_INTF_TX_PROPS32:
-		cmd = IPA_IOC_QUERY_INTF_TX_PROPS;
-		break;
-	case IPA_IOC_QUERY_INTF_RX_PROPS32:
-		cmd = IPA_IOC_QUERY_INTF_RX_PROPS;
-		break;
-	case IPA_IOC_QUERY_INTF_EXT_PROPS32:
-		cmd = IPA_IOC_QUERY_INTF_EXT_PROPS;
-		break;
-	case IPA_IOC_GET_HDR32:
-		cmd = IPA_IOC_GET_HDR;
-		break;
-	case IPA_IOC_ALLOC_NAT_MEM32:
-		if (copy_from_user((u8 *)&nat_mem32, (u8 *)arg,
-			sizeof(struct ipa_ioc_nat_alloc_mem32))) {
-			retval = -EFAULT;
-			goto ret;
-		}
-		memcpy(nat_mem.dev_name, nat_mem32.dev_name,
-				IPA_RESOURCE_NAME_MAX);
-		nat_mem.size = (size_t)nat_mem32.size;
-		nat_mem.offset = (off_t)nat_mem32.offset;
-
-		/* null terminate the string */
-		nat_mem.dev_name[IPA_RESOURCE_NAME_MAX - 1] = '\0';
-
-		if (allocate_nat_device(&nat_mem)) {
-			retval = -EFAULT;
-			goto ret;
-		}
-		nat_mem32.offset = (compat_off_t)nat_mem.offset;
-		if (copy_to_user((u8 *)arg, (u8 *)&nat_mem32,
-			sizeof(struct ipa_ioc_nat_alloc_mem32))) {
-			retval = -EFAULT;
-		}
-ret:
-		return retval;
-	case IPA_IOC_V4_INIT_NAT32:
-		cmd = IPA_IOC_V4_INIT_NAT;
-		break;
-	case IPA_IOC_NAT_DMA32:
-		cmd = IPA_IOC_NAT_DMA;
-		break;
-	case IPA_IOC_V4_DEL_NAT32:
-		cmd = IPA_IOC_V4_DEL_NAT;
-		break;
-	case IPA_IOC_GET_NAT_OFFSET32:
-		cmd = IPA_IOC_GET_NAT_OFFSET;
-		break;
-	case IPA_IOC_PULL_MSG32:
-		cmd = IPA_IOC_PULL_MSG;
-		break;
-	case IPA_IOC_RM_ADD_DEPENDENCY32:
-		cmd = IPA_IOC_RM_ADD_DEPENDENCY;
-		break;
-	case IPA_IOC_RM_DEL_DEPENDENCY32:
-		cmd = IPA_IOC_RM_DEL_DEPENDENCY;
-		break;
-	case IPA_IOC_GENERATE_FLT_EQ32:
-		cmd = IPA_IOC_GENERATE_FLT_EQ;
-		break;
-	case IPA_IOC_QUERY_RT_TBL_INDEX32:
-		cmd = IPA_IOC_QUERY_RT_TBL_INDEX;
-		break;
-	case IPA_IOC_WRITE_QMAPID32:
-		cmd = IPA_IOC_WRITE_QMAPID;
-		break;
-	case IPA_IOC_MDFY_FLT_RULE32:
-		cmd = IPA_IOC_MDFY_FLT_RULE;
-		break;
-	case IPA_IOC_NOTIFY_WAN_UPSTREAM_ROUTE_ADD32:
-		cmd = IPA_IOC_NOTIFY_WAN_UPSTREAM_ROUTE_ADD;
-		break;
-	case IPA_IOC_NOTIFY_WAN_UPSTREAM_ROUTE_DEL32:
-		cmd = IPA_IOC_NOTIFY_WAN_UPSTREAM_ROUTE_DEL;
-		break;
-	case IPA_IOC_NOTIFY_WAN_EMBMS_CONNECTED32:
-		cmd = IPA_IOC_NOTIFY_WAN_EMBMS_CONNECTED;
-		break;
-	case IPA_IOC_MDFY_RT_RULE32:
-		cmd = IPA_IOC_MDFY_RT_RULE;
-		break;
-	case IPA_IOC_COMMIT_HDR:
-	case IPA_IOC_RESET_HDR:
-	case IPA_IOC_COMMIT_RT:
-	case IPA_IOC_RESET_RT:
-	case IPA_IOC_COMMIT_FLT:
-	case IPA_IOC_RESET_FLT:
-	case IPA_IOC_DUMP:
-	case IPA_IOC_PUT_RT_TBL:
-	case IPA_IOC_PUT_HDR:
-	case IPA_IOC_SET_FLT:
-	case IPA_IOC_QUERY_EP_MAPPING:
-		break;
-	default:
-		return -ENOIOCTLCMD;
-	}
-	return ipa_ioctl(file, cmd, (unsigned long) compat_ptr(arg));
-}
-#endif
-
-static const struct file_operations ipa_drv_fops = {
-	.owner = THIS_MODULE,
-	.open = ipa_open,
-	.read = ipa_read,
-	.unlocked_ioctl = ipa_ioctl,
-#ifdef CONFIG_COMPAT
-	.compat_ioctl = compat_ipa_ioctl,
-#endif
-};
-
-static int ipa_get_clks(struct device *dev)
-{
-	ipa_clk = clk_get(dev, "core_clk");
-	if (IS_ERR(ipa_clk)) {
-		if (ipa_clk != ERR_PTR(-EPROBE_DEFER))
-			IPAERR("fail to get ipa clk\n");
-		return PTR_ERR(ipa_clk);
-	}
-
-	if (smmu_present && arm_smmu) {
-		smmu_clk = clk_get(dev, "smmu_clk");
-		if (IS_ERR(smmu_clk)) {
-			if (smmu_clk != ERR_PTR(-EPROBE_DEFER))
-				IPAERR("fail to get smmu clk\n");
-			return PTR_ERR(smmu_clk);
-		}
-
-		if (clk_get_rate(smmu_clk) == 0) {
-			long rate = clk_round_rate(smmu_clk, 1000);
-			clk_set_rate(smmu_clk, rate);
-		}
-	}
-
-	if (ipa_ctx->ipa_hw_type < IPA_HW_v2_0) {
-		ipa_cnoc_clk = clk_get(dev, "iface_clk");
-		if (IS_ERR(ipa_cnoc_clk)) {
-			ipa_cnoc_clk = NULL;
-			IPAERR("fail to get cnoc clk\n");
-			return -ENODEV;
-		}
-
-		ipa_clk_src = clk_get(dev, "core_src_clk");
-		if (IS_ERR(ipa_clk_src)) {
-			ipa_clk_src = NULL;
-			IPAERR("fail to get ipa clk src\n");
-			return -ENODEV;
-		}
-
-		sys_noc_ipa_axi_clk = clk_get(dev, "bus_clk");
-		if (IS_ERR(sys_noc_ipa_axi_clk)) {
-			sys_noc_ipa_axi_clk = NULL;
-			IPAERR("fail to get sys_noc_ipa_axi clk\n");
-			return -ENODEV;
-		}
-
-		ipa_inactivity_clk = clk_get(dev, "inactivity_clk");
-		if (IS_ERR(ipa_inactivity_clk)) {
-			ipa_inactivity_clk = NULL;
-			IPAERR("fail to get inactivity clk\n");
-			return -ENODEV;
-		}
-	}
-
-	return 0;
-}
-
-void _ipa_enable_clks_v2_0(void)
-{
-	IPADBG("enabling gcc_ipa_clk\n");
-	if (ipa_clk) {
-		clk_prepare(ipa_clk);
-		clk_enable(ipa_clk);
-		IPADBG("curr_ipa_clk_rate=%d", ipa_ctx->curr_ipa_clk_rate);
-		clk_set_rate(ipa_clk, ipa_ctx->curr_ipa_clk_rate);
-		ipa_uc_notify_clk_state(true);
-	} else {
-		WARN_ON(1);
-	}
-
-	if (smmu_clk)
-		clk_prepare_enable(smmu_clk);
-	/* Enable the BAM IRQ. */
-	ipa_sps_irq_control_all(true);
-	ipa_suspend_apps_pipes(false);
-}
-
-void _ipa_enable_clks_v1_1(void)
-{
-
-	if (ipa_cnoc_clk) {
-		clk_prepare(ipa_cnoc_clk);
-		clk_enable(ipa_cnoc_clk);
-		clk_set_rate(ipa_cnoc_clk, IPA_CNOC_CLK_RATE);
-	} else {
-		WARN_ON(1);
-	}
-
-	if (ipa_clk_src)
-		clk_set_rate(ipa_clk_src,
-				ipa_ctx->curr_ipa_clk_rate);
-	else
-		WARN_ON(1);
-
-	if (ipa_clk)
-		clk_prepare(ipa_clk);
-	else
-		WARN_ON(1);
-
-	if (sys_noc_ipa_axi_clk)
-		clk_prepare(sys_noc_ipa_axi_clk);
-	else
-		WARN_ON(1);
-
-	if (ipa_inactivity_clk)
-		clk_prepare(ipa_inactivity_clk);
-	else
-		WARN_ON(1);
-
-	if (ipa_clk)
-		clk_enable(ipa_clk);
-	else
-		WARN_ON(1);
-
-	if (sys_noc_ipa_axi_clk)
-		clk_enable(sys_noc_ipa_axi_clk);
-	else
-		WARN_ON(1);
-
-	if (ipa_inactivity_clk)
-		clk_enable(ipa_inactivity_clk);
-	else
-		WARN_ON(1);
-
-}
-
-static unsigned int ipa_get_bus_vote(void)
-{
-	unsigned int idx = 1;
-
-	if (ipa_ctx->curr_ipa_clk_rate == ipa_ctx->ctrl->ipa_clk_rate_svs) {
-		idx = 1;
-	} else if (ipa_ctx->curr_ipa_clk_rate ==
-			ipa_ctx->ctrl->ipa_clk_rate_nominal) {
-		if (ipa_ctx->ctrl->msm_bus_data_ptr->num_usecases <= 2)
-			idx = 1;
-		else
-			idx = 2;
-	} else if (ipa_ctx->curr_ipa_clk_rate ==
-			ipa_ctx->ctrl->ipa_clk_rate_turbo) {
-		idx = ipa_ctx->ctrl->msm_bus_data_ptr->num_usecases - 1;
-	} else {
-		WARN_ON(1);
-	}
-
-	IPADBG("curr %d idx %d\n", ipa_ctx->curr_ipa_clk_rate, idx);
-
-	return idx;
-}
-
-/**
-* ipa_enable_clks() - Turn on IPA clocks
-*
-* Return codes:
-* None
-*/
-void ipa_enable_clks(void)
-{
-	IPADBG("enabling IPA clocks and bus voting\n");
-
-	ipa_ctx->ctrl->ipa_enable_clks();
-
-	if (ipa_ctx->ipa_hw_mode != IPA_HW_MODE_VIRTUAL)
-		if (msm_bus_scale_client_update_request(ipa_ctx->ipa_bus_hdl,
-		    ipa_get_bus_vote()))
-			WARN_ON(1);
-}
-
-void _ipa_disable_clks_v1_1(void)
-{
-
-	if (ipa_inactivity_clk)
-		clk_disable_unprepare(ipa_inactivity_clk);
-	else
-		WARN_ON(1);
-
-	if (sys_noc_ipa_axi_clk)
-		clk_disable_unprepare(sys_noc_ipa_axi_clk);
-	else
-		WARN_ON(1);
-
-	if (ipa_clk)
-		clk_disable_unprepare(ipa_clk);
-	else
-		WARN_ON(1);
-
-	if (ipa_cnoc_clk)
-		clk_disable_unprepare(ipa_cnoc_clk);
-	else
-		WARN_ON(1);
-
-}
-
-void _ipa_disable_clks_v2_0(void)
-{
-	IPADBG("disabling gcc_ipa_clk\n");
-	ipa_suspend_apps_pipes(true);
-	ipa_sps_irq_control_all(false);
-	ipa_uc_notify_clk_state(false);
-	if (ipa_clk)
-		clk_disable_unprepare(ipa_clk);
-	else
-		WARN_ON(1);
-
-	if (smmu_clk)
-		clk_disable_unprepare(smmu_clk);
-}
-
-/**
-* ipa_disable_clks() - Turn off IPA clocks
-*
-* Return codes:
-* None
-*/
-void ipa_disable_clks(void)
-{
-	IPADBG("disabling IPA clocks and bus voting\n");
-
-	ipa_ctx->ctrl->ipa_disable_clks();
-
-	if (ipa_ctx->ipa_hw_mode != IPA_HW_MODE_VIRTUAL)
-		if (msm_bus_scale_client_update_request(ipa_ctx->ipa_bus_hdl,
-		    0))
-			WARN_ON(1);
-}
-
-/**
- * ipa_start_tag_process() - Send TAG packet and wait for it to come back
- *
- * This function is called prior to clock gating when active client counter
- * is 1. TAG process ensures that there are no packets inside IPA HW that
- * were not submitted to peer's BAM. During TAG process all aggregation frames
- * are (force) closed.
- *
- * Return codes:
- * None
- */
-static void ipa_start_tag_process(struct work_struct *work)
-{
-	int res;
-
-	IPADBG("starting TAG process\n");
-	/* close aggregation frames on all pipes */
-	res = ipa_tag_aggr_force_close(-1);
-	if (res)
-		IPAERR("ipa_tag_aggr_force_close failed %d\n", res);
-
-	ipa_dec_client_disable_clks();
-
-	IPADBG("TAG process done\n");
-	return;
-}
-
-/**
-* ipa_inc_client_enable_clks() - Increase active clients counter, and
-* enable ipa clocks if necessary
-*
-* Return codes:
-* None
-*/
-void ipa_inc_client_enable_clks(void)
-{
-	ipa_active_clients_lock();
-	ipa_ctx->ipa_active_clients.cnt++;
-	if (ipa_ctx->ipa_active_clients.cnt == 1)
-		ipa_enable_clks();
-	IPADBG("active clients = %d\n", ipa_ctx->ipa_active_clients.cnt);
-	ipa_active_clients_unlock();
-}
-
-/**
-* ipa_inc_client_enable_clks_no_block() - Only increment the number of active
-* clients if no asynchronous actions should be done. Asynchronous actions are
-* locking a mutex and waking up IPA HW.
-*
-* Return codes: 0 for success
-*		-EPERM if an asynchronous action should have been done
-*/
-int ipa_inc_client_enable_clks_no_block(void)
-{
-	int res = 0;
-	unsigned long flags;
-
-	if (ipa_active_clients_trylock(&flags) == 0)
-		return -EPERM;
-
-	if (ipa_ctx->ipa_active_clients.cnt == 0) {
-		res = -EPERM;
-		goto bail;
-	}
-
-	ipa_ctx->ipa_active_clients.cnt++;
-	IPADBG("active clients = %d\n", ipa_ctx->ipa_active_clients.cnt);
-bail:
-	ipa_active_clients_trylock_unlock(&flags);
-
-	return res;
-}
-
-/**
- * ipa_dec_client_disable_clks() - Decrease active clients counter
- *
- * In case that there are no active clients this function also starts
- * TAG process. When TAG progress ends ipa clocks will be gated.
- * start_tag_process_again flag is set during this function to signal TAG
- * process to start again as there was another client that may send data to ipa
- *
- * Return codes:
- * None
- */
-void ipa_dec_client_disable_clks(void)
-{
-	ipa_active_clients_lock();
-	ipa_ctx->ipa_active_clients.cnt--;
-	IPADBG("active clients = %d\n", ipa_ctx->ipa_active_clients.cnt);
-	if (ipa_ctx->ipa_active_clients.cnt == 0) {
-		if (ipa_ctx->tag_process_before_gating) {
-			ipa_ctx->tag_process_before_gating = false;
-			/*
-			 * When TAG process ends, active clients will be
-			 * decreased
-			 */
-			ipa_ctx->ipa_active_clients.cnt = 1;
-			queue_work(ipa_ctx->power_mgmt_wq, &ipa_tag_work);
-		} else {
-			ipa_disable_clks();
-		}
-	}
-	ipa_active_clients_unlock();
-}
-
-/**
-* ipa_inc_acquire_wakelock() - Increase active clients counter, and
-* acquire wakelock if necessary
-*
-* Return codes:
-* None
-*/
-void ipa_inc_acquire_wakelock(void)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&ipa_ctx->wakelock_ref_cnt.spinlock, flags);
-	ipa_ctx->wakelock_ref_cnt.cnt++;
-	if (ipa_ctx->wakelock_ref_cnt.cnt == 1)
-		__pm_stay_awake(&ipa_ctx->w_lock);
-	IPADBG("active wakelock ref cnt = %d\n", ipa_ctx->wakelock_ref_cnt.cnt);
-	spin_unlock_irqrestore(&ipa_ctx->wakelock_ref_cnt.spinlock, flags);
-}
-
-/**
- * ipa_dec_release_wakelock() - Decrease active clients counter
- *
- * In case if the ref count is 0, release the wakelock.
- *
- * Return codes:
- * None
- */
-void ipa_dec_release_wakelock(void)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&ipa_ctx->wakelock_ref_cnt.spinlock, flags);
-	ipa_ctx->wakelock_ref_cnt.cnt--;
-	IPADBG("active wakelock ref cnt = %d\n", ipa_ctx->wakelock_ref_cnt.cnt);
-	if (ipa_ctx->wakelock_ref_cnt.cnt == 0)
-		__pm_relax(&ipa_ctx->w_lock);
-	spin_unlock_irqrestore(&ipa_ctx->wakelock_ref_cnt.spinlock, flags);
-}
-
-static int ipa_setup_bam_cfg(const struct ipa_plat_drv_res *res)
-{
-	void *ipa_bam_mmio;
-	int reg_val;
-	int retval = 0;
-
-	ipa_bam_mmio = ioremap(res->ipa_mem_base + IPA_BAM_REG_BASE_OFST,
-			IPA_BAM_REMAP_SIZE);
-	if (!ipa_bam_mmio)
-		return -ENOMEM;
-	switch (ipa_ctx->ipa_hw_type) {
-	case IPA_HW_v1_1:
-		reg_val = IPA_BAM_CNFG_BITS_VALv1_1;
-		break;
-	case IPA_HW_v2_0:
-	case IPA_HW_v2_5:
-	case IPA_HW_v2_6L:
-		reg_val = IPA_BAM_CNFG_BITS_VALv2_0;
-		break;
-	default:
-		retval = -EPERM;
-		goto fail;
-	}
-	if (ipa_ctx->ipa_hw_type < IPA_HW_v2_5)
-		ipa_write_reg(ipa_bam_mmio, IPA_BAM_CNFG_BITS_OFST, reg_val);
-fail:
-	iounmap(ipa_bam_mmio);
-
-	return retval;
-}
-
-int ipa_set_required_perf_profile(enum ipa_voltage_level floor_voltage,
-				  u32 bandwidth_mbps)
-{
-	enum ipa_voltage_level needed_voltage;
-	u32 clk_rate;
-
-	IPADBG("floor_voltage=%d, bandwidth_mbps=%u",
-					floor_voltage, bandwidth_mbps);
-
-	if (floor_voltage < IPA_VOLTAGE_UNSPECIFIED ||
-		floor_voltage >= IPA_VOLTAGE_MAX) {
-		IPAERR("bad voltage\n");
-		return -EINVAL;
-	}
-
-	if (ipa_ctx->enable_clock_scaling) {
-		IPADBG("Clock scaling is enabled\n");
-		if (bandwidth_mbps >=
-			ipa_ctx->ctrl->clock_scaling_bw_threshold_turbo)
-			needed_voltage = IPA_VOLTAGE_TURBO;
-		else if (bandwidth_mbps >=
-			ipa_ctx->ctrl->clock_scaling_bw_threshold_nominal)
-			needed_voltage = IPA_VOLTAGE_NOMINAL;
-		else
-			needed_voltage = IPA_VOLTAGE_SVS;
-	} else {
-		IPADBG("Clock scaling is disabled\n");
-		needed_voltage = IPA_VOLTAGE_NOMINAL;
-	}
-
-	needed_voltage = max(needed_voltage, floor_voltage);
-	switch (needed_voltage) {
-	case IPA_VOLTAGE_SVS:
-		clk_rate = ipa_ctx->ctrl->ipa_clk_rate_svs;
-		break;
-	case IPA_VOLTAGE_NOMINAL:
-		clk_rate = ipa_ctx->ctrl->ipa_clk_rate_nominal;
-		break;
-	case IPA_VOLTAGE_TURBO:
-		clk_rate = ipa_ctx->ctrl->ipa_clk_rate_turbo;
-		break;
-	default:
-		IPAERR("bad voltage\n");
-		WARN_ON(1);
-		return -EFAULT;
-	}
-
-	if (clk_rate == ipa_ctx->curr_ipa_clk_rate) {
-		IPADBG("Same voltage\n");
-		return 0;
-	}
-
-	ipa_active_clients_lock();
-	ipa_ctx->curr_ipa_clk_rate = clk_rate;
-	IPADBG("setting clock rate to %u\n", ipa_ctx->curr_ipa_clk_rate);
-	if (ipa_ctx->ipa_active_clients.cnt > 0) {
-		clk_set_rate(ipa_clk, ipa_ctx->curr_ipa_clk_rate);
-		if (ipa_ctx->ipa_hw_mode != IPA_HW_MODE_VIRTUAL)
-			if (msm_bus_scale_client_update_request(
-			    ipa_ctx->ipa_bus_hdl, ipa_get_bus_vote()))
-				WARN_ON(1);
-	} else {
-		IPADBG("clocks are gated, not setting rate\n");
-	}
-	ipa_active_clients_unlock();
-	IPADBG("Done\n");
-	return 0;
-}
-
-static int ipa_init_flt_block(void)
-{
-	int result = 0;
-
-	/*
-	 * SW workaround for Improper Filter Behavior when neither Global nor
-	 * Pipe Rules are present => configure dummy global filter rule
-	 * always which results in a miss
-	 */
-	struct ipa_ioc_add_flt_rule *rules;
-	struct ipa_flt_rule_add *rule;
-	struct ipa_ioc_get_rt_tbl rt_lookup;
-	enum ipa_ip_type ip;
-
-	if (ipa_ctx->ipa_hw_type >= IPA_HW_v1_1) {
-		size_t sz = sizeof(struct ipa_ioc_add_flt_rule) +
-		   sizeof(struct ipa_flt_rule_add);
-
-		rules = kmalloc(sz, GFP_KERNEL);
-		if (rules == NULL) {
-			IPAERR("fail to alloc mem for dummy filter rule\n");
-			return -ENOMEM;
-		}
-
-		IPADBG("Adding global rules for IPv4 and IPv6");
-		for (ip = IPA_IP_v4; ip < IPA_IP_MAX; ip++) {
-			memset(&rt_lookup, 0,
-					sizeof(struct ipa_ioc_get_rt_tbl));
-			rt_lookup.ip = ip;
-			strlcpy(rt_lookup.name, IPA_DFLT_RT_TBL_NAME,
-					IPA_RESOURCE_NAME_MAX);
-			ipa_get_rt_tbl(&rt_lookup);
-			ipa_put_rt_tbl(rt_lookup.hdl);
-
-			memset(rules, 0, sz);
-			rule = &rules->rules[0];
-			rules->commit = 1;
-			rules->ip = ip;
-			rules->global = 1;
-			rules->num_rules = 1;
-			rule->at_rear = 1;
-			if (ip == IPA_IP_v4) {
-				rule->rule.attrib.attrib_mask =
-					IPA_FLT_PROTOCOL | IPA_FLT_DST_ADDR;
-				rule->rule.attrib.u.v4.protocol =
-				   IPA_INVALID_L4_PROTOCOL;
-				rule->rule.attrib.u.v4.dst_addr_mask = ~0;
-				rule->rule.attrib.u.v4.dst_addr = ~0;
-			} else if (ip == IPA_IP_v6) {
-				rule->rule.attrib.attrib_mask =
-					IPA_FLT_NEXT_HDR | IPA_FLT_DST_ADDR;
-				rule->rule.attrib.u.v6.next_hdr =
-					IPA_INVALID_L4_PROTOCOL;
-				rule->rule.attrib.u.v6.dst_addr_mask[0] = ~0;
-				rule->rule.attrib.u.v6.dst_addr_mask[1] = ~0;
-				rule->rule.attrib.u.v6.dst_addr_mask[2] = ~0;
-				rule->rule.attrib.u.v6.dst_addr_mask[3] = ~0;
-				rule->rule.attrib.u.v6.dst_addr[0] = ~0;
-				rule->rule.attrib.u.v6.dst_addr[1] = ~0;
-				rule->rule.attrib.u.v6.dst_addr[2] = ~0;
-				rule->rule.attrib.u.v6.dst_addr[3] = ~0;
-			} else {
-				result = -EINVAL;
-				WARN_ON(1);
-				break;
-			}
-			rule->rule.action = IPA_PASS_TO_ROUTING;
-			rule->rule.rt_tbl_hdl = rt_lookup.hdl;
-			rule->rule.retain_hdr = true;
-
-			if (ipa_add_flt_rule(rules) || rules->rules[0].status) {
-				result = -EINVAL;
-				WARN_ON(1);
-				break;
-			}
-		}
-		kfree(rules);
-	}
-	return result;
-}
-
-static int ipa_sps_process_irq_schedule_rel(void)
-{
-	return queue_delayed_work(ipa_ctx->sps_power_mgmt_wq,
-		&ipa_sps_release_resource_work,
-		msecs_to_jiffies(IPA_SPS_PROD_TIMEOUT_MSEC));
-}
-
-/**
-* ipa_suspend_handler() - Handles the suspend interrupt:
-* wakes up the suspended peripheral by requesting its consumer
-* @interrupt:		Interrupt type
-* @private_data:	The client's private data
-* @interrupt_data:	Interrupt specific information data
-*/
-void ipa_suspend_handler(enum ipa_irq_type interrupt,
-				void *private_data,
-				void *interrupt_data)
-{
-	enum ipa_rm_resource_name resource;
-	u32 suspend_data =
-		((struct ipa_tx_suspend_irq_data *)interrupt_data)->endpoints;
-	u32 bmsk = 1;
-	u32 i = 0;
-	int res;
-	struct ipa_ep_cfg_holb holb_cfg;
-
-	IPADBG("interrupt=%d, interrupt_data=%u\n", interrupt, suspend_data);
-	memset(&holb_cfg, 0, sizeof(holb_cfg));
-	holb_cfg.tmr_val = 0;
-
-	for (i = 0; i < ipa_ctx->ipa_num_pipes; i++) {
-		if ((suspend_data & bmsk) && (ipa_ctx->ep[i].valid)) {
-			if (IPA_CLIENT_IS_APPS_CONS(ipa_ctx->ep[i].client)) {
-				/*
-				 * pipe will be unsuspended as part of
-				 * enabling IPA clocks
-				 */
-				if (!atomic_read(
-					&ipa_ctx->sps_pm.dec_clients)
-					) {
-					ipa_inc_client_enable_clks();
-					IPADBG("Pipes un-suspended.\n");
-					IPADBG("Enter poll mode.\n");
-					atomic_set(
-						&ipa_ctx->sps_pm.dec_clients,
-						1);
-					ipa_sps_process_irq_schedule_rel();
-				}
-			} else {
-				resource = ipa_get_rm_resource_from_ep(i);
-				res = ipa_rm_request_resource_with_timer(
-					resource);
-				if (res == -EPERM &&
-				    IPA_CLIENT_IS_CONS(
-					ipa_ctx->ep[i].client)) {
-					holb_cfg.en = 1;
-					res = ipa_cfg_ep_holb_by_client(
-					   ipa_ctx->ep[i].client, &holb_cfg);
-					if (res) {
-						IPAERR("holb en fail\n");
-						IPAERR("IPAHW stall\n");
-						BUG();
-					}
-				}
-			}
-		}
-		bmsk = bmsk << 1;
-	}
-}
-
-/**
-* ipa_restore_suspend_handler() - restores the original suspend IRQ handler
-* as it was registered in the IPA init sequence.
-* Return codes:
-* 0: success
-* -EPERM: failed to remove current handler or failed to add original handler
-* */
-int ipa_restore_suspend_handler(void)
-{
-	int result = 0;
-
-	result  = ipa_remove_interrupt_handler(IPA_TX_SUSPEND_IRQ);
-	if (result) {
-		IPAERR("remove handler for suspend interrupt failed\n");
-		return -EPERM;
-	}
-
-	result = ipa_add_interrupt_handler(IPA_TX_SUSPEND_IRQ,
-			ipa_suspend_handler, true, NULL);
-	if (result) {
-		IPAERR("register handler for suspend interrupt failed\n");
-		result = -EPERM;
-	}
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_restore_suspend_handler);
-
-static int apps_cons_release_resource(void)
-{
-	return 0;
-}
-
-static int apps_cons_request_resource(void)
-{
-	return 0;
-}
-
-static void ipa_sps_release_resource(struct work_struct *work)
-{
-	/* check whether still need to decrease client usage */
-	if (atomic_read(&ipa_ctx->sps_pm.dec_clients)) {
-		if (atomic_read(&ipa_ctx->sps_pm.eot_activity)) {
-			IPADBG("EOT pending Re-scheduling\n");
-			ipa_sps_process_irq_schedule_rel();
-		} else {
-			atomic_set(&ipa_ctx->sps_pm.dec_clients, 0);
-			ipa_dec_client_disable_clks();
-		}
-	}
-	atomic_set(&ipa_ctx->sps_pm.eot_activity, 0);
-}
-
-int ipa_create_apps_resource(void)
-{
-	struct ipa_rm_create_params apps_cons_create_params;
-	struct ipa_rm_perf_profile profile;
-	int result = 0;
-
-	memset(&apps_cons_create_params, 0,
-				sizeof(apps_cons_create_params));
-	apps_cons_create_params.name = IPA_RM_RESOURCE_APPS_CONS;
-	apps_cons_create_params.request_resource = apps_cons_request_resource;
-	apps_cons_create_params.release_resource = apps_cons_release_resource;
-	result = ipa_rm_create_resource(&apps_cons_create_params);
-	if (result) {
-		IPAERR("ipa_rm_create_resource failed\n");
-		return result;
-	}
-
-	profile.max_supported_bandwidth_mbps = IPA_APPS_MAX_BW_IN_MBPS;
-	ipa_rm_set_perf_profile(IPA_RM_RESOURCE_APPS_CONS, &profile);
-
-	return result;
-}
-
-
-/**
-* ipa_init() - Initialize the IPA Driver
-* @resource_p:	contain platform specific values from DST file
-* @pdev:	The platform device structure representing the IPA driver
-*
-* Function initialization process:
-* - Allocate memory for the driver context data struct
-* - Initializing the ipa_ctx with:
-*    1)parsed values from the dts file
-*    2)parameters passed to the module initialization
-*    3)read HW values(such as core memory size)
-* - Map IPA core registers to CPU memory
-* - Restart IPA core(HW reset)
-* - Register IPA BAM to SPS driver and get a BAM handler
-* - Set configuration for IPA BAM via BAM_CNFG_BITS
-* - Initialize the look-aside caches(kmem_cache/slab) for filter,
-*   routing and IPA-tree
-* - Create memory pool with 4 objects for DMA operations(each object
-*   is 512Bytes long), this object will be use for tx(A5->IPA)
-* - Initialize lists head(routing,filter,hdr,system pipes)
-* - Initialize mutexes (for ipa_ctx and NAT memory mutexes)
-* - Initialize spinlocks (for list related to A5<->IPA pipes)
-* - Initialize 2 single-threaded work-queue named "ipa rx wq" and "ipa tx wq"
-* - Initialize Red-Black-Tree(s) for handles of header,routing rule,
-*   routing table ,filtering rule
-* - Setup all A5<->IPA pipes by calling to ipa_setup_a5_pipes
-* - Preparing the descriptors for System pipes
-* - Initialize the filter block by committing IPV4 and IPV6 default rules
-* - Create empty routing table in system memory(no committing)
-* - Initialize pipes memory pool with ipa_pipe_mem_init for supported platforms
-* - Create a char-device for IPA
-* - Initialize IPA RM (resource manager)
-*/
-static int ipa_init(const struct ipa_plat_drv_res *resource_p,
-		struct device *ipa_dev)
-{
-	int result = 0;
-	int i;
-	struct sps_bam_props bam_props = { 0 };
-	struct ipa_flt_tbl *flt_tbl;
-	struct ipa_rt_tbl_set *rset;
-
-	IPADBG("IPA Driver initialization started\n");
-
-	/*
-	 * since structure alignment is implementation dependent, add test to
-	 * avoid different and incompatible data layouts
-	 */
-	BUILD_BUG_ON(sizeof(struct ipa_hw_pkt_status) != IPA_PKT_STATUS_SIZE);
-
-	ipa_ctx = kzalloc(sizeof(*ipa_ctx), GFP_KERNEL);
-	if (!ipa_ctx) {
-		IPAERR(":kzalloc err.\n");
-		result = -ENOMEM;
-		goto fail_mem_ctx;
-	}
-
-	ipa_ctx->pdev = ipa_dev;
-	ipa_ctx->uc_pdev = ipa_dev;
-	ipa_ctx->smmu_present = smmu_present;
-	ipa_ctx->ipa_wrapper_base = resource_p->ipa_mem_base;
-	ipa_ctx->ipa_hw_type = resource_p->ipa_hw_type;
-	ipa_ctx->ipa_hw_mode = resource_p->ipa_hw_mode;
-	ipa_ctx->use_ipa_teth_bridge = resource_p->use_ipa_teth_bridge;
-	ipa_ctx->ipa_bam_remote_mode = resource_p->ipa_bam_remote_mode;
-	ipa_ctx->modem_cfg_emb_pipe_flt = resource_p->modem_cfg_emb_pipe_flt;
-	ipa_ctx->wan_rx_ring_size = resource_p->wan_rx_ring_size;
-	ipa_ctx->skip_uc_pipe_reset = resource_p->skip_uc_pipe_reset;
-	ipa_ctx->tethered_flow_control = resource_p->tethered_flow_control;
-
-	/* default aggregation parameters */
-	ipa_ctx->aggregation_type = IPA_MBIM_16;
-	ipa_ctx->aggregation_byte_limit = 1;
-	ipa_ctx->aggregation_time_limit = 0;
-
-	ipa_ctx->ctrl = kzalloc(sizeof(*ipa_ctx->ctrl), GFP_KERNEL);
-	if (!ipa_ctx->ctrl) {
-		IPAERR("memory allocation error for ctrl\n");
-		result = -ENOMEM;
-		goto fail_mem_ctrl;
-	}
-	result = ipa_controller_static_bind(ipa_ctx->ctrl,
-			ipa_ctx->ipa_hw_type);
-	if (result) {
-		IPAERR("fail to static bind IPA ctrl.\n");
-		result = -EFAULT;
-		goto fail_bind;
-	}
-
-	IPADBG("hdr_lcl=%u ip4_rt=%u ip6_rt=%u ip4_flt=%u ip6_flt=%u\n",
-	       ipa_ctx->hdr_tbl_lcl, ipa_ctx->ip4_rt_tbl_lcl,
-	       ipa_ctx->ip6_rt_tbl_lcl, ipa_ctx->ip4_flt_tbl_lcl,
-	       ipa_ctx->ip6_flt_tbl_lcl);
-
-	if (bus_scale_table) {
-		IPADBG("Use bus scaling info from device tree\n");
-		ipa_ctx->ctrl->msm_bus_data_ptr = bus_scale_table;
-	}
-
-	if (ipa_ctx->ipa_hw_mode != IPA_HW_MODE_VIRTUAL) {
-		/* get BUS handle */
-		ipa_ctx->ipa_bus_hdl =
-			msm_bus_scale_register_client(
-				ipa_ctx->ctrl->msm_bus_data_ptr);
-		if (!ipa_ctx->ipa_bus_hdl) {
-			IPAERR("fail to register with bus mgr!\n");
-			result = -ENODEV;
-			goto fail_bus_reg;
-		}
-	} else {
-		IPADBG("Skipping bus scaling registration on Virtual plat\n");
-	}
-
-	/* get IPA clocks */
-	result = ipa_get_clks(master_dev);
-	if (result)
-		goto fail_clk;
-
-	/* Enable ipa_ctx->enable_clock_scaling */
-	ipa_ctx->enable_clock_scaling = 1;
-	ipa_ctx->curr_ipa_clk_rate = ipa_ctx->ctrl->ipa_clk_rate_turbo;
-
-	/* enable IPA clocks explicitly to allow the initialization */
-	ipa_enable_clks();
-
-	/* setup IPA register access */
-	ipa_ctx->mmio = ioremap(resource_p->ipa_mem_base +
-			ipa_ctx->ctrl->ipa_reg_base_ofst,
-			resource_p->ipa_mem_size);
-	if (!ipa_ctx->mmio) {
-		IPAERR(":ipa-base ioremap err.\n");
-		result = -EFAULT;
-		goto fail_remap;
-	}
-
-	result = ipa_init_hw();
-	if (result) {
-		IPAERR(":error initializing HW.\n");
-		result = -ENODEV;
-		goto fail_init_hw;
-	}
-	IPADBG("IPA HW initialization sequence completed");
-
-	ipa_ctx->ipa_num_pipes = ipa_get_num_pipes();
-	ipa_ctx->ctrl->ipa_sram_read_settings();
-	IPADBG("SRAM, size: 0x%x, restricted bytes: 0x%x\n",
-		ipa_ctx->smem_sz, ipa_ctx->smem_restricted_bytes);
-
-	if (ipa_ctx->smem_reqd_sz >
-		ipa_ctx->smem_sz - ipa_ctx->smem_restricted_bytes) {
-		IPAERR("SW expect more core memory, needed %d, avail %d\n",
-			ipa_ctx->smem_reqd_sz, ipa_ctx->smem_sz -
-			ipa_ctx->smem_restricted_bytes);
-		result = -ENOMEM;
-		goto fail_init_hw;
-	}
-
-	mutex_init(&ipa_ctx->ipa_active_clients.mutex);
-	spin_lock_init(&ipa_ctx->ipa_active_clients.spinlock);
-	ipa_ctx->ipa_active_clients.cnt = 1;
-
-	/* Create workqueues for power management */
-	ipa_ctx->power_mgmt_wq =
-		create_singlethread_workqueue("ipa_power_mgmt");
-	if (!ipa_ctx->power_mgmt_wq) {
-		IPAERR("failed to create power mgmt wq\n");
-		result = -ENOMEM;
-		goto fail_init_hw;
-	}
-
-	ipa_ctx->sps_power_mgmt_wq =
-		create_singlethread_workqueue("sps_ipa_power_mgmt");
-	if (!ipa_ctx->sps_power_mgmt_wq) {
-		IPAERR("failed to create sps power mgmt wq\n");
-		result = -ENOMEM;
-		goto fail_create_sps_wq;
-	}
-
-	/* register IPA with SPS driver */
-	bam_props.phys_addr = resource_p->bam_mem_base;
-	bam_props.virt_size = resource_p->bam_mem_size;
-	bam_props.irq = resource_p->bam_irq;
-	bam_props.num_pipes = ipa_ctx->ipa_num_pipes;
-	bam_props.summing_threshold = IPA_SUMMING_THRESHOLD;
-	bam_props.event_threshold = IPA_EVENT_THRESHOLD;
-	bam_props.options |= SPS_BAM_NO_LOCAL_CLK_GATING;
-	if (ipa_ctx->ipa_hw_mode != IPA_HW_MODE_VIRTUAL)
-		bam_props.options |= SPS_BAM_OPT_IRQ_WAKEUP;
-	if (ipa_ctx->ipa_bam_remote_mode == true)
-		bam_props.manage |= SPS_BAM_MGR_DEVICE_REMOTE;
-	if (ipa_ctx->smmu_present)
-		bam_props.options |= SPS_BAM_SMMU_EN;
-	bam_props.ee = resource_p->ee;
-	bam_props.ipc_loglevel = SPS_IPC_LOGLEVEL;
-
-	result = sps_register_bam_device(&bam_props, &ipa_ctx->bam_handle);
-	if (result) {
-		IPAERR(":bam register err.\n");
-		result = -EPROBE_DEFER;
-		goto fail_register_bam_device;
-	}
-	IPADBG("IPA BAM is registered\n");
-
-	if (ipa_setup_bam_cfg(resource_p)) {
-		IPAERR(":bam cfg err.\n");
-		result = -ENODEV;
-		goto fail_flt_rule_cache;
-	}
-
-	/* init the lookaside cache */
-	ipa_ctx->flt_rule_cache = kmem_cache_create("IPA FLT",
-			sizeof(struct ipa_flt_entry), 0, 0, NULL);
-	if (!ipa_ctx->flt_rule_cache) {
-		IPAERR(":ipa flt cache create failed\n");
-		result = -ENOMEM;
-		goto fail_flt_rule_cache;
-	}
-	ipa_ctx->rt_rule_cache = kmem_cache_create("IPA RT",
-			sizeof(struct ipa_rt_entry), 0, 0, NULL);
-	if (!ipa_ctx->rt_rule_cache) {
-		IPAERR(":ipa rt cache create failed\n");
-		result = -ENOMEM;
-		goto fail_rt_rule_cache;
-	}
-	ipa_ctx->hdr_cache = kmem_cache_create("IPA HDR",
-			sizeof(struct ipa_hdr_entry), 0, 0, NULL);
-	if (!ipa_ctx->hdr_cache) {
-		IPAERR(":ipa hdr cache create failed\n");
-		result = -ENOMEM;
-		goto fail_hdr_cache;
-	}
-	ipa_ctx->hdr_offset_cache =
-	   kmem_cache_create("IPA HDR OFFSET",
-			   sizeof(struct ipa_hdr_offset_entry), 0, 0, NULL);
-	if (!ipa_ctx->hdr_offset_cache) {
-		IPAERR(":ipa hdr off cache create failed\n");
-		result = -ENOMEM;
-		goto fail_hdr_offset_cache;
-	}
-	ipa_ctx->hdr_proc_ctx_cache = kmem_cache_create("IPA HDR PROC CTX",
-		sizeof(struct ipa_hdr_proc_ctx_entry), 0, 0, NULL);
-	if (!ipa_ctx->hdr_proc_ctx_cache) {
-		IPAERR(":ipa hdr proc ctx cache create failed\n");
-		result = -ENOMEM;
-		goto fail_hdr_proc_ctx_cache;
-	}
-	ipa_ctx->hdr_proc_ctx_offset_cache =
-		kmem_cache_create("IPA HDR PROC CTX OFFSET",
-		sizeof(struct ipa_hdr_proc_ctx_offset_entry), 0, 0, NULL);
-	if (!ipa_ctx->hdr_proc_ctx_offset_cache) {
-		IPAERR(":ipa hdr proc ctx off cache create failed\n");
-		result = -ENOMEM;
-		goto fail_hdr_proc_ctx_offset_cache;
-	}
-	ipa_ctx->rt_tbl_cache = kmem_cache_create("IPA RT TBL",
-			sizeof(struct ipa_rt_tbl), 0, 0, NULL);
-	if (!ipa_ctx->rt_tbl_cache) {
-		IPAERR(":ipa rt tbl cache create failed\n");
-		result = -ENOMEM;
-		goto fail_rt_tbl_cache;
-	}
-	ipa_ctx->tx_pkt_wrapper_cache =
-	   kmem_cache_create("IPA TX PKT WRAPPER",
-			   sizeof(struct ipa_tx_pkt_wrapper), 0, 0, NULL);
-	if (!ipa_ctx->tx_pkt_wrapper_cache) {
-		IPAERR(":ipa tx pkt wrapper cache create failed\n");
-		result = -ENOMEM;
-		goto fail_tx_pkt_wrapper_cache;
-	}
-	ipa_ctx->rx_pkt_wrapper_cache =
-	   kmem_cache_create("IPA RX PKT WRAPPER",
-			   sizeof(struct ipa_rx_pkt_wrapper), 0, 0, NULL);
-	if (!ipa_ctx->rx_pkt_wrapper_cache) {
-		IPAERR(":ipa rx pkt wrapper cache create failed\n");
-		result = -ENOMEM;
-		goto fail_rx_pkt_wrapper_cache;
-	}
-
-	/* Setup DMA pool */
-	ipa_ctx->dma_pool = dma_pool_create("ipa_tx", ipa_ctx->pdev,
-		IPA_NUM_DESC_PER_SW_TX * sizeof(struct sps_iovec),
-		0, 0);
-	if (!ipa_ctx->dma_pool) {
-		IPAERR("cannot alloc DMA pool.\n");
-		result = -ENOMEM;
-		goto fail_dma_pool;
-	}
-
-	ipa_ctx->glob_flt_tbl[IPA_IP_v4].in_sys = !ipa_ctx->ip4_flt_tbl_lcl;
-	ipa_ctx->glob_flt_tbl[IPA_IP_v6].in_sys = !ipa_ctx->ip6_flt_tbl_lcl;
-
-	/* init the various list heads */
-	INIT_LIST_HEAD(&ipa_ctx->glob_flt_tbl[IPA_IP_v4].head_flt_rule_list);
-	INIT_LIST_HEAD(&ipa_ctx->glob_flt_tbl[IPA_IP_v6].head_flt_rule_list);
-	INIT_LIST_HEAD(&ipa_ctx->hdr_tbl.head_hdr_entry_list);
-	for (i = 0; i < IPA_HDR_BIN_MAX; i++) {
-		INIT_LIST_HEAD(&ipa_ctx->hdr_tbl.head_offset_list[i]);
-		INIT_LIST_HEAD(&ipa_ctx->hdr_tbl.head_free_offset_list[i]);
-	}
-	INIT_LIST_HEAD(&ipa_ctx->hdr_proc_ctx_tbl.head_proc_ctx_entry_list);
-	for (i = 0; i < IPA_HDR_PROC_CTX_BIN_MAX; i++) {
-		INIT_LIST_HEAD(&ipa_ctx->hdr_proc_ctx_tbl.head_offset_list[i]);
-		INIT_LIST_HEAD(&ipa_ctx->
-				hdr_proc_ctx_tbl.head_free_offset_list[i]);
-	}
-	INIT_LIST_HEAD(&ipa_ctx->rt_tbl_set[IPA_IP_v4].head_rt_tbl_list);
-	INIT_LIST_HEAD(&ipa_ctx->rt_tbl_set[IPA_IP_v6].head_rt_tbl_list);
-	for (i = 0; i < ipa_ctx->ipa_num_pipes; i++) {
-		flt_tbl = &ipa_ctx->flt_tbl[i][IPA_IP_v4];
-		INIT_LIST_HEAD(&flt_tbl->head_flt_rule_list);
-		flt_tbl->in_sys = !ipa_ctx->ip4_flt_tbl_lcl;
-
-		flt_tbl = &ipa_ctx->flt_tbl[i][IPA_IP_v6];
-		INIT_LIST_HEAD(&flt_tbl->head_flt_rule_list);
-		flt_tbl->in_sys = !ipa_ctx->ip6_flt_tbl_lcl;
-	}
-
-	rset = &ipa_ctx->reap_rt_tbl_set[IPA_IP_v4];
-	INIT_LIST_HEAD(&rset->head_rt_tbl_list);
-	rset = &ipa_ctx->reap_rt_tbl_set[IPA_IP_v6];
-	INIT_LIST_HEAD(&rset->head_rt_tbl_list);
-
-	INIT_LIST_HEAD(&ipa_ctx->intf_list);
-	INIT_LIST_HEAD(&ipa_ctx->msg_list);
-	INIT_LIST_HEAD(&ipa_ctx->pull_msg_list);
-	init_waitqueue_head(&ipa_ctx->msg_waitq);
-	mutex_init(&ipa_ctx->msg_lock);
-
-	mutex_init(&ipa_ctx->lock);
-	mutex_init(&ipa_ctx->nat_mem.lock);
-
-	idr_init(&ipa_ctx->ipa_idr);
-	spin_lock_init(&ipa_ctx->idr_lock);
-
-	/* wlan related member */
-	memset(&ipa_ctx->wc_memb, 0, sizeof(ipa_ctx->wc_memb));
-	spin_lock_init(&ipa_ctx->wc_memb.wlan_spinlock);
-	spin_lock_init(&ipa_ctx->wc_memb.ipa_tx_mul_spinlock);
-	INIT_LIST_HEAD(&ipa_ctx->wc_memb.wlan_comm_desc_list);
-	/*
-	 * setup an empty routing table in system memory, this will be used
-	 * to delete a routing table cleanly and safely
-	 */
-	ipa_ctx->empty_rt_tbl_mem.size = IPA_ROUTING_RULE_BYTE_SIZE;
-
-	ipa_ctx->empty_rt_tbl_mem.base =
-		dma_alloc_coherent(ipa_ctx->pdev,
-				ipa_ctx->empty_rt_tbl_mem.size,
-				    &ipa_ctx->empty_rt_tbl_mem.phys_base,
-				    GFP_KERNEL);
-	if (!ipa_ctx->empty_rt_tbl_mem.base) {
-		IPAERR("DMA buff alloc fail %d bytes for empty routing tbl\n",
-				ipa_ctx->empty_rt_tbl_mem.size);
-		result = -ENOMEM;
-		goto fail_apps_pipes;
-	}
-	memset(ipa_ctx->empty_rt_tbl_mem.base, 0,
-			ipa_ctx->empty_rt_tbl_mem.size);
-	IPADBG("empty routing table was allocated in system memory");
-
-	/* setup the A5-IPA pipes */
-	if (ipa_setup_apps_pipes()) {
-		IPAERR(":failed to setup IPA-Apps pipes.\n");
-		result = -ENODEV;
-		goto fail_empty_rt_tbl;
-	}
-	IPADBG("IPA System2Bam pipes were connected\n");
-
-	if (ipa_init_flt_block()) {
-		IPAERR("fail to setup dummy filter rules\n");
-		result = -ENODEV;
-		goto fail_empty_rt_tbl;
-	}
-	IPADBG("filter block was set with dummy filter rules");
-
-	/* setup the IPA pipe mem pool */
-	if (resource_p->ipa_pipe_mem_size)
-		ipa_pipe_mem_init(resource_p->ipa_pipe_mem_start_ofst,
-				resource_p->ipa_pipe_mem_size);
-
-	ipa_ctx->class = class_create(THIS_MODULE, DRV_NAME);
-
-	result = alloc_chrdev_region(&ipa_ctx->dev_num, 0, 1, DRV_NAME);
-	if (result) {
-		IPAERR("alloc_chrdev_region err.\n");
-		result = -ENODEV;
-		goto fail_alloc_chrdev_region;
-	}
-
-	ipa_ctx->dev = device_create(ipa_ctx->class, NULL, ipa_ctx->dev_num,
-			ipa_ctx, DRV_NAME);
-	if (IS_ERR(ipa_ctx->dev)) {
-		IPAERR(":device_create err.\n");
-		result = -ENODEV;
-		goto fail_device_create;
-	}
-
-	cdev_init(&ipa_ctx->cdev, &ipa_drv_fops);
-	ipa_ctx->cdev.owner = THIS_MODULE;
-	ipa_ctx->cdev.ops = &ipa_drv_fops;  /* from LDD3 */
-
-	result = cdev_add(&ipa_ctx->cdev, ipa_ctx->dev_num, 1);
-	if (result) {
-		IPAERR(":cdev_add err=%d\n", -result);
-		result = -ENODEV;
-		goto fail_cdev_add;
-	}
-	IPADBG("ipa cdev added successful. major:%d minor:%d\n",
-			MAJOR(ipa_ctx->dev_num),
-			MINOR(ipa_ctx->dev_num));
-
-	if (create_nat_device()) {
-		IPAERR("unable to create nat device\n");
-		result = -ENODEV;
-		goto fail_nat_dev_add;
-	}
-
-	/* Create workqueue for power management */
-	ipa_ctx->power_mgmt_wq =
-		create_singlethread_workqueue("ipa_power_mgmt");
-	if (!ipa_ctx->power_mgmt_wq) {
-		IPAERR("failed to create wq\n");
-		result = -ENOMEM;
-		goto fail_init_hw;
-	}
-
-	/* Create a wakeup source. */
-	wakeup_source_init(&ipa_ctx->w_lock, "IPA_WS");
-	spin_lock_init(&ipa_ctx->wakelock_ref_cnt.spinlock);
-
-	/* Initialize IPA RM (resource manager) */
-	result = ipa_rm_initialize();
-	if (result) {
-		IPAERR("RM initialization failed (%d)\n", -result);
-		result = -ENODEV;
-		goto fail_ipa_rm_init;
-	}
-	IPADBG("IPA resource manager initialized");
-
-	result = ipa_create_apps_resource();
-	if (result) {
-		IPAERR("Failed to create APPS_CONS resource\n");
-		result = -ENODEV;
-		goto fail_create_apps_resource;
-	}
-
-	/*register IPA IRQ handler*/
-	result = ipa_interrupts_init(resource_p->ipa_irq, 0,
-			master_dev);
-	if (result) {
-		IPAERR("ipa interrupts initialization failed\n");
-		result = -ENODEV;
-		goto fail_ipa_interrupts_init;
-	}
-
-	/*add handler for suspend interrupt*/
-	result = ipa_add_interrupt_handler(IPA_TX_SUSPEND_IRQ,
-			ipa_suspend_handler, false, NULL);
-	if (result) {
-		IPAERR("register handler for suspend interrupt failed\n");
-		result = -ENODEV;
-		goto fail_add_interrupt_handler;
-	}
-
-	if (ipa_ctx->use_ipa_teth_bridge) {
-		/* Initialize the tethering bridge driver */
-		result = teth_bridge_driver_init();
-		if (result) {
-			IPAERR(":teth_bridge init failed (%d)\n", -result);
-			result = -ENODEV;
-			goto fail_add_interrupt_handler;
-		}
-		IPADBG("teth_bridge initialized");
-	}
-
-	ipa_debugfs_init();
-
-	result = ipa_uc_interface_init();
-	if (result)
-		IPAERR(":ipa Uc interface init failed (%d)\n", -result);
-	else
-		IPADBG(":ipa Uc interface init ok\n");
-
-	result = ipa_wdi_init();
-	if (result)
-		IPAERR(":wdi init failed (%d)\n", -result);
-	else
-		IPADBG(":wdi init ok\n");
-
-	ipa_ctx->q6_proxy_clk_vote_valid = true;
-
-	ipa_register_panic_uc_hdlr();
-	ipa_register_panic_gen_notifier();
-
-	pr_info("IPA driver initialization was successful.\n");
-
-	return 0;
-
-fail_add_interrupt_handler:
-	free_irq(resource_p->ipa_irq, master_dev);
-fail_ipa_interrupts_init:
-	ipa_rm_delete_resource(IPA_RM_RESOURCE_APPS_CONS);
-fail_create_apps_resource:
-	ipa_rm_exit();
-fail_ipa_rm_init:
-fail_nat_dev_add:
-	cdev_del(&ipa_ctx->cdev);
-fail_cdev_add:
-	device_destroy(ipa_ctx->class, ipa_ctx->dev_num);
-fail_device_create:
-	unregister_chrdev_region(ipa_ctx->dev_num, 1);
-fail_alloc_chrdev_region:
-	if (ipa_ctx->pipe_mem_pool)
-		gen_pool_destroy(ipa_ctx->pipe_mem_pool);
-fail_empty_rt_tbl:
-	ipa_teardown_apps_pipes();
-	dma_free_coherent(ipa_ctx->pdev,
-			  ipa_ctx->empty_rt_tbl_mem.size,
-			  ipa_ctx->empty_rt_tbl_mem.base,
-			  ipa_ctx->empty_rt_tbl_mem.phys_base);
-fail_apps_pipes:
-	idr_destroy(&ipa_ctx->ipa_idr);
-fail_dma_pool:
-	kmem_cache_destroy(ipa_ctx->rx_pkt_wrapper_cache);
-fail_rx_pkt_wrapper_cache:
-	kmem_cache_destroy(ipa_ctx->tx_pkt_wrapper_cache);
-fail_tx_pkt_wrapper_cache:
-	kmem_cache_destroy(ipa_ctx->rt_tbl_cache);
-fail_rt_tbl_cache:
-	kmem_cache_destroy(ipa_ctx->hdr_proc_ctx_offset_cache);
-fail_hdr_proc_ctx_offset_cache:
-	kmem_cache_destroy(ipa_ctx->hdr_proc_ctx_cache);
-fail_hdr_proc_ctx_cache:
-	kmem_cache_destroy(ipa_ctx->hdr_offset_cache);
-fail_hdr_offset_cache:
-	kmem_cache_destroy(ipa_ctx->hdr_cache);
-fail_hdr_cache:
-	kmem_cache_destroy(ipa_ctx->rt_rule_cache);
-fail_rt_rule_cache:
-	kmem_cache_destroy(ipa_ctx->flt_rule_cache);
-fail_flt_rule_cache:
-	sps_deregister_bam_device(ipa_ctx->bam_handle);
-fail_register_bam_device:
-	destroy_workqueue(ipa_ctx->sps_power_mgmt_wq);
-fail_create_sps_wq:
-	destroy_workqueue(ipa_ctx->power_mgmt_wq);
-fail_init_hw:
-	iounmap(ipa_ctx->mmio);
-fail_remap:
-	ipa_disable_clks();
-fail_clk:
-	msm_bus_scale_unregister_client(ipa_ctx->ipa_bus_hdl);
-fail_bus_reg:
-	if (bus_scale_table) {
-		msm_bus_cl_clear_pdata(bus_scale_table);
-		bus_scale_table = NULL;
-	}
-fail_bind:
-	kfree(ipa_ctx->ctrl);
-fail_mem_ctrl:
-	kfree(ipa_ctx);
-	ipa_ctx = NULL;
-fail_mem_ctx:
-	return result;
-}
-
-static int get_ipa_dts_configuration(struct platform_device *pdev,
-		struct ipa_plat_drv_res *ipa_drv_res)
-{
-	int result;
-	struct resource *resource;
-
-	/* initialize ipa_res */
-	ipa_drv_res->ipa_pipe_mem_start_ofst = IPA_PIPE_MEM_START_OFST;
-	ipa_drv_res->ipa_pipe_mem_size = IPA_PIPE_MEM_SIZE;
-	ipa_drv_res->ipa_hw_type = 0;
-	ipa_drv_res->ipa_hw_mode = 0;
-	ipa_drv_res->ipa_bam_remote_mode = false;
-	ipa_drv_res->modem_cfg_emb_pipe_flt = false;
-	ipa_drv_res->wan_rx_ring_size = IPA_GENERIC_RX_POOL_SZ;
-
-	smmu_disable_htw = of_property_read_bool(pdev->dev.of_node,
-			"qcom,smmu-disable-htw");
-
-	/* Get IPA HW Version */
-	result = of_property_read_u32(pdev->dev.of_node, "qcom,ipa-hw-ver",
-					&ipa_drv_res->ipa_hw_type);
-	if ((result) || (ipa_drv_res->ipa_hw_type == 0)) {
-		IPAERR(":get resource failed for ipa-hw-ver!\n");
-		return -ENODEV;
-	}
-	IPADBG(": ipa_hw_type = %d", ipa_drv_res->ipa_hw_type);
-
-	/* Get IPA HW mode */
-	result = of_property_read_u32(pdev->dev.of_node, "qcom,ipa-hw-mode",
-			&ipa_drv_res->ipa_hw_mode);
-	if (result)
-		IPADBG("using default (IPA_MODE_NORMAL) for ipa-hw-mode\n");
-	else
-		IPADBG(": found ipa_drv_res->ipa_hw_mode = %d",
-				ipa_drv_res->ipa_hw_mode);
-
-	/* Get IPA WAN RX pool sizee */
-	result = of_property_read_u32(pdev->dev.of_node,
-			"qcom,wan-rx-ring-size",
-			&ipa_drv_res->wan_rx_ring_size);
-	if (result)
-		IPADBG("using default for wan-rx-ring-size\n");
-	else
-		IPADBG(": found ipa_drv_res->wan-rx-ring-size = %u",
-				ipa_drv_res->wan_rx_ring_size);
-
-	ipa_drv_res->use_ipa_teth_bridge =
-			of_property_read_bool(pdev->dev.of_node,
-			"qcom,use-ipa-tethering-bridge");
-	IPADBG(": using TBDr = %s",
-		ipa_drv_res->use_ipa_teth_bridge
-		? "True" : "False");
-
-	ipa_drv_res->ipa_bam_remote_mode =
-			of_property_read_bool(pdev->dev.of_node,
-			"qcom,ipa-bam-remote-mode");
-	IPADBG(": ipa bam remote mode = %s\n",
-			ipa_drv_res->ipa_bam_remote_mode
-			? "True" : "False");
-
-	ipa_drv_res->modem_cfg_emb_pipe_flt =
-			of_property_read_bool(pdev->dev.of_node,
-			"qcom,modem-cfg-emb-pipe-flt");
-	IPADBG(": modem configure embedded pipe filtering = %s\n",
-			ipa_drv_res->modem_cfg_emb_pipe_flt
-			? "True" : "False");
-
-	ipa_drv_res->skip_uc_pipe_reset =
-		of_property_read_bool(pdev->dev.of_node,
-		"qcom,skip-uc-pipe-reset");
-	IPADBG(": skip uC pipe reset = %s\n",
-		ipa_drv_res->skip_uc_pipe_reset
-		? "True" : "False");
-
-	ipa_drv_res->tethered_flow_control =
-		of_property_read_bool(pdev->dev.of_node,
-		"qcom,tethered-flow-control");
-	IPADBG(": Use apps based flow control = %s\n",
-		ipa_drv_res->tethered_flow_control
-		? "True" : "False");
-
-	/* Get IPA wrapper address */
-	resource = platform_get_resource_byname(pdev, IORESOURCE_MEM,
-			"ipa-base");
-	if (!resource) {
-		IPAERR(":get resource failed for ipa-base!\n");
-		return -ENODEV;
-	} else {
-		ipa_drv_res->ipa_mem_base = resource->start;
-		ipa_drv_res->ipa_mem_size = resource_size(resource);
-		IPADBG(": ipa-base = 0x%x, size = 0x%x\n",
-				ipa_drv_res->ipa_mem_base,
-				ipa_drv_res->ipa_mem_size);
-	}
-
-	/* Get IPA BAM address */
-	resource = platform_get_resource_byname(pdev, IORESOURCE_MEM,
-			"bam-base");
-	if (!resource) {
-		IPAERR(":get resource failed for bam-base!\n");
-		return -ENODEV;
-	} else {
-		ipa_drv_res->bam_mem_base = resource->start;
-		ipa_drv_res->bam_mem_size = resource_size(resource);
-		IPADBG(": bam-base = 0x%x, size = 0x%x\n",
-				ipa_drv_res->bam_mem_base,
-				ipa_drv_res->bam_mem_size);
-	}
-
-	/* Get IPA pipe mem start ofst */
-	resource = platform_get_resource_byname(pdev, IORESOURCE_MEM,
-			"ipa-pipe-mem");
-	if (!resource) {
-		IPADBG(":not using pipe memory - resource nonexisting\n");
-	} else {
-		ipa_drv_res->ipa_pipe_mem_start_ofst = resource->start;
-		ipa_drv_res->ipa_pipe_mem_size = resource_size(resource);
-		IPADBG(":using pipe memory - at 0x%x of size 0x%x\n",
-				ipa_drv_res->ipa_pipe_mem_start_ofst,
-				ipa_drv_res->ipa_pipe_mem_size);
-	}
-
-	/* Get IPA IRQ number */
-	resource = platform_get_resource_byname(pdev, IORESOURCE_IRQ,
-			"ipa-irq");
-	if (!resource) {
-		IPAERR(":get resource failed for ipa-irq!\n");
-		return -ENODEV;
-	} else {
-		ipa_drv_res->ipa_irq = resource->start;
-		IPADBG(":ipa-irq = %d\n", ipa_drv_res->ipa_irq);
-	}
-
-	/* Get IPA BAM IRQ number */
-	resource = platform_get_resource_byname(pdev, IORESOURCE_IRQ,
-			"bam-irq");
-	if (!resource) {
-		IPAERR(":get resource failed for bam-irq!\n");
-		return -ENODEV;
-	} else {
-		ipa_drv_res->bam_irq = resource->start;
-		IPADBG(":ibam-irq = %d\n", ipa_drv_res->bam_irq);
-	}
-
-	result = of_property_read_u32(pdev->dev.of_node, "qcom,ee",
-			&ipa_drv_res->ee);
-	if (result)
-		ipa_drv_res->ee = 0;
-
-	return 0;
-}
-
-int ipa_smmu_map_peer_bam(unsigned long dev)
-{
-	phys_addr_t base;
-	u32 size;
-	struct iommu_domain *smmu_domain;
-
-	if (ipa_ctx->smmu_present) {
-		if (ipa_ctx->peer_bam_map_cnt == 0) {
-			if (sps_get_bam_addr(dev, &base, &size)) {
-				IPAERR("Fail to get addr\n");
-				return -EINVAL;
-			}
-			smmu_domain = ipa_get_smmu_domain();
-			if (smmu_domain != NULL) {
-				if (iommu_map(smmu_domain,
-					IPA_SMMU_AP_VA_END,
-					rounddown(base, PAGE_SIZE),
-					roundup(size + base -
-					rounddown(base, PAGE_SIZE), PAGE_SIZE),
-					IOMMU_READ | IOMMU_WRITE |
-					IOMMU_DEVICE)) {
-					IPAERR("Fail to iommu_map\n");
-					return -EINVAL;
-				}
-			}
-
-			ipa_ctx->peer_bam_iova = IPA_SMMU_AP_VA_END;
-			ipa_ctx->peer_bam_pa = base;
-			ipa_ctx->peer_bam_map_size = size;
-			ipa_ctx->peer_bam_dev = dev;
-
-			IPADBG("Peer bam %lu mapped\n", dev);
-		} else {
-			WARN_ON(dev != ipa_ctx->peer_bam_dev);
-		}
-
-		ipa_ctx->peer_bam_map_cnt++;
-	}
-
-	return 0;
-}
-
-int ipa_smmu_unmap_peer_bam(unsigned long dev)
-{
-	size_t len;
-	struct iommu_domain *smmu_domain;
-
-	if (ipa_ctx->smmu_present) {
-		WARN_ON(dev != ipa_ctx->peer_bam_dev);
-		ipa_ctx->peer_bam_map_cnt--;
-		if (ipa_ctx->peer_bam_map_cnt == 0) {
-			len = roundup(ipa_ctx->peer_bam_map_size +
-					ipa_ctx->peer_bam_pa -
-					rounddown(ipa_ctx->peer_bam_pa,
-						PAGE_SIZE), PAGE_SIZE);
-			smmu_domain = ipa_get_smmu_domain();
-			if (smmu_domain != NULL) {
-				if (iommu_unmap(smmu_domain,
-					IPA_SMMU_AP_VA_END, len) != len) {
-					IPAERR("Fail to iommu_unmap\n");
-					return -EINVAL;
-				}
-				IPADBG("Peer bam %lu unmapped\n", dev);
-			}
-		}
-	}
-
-	return 0;
-}
-
-static int ipa_smmu_wlan_cb_probe(struct device *dev)
-{
-	struct ipa_smmu_cb_ctx *cb = &smmu_cb[IPA_SMMU_CB_WLAN];
-	int disable_htw = 1;
-	int atomic_ctx = 1;
-	int ret;
-
-	IPADBG("sub pdev=%p\n", dev);
-
-	cb->dev = dev;
-	cb->iommu = iommu_domain_alloc(&platform_bus_type);
-	if (!cb->iommu) {
-		IPAERR("could not alloc iommu domain\n");
-		/* assume this failure is because iommu driver is not ready */
-		return -EPROBE_DEFER;
-	}
-
-	if (smmu_disable_htw) {
-		ret = iommu_domain_set_attr(cb->iommu,
-			DOMAIN_ATTR_COHERENT_HTW_DISABLE,
-			&disable_htw);
-		if (ret) {
-			IPAERR("couldn't disable coherent HTW\n");
-			return -EIO;
-		}
-	}
-
-	if (iommu_domain_set_attr(cb->iommu,
-				DOMAIN_ATTR_ATOMIC,
-				&atomic_ctx)) {
-		IPAERR("couldn't disable coherent HTW\n");
-		return -EIO;
-	}
-
-	ret = iommu_attach_device(cb->iommu, dev);
-	if (ret) {
-		IPAERR("could not attach device ret=%d\n", ret);
-		return ret;
-	}
-
-	IPADBG("map IPA region to WLAN_CB IOMMU\n");
-	ret = iommu_map(cb->iommu, 0x680000, 0x680000,
-			0x64000,
-			IOMMU_READ | IOMMU_WRITE | IOMMU_DEVICE);
-	if (ret) {
-		IPAERR("map IPA to WLAN_CB IOMMU failed ret=%d\n",
-			ret);
-		return ret;
-	}
-
-	cb->valid = true;
-
-	return 0;
-}
-
-static int ipa_smmu_uc_cb_probe(struct device *dev)
-{
-	struct ipa_smmu_cb_ctx *cb = &smmu_cb[IPA_SMMU_CB_UC];
-	int order = 0;
-	int disable_htw = 1;
-	int ret;
-
-	IPADBG("sub pdev=%p\n", dev);
-
-	if (dma_set_mask(dev, DMA_BIT_MASK(32)) ||
-		    dma_set_coherent_mask(dev, DMA_BIT_MASK(32))) {
-		IPAERR("DMA set mask failed\n");
-		return -EOPNOTSUPP;
-	}
-
-	cb->dev = dev;
-	cb->mapping = arm_iommu_create_mapping(&platform_bus_type,
-			IPA_SMMU_UC_VA_START, IPA_SMMU_UC_VA_SIZE, order);
-	if (IS_ERR(cb->mapping)) {
-		IPADBG("Fail to create mapping\n");
-		/* assume this failure is because iommu driver is not ready */
-		return -EPROBE_DEFER;
-	}
-
-	ret = arm_iommu_attach_device(cb->dev, cb->mapping);
-	if (ret) {
-		IPAERR("could not attach device ret=%d\n", ret);
-		return ret;
-	}
-
-	if (smmu_disable_htw) {
-		if (iommu_domain_set_attr(cb->mapping->domain,
-				DOMAIN_ATTR_COHERENT_HTW_DISABLE,
-				 &disable_htw)) {
-			IPAERR("couldn't disable coherent HTW\n");
-			arm_iommu_detach_device(cb->dev);
-			return -EIO;
-		}
-	}
-
-	cb->valid = true;
-	cb->next_addr = IPA_SMMU_UC_VA_END;
-	ipa_ctx->uc_pdev = dev;
-
-	return 0;
-}
-
-static int ipa_smmu_ap_cb_probe(struct device *dev)
-{
-	struct ipa_smmu_cb_ctx *cb = &smmu_cb[IPA_SMMU_CB_AP];
-	int order = 0;
-	int result;
-	int disable_htw = 1;
-
-	IPADBG("sub pdev=%p\n", dev);
-
-	if (dma_set_mask(dev, DMA_BIT_MASK(32)) ||
-		    dma_set_coherent_mask(dev, DMA_BIT_MASK(32))) {
-		IPAERR("DMA set mask failed\n");
-		return -EOPNOTSUPP;
-	}
-
-	cb->dev = dev;
-	cb->mapping = arm_iommu_create_mapping(&platform_bus_type,
-			IPA_SMMU_AP_VA_START, IPA_SMMU_AP_VA_SIZE, order);
-	if (IS_ERR(cb->mapping)) {
-		IPADBG("Fail to create mapping\n");
-		/* assume this failure is because iommu driver is not ready */
-		return -EPROBE_DEFER;
-	}
-
-	result = arm_iommu_attach_device(cb->dev, cb->mapping);
-	if (result) {
-		IPAERR("couldn't attach to IOMMU ret=%d\n", result);
-		return result;
-	}
-
-	IPADBG("map IPA region to AP_CB IOMMU\n");
-	result = iommu_map(cb->mapping->domain, 0x680000, 0x680000,
-		0x64000,
-			IOMMU_READ | IOMMU_WRITE | IOMMU_DEVICE);
-	if (result) {
-		IPAERR("map IPA region to AP_CB IOMMU failed ret=%d\n",
-			result);
-		return result;
-	}
-
-	if (smmu_disable_htw) {
-		if (iommu_domain_set_attr(cb->mapping->domain,
-				DOMAIN_ATTR_COHERENT_HTW_DISABLE,
-				 &disable_htw)) {
-			IPAERR("couldn't disable coherent HTW\n");
-			arm_iommu_detach_device(cb->dev);
-			return -EIO;
-		}
-	}
-
-	cb->valid = true;
-	smmu_present = true;
-
-	if (!bus_scale_table)
-		bus_scale_table = msm_bus_cl_get_pdata(ipa_pdev);
-
-	/* Proceed to real initialization */
-	result = ipa_init(&ipa_res, dev);
-	if (result) {
-		IPAERR("ipa_init failed\n");
-		arm_iommu_detach_device(cb->dev);
-		arm_iommu_release_mapping(cb->mapping);
-		return result;
-	}
-
-	return result;
-}
-
-#define IPA_QSMMU_AP_CB_LABEL "ipa_shared"
-#define IPA_QSMMU_WLAN_CB_LABEL "ipa_wlan"
-#define IPA_QSMMU_UC_CB_LABEL "ipa_uc"
-
-static int ipa_qsmmu_setup(struct device *dev)
-{
-	struct device *d;
-	int result = 0;
-
-	d = msm_iommu_get_ctx(IPA_QSMMU_AP_CB_LABEL);
-	if (IS_ERR_OR_NULL(d)) {
-		if (d != ERR_PTR(-EPROBE_DEFER))
-			IPAERR("fail to get ipa ap cb\n");
-		return PTR_ERR(d);
-	} else {
-		result = ipa_smmu_ap_cb_probe(d);
-	}
-
-	/* TODO: add err handling for WLAN and uc */
-	d = msm_iommu_get_ctx(IPA_QSMMU_WLAN_CB_LABEL);
-	if (IS_ERR_OR_NULL(d))
-		IPAERR("fail to get ipa wlan cb\n");
-	else
-		ipa_smmu_wlan_cb_probe(d);
-
-	d = msm_iommu_get_ctx(IPA_QSMMU_UC_CB_LABEL);
-	if (IS_ERR_OR_NULL(d))
-		IPAERR("fail to get ipa uc cb\n");
-	else
-		ipa_smmu_uc_cb_probe(d);
-
-	return result;
-}
-
-static int ipa_plat_drv_probe(struct platform_device *pdev_p)
-{
-	int result;
-	struct device *dev = &pdev_p->dev;
-
-	IPADBG("IPA driver probing started\n");
-
-	if (of_device_is_compatible(dev->of_node, "qcom,ipa-smmu-ap-cb"))
-		return ipa_smmu_ap_cb_probe(dev);
-
-	if (of_device_is_compatible(dev->of_node, "qcom,ipa-smmu-wlan-cb"))
-		return ipa_smmu_wlan_cb_probe(dev);
-
-	if (of_device_is_compatible(dev->of_node, "qcom,ipa-smmu-uc-cb"))
-		return ipa_smmu_uc_cb_probe(dev);
-
-	master_dev = dev;
-	if (!ipa_pdev)
-		ipa_pdev = pdev_p;
-
-	result = get_ipa_dts_configuration(pdev_p, &ipa_res);
-	if (result) {
-		IPAERR("IPA dts parsing failed\n");
-		return result;
-	}
-
-	if (of_property_read_bool(pdev_p->dev.of_node, "qcom,arm-smmu")) {
-		arm_smmu = true;
-		result = of_platform_populate(pdev_p->dev.of_node,
-				ipa_plat_drv_match, NULL, &pdev_p->dev);
-	} else if (of_property_read_bool(pdev_p->dev.of_node,
-				"qcom,msm-smmu")) {
-		result = ipa_qsmmu_setup(dev);
-	} else {
-		if (dma_set_mask(&pdev_p->dev, DMA_BIT_MASK(32)) ||
-			    dma_set_coherent_mask(&pdev_p->dev,
-			    DMA_BIT_MASK(32))) {
-			IPAERR("DMA set mask failed\n");
-			return -EOPNOTSUPP;
-		}
-
-		if (!bus_scale_table)
-			bus_scale_table = msm_bus_cl_get_pdata(pdev_p);
-
-		/* Proceed to real initialization */
-		result = ipa_init(&ipa_res, dev);
-		if (result) {
-			IPAERR("ipa_init failed\n");
-			return result;
-		}
-	}
-
-	return result;
-}
-
-/**
- * ipa_ap_suspend() - suspend callback for runtime_pm
- * @dev: pointer to device
- *
- * This callback will be invoked by the runtime_pm framework when an AP suspend
- * operation is invoked, usually by pressing a suspend button.
- *
- * Returns -EAGAIN to runtime_pm framework in case IPA is in use by AP.
- * This will postpone the suspend operation until IPA is no longer used by AP.
-*/
-static int ipa_ap_suspend(struct device *dev)
-{
-	int i;
-
-	IPADBG("Enter...\n");
-
-	/* In case there is a tx/rx handler in polling mode fail to suspend */
-	for (i = 0; i < ipa_ctx->ipa_num_pipes; i++) {
-		if (ipa_ctx->ep[i].sys &&
-			atomic_read(&ipa_ctx->ep[i].sys->curr_polling_state)) {
-			IPAERR("EP %d is in polling state, do not suspend\n",
-				i);
-			return -EAGAIN;
-		}
-	}
-
-	/* release SPS IPA resource without waiting for inactivity timer */
-	atomic_set(&ipa_ctx->sps_pm.eot_activity, 0);
-	ipa_sps_release_resource(NULL);
-	IPADBG("Exit\n");
-
-	return 0;
-}
-
-/**
-* ipa_ap_resume() - resume callback for runtime_pm
-* @dev: pointer to device
-*
-* This callback will be invoked by the runtime_pm framework when an AP resume
-* operation is invoked.
-*
-* Always returns 0 since resume should always succeed.
-*/
-static int ipa_ap_resume(struct device *dev)
-{
-	return 0;
-}
-
-static const struct dev_pm_ops ipa_pm_ops = {
-	.suspend_noirq = ipa_ap_suspend,
-	.resume_noirq = ipa_ap_resume,
-};
-
-static struct platform_driver ipa_plat_drv = {
-	.probe = ipa_plat_drv_probe,
-	.driver = {
-		.name = DRV_NAME,
-		.owner = THIS_MODULE,
-		.pm = &ipa_pm_ops,
-		.of_match_table = ipa_plat_drv_match,
-	},
-};
-
-struct ipa_context *ipa_get_ctx(void)
-{
-	return ipa_ctx;
-}
-
-static int __init ipa_module_init(void)
-{
-	IPADBG("IPA module init\n");
-
-	/* Register as a platform device driver */
-	return platform_driver_register(&ipa_plat_drv);
-}
-subsys_initcall(ipa_module_init);
-
-MODULE_LICENSE("GPL v2");
-MODULE_DESCRIPTION("IPA HW device driver");
-
diff --git a/drivers/platform/msm/ipa/ipa_client.c b/drivers/platform/msm/ipa/ipa_client.c
deleted file mode 100644
index 1001301..00000000
--- a/drivers/platform/msm/ipa/ipa_client.c
+++ /dev/null
@@ -1,736 +0,0 @@
-/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-#include <asm/barrier.h>
-#include <linux/delay.h>
-#include <linux/device.h>
-#include "ipa_i.h"
-
-/*
- * These values were determined empirically and shows good E2E bi-
- * directional throughputs
- */
-#define IPA_HOLB_TMR_EN 0x1
-#define IPA_HOLB_TMR_DIS 0x0
-#define IPA_HOLB_TMR_DEFAULT_VAL 0x1ff
-
-#define IPA_PKT_FLUSH_TO_US 100
-
-int ipa_enable_data_path(u32 clnt_hdl)
-{
-	struct ipa_ep_context *ep = &ipa_ctx->ep[clnt_hdl];
-	struct ipa_ep_cfg_holb holb_cfg;
-	struct ipa_ep_cfg_ctrl ep_cfg_ctrl;
-	int res = 0;
-
-	IPADBG("Enabling data path\n");
-	/* From IPA 2.0, disable HOLB */
-	if ((ipa_ctx->ipa_hw_type >= IPA_HW_v2_0) &&
-		IPA_CLIENT_IS_CONS(ep->client)) {
-		memset(&holb_cfg, 0 , sizeof(holb_cfg));
-		holb_cfg.en = IPA_HOLB_TMR_DIS;
-		holb_cfg.tmr_val = 0;
-		res = ipa_cfg_ep_holb(clnt_hdl, &holb_cfg);
-	}
-
-	/* Enable the pipe */
-	if (IPA_CLIENT_IS_CONS(ep->client) &&
-	    (ep->keep_ipa_awake ||
-	     ipa_ctx->resume_on_connect[ep->client] ||
-	     !ipa_should_pipe_be_suspended(ep->client))) {
-		memset(&ep_cfg_ctrl, 0 , sizeof(ep_cfg_ctrl));
-		ep_cfg_ctrl.ipa_ep_suspend = false;
-		ipa_cfg_ep_ctrl(clnt_hdl, &ep_cfg_ctrl);
-	}
-
-	return res;
-}
-
-int ipa_disable_data_path(u32 clnt_hdl)
-{
-	struct ipa_ep_context *ep = &ipa_ctx->ep[clnt_hdl];
-	struct ipa_ep_cfg_holb holb_cfg;
-	struct ipa_ep_cfg_ctrl ep_cfg_ctrl;
-	u32 aggr_init;
-	int res = 0;
-
-	IPADBG("Disabling data path\n");
-	/* On IPA 2.0, enable HOLB in order to prevent IPA from stalling */
-	if ((ipa_ctx->ipa_hw_type >= IPA_HW_v2_0) &&
-		IPA_CLIENT_IS_CONS(ep->client)) {
-		memset(&holb_cfg, 0, sizeof(holb_cfg));
-		holb_cfg.en = IPA_HOLB_TMR_EN;
-		holb_cfg.tmr_val = 0;
-		res = ipa_cfg_ep_holb(clnt_hdl, &holb_cfg);
-	}
-
-	/* Suspend the pipe */
-	if (IPA_CLIENT_IS_CONS(ep->client)) {
-		memset(&ep_cfg_ctrl, 0 , sizeof(struct ipa_ep_cfg_ctrl));
-		ep_cfg_ctrl.ipa_ep_suspend = true;
-		ipa_cfg_ep_ctrl(clnt_hdl, &ep_cfg_ctrl);
-	}
-
-	udelay(IPA_PKT_FLUSH_TO_US);
-	aggr_init = ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_AGGR_N_OFST_v2_0(clnt_hdl));
-	if (((aggr_init & IPA_ENDP_INIT_AGGR_N_AGGR_EN_BMSK) >>
-	    IPA_ENDP_INIT_AGGR_N_AGGR_EN_SHFT) == IPA_ENABLE_AGGR)
-		ipa_tag_aggr_force_close(clnt_hdl);
-
-	return res;
-}
-
-static int ipa_connect_configure_sps(const struct ipa_connect_params *in,
-				     struct ipa_ep_context *ep, int ipa_ep_idx)
-{
-	int result = -EFAULT;
-
-	/* Default Config */
-	ep->ep_hdl = sps_alloc_endpoint();
-
-	if (ipa_smmu_map_peer_bam(in->client_bam_hdl)) {
-		IPAERR("fail to iommu map peer BAM.\n");
-		return -EFAULT;
-	}
-
-	if (ep->ep_hdl == NULL) {
-		IPAERR("SPS EP alloc failed EP.\n");
-		return -EFAULT;
-	}
-
-	result = sps_get_config(ep->ep_hdl,
-		&ep->connect);
-	if (result) {
-		IPAERR("fail to get config.\n");
-		return -EFAULT;
-	}
-
-	/* Specific Config */
-	if (IPA_CLIENT_IS_CONS(in->client)) {
-		ep->connect.mode = SPS_MODE_SRC;
-		ep->connect.destination =
-			in->client_bam_hdl;
-		ep->connect.dest_iova = ipa_ctx->peer_bam_iova;
-		ep->connect.source = ipa_ctx->bam_handle;
-		ep->connect.dest_pipe_index =
-			in->client_ep_idx;
-		ep->connect.src_pipe_index = ipa_ep_idx;
-	} else {
-		ep->connect.mode = SPS_MODE_DEST;
-		ep->connect.source = in->client_bam_hdl;
-		ep->connect.source_iova = ipa_ctx->peer_bam_iova;
-		ep->connect.destination = ipa_ctx->bam_handle;
-		ep->connect.src_pipe_index = in->client_ep_idx;
-		ep->connect.dest_pipe_index = ipa_ep_idx;
-	}
-
-	return 0;
-}
-
-static int ipa_connect_allocate_fifo(const struct ipa_connect_params *in,
-				     struct sps_mem_buffer *mem_buff_ptr,
-				     bool *fifo_in_pipe_mem_ptr,
-				     u32 *fifo_pipe_mem_ofst_ptr,
-				     u32 fifo_size, int ipa_ep_idx)
-{
-	dma_addr_t dma_addr;
-	u32 ofst;
-	int result = -EFAULT;
-	struct iommu_domain *smmu_domain;
-
-	mem_buff_ptr->size = fifo_size;
-	if (in->pipe_mem_preferred) {
-		if (ipa_pipe_mem_alloc(&ofst, fifo_size)) {
-			IPAERR("FIFO pipe mem alloc fail ep %u\n",
-				ipa_ep_idx);
-			mem_buff_ptr->base =
-				dma_alloc_coherent(ipa_ctx->pdev,
-				mem_buff_ptr->size,
-				&dma_addr, GFP_KERNEL);
-		} else {
-			memset(mem_buff_ptr, 0, sizeof(struct sps_mem_buffer));
-			result = sps_setup_bam2bam_fifo(mem_buff_ptr, ofst,
-				fifo_size, 1);
-			WARN_ON(result);
-			*fifo_in_pipe_mem_ptr = 1;
-			dma_addr = mem_buff_ptr->phys_base;
-			*fifo_pipe_mem_ofst_ptr = ofst;
-		}
-	} else {
-		mem_buff_ptr->base =
-			dma_alloc_coherent(ipa_ctx->pdev, mem_buff_ptr->size,
-			&dma_addr, GFP_KERNEL);
-	}
-	if (!ipa_ctx->smmu_present) {
-		mem_buff_ptr->phys_base = dma_addr;
-	} else {
-		mem_buff_ptr->iova = dma_addr;
-		smmu_domain = ipa_get_smmu_domain();
-		if (smmu_domain != NULL) {
-			mem_buff_ptr->phys_base =
-				iommu_iova_to_phys(smmu_domain, dma_addr);
-		}
-	}
-	if (mem_buff_ptr->base == NULL) {
-		IPAERR("fail to get DMA memory.\n");
-		return -EFAULT;
-	}
-
-	return 0;
-}
-
-/**
- * ipa_connect() - low-level IPA client connect
- * @in:	[in] input parameters from client
- * @sps:	[out] sps output from IPA needed by client for sps_connect
- * @clnt_hdl:	[out] opaque client handle assigned by IPA to client
- *
- * Should be called by the driver of the peripheral that wants to connect to
- * IPA in BAM-BAM mode. these peripherals are USB and HSIC. this api
- * expects caller to take responsibility to add any needed headers, routing
- * and filtering tables and rules as needed.
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_connect(const struct ipa_connect_params *in, struct ipa_sps_params *sps,
-		u32 *clnt_hdl)
-{
-	int ipa_ep_idx;
-	int result = -EFAULT;
-	struct ipa_ep_context *ep;
-	struct ipa_ep_cfg_status ep_status;
-	unsigned long base;
-	struct iommu_domain *smmu_domain;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	IPADBG("connecting client\n");
-
-	if (in == NULL || sps == NULL || clnt_hdl == NULL ||
-	    in->client >= IPA_CLIENT_MAX ||
-	    in->desc_fifo_sz == 0 || in->data_fifo_sz == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	ipa_ep_idx = ipa_get_ep_mapping(in->client);
-	if (ipa_ep_idx == -1) {
-		IPAERR("fail to alloc EP.\n");
-		goto fail;
-	}
-
-	ep = &ipa_ctx->ep[ipa_ep_idx];
-
-	if (ep->valid) {
-		IPAERR("EP already allocated.\n");
-		goto fail;
-	}
-
-	memset(&ipa_ctx->ep[ipa_ep_idx], 0, sizeof(struct ipa_ep_context));
-	ipa_inc_client_enable_clks();
-
-	ep->skip_ep_cfg = in->skip_ep_cfg;
-	ep->valid = 1;
-	ep->client = in->client;
-	ep->client_notify = in->notify;
-	ep->priv = in->priv;
-	ep->keep_ipa_awake = in->keep_ipa_awake;
-
-	/* Notify uc to start monitoring holb on USB BAM Producer pipe. */
-	if (IPA_CLIENT_IS_USB_CONS(in->client)) {
-		ipa_uc_monitor_holb(in->client, true);
-		IPADBG("Enabling holb monitor for client:%d", in->client);
-	}
-
-	result = ipa_enable_data_path(ipa_ep_idx);
-	if (result) {
-		IPAERR("enable data path failed res=%d clnt=%d.\n", result,
-				ipa_ep_idx);
-		goto ipa_cfg_ep_fail;
-	}
-
-	if (!ep->skip_ep_cfg) {
-		if (ipa_cfg_ep(ipa_ep_idx, &in->ipa_ep_cfg)) {
-			IPAERR("fail to configure EP.\n");
-			goto ipa_cfg_ep_fail;
-		}
-		/* Setting EP status 0 */
-		memset(&ep_status, 0, sizeof(ep_status));
-		if (ipa_cfg_ep_status(ipa_ep_idx, &ep_status)) {
-			IPAERR("fail to configure status of EP.\n");
-			goto ipa_cfg_ep_fail;
-		}
-		IPADBG("ep configuration successful\n");
-	} else {
-		IPADBG("Skipping endpoint configuration.\n");
-	}
-
-	result = ipa_connect_configure_sps(in, ep, ipa_ep_idx);
-	if (result) {
-		IPAERR("fail to configure SPS.\n");
-		goto ipa_cfg_ep_fail;
-	}
-
-	if (ipa_ctx->smmu_present &&
-			(in->desc.base == NULL ||
-			 in->data.base == NULL)) {
-		IPAERR(" allocate FIFOs data_fifo=0x%p desc_fifo=0x%p.\n",
-				in->data.base, in->desc.base);
-		goto desc_mem_alloc_fail;
-	}
-
-	if (in->desc.base == NULL) {
-		result = ipa_connect_allocate_fifo(in, &ep->connect.desc,
-						  &ep->desc_fifo_in_pipe_mem,
-						  &ep->desc_fifo_pipe_mem_ofst,
-						  in->desc_fifo_sz, ipa_ep_idx);
-		if (result) {
-			IPAERR("fail to allocate DESC FIFO.\n");
-			goto desc_mem_alloc_fail;
-		}
-	} else {
-		IPADBG("client allocated DESC FIFO\n");
-		ep->connect.desc = in->desc;
-		ep->desc_fifo_client_allocated = 1;
-	}
-	IPADBG("Descriptor FIFO pa=%pa, size=%d\n", &ep->connect.desc.phys_base,
-	       ep->connect.desc.size);
-
-	if (in->data.base == NULL) {
-		result = ipa_connect_allocate_fifo(in, &ep->connect.data,
-						&ep->data_fifo_in_pipe_mem,
-						&ep->data_fifo_pipe_mem_ofst,
-						in->data_fifo_sz, ipa_ep_idx);
-		if (result) {
-			IPAERR("fail to allocate DATA FIFO.\n");
-			goto data_mem_alloc_fail;
-		}
-	} else {
-		IPADBG("client allocated DATA FIFO\n");
-		ep->connect.data = in->data;
-		ep->data_fifo_client_allocated = 1;
-	}
-	IPADBG("Data FIFO pa=%pa, size=%d\n", &ep->connect.data.phys_base,
-	       ep->connect.data.size);
-
-	if (ipa_ctx->smmu_present) {
-		ep->connect.data.iova = ep->connect.data.phys_base;
-		base = ep->connect.data.iova;
-		smmu_domain = ipa_get_smmu_domain();
-		if (smmu_domain != NULL) {
-			if (iommu_map(smmu_domain,
-				rounddown(base, PAGE_SIZE),
-				rounddown(base, PAGE_SIZE),
-				roundup(ep->connect.data.size + base -
-					rounddown(base, PAGE_SIZE), PAGE_SIZE),
-				IOMMU_READ | IOMMU_WRITE)) {
-				IPAERR("Fail to iommu_map data FIFO\n");
-				goto iommu_map_data_fail;
-			}
-		}
-		ep->connect.desc.iova = ep->connect.desc.phys_base;
-		base = ep->connect.desc.iova;
-		if (smmu_domain != NULL) {
-			if (iommu_map(smmu_domain,
-				rounddown(base, PAGE_SIZE),
-				rounddown(base, PAGE_SIZE),
-				roundup(ep->connect.desc.size + base -
-					rounddown(base, PAGE_SIZE), PAGE_SIZE),
-				IOMMU_READ | IOMMU_WRITE)) {
-				IPAERR("Fail to iommu_map desc FIFO\n");
-				goto iommu_map_desc_fail;
-			}
-		}
-	}
-
-	if ((ipa_ctx->ipa_hw_type == IPA_HW_v2_0 ||
-		ipa_ctx->ipa_hw_type == IPA_HW_v2_5) &&
-		IPA_CLIENT_IS_USB_CONS(in->client))
-		ep->connect.event_thresh = IPA_USB_EVENT_THRESHOLD;
-	else
-		ep->connect.event_thresh = IPA_EVENT_THRESHOLD;
-	ep->connect.options = SPS_O_AUTO_ENABLE;    /* BAM-to-BAM */
-
-	result = ipa_sps_connect_safe(ep->ep_hdl, &ep->connect, in->client);
-	if (result) {
-		IPAERR("sps_connect fails.\n");
-		goto sps_connect_fail;
-	}
-
-	sps->ipa_bam_hdl = ipa_ctx->bam_handle;
-	sps->ipa_ep_idx = ipa_ep_idx;
-	*clnt_hdl = ipa_ep_idx;
-	memcpy(&sps->desc, &ep->connect.desc, sizeof(struct sps_mem_buffer));
-	memcpy(&sps->data, &ep->connect.data, sizeof(struct sps_mem_buffer));
-
-	ipa_ctx->skip_ep_cfg_shadow[ipa_ep_idx] = ep->skip_ep_cfg;
-	if (!ep->skip_ep_cfg && IPA_CLIENT_IS_PROD(in->client))
-		ipa_install_dflt_flt_rules(ipa_ep_idx);
-
-	if (!ep->keep_ipa_awake)
-		ipa_dec_client_disable_clks();
-
-	IPADBG("client %d (ep: %d) connected\n", in->client, ipa_ep_idx);
-
-	return 0;
-
-sps_connect_fail:
-	if (ipa_ctx->smmu_present) {
-		base = ep->connect.desc.iova;
-		smmu_domain = ipa_get_smmu_domain();
-		if (smmu_domain != NULL) {
-			iommu_unmap(smmu_domain,
-				rounddown(base, PAGE_SIZE),
-				roundup(ep->connect.desc.size + base -
-					rounddown(base, PAGE_SIZE), PAGE_SIZE));
-		}
-	}
-iommu_map_desc_fail:
-	if (ipa_ctx->smmu_present) {
-		base = ep->connect.data.iova;
-		smmu_domain = ipa_get_smmu_domain();
-		if (smmu_domain != NULL) {
-			iommu_unmap(smmu_domain,
-				rounddown(base, PAGE_SIZE),
-				roundup(ep->connect.data.size + base -
-					rounddown(base, PAGE_SIZE), PAGE_SIZE));
-		}
-	}
-iommu_map_data_fail:
-	if (!ep->data_fifo_client_allocated) {
-		if (!ep->data_fifo_in_pipe_mem)
-			dma_free_coherent(ipa_ctx->pdev,
-				  ep->connect.data.size,
-				  ep->connect.data.base,
-				  ep->connect.data.phys_base);
-		else
-			ipa_pipe_mem_free(ep->data_fifo_pipe_mem_ofst,
-				  ep->connect.data.size);
-	}
-data_mem_alloc_fail:
-	if (!ep->desc_fifo_client_allocated) {
-		if (!ep->desc_fifo_in_pipe_mem)
-			dma_free_coherent(ipa_ctx->pdev,
-				  ep->connect.desc.size,
-				  ep->connect.desc.base,
-				  ep->connect.desc.phys_base);
-		else
-			ipa_pipe_mem_free(ep->desc_fifo_pipe_mem_ofst,
-				  ep->connect.desc.size);
-	}
-desc_mem_alloc_fail:
-	sps_free_endpoint(ep->ep_hdl);
-ipa_cfg_ep_fail:
-	memset(&ipa_ctx->ep[ipa_ep_idx], 0, sizeof(struct ipa_ep_context));
-	ipa_dec_client_disable_clks();
-fail:
-	return result;
-}
-EXPORT_SYMBOL(ipa_connect);
-
-/**
- * ipa_disconnect() - low-level IPA client disconnect
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- *
- * Should be called by the driver of the peripheral that wants to disconnect
- * from IPA in BAM-BAM mode. this api expects caller to take responsibility to
- * free any needed headers, routing and filtering tables and rules as needed.
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_disconnect(u32 clnt_hdl)
-{
-	int result;
-	struct ipa_ep_context *ep;
-	unsigned long peer_bam;
-	unsigned long base;
-	struct iommu_domain *smmu_domain;
-	struct ipa_disable_force_clear_datapath_req_msg_v01 req = {0};
-	int res;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-		ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (!ep->keep_ipa_awake)
-		ipa_inc_client_enable_clks();
-
-	/* Set Disconnect in Progress flag. */
-	spin_lock(&ipa_ctx->disconnect_lock);
-	ep->disconnect_in_progress = true;
-	spin_unlock(&ipa_ctx->disconnect_lock);
-
-	/* Notify uc to stop monitoring holb on USB BAM Producer pipe. */
-	if (IPA_CLIENT_IS_USB_CONS(ep->client)) {
-		ipa_uc_monitor_holb(ep->client, false);
-		IPADBG("Disabling holb monitor for client: %d\n", ep->client);
-	}
-
-	result = ipa_disable_data_path(clnt_hdl);
-	if (result) {
-		IPAERR("disable data path failed res=%d clnt=%d.\n", result,
-				clnt_hdl);
-		return -EPERM;
-	}
-
-	result = sps_disconnect(ep->ep_hdl);
-	if (result) {
-		IPAERR("SPS disconnect failed.\n");
-		return -EPERM;
-	}
-
-	if (IPA_CLIENT_IS_CONS(ep->client))
-		peer_bam = ep->connect.destination;
-	else
-		peer_bam = ep->connect.source;
-
-	if (ipa_smmu_unmap_peer_bam(peer_bam)) {
-		IPAERR("fail to iommu unmap peer BAM.\n");
-		return -EPERM;
-	}
-
-	if (!ep->desc_fifo_client_allocated &&
-	     ep->connect.desc.base) {
-		if (!ep->desc_fifo_in_pipe_mem)
-			dma_free_coherent(ipa_ctx->pdev,
-					  ep->connect.desc.size,
-					  ep->connect.desc.base,
-					  ep->connect.desc.phys_base);
-		else
-			ipa_pipe_mem_free(ep->desc_fifo_pipe_mem_ofst,
-					  ep->connect.desc.size);
-	}
-
-	if (!ep->data_fifo_client_allocated &&
-	     ep->connect.data.base) {
-		if (!ep->data_fifo_in_pipe_mem)
-			dma_free_coherent(ipa_ctx->pdev,
-					  ep->connect.data.size,
-					  ep->connect.data.base,
-					  ep->connect.data.phys_base);
-		else
-			ipa_pipe_mem_free(ep->data_fifo_pipe_mem_ofst,
-					  ep->connect.data.size);
-	}
-
-	if (ipa_ctx->smmu_present) {
-		base = ep->connect.desc.iova;
-		smmu_domain = ipa_get_smmu_domain();
-		if (smmu_domain != NULL) {
-			iommu_unmap(smmu_domain,
-				rounddown(base, PAGE_SIZE),
-				roundup(ep->connect.desc.size + base -
-					rounddown(base, PAGE_SIZE), PAGE_SIZE));
-		}
-	}
-
-	if (ipa_ctx->smmu_present) {
-		base = ep->connect.data.iova;
-		smmu_domain = ipa_get_smmu_domain();
-		if (smmu_domain != NULL) {
-			iommu_unmap(smmu_domain,
-				rounddown(base, PAGE_SIZE),
-				roundup(ep->connect.data.size + base -
-					rounddown(base, PAGE_SIZE), PAGE_SIZE));
-		}
-	}
-
-	result = sps_free_endpoint(ep->ep_hdl);
-	if (result) {
-		IPAERR("SPS de-alloc EP failed.\n");
-		return -EPERM;
-	}
-
-	ipa_delete_dflt_flt_rules(clnt_hdl);
-
-	/* If APPS flow control is not enabled, send a message to modem to
-	 * enable flow control honoring.
-	 */
-	if (!ipa_ctx->tethered_flow_control && ep->qmi_request_sent) {
-		/* Send a message to modem to disable flow control honoring. */
-		req.request_id = clnt_hdl;
-		res = qmi_disable_force_clear_datapath_send(&req);
-		if (res) {
-			IPADBG("disable_force_clear_datapath failed %d\n",
-				res);
-		}
-	}
-
-	spin_lock(&ipa_ctx->disconnect_lock);
-	memset(&ipa_ctx->ep[clnt_hdl], 0, sizeof(struct ipa_ep_context));
-	spin_unlock(&ipa_ctx->disconnect_lock);
-
-	ipa_dec_client_disable_clks();
-
-	IPADBG("client (ep: %d) disconnected\n", clnt_hdl);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_disconnect);
-
-/**
-* ipa_reset_endpoint() - reset an endpoint from BAM perspective
-* @clnt_hdl: [in] IPA client handle
-*
-* Returns:	0 on success, negative on failure
-*
-* Note:	Should not be called from atomic context
-*/
-int ipa_reset_endpoint(u32 clnt_hdl)
-{
-	int res;
-	struct ipa_ep_context *ep;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes) {
-		IPAERR("Bad parameters.\n");
-		return -EFAULT;
-	}
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	ipa_inc_client_enable_clks();
-	res = sps_disconnect(ep->ep_hdl);
-	if (res) {
-		IPAERR("sps_disconnect() failed, res=%d.\n", res);
-		goto bail;
-	} else {
-		res = ipa_sps_connect_safe(ep->ep_hdl, &ep->connect,
-			ep->client);
-		if (res) {
-			IPAERR("sps_connect() failed, res=%d.\n", res);
-			goto bail;
-		}
-	}
-
-bail:
-	ipa_dec_client_disable_clks();
-
-	return res;
-}
-EXPORT_SYMBOL(ipa_reset_endpoint);
-
-/**
- * ipa_sps_connect_safe() - connect endpoint from BAM prespective
- * @h: [in] sps pipe handle
- * @connect: [in] sps connect parameters
- * @ipa_client: [in] ipa client handle representing the pipe
- *
- * This function connects a BAM pipe using SPS driver sps_connect() API
- * and by requesting uC interface to reset the pipe, avoids an IPA HW
- * limitation that does not allow reseting a BAM pipe during traffic in
- * IPA TX command queue.
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_sps_connect_safe(struct sps_pipe *h, struct sps_connect *connect,
-			 enum ipa_client_type ipa_client)
-{
-	int res;
-
-	if (ipa_ctx->ipa_hw_type > IPA_HW_v2_5 || ipa_ctx->skip_uc_pipe_reset) {
-		IPADBG("uC pipe reset is not required\n");
-	} else {
-		res = ipa_uc_reset_pipe(ipa_client);
-		if (res)
-			return res;
-	}
-	return sps_connect(h, connect);
-}
-EXPORT_SYMBOL(ipa_sps_connect_safe);
-/**
- * ipa_clear_endpoint_delay() - Remove ep delay set on the IPA pipe before
- * client disconnect.
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- *
- * Should be called by the driver of the peripheral that wants to remove
- * ep delay on IPA consumer ipe before disconnect in BAM-BAM mode. this api
- * expects caller to take responsibility to free any needed headers, routing
- * and filtering tables and rules as needed.
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_clear_endpoint_delay(u32 clnt_hdl)
-{
-	struct ipa_ep_context *ep;
-	struct ipa_ep_cfg_ctrl ep_ctrl = {0};
-	struct ipa_enable_force_clear_datapath_req_msg_v01 req = {0};
-	int res;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-		ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (!ipa_ctx->tethered_flow_control) {
-		IPADBG("APPS flow control is not enabled\n");
-		/* Send a message to modem to disable flow control honoring. */
-		req.request_id = clnt_hdl;
-		req.source_pipe_bitmask = 1 << clnt_hdl;
-		res = qmi_enable_force_clear_datapath_send(&req);
-		if (res) {
-			IPADBG("enable_force_clear_datapath failed %d\n",
-				res);
-		}
-		ep->qmi_request_sent = true;
-	}
-
-
-	ipa_inc_client_enable_clks();
-	/* Set disconnect in progress flag so further flow control events are
-	 * not honored.
-	 */
-	spin_lock(&ipa_ctx->disconnect_lock);
-	ep->disconnect_in_progress = true;
-	spin_unlock(&ipa_ctx->disconnect_lock);
-
-	/* If flow is disabled at this point, restore the ep state.*/
-	ep_ctrl.ipa_ep_delay = false;
-	ep_ctrl.ipa_ep_suspend = false;
-	ipa_cfg_ep_ctrl(clnt_hdl, &ep_ctrl);
-
-	ipa_dec_client_disable_clks();
-
-	IPADBG("client (ep: %d) removed ep delay\n", clnt_hdl);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_clear_endpoint_delay);
diff --git a/drivers/platform/msm/ipa/ipa_debugfs.c b/drivers/platform/msm/ipa/ipa_debugfs.c
deleted file mode 100644
index 38aa517..00000000
--- a/drivers/platform/msm/ipa/ipa_debugfs.c
+++ /dev/null
@@ -1,2024 +0,0 @@
-/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifdef CONFIG_DEBUG_FS
-
-#include <linux/debugfs.h>
-#include <linux/kernel.h>
-#include <linux/stringify.h>
-#include "ipa_i.h"
-#include "ipa_rm_i.h"
-
-#define IPA_MAX_MSG_LEN 4096
-#define IPA_DBG_CNTR_ON 127265
-#define IPA_DBG_CNTR_OFF 127264
-
-const char *ipa_excp_name[] = {
-	__stringify_1(IPA_A5_MUX_HDR_EXCP_RSVD0),
-	__stringify_1(IPA_A5_MUX_HDR_EXCP_RSVD1),
-	__stringify_1(IPA_A5_MUX_HDR_EXCP_FLAG_IHL),
-	__stringify_1(IPA_A5_MUX_HDR_EXCP_FLAG_REPLICATED),
-	__stringify_1(IPA_A5_MUX_HDR_EXCP_FLAG_TAG),
-	__stringify_1(IPA_A5_MUX_HDR_EXCP_FLAG_SW_FLT),
-	__stringify_1(IPA_A5_MUX_HDR_EXCP_FLAG_NAT),
-	__stringify_1(IPA_A5_MUX_HDR_EXCP_FLAG_IP),
-};
-
-const char *ipa_status_excp_name[] = {
-	__stringify_1(IPA_EXCP_DEAGGR),
-	__stringify_1(IPA_EXCP_REPLICATION),
-	__stringify_1(IPA_EXCP_IP),
-	__stringify_1(IPA_EXCP_IHL),
-	__stringify_1(IPA_EXCP_FRAG_MISS),
-	__stringify_1(IPA_EXCP_SW),
-	__stringify_1(IPA_EXCP_NAT),
-	__stringify_1(IPA_EXCP_NONE),
-};
-
-const char *ipa_event_name[] = {
-	__stringify(WLAN_CLIENT_CONNECT),
-	__stringify(WLAN_CLIENT_DISCONNECT),
-	__stringify(WLAN_CLIENT_POWER_SAVE_MODE),
-	__stringify(WLAN_CLIENT_NORMAL_MODE),
-	__stringify(SW_ROUTING_ENABLE),
-	__stringify(SW_ROUTING_DISABLE),
-	__stringify(WLAN_AP_CONNECT),
-	__stringify(WLAN_AP_DISCONNECT),
-	__stringify(WLAN_STA_CONNECT),
-	__stringify(WLAN_STA_DISCONNECT),
-	__stringify(WLAN_CLIENT_CONNECT_EX),
-	__stringify(WLAN_SWITCH_TO_SCC),
-	__stringify(WLAN_SWITCH_TO_MCC),
-	__stringify(WLAN_WDI_ENABLE),
-	__stringify(WLAN_WDI_DISABLE),
-	__stringify(WAN_UPSTREAM_ROUTE_ADD),
-	__stringify(WAN_UPSTREAM_ROUTE_DEL),
-	__stringify(WAN_EMBMS_CONNECT),
-	__stringify(WAN_XLAT_CONNECT),
-	__stringify(ECM_CONNECT),
-	__stringify(ECM_DISCONNECT),
-	__stringify(IPA_TETHERING_STATS_UPDATE_STATS),
-	__stringify(IPA_TETHERING_STATS_UPDATE_NETWORK_STATS),
-};
-
-const char *ipa_hdr_l2_type_name[] = {
-	__stringify(IPA_HDR_L2_NONE),
-	__stringify(IPA_HDR_L2_ETHERNET_II),
-	__stringify(IPA_HDR_L2_802_3),
-};
-
-const char *ipa_hdr_proc_type_name[] = {
-	__stringify(IPA_HDR_PROC_NONE),
-	__stringify(IPA_HDR_PROC_ETHII_TO_ETHII),
-	__stringify(IPA_HDR_PROC_ETHII_TO_802_3),
-	__stringify(IPA_HDR_PROC_802_3_TO_ETHII),
-	__stringify(IPA_HDR_PROC_802_3_TO_802_3),
-};
-
-const char *ipa_ep_client_type[] = {
-	__stringify(IPA_CLIENT_PROD),
-	__stringify(IPA_CLIENT_WLAN1_PROD),
-	__stringify(IPA_CLIENT_HSIC2_PROD),
-	__stringify(IPA_CLIENT_USB2_PROD),
-	__stringify(IPA_CLIENT_HSIC3_PROD),
-	__stringify(IPA_CLIENT_USB3_PROD),
-	__stringify(IPA_CLIENT_HSIC4_PROD),
-	__stringify(IPA_CLIENT_USB4_PROD),
-	__stringify(IPA_CLIENT_HSIC5_PROD),
-	__stringify(IPA_CLIENT_USB_PROD),
-	__stringify(IPA_CLIENT_A5_WLAN_AMPDU_PROD),
-	__stringify(IPA_CLIENT_A2_EMBEDDED_PROD),
-	__stringify(IPA_CLIENT_A2_TETHERED_PROD),
-	__stringify(IPA_CLIENT_APPS_LAN_WAN_PROD),
-	__stringify(IPA_CLIENT_APPS_CMD_PROD),
-	__stringify(IPA_CLIENT_ODU_PROD),
-	__stringify(IPA_CLIENT_MHI_PROD),
-	__stringify(IPA_CLIENT_Q6_LAN_PROD),
-	__stringify(IPA_CLIENT_Q6_CMD_PROD),
-	__stringify(IPA_CLIENT_MEMCPY_DMA_SYNC_PROD),
-	__stringify(IPA_CLIENT_MEMCPY_DMA_ASYNC_PROD),
-	__stringify(IPA_CLIENT_Q6_DECOMP_PROD),
-	__stringify(IPA_CLIENT_Q6_DECOMP2_PROD),
-	__stringify(IPA_CLIENT_TEST_PROD),
-	__stringify(IPA_CLIENT_TEST1_PROD),
-	__stringify(IPA_CLIENT_TEST2_PROD),
-	__stringify(IPA_CLIENT_TEST3_PROD),
-	__stringify(IPA_CLIENT_TEST4_PROD),
-	__stringify(IPA_CLIENT_CONS),
-	__stringify(IPA_CLIENT_WLAN1_CONS),
-	__stringify(IPA_CLIENT_HSIC2_CONS),
-	__stringify(IPA_CLIENT_USB2_CONS),
-	__stringify(IPA_CLIENT_WLAN2_CONS),
-	__stringify(IPA_CLIENT_HSIC3_CONS),
-	__stringify(IPA_CLIENT_USB3_CONS),
-	__stringify(IPA_CLIENT_WLAN3_CONS),
-	__stringify(IPA_CLIENT_HSIC4_CONS),
-	__stringify(IPA_CLIENT_USB4_CONS),
-	__stringify(IPA_CLIENT_WLAN4_CONS),
-	__stringify(IPA_CLIENT_HSIC5_CONS),
-	__stringify(IPA_CLIENT_USB_CONS),
-	__stringify(IPA_CLIENT_USB_DPL_CONS),
-	__stringify(IPA_CLIENT_A2_EMBEDDED_CONS),
-	__stringify(IPA_CLIENT_A2_TETHERED_CONS),
-	__stringify(IPA_CLIENT_A5_LAN_WAN_CONS),
-	__stringify(IPA_CLIENT_APPS_LAN_CONS),
-	__stringify(IPA_CLIENT_APPS_WAN_CONS),
-	__stringify(IPA_CLIENT_ODU_EMB_CONS),
-	__stringify(IPA_CLIENT_ODU_TETH_CONS),
-	__stringify(IPA_CLIENT_MHI_CONS),
-	__stringify(IPA_CLIENT_Q6_LAN_CONS),
-	__stringify(IPA_CLIENT_Q6_WAN_CONS),
-	__stringify(IPA_CLIENT_Q6_DUN_CONS),
-	__stringify(IPA_CLIENT_MEMCPY_DMA_SYNC_CONS),
-	__stringify(IPA_CLIENT_MEMCPY_DMA_ASYNC_CONS),
-	__stringify(IPA_CLIENT_Q6_DECOMP_CONS),
-	__stringify(IPA_CLIENT_Q6_DECOMP2_CONS),
-	__stringify(IPA_CLIENT_TEST_CONS),
-	__stringify(IPA_CLIENT_TEST1_CONS),
-	__stringify(IPA_CLIENT_TEST2_CONS),
-	__stringify(IPA_CLIENT_TEST3_CONS),
-	__stringify(IPA_CLIENT_TEST4_CONS),
-	__stringify(IPA_CLIENT_MAX),
-};
-
-static struct dentry *dent;
-static struct dentry *dfile_gen_reg;
-static struct dentry *dfile_ep_reg;
-static struct dentry *dfile_keep_awake;
-static struct dentry *dfile_ep_holb;
-static struct dentry *dfile_hdr;
-static struct dentry *dfile_proc_ctx;
-static struct dentry *dfile_ip4_rt;
-static struct dentry *dfile_ip6_rt;
-static struct dentry *dfile_ip4_flt;
-static struct dentry *dfile_ip6_flt;
-static struct dentry *dfile_stats;
-static struct dentry *dfile_wstats;
-static struct dentry *dfile_wdi_stats;
-static struct dentry *dfile_dbg_cnt;
-static struct dentry *dfile_msg;
-static struct dentry *dfile_ip4_nat;
-static struct dentry *dfile_rm_stats;
-static struct dentry *dfile_enable_panic_dump;
-static char dbg_buff[IPA_MAX_MSG_LEN];
-static s8 ep_reg_idx;
-static int ipa_enable_panic_dump;
-
-int _ipa_read_gen_reg_v1_1(char *buff, int max_len)
-{
-	return scnprintf(dbg_buff, IPA_MAX_MSG_LEN,
-			"IPA_VERSION=0x%x\n"
-			"IPA_COMP_HW_VERSION=0x%x\n"
-			"IPA_ROUTE=0x%x\n"
-			"IPA_FILTER=0x%x\n"
-			"IPA_SHARED_MEM_SIZE=0x%x\n",
-			ipa_read_reg(ipa_ctx->mmio, IPA_VERSION_OFST),
-			ipa_read_reg(ipa_ctx->mmio, IPA_COMP_HW_VERSION_OFST),
-			ipa_read_reg(ipa_ctx->mmio, IPA_ROUTE_OFST_v1_1),
-			ipa_read_reg(ipa_ctx->mmio, IPA_FILTER_OFST_v1_1),
-			ipa_read_reg(ipa_ctx->mmio,
-					IPA_SHARED_MEM_SIZE_OFST_v1_1));
-}
-
-int _ipa_read_gen_reg_v2_0(char *buff, int max_len)
-{
-	return scnprintf(dbg_buff, IPA_MAX_MSG_LEN,
-			"IPA_VERSION=0x%x\n"
-			"IPA_COMP_HW_VERSION=0x%x\n"
-			"IPA_ROUTE=0x%x\n"
-			"IPA_FILTER=0x%x\n"
-			"IPA_SHARED_MEM_RESTRICTED=0x%x\n"
-			"IPA_SHARED_MEM_SIZE=0x%x\n",
-			ipa_read_reg(ipa_ctx->mmio, IPA_VERSION_OFST),
-			ipa_read_reg(ipa_ctx->mmio, IPA_COMP_HW_VERSION_OFST),
-			ipa_read_reg(ipa_ctx->mmio, IPA_ROUTE_OFST_v1_1),
-			ipa_read_reg(ipa_ctx->mmio, IPA_FILTER_OFST_v1_1),
-			ipa_read_reg_field(ipa_ctx->mmio,
-				IPA_SHARED_MEM_SIZE_OFST_v2_0,
-				IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_BMSK_v2_0,
-				IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_SHFT_v2_0),
-			ipa_read_reg_field(ipa_ctx->mmio,
-				IPA_SHARED_MEM_SIZE_OFST_v2_0,
-				IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_BMSK_v2_0,
-				IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_SHFT_v2_0));
-}
-
-static ssize_t ipa_read_gen_reg(struct file *file, char __user *ubuf,
-		size_t count, loff_t *ppos)
-{
-	int nbytes;
-
-	ipa_inc_client_enable_clks();
-	nbytes = ipa_ctx->ctrl->ipa_read_gen_reg(dbg_buff, IPA_MAX_MSG_LEN);
-	ipa_dec_client_disable_clks();
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, nbytes);
-}
-
-static ssize_t ipa_write_ep_holb(struct file *file,
-		const char __user *buf, size_t count, loff_t *ppos)
-{
-	struct ipa_ep_cfg_holb holb;
-	u32 en;
-	u32 tmr_val;
-	u32 ep_idx;
-	unsigned long missing;
-	char *sptr, *token;
-
-	if (sizeof(dbg_buff) < count + 1)
-		return -EFAULT;
-
-	missing = copy_from_user(dbg_buff, buf, count);
-	if (missing)
-		return -EFAULT;
-
-	dbg_buff[count] = '\0';
-
-	sptr = dbg_buff;
-
-	token = strsep(&sptr, " ");
-	if (!token)
-		return -EINVAL;
-	if (kstrtou32(token, 0, &ep_idx))
-		return -EINVAL;
-
-	token = strsep(&sptr, " ");
-	if (!token)
-		return -EINVAL;
-	if (kstrtou32(token, 0, &en))
-		return -EINVAL;
-
-	token = strsep(&sptr, " ");
-	if (!token)
-		return -EINVAL;
-	if (kstrtou32(token, 0, &tmr_val))
-		return -EINVAL;
-
-	holb.en = en;
-	holb.tmr_val = tmr_val;
-
-	ipa_cfg_ep_holb(ep_idx, &holb);
-
-	return count;
-}
-
-static ssize_t ipa_write_ep_reg(struct file *file, const char __user *buf,
-		size_t count, loff_t *ppos)
-{
-	unsigned long missing;
-	s8 option = 0;
-
-	if (sizeof(dbg_buff) < count + 1)
-		return -EFAULT;
-
-	missing = copy_from_user(dbg_buff, buf, count);
-	if (missing)
-		return -EFAULT;
-
-	dbg_buff[count] = '\0';
-	if (kstrtos8(dbg_buff, 0, &option))
-		return -EFAULT;
-
-	if (option >= ipa_ctx->ipa_num_pipes) {
-		IPAERR("bad pipe specified %u\n", option);
-		return count;
-	}
-
-	ep_reg_idx = option;
-
-	return count;
-}
-
-int _ipa_read_ep_reg_v1_1(char *buf, int max_len, int pipe)
-{
-	return scnprintf(buf, max_len,
-			"IPA_ENDP_INIT_NAT_%u=0x%x\n"
-			"IPA_ENDP_INIT_HDR_%u=0x%x\n"
-			"IPA_ENDP_INIT_MODE_%u=0x%x\n"
-			"IPA_ENDP_INIT_AGGR_%u=0x%x\n"
-			"IPA_ENDP_INIT_ROUTE_%u=0x%x\n"
-			"IPA_ENDP_INIT_CTRL_%u=0x%x\n"
-			"IPA_ENDP_INIT_HOL_EN_%u=0x%x\n"
-			"IPA_ENDP_INIT_HOL_TIMER_%u=0x%x\n",
-			pipe, ipa_read_reg(ipa_ctx->mmio,
-				IPA_ENDP_INIT_NAT_N_OFST_v1_1(pipe)),
-			pipe, ipa_read_reg(ipa_ctx->mmio,
-				IPA_ENDP_INIT_HDR_N_OFST_v1_1(pipe)),
-			pipe, ipa_read_reg(ipa_ctx->mmio,
-				IPA_ENDP_INIT_MODE_N_OFST_v1_1(pipe)),
-			pipe, ipa_read_reg(ipa_ctx->mmio,
-				IPA_ENDP_INIT_AGGR_N_OFST_v1_1(pipe)),
-			pipe, ipa_read_reg(ipa_ctx->mmio,
-				IPA_ENDP_INIT_ROUTE_N_OFST_v1_1(pipe)),
-			pipe, ipa_read_reg(ipa_ctx->mmio,
-				IPA_ENDP_INIT_CTRL_N_OFST(pipe)),
-			pipe, ipa_read_reg(ipa_ctx->mmio,
-				IPA_ENDP_INIT_HOL_BLOCK_EN_N_OFST_v1_1(pipe)),
-			pipe, ipa_read_reg(ipa_ctx->mmio,
-				IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_OFST_v1_1(pipe))
-				);
-}
-
-int _ipa_read_ep_reg_v2_0(char *buf, int max_len, int pipe)
-{
-	return scnprintf(
-		dbg_buff, IPA_MAX_MSG_LEN,
-		"IPA_ENDP_INIT_NAT_%u=0x%x\n"
-		"IPA_ENDP_INIT_HDR_%u=0x%x\n"
-		"IPA_ENDP_INIT_HDR_EXT_%u=0x%x\n"
-		"IPA_ENDP_INIT_MODE_%u=0x%x\n"
-		"IPA_ENDP_INIT_AGGR_%u=0x%x\n"
-		"IPA_ENDP_INIT_ROUTE_%u=0x%x\n"
-		"IPA_ENDP_INIT_CTRL_%u=0x%x\n"
-		"IPA_ENDP_INIT_HOL_EN_%u=0x%x\n"
-		"IPA_ENDP_INIT_HOL_TIMER_%u=0x%x\n"
-		"IPA_ENDP_INIT_DEAGGR_%u=0x%x\n"
-		"IPA_ENDP_INIT_CFG_%u=0x%x\n",
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_NAT_N_OFST_v2_0(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HDR_N_OFST_v2_0(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HDR_EXT_n_OFST_v2_0(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_MODE_N_OFST_v2_0(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_AGGR_N_OFST_v2_0(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_ROUTE_N_OFST_v2_0(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_CTRL_N_OFST(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HOL_BLOCK_EN_N_OFST_v2_0(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_OFST_v2_0(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_DEAGGR_n_OFST_v2_0(pipe)),
-		pipe, ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_CFG_n_OFST(pipe)));
-}
-
-static ssize_t ipa_read_ep_reg(struct file *file, char __user *ubuf,
-		size_t count, loff_t *ppos)
-{
-	int nbytes;
-	int i;
-	int start_idx;
-	int end_idx;
-	int size = 0;
-	int ret;
-	loff_t pos;
-
-	/* negative ep_reg_idx means all registers */
-	if (ep_reg_idx < 0) {
-		start_idx = 0;
-		end_idx = ipa_ctx->ipa_num_pipes;
-	} else {
-		start_idx = ep_reg_idx;
-		end_idx = start_idx + 1;
-	}
-	pos = *ppos;
-	ipa_inc_client_enable_clks();
-	for (i = start_idx; i < end_idx; i++) {
-
-		nbytes = ipa_ctx->ctrl->ipa_read_ep_reg(dbg_buff,
-				IPA_MAX_MSG_LEN, i);
-
-		*ppos = pos;
-		ret = simple_read_from_buffer(ubuf, count, ppos, dbg_buff,
-					      nbytes);
-		if (ret < 0) {
-			ipa_dec_client_disable_clks();
-			return ret;
-		}
-
-		size += ret;
-		ubuf += nbytes;
-		count -= nbytes;
-	}
-	ipa_dec_client_disable_clks();
-
-	*ppos = pos + size;
-	return size;
-}
-
-void print_pipe_info(int pipe_num, char *dbg_buff, int size)
-{
-	int result;
-	u32 free_slots;
-	u32 is_empty;
-
-	pr_err("pipe num:%d\npipe client type:%s\n", pipe_num,
-		ipa_ep_client_type[ipa_ctx->ep[pipe_num].client]);
-	result = sps_is_pipe_empty(ipa_ctx->ep[pipe_num].ep_hdl,
-		&is_empty);
-	if (result) {
-		pr_err("error reading pipe fifo free slots\n");
-	} else {
-		if (is_empty)
-			scnprintf(dbg_buff, size, "bam pipe is EMPTY,");
-		else
-			scnprintf(dbg_buff, size, "bam pipe is NOT-EMPTY,");
-		if (sps_get_unused_desc_num(ipa_ctx->ep[pipe_num].ep_hdl,
-					    &free_slots))
-			pr_err("%s error reading unused descriptors in FIFO\n",
-				dbg_buff);
-		else
-			pr_err("%s unused descriptors in descriptor FIFO: %d\n",
-				dbg_buff, free_slots);
-	}
-	ipa_ctx->ctrl->ipa_read_ep_reg(dbg_buff, size, pipe_num);
-	pr_err("%s\n", dbg_buff);
-}
-
-void ipa_print_active_ep_reg(char *dbg_buff, int size)
-{
-	int start_idx = 0;
-	int end_idx;
-	int i;
-	int count = 0;
-
-	end_idx = ipa_ctx->ipa_num_pipes;
-
-	ipa_inc_client_enable_clks();
-
-	for (i = start_idx; i < end_idx; i++)
-		if (ipa_ctx->ep[i].valid)
-			count++;
-	pr_err("number of active pipes:%d\n\n", count);
-	for (i = start_idx; i < end_idx; i++) {
-		if (ipa_ctx->ep[i].valid)
-			print_pipe_info(i, dbg_buff, size);
-	}
-
-	ipa_dec_client_disable_clks();
-}
-
-static ssize_t ipa_write_keep_awake(struct file *file, const char __user *buf,
-	size_t count, loff_t *ppos)
-{
-	unsigned long missing;
-	s8 option = 0;
-
-	if (sizeof(dbg_buff) < count + 1)
-		return -EFAULT;
-
-	missing = copy_from_user(dbg_buff, buf, count);
-	if (missing)
-		return -EFAULT;
-
-	dbg_buff[count] = '\0';
-	if (kstrtos8(dbg_buff, 0, &option))
-		return -EFAULT;
-
-	if (option == 1)
-		ipa_inc_client_enable_clks();
-	else if (option == 0)
-		ipa_dec_client_disable_clks();
-	else
-		return -EFAULT;
-
-	return count;
-}
-
-static ssize_t ipa_read_keep_awake(struct file *file, char __user *ubuf,
-	size_t count, loff_t *ppos)
-{
-	int nbytes;
-
-	ipa_active_clients_lock();
-	if (ipa_ctx->ipa_active_clients.cnt)
-		nbytes = scnprintf(dbg_buff, IPA_MAX_MSG_LEN,
-				"IPA APPS power state is ON\n");
-	else
-		nbytes = scnprintf(dbg_buff, IPA_MAX_MSG_LEN,
-				"IPA APPS power state is OFF\n");
-	ipa_active_clients_unlock();
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, nbytes);
-}
-
-int ipa_get_hdr_buff(char *dbg_buff, int size)
-{
-	int nbytes = 0;
-	int i = 0;
-	struct ipa_hdr_entry *entry;
-
-	*dbg_buff = 0;
-	mutex_lock(&ipa_ctx->lock);
-
-	if (ipa_ctx->hdr_tbl_lcl)
-		pr_err("Table resides on local memory\n");
-	else
-		pr_err("Table resides on system (ddr) memory\n");
-
-	list_for_each_entry(entry, &ipa_ctx->hdr_tbl.head_hdr_entry_list,
-			link) {
-		nbytes = scnprintf(
-			dbg_buff,
-			size,
-			"name:%s len=%d ref=%d partial=%d type=%s ",
-			entry->name,
-			entry->hdr_len,
-			entry->ref_cnt,
-			entry->is_partial,
-			ipa_hdr_l2_type_name[entry->type]);
-
-		if (entry->is_hdr_proc_ctx) {
-			nbytes += scnprintf(
-				dbg_buff + nbytes,
-				size - nbytes,
-				"phys_base=0x%pa ",
-				&entry->phys_base);
-		} else {
-			nbytes += scnprintf(
-				dbg_buff + nbytes,
-				size - nbytes,
-				"ofst=%u ",
-				entry->offset_entry->offset >> 2);
-		}
-		for (i = 0; i < entry->hdr_len; i++) {
-			scnprintf(dbg_buff + nbytes + i * 2,
-				size - nbytes - i * 2,
-				"%02x", entry->hdr[i]);
-		}
-		scnprintf(dbg_buff + nbytes + entry->hdr_len * 2,
-			size - nbytes - entry->hdr_len * 2,
-			"\n");
-		pr_err("%s", dbg_buff);
-	}
-	mutex_unlock(&ipa_ctx->lock);
-
-	return nbytes;
-}
-
-static ssize_t ipa_read_hdr(struct file *file, char __user *ubuf, size_t count,
-	loff_t *ppos)
-{
-	ipa_get_hdr_buff(dbg_buff, IPA_MAX_MSG_LEN);
-
-	return 0;
-}
-
-static int ipa_attrib_dump(struct ipa_rule_attrib *attrib,
-		enum ipa_ip_type ip)
-{
-	uint32_t addr[4];
-	uint32_t mask[4];
-	int i;
-
-	if (attrib->attrib_mask & IPA_FLT_TOS_MASKED)
-		pr_err("tos_value:%d ", attrib->tos_value);
-
-	if (attrib->attrib_mask & IPA_FLT_TOS_MASKED)
-		pr_err("tos_mask:%d ", attrib->tos_mask);
-
-	if (attrib->attrib_mask & IPA_FLT_PROTOCOL)
-		pr_err("protocol:%d ", attrib->u.v4.protocol);
-
-	if (attrib->attrib_mask & IPA_FLT_SRC_ADDR) {
-		if (ip == IPA_IP_v4) {
-			addr[0] = htonl(attrib->u.v4.src_addr);
-			mask[0] = htonl(attrib->u.v4.src_addr_mask);
-			pr_err(
-					"src_addr:%pI4 src_addr_mask:%pI4 ",
-					addr + 0, mask + 0);
-		} else if (ip == IPA_IP_v6) {
-			for (i = 0; i < 4; i++) {
-				addr[i] = htonl(attrib->u.v6.src_addr[i]);
-				mask[i] = htonl(attrib->u.v6.src_addr_mask[i]);
-			}
-			pr_err(
-					   "src_addr:%pI6 src_addr_mask:%pI6 ",
-					   addr + 0, mask + 0);
-		} else {
-			WARN_ON(1);
-		}
-	}
-	if (attrib->attrib_mask & IPA_FLT_DST_ADDR) {
-		if (ip == IPA_IP_v4) {
-			addr[0] = htonl(attrib->u.v4.dst_addr);
-			mask[0] = htonl(attrib->u.v4.dst_addr_mask);
-			pr_err(
-					   "dst_addr:%pI4 dst_addr_mask:%pI4 ",
-					   addr + 0, mask + 0);
-		} else if (ip == IPA_IP_v6) {
-			for (i = 0; i < 4; i++) {
-				addr[i] = htonl(attrib->u.v6.dst_addr[i]);
-				mask[i] = htonl(attrib->u.v6.dst_addr_mask[i]);
-			}
-			pr_err(
-					   "dst_addr:%pI6 dst_addr_mask:%pI6 ",
-					   addr + 0, mask + 0);
-		} else {
-			WARN_ON(1);
-		}
-	}
-	if (attrib->attrib_mask & IPA_FLT_SRC_PORT_RANGE) {
-		pr_err("src_port_range:%u %u ",
-				   attrib->src_port_lo,
-			     attrib->src_port_hi);
-	}
-	if (attrib->attrib_mask & IPA_FLT_DST_PORT_RANGE) {
-		pr_err("dst_port_range:%u %u ",
-				   attrib->dst_port_lo,
-			     attrib->dst_port_hi);
-	}
-	if (attrib->attrib_mask & IPA_FLT_TYPE)
-		pr_err("type:%d ", attrib->type);
-
-	if (attrib->attrib_mask & IPA_FLT_CODE)
-		pr_err("code:%d ", attrib->code);
-
-	if (attrib->attrib_mask & IPA_FLT_SPI)
-		pr_err("spi:%x ", attrib->spi);
-
-	if (attrib->attrib_mask & IPA_FLT_SRC_PORT)
-		pr_err("src_port:%u ", attrib->src_port);
-
-	if (attrib->attrib_mask & IPA_FLT_DST_PORT)
-		pr_err("dst_port:%u ", attrib->dst_port);
-
-	if (attrib->attrib_mask & IPA_FLT_TC)
-		pr_err("tc:%d ", attrib->u.v6.tc);
-
-	if (attrib->attrib_mask & IPA_FLT_FLOW_LABEL)
-		pr_err("flow_label:%x ", attrib->u.v6.flow_label);
-
-	if (attrib->attrib_mask & IPA_FLT_NEXT_HDR)
-		pr_err("next_hdr:%d ", attrib->u.v6.next_hdr);
-
-	if (attrib->attrib_mask & IPA_FLT_META_DATA) {
-		pr_err(
-				   "metadata:%x metadata_mask:%x",
-				   attrib->meta_data, attrib->meta_data_mask);
-	}
-
-	if (attrib->attrib_mask & IPA_FLT_FRAGMENT)
-		pr_err("frg ");
-
-	if ((attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_ETHER_II) ||
-		(attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_802_3)) {
-		pr_err("src_mac_addr:%pM ", attrib->src_mac_addr);
-	}
-
-	if ((attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_ETHER_II) ||
-		(attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_802_3)) {
-		pr_err("dst_mac_addr:%pM ", attrib->dst_mac_addr);
-	}
-
-	if (attrib->attrib_mask & IPA_FLT_MAC_ETHER_TYPE)
-		pr_err("ether_type:%x ", attrib->ether_type);
-
-	pr_err("\n");
-	return 0;
-}
-
-static int ipa_attrib_dump_eq(struct ipa_ipfltri_rule_eq *attrib)
-{
-	uint8_t addr[16];
-	uint8_t mask[16];
-	int i;
-	int j;
-
-	if (attrib->tos_eq_present)
-		pr_err("tos_value:%d ", attrib->tos_eq);
-
-	if (attrib->protocol_eq_present)
-		pr_err("protocol:%d ", attrib->protocol_eq);
-
-	for (i = 0; i < attrib->num_ihl_offset_range_16; i++) {
-		pr_err(
-			   "(ihl_ofst_range16: ofst:%u lo:%u hi:%u) ",
-			   attrib->ihl_offset_range_16[i].offset,
-			   attrib->ihl_offset_range_16[i].range_low,
-			   attrib->ihl_offset_range_16[i].range_high);
-	}
-
-	for (i = 0; i < attrib->num_offset_meq_32; i++) {
-		pr_err(
-			   "(ofst_meq32: ofst:%u mask:0x%x val:0x%x) ",
-			   attrib->offset_meq_32[i].offset,
-			   attrib->offset_meq_32[i].mask,
-			   attrib->offset_meq_32[i].value);
-	}
-
-	if (attrib->tc_eq_present)
-		pr_err("tc:%d ", attrib->tc_eq);
-
-	if (attrib->fl_eq_present)
-		pr_err("flow_label:%d ", attrib->fl_eq);
-
-	if (attrib->ihl_offset_eq_16_present) {
-		pr_err(
-				"(ihl_ofst_eq16:%d val:0x%x) ",
-				attrib->ihl_offset_eq_16.offset,
-				attrib->ihl_offset_eq_16.value);
-	}
-
-	for (i = 0; i < attrib->num_ihl_offset_meq_32; i++) {
-		pr_err(
-				"(ihl_ofst_meq32: ofts:%d mask:0x%x val:0x%x) ",
-				attrib->ihl_offset_meq_32[i].offset,
-				attrib->ihl_offset_meq_32[i].mask,
-				attrib->ihl_offset_meq_32[i].value);
-	}
-
-	for (i = 0; i < attrib->num_offset_meq_128; i++) {
-		for (j = 0; j < 16; j++) {
-			addr[j] = attrib->offset_meq_128[i].value[j];
-			mask[j] = attrib->offset_meq_128[i].mask[j];
-		}
-		pr_err(
-				"(ofst_meq128: ofst:%d mask:%pI6 val:%pI6) ",
-				attrib->offset_meq_128[i].offset,
-				mask + 0,
-				addr + 0);
-	}
-
-	if (attrib->metadata_meq32_present) {
-		pr_err(
-				"(metadata: ofst:%u mask:0x%x val:0x%x) ",
-				attrib->metadata_meq32.offset,
-				attrib->metadata_meq32.mask,
-				attrib->metadata_meq32.value);
-	}
-
-	if (attrib->ipv4_frag_eq_present)
-		pr_err("frg ");
-
-	pr_err("\n");
-	return 0;
-}
-
-static int ipa_open_dbg(struct inode *inode, struct file *file)
-{
-	file->private_data = inode->i_private;
-	return 0;
-}
-
-void ipa_print_rt(enum ipa_ip_type ip)
-{
-	int i = 0;
-	struct ipa_rt_tbl *tbl;
-	struct ipa_rt_entry *entry;
-	struct ipa_rt_tbl_set *set;
-	u32 ofst;
-	u32 ofst_words;
-
-	set = &ipa_ctx->rt_tbl_set[ip];
-
-	mutex_lock(&ipa_ctx->lock);
-
-	if (ip == IPA_IP_v6) {
-		if (ipa_ctx->ip6_rt_tbl_lcl)
-			pr_err("Table resides on local memory\n");
-		else
-			pr_err("Table resides on system (ddr) memory\n");
-	} else if (ip == IPA_IP_v4) {
-		if (ipa_ctx->ip4_rt_tbl_lcl)
-			pr_err("Table resides on local memory\n");
-		else
-			pr_err("Table resides on system (ddr) memory\n");
-	}
-
-	list_for_each_entry(tbl, &set->head_rt_tbl_list, link) {
-		i = 0;
-		list_for_each_entry(entry, &tbl->head_rt_rule_list, link) {
-			if (entry->proc_ctx) {
-				ofst = entry->proc_ctx->offset_entry->offset;
-				ofst_words =
-					(ofst +
-					ipa_ctx->hdr_proc_ctx_tbl.start_offset)
-					>> 5;
-
-				pr_err("tbl_idx:%d tbl_name:%s tbl_ref:%u ",
-					entry->tbl->idx, entry->tbl->name,
-					entry->tbl->ref_cnt);
-				pr_err("rule_idx:%d dst:%d ep:%d S:%u ",
-					i, entry->rule.dst,
-					ipa_get_ep_mapping(entry->rule.dst),
-					!ipa_ctx->hdr_tbl_lcl);
-				pr_err("proc_ctx[32B]:%u attrib_mask:%08x ",
-					ofst_words,
-					entry->rule.attrib.attrib_mask);
-			} else {
-				if (entry->hdr)
-					ofst = entry->hdr->offset_entry->offset;
-				else
-					ofst = 0;
-
-				pr_err("tbl_idx:%d tbl_name:%s tbl_ref:%u ",
-					entry->tbl->idx, entry->tbl->name,
-					entry->tbl->ref_cnt);
-				pr_err("rule_idx:%d dst:%d ep:%d S:%u ",
-					i, entry->rule.dst,
-					ipa_get_ep_mapping(entry->rule.dst),
-					!ipa_ctx->hdr_tbl_lcl);
-				pr_err("hdr_ofst[words]:%u attrib_mask:%08x ",
-					ofst >> 2,
-					entry->rule.attrib.attrib_mask);
-			}
-
-			ipa_attrib_dump(&entry->rule.attrib, ip);
-			i++;
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-}
-
-static ssize_t ipa_read_rt(struct file *file, char __user *ubuf, size_t count,
-	loff_t *ppos)
-{
-	enum ipa_ip_type ip;
-
-	ip = (enum ipa_ip_type)file->private_data;
-	ipa_print_rt(ip);
-
-	return 0;
-}
-
-int ipa_get_proc_ctx(char *dbg_buff, int size)
-{
-	int nbytes = 0;
-	struct ipa_hdr_proc_ctx_tbl *tbl;
-	struct ipa_hdr_proc_ctx_entry *entry;
-	u32 ofst_words;
-
-	tbl = &ipa_ctx->hdr_proc_ctx_tbl;
-	*dbg_buff = 0;
-
-	mutex_lock(&ipa_ctx->lock);
-
-	if (ipa_ctx->hdr_proc_ctx_tbl_lcl)
-		pr_info("Table resides on local memory\n");
-	else
-		pr_info("Table resides on system(ddr) memory\n");
-
-	list_for_each_entry(entry, &tbl->head_proc_ctx_entry_list, link) {
-		ofst_words = (entry->offset_entry->offset +
-			ipa_ctx->hdr_proc_ctx_tbl.start_offset)
-			>> 5;
-		if (entry->hdr->is_hdr_proc_ctx) {
-			nbytes += scnprintf(dbg_buff + nbytes,
-				size - nbytes,
-				"id:%u hdr_proc_type:%s proc_ctx[32B]:%u ",
-				entry->id,
-				ipa_hdr_proc_type_name[entry->type],
-				ofst_words);
-			nbytes += scnprintf(dbg_buff + nbytes,
-				size - nbytes,
-				"hdr_phys_base:0x%pa\n",
-				&entry->hdr->phys_base);
-		} else {
-			nbytes += scnprintf(dbg_buff + nbytes,
-				size - nbytes,
-				"id:%u hdr_proc_type:%s proc_ctx[32B]:%u ",
-				entry->id,
-				ipa_hdr_proc_type_name[entry->type],
-				ofst_words);
-			nbytes += scnprintf(dbg_buff + nbytes,
-				size - nbytes,
-				"hdr[words]:%u\n",
-				entry->hdr->offset_entry->offset >> 2);
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-
-	return nbytes;
-}
-
-static ssize_t ipa_read_proc_ctx(struct file *file, char __user *ubuf,
-	size_t count, loff_t *ppos)
-{
-	int ret;
-
-	ret = ipa_get_proc_ctx(dbg_buff, IPA_MAX_MSG_LEN);
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, ret);
-}
-
-void ipa_print_flt(enum ipa_ip_type ip)
-{
-	int i;
-	int j;
-	struct ipa_flt_tbl *tbl;
-	struct ipa_flt_entry *entry;
-	struct ipa_rt_tbl *rt_tbl;
-	u32 rt_tbl_idx;
-	u32 bitmap;
-	bool eq;
-
-	tbl = &ipa_ctx->glob_flt_tbl[ip];
-	mutex_lock(&ipa_ctx->lock);
-	i = 0;
-	list_for_each_entry(entry, &tbl->head_flt_rule_list, link) {
-		if (entry->rule.eq_attrib_type) {
-			rt_tbl_idx = entry->rule.rt_tbl_idx;
-			bitmap = entry->rule.eq_attrib.rule_eq_bitmap;
-			eq = true;
-		} else {
-			rt_tbl = ipa_id_find(entry->rule.rt_tbl_hdl);
-			if (rt_tbl)
-				rt_tbl_idx = rt_tbl->idx;
-			else
-				rt_tbl_idx = ~0;
-			bitmap = entry->rule.attrib.attrib_mask;
-			eq = false;
-		}
-		pr_err("ep_idx:global rule_idx:%d act:%d rt_tbl_idx:%d ",
-			i, entry->rule.action, rt_tbl_idx);
-		pr_err("attrib_mask:%08x to_uc:%d, retain_hdr:%d eq:%d ",
-			bitmap, entry->rule.to_uc, entry->rule.retain_hdr, eq);
-		if (eq)
-			ipa_attrib_dump_eq(
-				&entry->rule.eq_attrib);
-		else
-			ipa_attrib_dump(
-				&entry->rule.attrib, ip);
-		i++;
-	}
-
-	for (j = 0; j < ipa_ctx->ipa_num_pipes; j++) {
-		tbl = &ipa_ctx->flt_tbl[j][ip];
-		i = 0;
-		list_for_each_entry(entry, &tbl->head_flt_rule_list, link) {
-			if (entry->rule.eq_attrib_type) {
-				rt_tbl_idx = entry->rule.rt_tbl_idx;
-				bitmap = entry->rule.eq_attrib.rule_eq_bitmap;
-				eq = true;
-			} else {
-				rt_tbl = ipa_id_find(entry->rule.rt_tbl_hdl);
-				if (rt_tbl)
-					rt_tbl_idx = rt_tbl->idx;
-				else
-					rt_tbl_idx = ~0;
-				bitmap = entry->rule.attrib.attrib_mask;
-				eq = false;
-			}
-			pr_err("ep_idx:%d rule_idx:%d act:%d rt_tbl_idx:%d ",
-				j, i, entry->rule.action, rt_tbl_idx);
-			pr_err("attrib_mask:%08x to_uc:%d, retain_hdr:%d ",
-				bitmap, entry->rule.to_uc,
-				entry->rule.retain_hdr);
-			pr_err("eq:%d ", eq);
-			if (eq)
-				ipa_attrib_dump_eq(
-					&entry->rule.eq_attrib);
-			else
-				ipa_attrib_dump(
-					&entry->rule.attrib, ip);
-			i++;
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-}
-
-static ssize_t ipa_read_flt(struct file *file, char __user *ubuf, size_t count,
-	loff_t *ppos)
-{
-	enum ipa_ip_type ip;
-
-	ip = (enum ipa_ip_type)file->private_data;
-	ipa_print_flt(ip);
-
-	return 0;
-}
-
-int ipa_get_stats(char *dbg_buff, int size)
-{
-	int nbytes;
-	int i;
-	int cnt = 0;
-	uint connect = 0;
-
-	for (i = 0; i < ipa_ctx->ipa_num_pipes; i++)
-		connect |= (ipa_ctx->ep[i].valid << i);
-
-	if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_0) {
-		nbytes = scnprintf(dbg_buff, size,
-			"sw_tx=%u\n"
-			"hw_tx=%u\n"
-			"tx_compl=%u\n"
-			"wan_rx=%u\n"
-			"stat_compl=%u\n"
-			"lan_aggr_close=%u\n"
-			"wan_aggr_close=%u\n"
-			"act_clnt=%u\n"
-			"con_clnt_bmap=0x%x\n"
-			"wan_rx_empty=%u\n"
-			"wan_repl_rx_empty=%u\n"
-			"lan_rx_empty=%u\n"
-			"lan_repl_rx_empty=%u\n"
-			"flow_enable=%u\n"
-			"flow_disable=%u\n",
-			ipa_ctx->stats.tx_sw_pkts,
-			ipa_ctx->stats.tx_hw_pkts,
-			ipa_ctx->stats.tx_pkts_compl,
-			ipa_ctx->stats.rx_pkts,
-			ipa_ctx->stats.stat_compl,
-			ipa_ctx->stats.aggr_close,
-			ipa_ctx->stats.wan_aggr_close,
-			ipa_ctx->ipa_active_clients.cnt,
-			connect,
-			ipa_ctx->stats.wan_rx_empty,
-			ipa_ctx->stats.wan_repl_rx_empty,
-			ipa_ctx->stats.lan_rx_empty,
-			ipa_ctx->stats.lan_repl_rx_empty,
-			ipa_ctx->stats.flow_enable,
-			ipa_ctx->stats.flow_disable);
-		cnt += nbytes;
-
-		for (i = 0; i < MAX_NUM_EXCP; i++) {
-			nbytes = scnprintf(dbg_buff + cnt,
-				size - cnt,
-				"lan_rx_excp[%u:%20s]=%u\n", i,
-				ipa_status_excp_name[i],
-				ipa_ctx->stats.rx_excp_pkts[i]);
-			cnt += nbytes;
-		}
-	} else {
-		nbytes = scnprintf(dbg_buff, size,
-			"sw_tx=%u\n"
-			"hw_tx=%u\n"
-			"rx=%u\n"
-			"rx_repl_repost=%u\n"
-			"rx_q_len=%u\n"
-			"act_clnt=%u\n"
-			"con_clnt_bmap=0x%x\n",
-			ipa_ctx->stats.tx_sw_pkts,
-			ipa_ctx->stats.tx_hw_pkts,
-			ipa_ctx->stats.rx_pkts,
-			ipa_ctx->stats.rx_repl_repost,
-			ipa_ctx->stats.rx_q_len,
-			ipa_ctx->ipa_active_clients.cnt,
-			connect);
-		cnt += nbytes;
-
-		for (i = 0; i < MAX_NUM_EXCP; i++) {
-			nbytes = scnprintf(dbg_buff + cnt,
-				size - cnt,
-				"rx_excp[%u:%35s]=%u\n", i, ipa_excp_name[i],
-				ipa_ctx->stats.rx_excp_pkts[i]);
-			cnt += nbytes;
-		}
-	}
-
-	return cnt;
-}
-
-static ssize_t ipa_read_stats(struct file *file, char __user *ubuf,
-	size_t count, loff_t *ppos)
-{
-	int ret;
-
-	ret = ipa_get_stats(dbg_buff, IPA_MAX_MSG_LEN);
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, ret);
-}
-
-int ipa_get_wstats(char *dbg_buff, int size)
-{
-#define HEAD_FRMT_STR "%25s\n"
-#define FRMT_STR "%25s %10u\n"
-#define FRMT_STR1 "%25s %10u\n\n"
-
-	int cnt = 0;
-	int nbytes;
-	int ipa_ep_idx;
-	enum ipa_client_type client = IPA_CLIENT_WLAN1_PROD;
-	struct ipa_ep_context *ep;
-
-	do {
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			HEAD_FRMT_STR, "Client IPA_CLIENT_WLAN1_PROD Stats:");
-		cnt += nbytes;
-
-		ipa_ep_idx = ipa_get_ep_mapping(client);
-		if (ipa_ep_idx == -1) {
-			nbytes = scnprintf(dbg_buff + cnt,
-				size - cnt, HEAD_FRMT_STR, "Not up");
-			cnt += nbytes;
-			break;
-		}
-
-		ep = &ipa_ctx->ep[ipa_ep_idx];
-		if (ep->valid != 1) {
-			nbytes = scnprintf(dbg_buff + cnt,
-				size - cnt, HEAD_FRMT_STR, "Not up");
-			cnt += nbytes;
-			break;
-		}
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR, "Avail Fifo Desc:",
-			atomic_read(&ep->avail_fifo_desc));
-		cnt += nbytes;
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR, "Rx Pkts Rcvd:", ep->wstats.rx_pkts_rcvd);
-		cnt += nbytes;
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR, "Rx Pkts Status Rcvd:",
-			ep->wstats.rx_pkts_status_rcvd);
-		cnt += nbytes;
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR, "Rx DH Rcvd:", ep->wstats.rx_hd_rcvd);
-		cnt += nbytes;
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR, "Rx DH Processed:",
-			ep->wstats.rx_hd_processed);
-		cnt += nbytes;
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR, "Rx DH Sent Back:", ep->wstats.rx_hd_reply);
-		cnt += nbytes;
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR, "Rx Pkt Leak:", ep->wstats.rx_pkt_leak);
-		cnt += nbytes;
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR1, "Rx DP Fail:", ep->wstats.rx_dp_fail);
-		cnt += nbytes;
-
-	} while (0);
-
-	client = IPA_CLIENT_WLAN1_CONS;
-	nbytes = scnprintf(dbg_buff + cnt, size - cnt, HEAD_FRMT_STR,
-		"Client IPA_CLIENT_WLAN1_CONS Stats:");
-	cnt += nbytes;
-	while (1) {
-		ipa_ep_idx = ipa_get_ep_mapping(client);
-		if (ipa_ep_idx == -1) {
-			nbytes = scnprintf(dbg_buff + cnt,
-				size - cnt, HEAD_FRMT_STR, "Not up");
-			cnt += nbytes;
-			goto nxt_clnt_cons;
-		}
-
-		ep = &ipa_ctx->ep[ipa_ep_idx];
-		if (ep->valid != 1) {
-			nbytes = scnprintf(dbg_buff + cnt,
-				size - cnt, HEAD_FRMT_STR, "Not up");
-			cnt += nbytes;
-			goto nxt_clnt_cons;
-		}
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR, "Tx Pkts Received:", ep->wstats.tx_pkts_rcvd);
-		cnt += nbytes;
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR, "Tx Pkts Sent:", ep->wstats.tx_pkts_sent);
-		cnt += nbytes;
-
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			FRMT_STR1, "Tx Pkts Dropped:",
-			ep->wstats.tx_pkts_dropped);
-		cnt += nbytes;
-
-nxt_clnt_cons:
-			switch (client) {
-			case IPA_CLIENT_WLAN1_CONS:
-				client = IPA_CLIENT_WLAN2_CONS;
-				nbytes = scnprintf(dbg_buff + cnt,
-					size - cnt, HEAD_FRMT_STR,
-					"Client IPA_CLIENT_WLAN2_CONS Stats:");
-				cnt += nbytes;
-				continue;
-			case IPA_CLIENT_WLAN2_CONS:
-				client = IPA_CLIENT_WLAN3_CONS;
-				nbytes = scnprintf(dbg_buff + cnt,
-					size - cnt, HEAD_FRMT_STR,
-					"Client IPA_CLIENT_WLAN3_CONS Stats:");
-				cnt += nbytes;
-				continue;
-			case IPA_CLIENT_WLAN3_CONS:
-				client = IPA_CLIENT_WLAN4_CONS;
-				nbytes = scnprintf(dbg_buff + cnt,
-					size - cnt, HEAD_FRMT_STR,
-					"Client IPA_CLIENT_WLAN4_CONS Stats:");
-				cnt += nbytes;
-				continue;
-			case IPA_CLIENT_WLAN4_CONS:
-			default:
-				break;
-			}
-		break;
-	}
-
-	nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-		"\n"HEAD_FRMT_STR, "All Wlan Consumer pipes stats:");
-	cnt += nbytes;
-
-	nbytes = scnprintf(dbg_buff + cnt, size - cnt, FRMT_STR,
-		"Tx Comm Buff Allocated:",
-		ipa_ctx->wc_memb.wlan_comm_total_cnt);
-	cnt += nbytes;
-
-	nbytes = scnprintf(dbg_buff + cnt, size - cnt, FRMT_STR,
-		"Tx Comm Buff Avail:", ipa_ctx->wc_memb.wlan_comm_free_cnt);
-	cnt += nbytes;
-
-	nbytes = scnprintf(dbg_buff + cnt, size - cnt, FRMT_STR1,
-		"Total Tx Pkts Freed:", ipa_ctx->wc_memb.total_tx_pkts_freed);
-	cnt += nbytes;
-
-	return cnt;
-}
-
-static ssize_t ipa_read_wstats(struct file *file, char __user *ubuf,
-	size_t count, loff_t *ppos)
-{
-	int ret = 0;
-
-	ret = ipa_get_wstats(dbg_buff, IPA_MAX_MSG_LEN);
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, ret);
-}
-
-int ipa_get_wdi(char *dbg_buff, int size)
-{
-	struct IpaHwStatsWDIInfoData_t stats;
-	int nbytes;
-	int cnt = 0;
-
-	if (!ipa_get_wdi_stats(&stats)) {
-		nbytes = scnprintf(dbg_buff, size,
-			"TX num_pkts_processed=%u\n"
-			"TX copy_engine_doorbell_value=%u\n"
-			"TX num_db_fired=%u\n"
-			"TX ringFull=%u\n"
-			"TX ringEmpty=%u\n"
-			"TX ringUsageHigh=%u\n"
-			"TX ringUsageLow=%u\n"
-			"TX RingUtilCount=%u\n"
-			"TX bamFifoFull=%u\n"
-			"TX bamFifoEmpty=%u\n"
-			"TX bamFifoUsageHigh=%u\n"
-			"TX bamFifoUsageLow=%u\n"
-			"TX bamUtilCount=%u\n"
-			"TX num_db=%u\n"
-			"TX num_unexpected_db=%u\n"
-			"TX num_bam_int_handled=%u\n"
-			"TX num_bam_int_in_non_runnning_state=%u\n"
-			"TX num_qmb_int_handled=%u\n"
-			"TX num_bam_int_handled_while_wait_for_bam=%u\n",
-			stats.tx_ch_stats.num_pkts_processed,
-			stats.tx_ch_stats.copy_engine_doorbell_value,
-			stats.tx_ch_stats.num_db_fired,
-			stats.tx_ch_stats.tx_comp_ring_stats.ringFull,
-			stats.tx_ch_stats.tx_comp_ring_stats.ringEmpty,
-			stats.tx_ch_stats.tx_comp_ring_stats.ringUsageHigh,
-			stats.tx_ch_stats.tx_comp_ring_stats.ringUsageLow,
-			stats.tx_ch_stats.tx_comp_ring_stats.RingUtilCount,
-			stats.tx_ch_stats.bam_stats.bamFifoFull,
-			stats.tx_ch_stats.bam_stats.bamFifoEmpty,
-			stats.tx_ch_stats.bam_stats.bamFifoUsageHigh,
-			stats.tx_ch_stats.bam_stats.bamFifoUsageLow,
-			stats.tx_ch_stats.bam_stats.bamUtilCount,
-			stats.tx_ch_stats.num_db,
-			stats.tx_ch_stats.num_unexpected_db,
-			stats.tx_ch_stats.num_bam_int_handled,
-			stats.tx_ch_stats.num_bam_int_in_non_runnning_state,
-			stats.tx_ch_stats.num_qmb_int_handled,
-			stats.tx_ch_stats.
-				num_bam_int_handled_while_wait_for_bam);
-		cnt += nbytes;
-		nbytes = scnprintf(dbg_buff + cnt, size - cnt,
-			"RX max_outstanding_pkts=%u\n"
-			"RX num_pkts_processed=%u\n"
-			"RX rx_ring_rp_value=%u\n"
-			"RX ringFull=%u\n"
-			"RX ringEmpty=%u\n"
-			"RX ringUsageHigh=%u\n"
-			"RX ringUsageLow=%u\n"
-			"RX RingUtilCount=%u\n"
-			"RX bamFifoFull=%u\n"
-			"RX bamFifoEmpty=%u\n"
-			"RX bamFifoUsageHigh=%u\n"
-			"RX bamFifoUsageLow=%u\n"
-			"RX bamUtilCount=%u\n"
-			"RX num_bam_int_handled=%u\n"
-			"RX num_db=%u\n"
-			"RX num_unexpected_db=%u\n"
-			"RX num_pkts_in_dis_uninit_state=%u\n"
-			"num_ic_inj_vdev_change=%u\n"
-			"num_ic_inj_fw_desc_change=%u\n"
-			"RX reserved1=%u\n"
-			"RX reserved2=%u\n",
-			stats.rx_ch_stats.max_outstanding_pkts,
-			stats.rx_ch_stats.num_pkts_processed,
-			stats.rx_ch_stats.rx_ring_rp_value,
-			stats.rx_ch_stats.rx_ind_ring_stats.ringFull,
-			stats.rx_ch_stats.rx_ind_ring_stats.ringEmpty,
-			stats.rx_ch_stats.rx_ind_ring_stats.ringUsageHigh,
-			stats.rx_ch_stats.rx_ind_ring_stats.ringUsageLow,
-			stats.rx_ch_stats.rx_ind_ring_stats.RingUtilCount,
-			stats.rx_ch_stats.bam_stats.bamFifoFull,
-			stats.rx_ch_stats.bam_stats.bamFifoEmpty,
-			stats.rx_ch_stats.bam_stats.bamFifoUsageHigh,
-			stats.rx_ch_stats.bam_stats.bamFifoUsageLow,
-			stats.rx_ch_stats.bam_stats.bamUtilCount,
-			stats.rx_ch_stats.num_bam_int_handled,
-			stats.rx_ch_stats.num_db,
-			stats.rx_ch_stats.num_unexpected_db,
-			stats.rx_ch_stats.num_pkts_in_dis_uninit_state,
-			stats.rx_ch_stats.num_ic_inj_vdev_change,
-			stats.rx_ch_stats.num_ic_inj_fw_desc_change,
-			stats.rx_ch_stats.reserved1,
-			stats.rx_ch_stats.reserved2);
-		cnt += nbytes;
-	} else {
-		nbytes = scnprintf(dbg_buff, size,
-				"Fail to read WDI stats\n");
-		cnt += nbytes;
-	}
-
-	return cnt;
-}
-
-static ssize_t ipa_read_wdi(struct file *file, char __user *ubuf,
-		size_t count, loff_t *ppos)
-{
-	int ret;
-
-	ret = ipa_get_wdi(dbg_buff, IPA_MAX_MSG_LEN);
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, ret);
-}
-
-void _ipa_write_dbg_cnt_v1_1(int option)
-{
-	if (option == 1)
-		ipa_write_reg(ipa_ctx->mmio, IPA_DEBUG_CNT_CTRL_N_OFST_v1_1(0),
-				IPA_DBG_CNTR_ON);
-	else
-		ipa_write_reg(ipa_ctx->mmio, IPA_DEBUG_CNT_CTRL_N_OFST_v1_1(0),
-				IPA_DBG_CNTR_OFF);
-}
-
-void _ipa_write_dbg_cnt_v2_0(int option)
-{
-	if (option == 1)
-		ipa_write_reg(ipa_ctx->mmio, IPA_DEBUG_CNT_CTRL_N_OFST_v2_0(0),
-				IPA_DBG_CNTR_ON);
-	else
-		ipa_write_reg(ipa_ctx->mmio, IPA_DEBUG_CNT_CTRL_N_OFST_v2_0(0),
-				IPA_DBG_CNTR_OFF);
-}
-
-static ssize_t ipa_write_dbg_cnt(struct file *file, const char __user *buf,
-		size_t count, loff_t *ppos)
-{
-	unsigned long missing;
-	u32 option = 0;
-
-	if (sizeof(dbg_buff) < count + 1)
-		return -EFAULT;
-
-	missing = copy_from_user(dbg_buff, buf, count);
-	if (missing)
-		return -EFAULT;
-
-	dbg_buff[count] = '\0';
-	if (kstrtou32(dbg_buff, 0, &option))
-		return -EFAULT;
-
-	ipa_inc_client_enable_clks();
-	ipa_ctx->ctrl->ipa_write_dbg_cnt(option);
-	ipa_dec_client_disable_clks();
-
-	return count;
-}
-
-int _ipa_read_dbg_cnt_v1_1(char *buf, int max_len)
-{
-	int regval;
-
-	regval = ipa_read_reg(ipa_ctx->mmio,
-			IPA_DEBUG_CNT_REG_N_OFST_v1_1(0));
-
-	return scnprintf(buf, max_len,
-			"IPA_DEBUG_CNT_REG_0=0x%x\n", regval);
-}
-
-int _ipa_read_dbg_cnt_v2_0(char *buf, int max_len)
-{
-	int regval;
-
-	regval = ipa_read_reg(ipa_ctx->mmio,
-			IPA_DEBUG_CNT_REG_N_OFST_v2_0(0));
-
-	return scnprintf(buf, max_len,
-			"IPA_DEBUG_CNT_REG_0=0x%x\n", regval);
-}
-
-static ssize_t ipa_read_dbg_cnt(struct file *file, char __user *ubuf,
-		size_t count, loff_t *ppos)
-{
-	int nbytes;
-
-	ipa_inc_client_enable_clks();
-	nbytes = ipa_ctx->ctrl->ipa_read_dbg_cnt(dbg_buff, IPA_MAX_MSG_LEN);
-	ipa_dec_client_disable_clks();
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, nbytes);
-}
-
-static ssize_t ipa_read_msg(struct file *file, char __user *ubuf,
-		size_t count, loff_t *ppos)
-{
-	int nbytes;
-	int cnt = 0;
-	int i;
-
-	for (i = 0; i < IPA_EVENT_MAX_NUM; i++) {
-		nbytes = scnprintf(dbg_buff + cnt, IPA_MAX_MSG_LEN - cnt,
-				"msg[%u:%27s] W:%u R:%u\n", i,
-				ipa_event_name[i],
-				ipa_ctx->stats.msg_w[i],
-				ipa_ctx->stats.msg_r[i]);
-		cnt += nbytes;
-	}
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, cnt);
-}
-
-void ipa_print_ip4_nat(void)
-{
-#define ENTRY_U32_FIELDS 8
-#define NAT_ENTRY_ENABLE 0x8000
-#define NAT_ENTRY_RST_FIN_BIT 0x4000
-#define BASE_TABLE 0
-#define EXPANSION_TABLE 1
-
-	u32 *base_tbl, *indx_tbl;
-	u32 tbl_size, *tmp;
-	u32 value, i, j, rule_id;
-	u16 enable, tbl_entry, flag;
-	u32 no_entrys = 0;
-
-	value = ipa_ctx->nat_mem.public_ip_addr;
-	pr_err(
-				"Table IP Address:%d.%d.%d.%d\n",
-				((value & 0xFF000000) >> 24),
-				((value & 0x00FF0000) >> 16),
-				((value & 0x0000FF00) >> 8),
-				((value & 0x000000FF)));
-
-	pr_err("Table Size:%d\n",
-				ipa_ctx->nat_mem.size_base_tables);
-
-	pr_err("Expansion Table Size:%d\n",
-				ipa_ctx->nat_mem.size_expansion_tables-1);
-
-	if (!ipa_ctx->nat_mem.is_sys_mem)
-		pr_err("Not supported for local(shared) memory\n");
-
-	/* Print Base tables */
-	rule_id = 0;
-	for (j = 0; j < 2; j++) {
-		if (j == BASE_TABLE) {
-			tbl_size = ipa_ctx->nat_mem.size_base_tables;
-			base_tbl = (u32 *)ipa_ctx->nat_mem.ipv4_rules_addr;
-
-			pr_err("\nBase Table:\n");
-		} else {
-			tbl_size = ipa_ctx->nat_mem.size_expansion_tables-1;
-			base_tbl =
-			 (u32 *)ipa_ctx->nat_mem.ipv4_expansion_rules_addr;
-
-			pr_err("\nExpansion Base Table:\n");
-		}
-
-		if (base_tbl != NULL) {
-			for (i = 0; i <= tbl_size; i++, rule_id++) {
-				tmp = base_tbl;
-				value = tmp[4];
-				enable = ((value & 0xFFFF0000) >> 16);
-
-				if (enable & NAT_ENTRY_ENABLE) {
-					no_entrys++;
-					pr_err("Rule:%d ", rule_id);
-
-					value = *tmp;
-					pr_err(
-						"Private_IP:%d.%d.%d.%d ",
-						((value & 0xFF000000) >> 24),
-						((value & 0x00FF0000) >> 16),
-						((value & 0x0000FF00) >> 8),
-						((value & 0x000000FF)));
-					tmp++;
-
-					value = *tmp;
-					pr_err(
-						"Target_IP:%d.%d.%d.%d ",
-						((value & 0xFF000000) >> 24),
-						((value & 0x00FF0000) >> 16),
-						((value & 0x0000FF00) >> 8),
-						((value & 0x000000FF)));
-					tmp++;
-
-					value = *tmp;
-					pr_err(
-						"Next_Index:%d  Public_Port:%d ",
-						(value & 0x0000FFFF),
-						((value & 0xFFFF0000) >> 16));
-					tmp++;
-
-					value = *tmp;
-					pr_err(
-						"Private_Port:%d  Target_Port:%d ",
-						(value & 0x0000FFFF),
-						((value & 0xFFFF0000) >> 16));
-					tmp++;
-
-					value = *tmp;
-					flag = ((value & 0xFFFF0000) >> 16);
-					if (flag & NAT_ENTRY_RST_FIN_BIT) {
-						pr_err(
-								"IP_CKSM_delta:0x%x  Flags:%s ",
-							  (value & 0x0000FFFF),
-								"Direct_To_A5");
-					} else {
-						pr_err(
-							"IP_CKSM_delta:0x%x  Flags:%s ",
-							(value & 0x0000FFFF),
-							"Fwd_to_route");
-					}
-					tmp++;
-
-					value = *tmp;
-					pr_err(
-						"Time_stamp:0x%x Proto:%d ",
-						(value & 0x00FFFFFF),
-						((value & 0xFF000000) >> 24));
-					tmp++;
-
-					value = *tmp;
-					pr_err(
-						"Prev_Index:%d  Indx_tbl_entry:%d ",
-						(value & 0x0000FFFF),
-						((value & 0xFFFF0000) >> 16));
-					tmp++;
-
-					value = *tmp;
-					pr_err(
-						"TCP_UDP_cksum_delta:0x%x\n",
-						((value & 0xFFFF0000) >> 16));
-				}
-
-				base_tbl += ENTRY_U32_FIELDS;
-
-			}
-		}
-	}
-
-	/* Print Index tables */
-	rule_id = 0;
-	for (j = 0; j < 2; j++) {
-		if (j == BASE_TABLE) {
-			tbl_size = ipa_ctx->nat_mem.size_base_tables;
-			indx_tbl = (u32 *)ipa_ctx->nat_mem.index_table_addr;
-
-			pr_err("\nIndex Table:\n");
-		} else {
-			tbl_size = ipa_ctx->nat_mem.size_expansion_tables-1;
-			indx_tbl =
-			 (u32 *)ipa_ctx->nat_mem.index_table_expansion_addr;
-
-			pr_err("\nExpansion Index Table:\n");
-		}
-
-		if (indx_tbl != NULL) {
-			for (i = 0; i <= tbl_size; i++, rule_id++) {
-				tmp = indx_tbl;
-				value = *tmp;
-				tbl_entry = (value & 0x0000FFFF);
-
-				if (tbl_entry) {
-					pr_err("Rule:%d ", rule_id);
-
-					value = *tmp;
-					pr_err(
-						"Table_Entry:%d  Next_Index:%d\n",
-						tbl_entry,
-						((value & 0xFFFF0000) >> 16));
-				}
-
-				indx_tbl++;
-			}
-		}
-	}
-	pr_err("Current No. Nat Entries: %d\n", no_entrys);
-}
-
-static ssize_t ipa_read_nat4(struct file *file,
-		char __user *ubuf, size_t count,
-		loff_t *ppos) {
-	ipa_print_ip4_nat();
-
-	return 0;
-}
-
-static ssize_t ipa_rm_read_stats(struct file *file, char __user *ubuf,
-		size_t count, loff_t *ppos)
-{
-	int result, nbytes, cnt = 0;
-	result = ipa_rm_stat(dbg_buff, IPA_MAX_MSG_LEN);
-	if (result < 0) {
-		nbytes = scnprintf(dbg_buff, IPA_MAX_MSG_LEN,
-				"Error in printing RM stat %d\n", result);
-		cnt += nbytes;
-	} else
-		cnt += result;
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, cnt);
-}
-
-static ssize_t write_ipa_enable_panic(struct file *file, const char __user *buf,
-	size_t count, loff_t *ppos)
-{
-	if (copy_from_user(dbg_buff, buf, count))
-		return -EFAULT;
-
-	if (dbg_buff[0] != '0') {
-		ipa_enable_panic_dump = 1;
-		pr_err("ipa panic crash dump enabled\n");
-	} else {
-		ipa_enable_panic_dump = 0;
-		pr_err("ipa panic crash dump disabled\n");
-	}
-
-	return count;
-}
-
-static int ipa_gen_panic_notifier(struct notifier_block *this,
-	unsigned long event, void *ptr)
-{
-	int result;
-
-	if (!ipa_enable_panic_dump)
-		goto exit_panic_notifier;
-
-	pr_err("---------------- IPA INFO PANIC DUMP START ---------------");
-
-	result = ipa_rm_stat(dbg_buff, IPA_MAX_MSG_LEN);
-	if (result < 0) {
-		scnprintf(dbg_buff, IPA_MAX_MSG_LEN,
-			"Error in printing RM stats %d\n", result);
-	}
-	pr_err("RM stats:\n%s", dbg_buff);
-	pr_err("----------------------------------------------------------\n");
-	ipa_get_stats(dbg_buff, IPA_MAX_MSG_LEN);
-	pr_err("IPA statistics:\n%s", dbg_buff);
-	pr_err("----------------------------------------------------------\n");
-	pr_err("IPA proccessing context stats:\n");
-	ipa_get_proc_ctx(dbg_buff, IPA_MAX_MSG_LEN);
-	pr_err("%s", dbg_buff);
-	pr_err("----------------------------------------------------------\n");
-	pr_err("IPA hdr stats:\n");
-	ipa_get_hdr_buff(dbg_buff, IPA_MAX_MSG_LEN);
-	pr_err("%s", dbg_buff);
-	pr_err("----------------------------------------------------------\n");
-	pr_err("IPA wdi stats:\n");
-	ipa_get_wdi(dbg_buff, IPA_MAX_MSG_LEN);
-	pr_err("%s", dbg_buff);
-	pr_err("----------------------------------------------------------\n");
-	ipa_get_wstats(dbg_buff, IPA_MAX_MSG_LEN);
-	pr_err("IPA wstats:\n%s", dbg_buff);
-	pr_err("----------------------------------------------------------\n");
-	pr_err("IPA IPv4 filtering table SW view:\n");
-	ipa_print_flt(IPA_IP_v4);
-	pr_err("----------------------------------------------------------\n");
-	pr_err("IPA IPv6 filtering table SW view:\n");
-	ipa_print_flt(IPA_IP_v6);
-	pr_err("----------------------------------------------------------\n");
-	pr_err("IPA IPv4 routing table SW view:\n");
-	ipa_print_rt(IPA_IP_v4);
-	pr_err("----------------------------------------------------------\n");
-	pr_err("IPA IPv6 routing table SW view:\n");
-	ipa_print_rt(IPA_IP_v6);
-	pr_err("----------------------------------------------------------\n");
-	pr_err("IPA IPv4 NAT table SW view:\n");
-	ipa_print_ip4_nat();
-	pr_err("----------------------------------------------------------\n");
-	pr_err("IPA active end-point registers:\n");
-	ipa_print_active_ep_reg(dbg_buff, IPA_MAX_MSG_LEN);
-	pr_err("----------------------------------------------------------\n");
-
-exit_panic_notifier:
-	return NOTIFY_DONE;
-}
-
-static struct notifier_block ipa_panic_blk = {
-	.notifier_call = ipa_gen_panic_notifier,
-};
-
-void ipa_register_panic_gen_notifier(void)
-{
-	atomic_notifier_chain_register(&panic_notifier_list, &ipa_panic_blk);
-}
-
-const struct file_operations ipa_gen_reg_ops = {
-	.read = ipa_read_gen_reg,
-};
-
-const struct file_operations ipa_ep_reg_ops = {
-	.read = ipa_read_ep_reg,
-	.write = ipa_write_ep_reg,
-};
-
-const struct file_operations ipa_keep_awake_ops = {
-	.read = ipa_read_keep_awake,
-	.write = ipa_write_keep_awake,
-};
-
-const struct file_operations ipa_ep_holb_ops = {
-	.write = ipa_write_ep_holb,
-};
-
-const struct file_operations ipa_hdr_ops = {
-	.read = ipa_read_hdr,
-};
-
-const struct file_operations ipa_rt_ops = {
-	.read = ipa_read_rt,
-	.open = ipa_open_dbg,
-};
-
-const struct file_operations ipa_proc_ctx_ops = {
-	.read = ipa_read_proc_ctx,
-};
-
-const struct file_operations ipa_flt_ops = {
-	.read = ipa_read_flt,
-	.open = ipa_open_dbg,
-};
-
-const struct file_operations ipa_stats_ops = {
-	.read = ipa_read_stats,
-};
-
-const struct file_operations ipa_wstats_ops = {
-	.read = ipa_read_wstats,
-};
-
-const struct file_operations ipa_wdi_ops = {
-	.read = ipa_read_wdi,
-};
-
-const struct file_operations ipa_msg_ops = {
-	.read = ipa_read_msg,
-};
-
-const struct file_operations ipa_dbg_cnt_ops = {
-	.read = ipa_read_dbg_cnt,
-	.write = ipa_write_dbg_cnt,
-};
-
-const struct file_operations ipa_nat4_ops = {
-	.read = ipa_read_nat4,
-};
-
-const struct file_operations ipa_rm_stats = {
-	.read = ipa_rm_read_stats,
-};
-
-const struct file_operations ipa_panic_ops = {
-	.write = write_ipa_enable_panic,
-};
-
-void ipa_debugfs_init(void)
-{
-	const mode_t read_only_mode = S_IRUSR | S_IRGRP | S_IROTH;
-	const mode_t read_write_mode = S_IRUSR | S_IRGRP | S_IROTH |
-			S_IWUSR | S_IWGRP;
-	const mode_t write_only_mode = S_IWUSR | S_IWGRP;
-	struct dentry *file;
-
-	ipa_enable_panic_dump = 0;
-
-	dent = debugfs_create_dir("ipa", 0);
-	if (IS_ERR(dent)) {
-		IPAERR("fail to create folder in debug_fs.\n");
-		return;
-	}
-
-	file = debugfs_create_u32("hw_type", read_only_mode,
-			dent, &ipa_ctx->ipa_hw_type);
-	if (!file) {
-		IPAERR("could not create hw_type file\n");
-		goto fail;
-	}
-
-
-	dfile_gen_reg = debugfs_create_file("gen_reg", read_only_mode, dent, 0,
-			&ipa_gen_reg_ops);
-	if (!dfile_gen_reg || IS_ERR(dfile_gen_reg)) {
-		IPAERR("fail to create file for debug_fs gen_reg\n");
-		goto fail;
-	}
-
-	dfile_ep_reg = debugfs_create_file("ep_reg", read_write_mode, dent, 0,
-			&ipa_ep_reg_ops);
-	if (!dfile_ep_reg || IS_ERR(dfile_ep_reg)) {
-		IPAERR("fail to create file for debug_fs ep_reg\n");
-		goto fail;
-	}
-
-	dfile_keep_awake = debugfs_create_file("keep_awake", read_write_mode,
-			dent, 0, &ipa_keep_awake_ops);
-	if (!dfile_keep_awake || IS_ERR(dfile_keep_awake)) {
-		IPAERR("fail to create file for debug_fs dfile_keep_awake\n");
-		goto fail;
-	}
-
-	dfile_ep_holb = debugfs_create_file("holb", write_only_mode, dent,
-			0, &ipa_ep_holb_ops);
-	if (!dfile_ep_holb || IS_ERR(dfile_ep_holb)) {
-		IPAERR("fail to create file for debug_fs dfile_ep_hol_en\n");
-		goto fail;
-	}
-
-	dfile_hdr = debugfs_create_file("hdr", read_only_mode, dent, 0,
-			&ipa_hdr_ops);
-	if (!dfile_hdr || IS_ERR(dfile_hdr)) {
-		IPAERR("fail to create file for debug_fs hdr\n");
-		goto fail;
-	}
-
-	dfile_proc_ctx = debugfs_create_file("proc_ctx", read_only_mode, dent,
-		0, &ipa_proc_ctx_ops);
-	if (!dfile_hdr || IS_ERR(dfile_hdr)) {
-		IPAERR("fail to create file for debug_fs proc_ctx\n");
-		goto fail;
-	}
-
-	dfile_ip4_rt = debugfs_create_file("ip4_rt", read_only_mode, dent,
-			(void *)IPA_IP_v4, &ipa_rt_ops);
-	if (!dfile_ip4_rt || IS_ERR(dfile_ip4_rt)) {
-		IPAERR("fail to create file for debug_fs ip4 rt\n");
-		goto fail;
-	}
-
-	dfile_ip6_rt = debugfs_create_file("ip6_rt", read_only_mode, dent,
-			(void *)IPA_IP_v6, &ipa_rt_ops);
-	if (!dfile_ip6_rt || IS_ERR(dfile_ip6_rt)) {
-		IPAERR("fail to create file for debug_fs ip6:w" " rt\n");
-		goto fail;
-	}
-
-	dfile_ip4_flt = debugfs_create_file("ip4_flt", read_only_mode, dent,
-			(void *)IPA_IP_v4, &ipa_flt_ops);
-	if (!dfile_ip4_flt || IS_ERR(dfile_ip4_flt)) {
-		IPAERR("fail to create file for debug_fs ip4 flt\n");
-		goto fail;
-	}
-
-	dfile_ip6_flt = debugfs_create_file("ip6_flt", read_only_mode, dent,
-			(void *)IPA_IP_v6, &ipa_flt_ops);
-	if (!dfile_ip6_flt || IS_ERR(dfile_ip6_flt)) {
-		IPAERR("fail to create file for debug_fs ip6 flt\n");
-		goto fail;
-	}
-
-	dfile_stats = debugfs_create_file("stats", read_only_mode, dent, 0,
-			&ipa_stats_ops);
-	if (!dfile_stats || IS_ERR(dfile_stats)) {
-		IPAERR("fail to create file for debug_fs stats\n");
-		goto fail;
-	}
-
-	dfile_wstats = debugfs_create_file("wstats", read_only_mode,
-			dent, 0, &ipa_wstats_ops);
-	if (!dfile_wstats || IS_ERR(dfile_wstats)) {
-		IPAERR("fail to create file for debug_fs wstats\n");
-		goto fail;
-	}
-
-	dfile_wdi_stats = debugfs_create_file("wdi", read_only_mode, dent, 0,
-			&ipa_wdi_ops);
-	if (!dfile_wdi_stats || IS_ERR(dfile_wdi_stats)) {
-		IPAERR("fail to create file for debug_fs wdi stats\n");
-		goto fail;
-	}
-
-	dfile_dbg_cnt = debugfs_create_file("dbg_cnt", read_write_mode, dent, 0,
-			&ipa_dbg_cnt_ops);
-	if (!dfile_dbg_cnt || IS_ERR(dfile_dbg_cnt)) {
-		IPAERR("fail to create file for debug_fs dbg_cnt\n");
-		goto fail;
-	}
-
-	dfile_msg = debugfs_create_file("msg", read_only_mode, dent, 0,
-			&ipa_msg_ops);
-	if (!dfile_msg || IS_ERR(dfile_msg)) {
-		IPAERR("fail to create file for debug_fs msg\n");
-		goto fail;
-	}
-
-	dfile_ip4_nat = debugfs_create_file("ip4_nat", read_only_mode, dent,
-			0, &ipa_nat4_ops);
-	if (!dfile_ip4_nat || IS_ERR(dfile_ip4_nat)) {
-		IPAERR("fail to create file for debug_fs ip4 nat\n");
-		goto fail;
-	}
-
-	dfile_rm_stats = debugfs_create_file("rm_stats",
-			read_only_mode, dent, 0, &ipa_rm_stats);
-	if (!dfile_rm_stats || IS_ERR(dfile_rm_stats)) {
-		IPAERR("fail to create file for debug_fs rm_stats\n");
-		goto fail;
-	}
-
-	dfile_enable_panic_dump = debugfs_create_file("enable_panic_dump",
-		write_only_mode, dent, 0, &ipa_panic_ops);
-	if (!dfile_enable_panic_dump || IS_ERR(dfile_enable_panic_dump)) {
-		IPAERR("fail to create file for enable panic dump\n");
-		goto fail;
-	}
-
-	file = debugfs_create_u32("enable_clock_scaling", read_write_mode,
-		dent, &ipa_ctx->enable_clock_scaling);
-	if (!file) {
-		IPAERR("could not create enable_clock_scaling file\n");
-		goto fail;
-	}
-
-	file = debugfs_create_u32("clock_scaling_bw_threshold_nominal_mbps",
-		read_write_mode, dent,
-		&ipa_ctx->ctrl->clock_scaling_bw_threshold_nominal);
-	if (!file) {
-		IPAERR("could not create bw_threshold_nominal_mbps\n");
-		goto fail;
-	}
-
-	file = debugfs_create_u32("clock_scaling_bw_threshold_turbo_mbps",
-		read_write_mode, dent,
-		&ipa_ctx->ctrl->clock_scaling_bw_threshold_turbo);
-	if (!file) {
-		IPAERR("could not create bw_threshold_turbo_mbps\n");
-		goto fail;
-	}
-
-	return;
-
-fail:
-	debugfs_remove_recursive(dent);
-}
-
-void ipa_debugfs_remove(void)
-{
-	if (IS_ERR(dent)) {
-		IPAERR("ipa_debugfs_remove: folder was not created.\n");
-		return;
-	}
-	debugfs_remove_recursive(dent);
-}
-
-#else /* !CONFIG_DEBUG_FS */
-void ipa_debugfs_init(void) {}
-void ipa_debugfs_remove(void) {}
-int _ipa_read_dbg_cnt_v1_1(char *buf, int max_len)
-{
-	return 0;
-}
-int _ipa_read_ep_reg_v1_1(char *buf, int max_len, int pipe)
-{
-	return 0;
-}
-int _ipa_read_gen_reg_v1_1(char *buff, int max_len)
-{
-	return 0;
-}
-void _ipa_write_dbg_cnt_v1_1(int option) {}
-int _ipa_read_gen_reg_v2_0(char *buff, int max_len)
-{
-	return 0;
-}
-int _ipa_read_ep_reg_v2_0(char *buf, int max_len, int pipe)
-{
-	return 0;
-}
-void _ipa_write_dbg_cnt_v2_0(int option) {}
-int _ipa_read_dbg_cnt_v2_0(char *buf, int max_len)
-{
-	return 0;
-}
-void ipa_register_panic_gen_notifier(void) {}
-#endif
-
diff --git a/drivers/platform/msm/ipa/ipa_dma.c b/drivers/platform/msm/ipa/ipa_dma.c
deleted file mode 100644
index ceff291..00000000
--- a/drivers/platform/msm/ipa/ipa_dma.c
+++ /dev/null
@@ -1,880 +0,0 @@
-/* Copyright (c) 2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- */
-
-
-#include <linux/debugfs.h>
-#include <linux/export.h>
-#include <linux/delay.h>
-#include <linux/kernel.h>
-#include <linux/msm_ipa.h>
-#include <linux/mutex.h>
-#include <linux/ipa.h>
-#include "ipa_i.h"
-
-#define IPA_DMA_POLLING_MIN_SLEEP_RX 1010
-#define IPA_DMA_POLLING_MAX_SLEEP_RX 1050
-#define IPA_DMA_SYS_DESC_MAX_FIFO_SZ 0x7FF8
-#define IPA_DMA_MAX_PKT_SZ 0xFFFF
-#define IPA_DMA_MAX_PENDING_SYNC (IPA_SYS_DESC_FIFO_SZ / \
-	sizeof(struct sps_iovec) - 1)
-#define IPA_DMA_MAX_PENDING_ASYNC (IPA_DMA_SYS_DESC_MAX_FIFO_SZ / \
-	sizeof(struct sps_iovec) - 1)
-
-#define IPADMA_DRV_NAME "ipa_dma"
-
-#define IPADMA_DBG(fmt, args...) \
-	pr_debug(IPADMA_DRV_NAME " %s:%d " fmt, \
-		 __func__, __LINE__, ## args)
-#define IPADMA_ERR(fmt, args...) \
-	pr_err(IPADMA_DRV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-
-#define IPADMA_FUNC_ENTRY() \
-	IPADMA_DBG("ENTRY\n")
-
-#define IPADMA_FUNC_EXIT() \
-	IPADMA_DBG("EXIT\n")
-
-#define IS_INIT(msg) \
-	do { \
-		if (ipa_dma_ctx == NULL) { \
-			IPADMA_ERR("IPADMA isn't initialized "msg"\n"); \
-			return -EPERM; \
-		} \
-	} \
-	while (0)
-
-#define OVERLAPPING_CHECK(addr1, addr2, len)\
-	do { \
-		if ((max(addr1, addr2) - min(addr1, addr2)) < len) { \
-			IPADMA_ERR("invalid addresses - " \
-			"overlapping buffers\n"); \
-			return -EINVAL; \
-		} \
-	} \
-	while (0)
-
-#define LEN_CHECK(len) \
-	do { \
-		if (len > IPA_DMA_MAX_PKT_SZ || len <= 0) {\
-			IPADMA_ERR("invalid len, %d\n", len);\
-			return	-EINVAL;\
-		} \
-	} \
-	while (0)
-
-#ifdef CONFIG_DEBUG_FS
-#define IPADMA_MAX_MSG_LEN 1024
-static char dbg_buff[IPADMA_MAX_MSG_LEN];
-static void ipa_dma_debugfs_init(void);
-static void ipa_dma_debugfs_destroy(void);
-#else
-static void ipa_dma_debugfs_init(void) {}
-static void ipa_dma_debugfs_destroy(void) {}
-#endif
-
-/**
- * struct ipa_dma_xfer_wrapper - IPADMA transfer descr wrapper
- * @phys_addr_src: physical address of the source data to copy
- * @phys_addr_dest: physical address to store the copied data
- * @len: len in bytes to copy
- * @link: linked to the wrappers list on the proper(sync/async) cons pipe
- * @xfer_done: completion object for sync_memcpy completion
- * @callback: IPADMA client provided completion callback
- * @user1: cookie1 for above callback
- *
- * This struct can wrap both sync and async memcpy transfers descriptors.
- */
-struct ipa_dma_xfer_wrapper {
-	phys_addr_t phys_addr_src;
-	phys_addr_t phys_addr_dest;
-	u16 len;
-	struct list_head link;
-	struct completion xfer_done;
-	void (*callback)(void *user1);
-	void *user1;
-};
-
-/**
- * struct ipa_dma_ctx -IPADMA driver context information
- * @is_enabled:is ipa_dma enabled?
- * @destroy_pending: destroy ipa_dma after handling all pending memcpy
- * @ipa_dma_xfer_wrapper_cache: cache of ipa_dma_xfer_wrapper structs
- * @sync_lock: lock for synchronisation in sync_memcpy
- * @async_lock: lock for synchronisation in async_memcpy
- * @enable_lock: lock for is_enabled
- * @pending_lock: lock for synchronize is_enable and pending_cnt
- * @done: no pending works-ipadma can be destroyed
- * @ipa_dma_sync_prod_hdl: handle of sync memcpy producer
- * @ipa_dma_async_prod_hdl:handle of async memcpy producer
- * @ipa_dma_sync_cons_hdl: handle of sync memcpy consumer
- * @sync_memcpy_pending_cnt: number of pending sync memcopy operations
- * @async_memcpy_pending_cnt: number of pending async memcopy operations
- * @uc_memcpy_pending_cnt: number of pending uc memcopy operations
- * @total_sync_memcpy: total number of sync memcpy (statistics)
- * @total_async_memcpy: total number of async memcpy (statistics)
- * @total_uc_memcpy: total number of uc memcpy (statistics)
- */
-struct ipa_dma_ctx {
-	bool is_enabled;
-	bool destroy_pending;
-	struct kmem_cache *ipa_dma_xfer_wrapper_cache;
-	struct mutex sync_lock;
-	spinlock_t async_lock;
-	struct mutex enable_lock;
-	spinlock_t pending_lock;
-	struct completion done;
-	u32 ipa_dma_sync_prod_hdl;
-	u32 ipa_dma_async_prod_hdl;
-	u32 ipa_dma_sync_cons_hdl;
-	u32 ipa_dma_async_cons_hdl;
-	atomic_t sync_memcpy_pending_cnt;
-	atomic_t async_memcpy_pending_cnt;
-	atomic_t uc_memcpy_pending_cnt;
-	atomic_t total_sync_memcpy;
-	atomic_t total_async_memcpy;
-	atomic_t total_uc_memcpy;
-};
-static struct ipa_dma_ctx *ipa_dma_ctx;
-
-/**
- * ipa_dma_init() -Initialize IPADMA.
- *
- * This function initialize all IPADMA internal data and connect in dma:
- *	MEMCPY_DMA_SYNC_PROD ->MEMCPY_DMA_SYNC_CONS
- *	MEMCPY_DMA_ASYNC_PROD->MEMCPY_DMA_SYNC_CONS
- *
- * Return codes: 0: success
- *		-EFAULT: IPADMA is already initialized
- *		-ENOMEM: allocating memory error
- *		-EPERM: pipe connection failed
- */
-int ipa_dma_init(void)
-{
-	struct ipa_dma_ctx *ipa_dma_ctx_t;
-	struct ipa_sys_connect_params sys_in;
-	int res = 0;
-
-	IPADMA_FUNC_ENTRY();
-
-	if (ipa_dma_ctx) {
-		IPADMA_ERR("Already initialized.\n");
-		return -EFAULT;
-	}
-	ipa_dma_ctx_t = kzalloc(sizeof(*(ipa_dma_ctx)), GFP_KERNEL);
-
-	if (!ipa_dma_ctx_t) {
-		IPADMA_ERR("kzalloc error.\n");
-		return -ENOMEM;
-	}
-
-	ipa_dma_ctx_t->ipa_dma_xfer_wrapper_cache =
-		kmem_cache_create("IPA DMA XFER WRAPPER",
-			sizeof(struct ipa_dma_xfer_wrapper), 0, 0, NULL);
-	if (!ipa_dma_ctx_t->ipa_dma_xfer_wrapper_cache) {
-		IPAERR(":failed to create ipa dma xfer wrapper cache.\n");
-		res = -ENOMEM;
-		goto fail_mem_ctrl;
-	}
-
-	mutex_init(&ipa_dma_ctx_t->enable_lock);
-	spin_lock_init(&ipa_dma_ctx_t->async_lock);
-	mutex_init(&ipa_dma_ctx_t->sync_lock);
-	spin_lock_init(&ipa_dma_ctx_t->pending_lock);
-	init_completion(&ipa_dma_ctx_t->done);
-	ipa_dma_ctx_t->is_enabled = false;
-	ipa_dma_ctx_t->destroy_pending = false;
-	atomic_set(&ipa_dma_ctx_t->async_memcpy_pending_cnt, 0);
-	atomic_set(&ipa_dma_ctx_t->sync_memcpy_pending_cnt, 0);
-	atomic_set(&ipa_dma_ctx_t->uc_memcpy_pending_cnt, 0);
-	atomic_set(&ipa_dma_ctx_t->total_async_memcpy, 0);
-	atomic_set(&ipa_dma_ctx_t->total_sync_memcpy, 0);
-	atomic_set(&ipa_dma_ctx_t->total_uc_memcpy, 0);
-
-	/* IPADMA SYNC PROD-source for sync memcpy */
-	memset(&sys_in, 0, sizeof(struct ipa_sys_connect_params));
-	sys_in.client = IPA_CLIENT_MEMCPY_DMA_SYNC_PROD;
-	sys_in.desc_fifo_sz = IPA_SYS_DESC_FIFO_SZ;
-	sys_in.ipa_ep_cfg.mode.mode = IPA_DMA;
-	sys_in.ipa_ep_cfg.mode.dst = IPA_CLIENT_MEMCPY_DMA_SYNC_CONS;
-	sys_in.skip_ep_cfg = false;
-	if (ipa_setup_sys_pipe(&sys_in,
-		&ipa_dma_ctx_t->ipa_dma_sync_prod_hdl)) {
-		IPADMA_ERR(":setup sync prod pipe failed\n");
-		res = -EPERM;
-		goto fail_sync_prod;
-	}
-
-	/* IPADMA SYNC CONS-destination for sync memcpy */
-	memset(&sys_in, 0, sizeof(struct ipa_sys_connect_params));
-	sys_in.client = IPA_CLIENT_MEMCPY_DMA_SYNC_CONS;
-	sys_in.desc_fifo_sz = IPA_SYS_DESC_FIFO_SZ;
-	sys_in.skip_ep_cfg = false;
-	sys_in.ipa_ep_cfg.mode.mode = IPA_BASIC;
-	sys_in.notify = NULL;
-	sys_in.priv = NULL;
-	if (ipa_setup_sys_pipe(&sys_in,
-		&ipa_dma_ctx_t->ipa_dma_sync_cons_hdl)) {
-		IPADMA_ERR(":setup sync cons pipe failed.\n");
-		res = -EPERM;
-		goto fail_sync_cons;
-	}
-
-	IPADMA_DBG("SYNC MEMCPY pipes are connected\n");
-
-	/* IPADMA ASYNC PROD-source for sync memcpy */
-	memset(&sys_in, 0, sizeof(struct ipa_sys_connect_params));
-	sys_in.client = IPA_CLIENT_MEMCPY_DMA_ASYNC_PROD;
-	sys_in.desc_fifo_sz = IPA_DMA_SYS_DESC_MAX_FIFO_SZ;
-	sys_in.ipa_ep_cfg.mode.mode = IPA_DMA;
-	sys_in.ipa_ep_cfg.mode.dst = IPA_CLIENT_MEMCPY_DMA_ASYNC_CONS;
-	sys_in.skip_ep_cfg = false;
-	sys_in.notify = NULL;
-	if (ipa_setup_sys_pipe(&sys_in,
-		&ipa_dma_ctx_t->ipa_dma_async_prod_hdl)) {
-		IPADMA_ERR(":setup async prod pipe failed.\n");
-		res = -EPERM;
-		goto fail_async_prod;
-	}
-
-	/* IPADMA ASYNC CONS-destination for sync memcpy */
-	memset(&sys_in, 0, sizeof(struct ipa_sys_connect_params));
-	sys_in.client = IPA_CLIENT_MEMCPY_DMA_ASYNC_CONS;
-	sys_in.desc_fifo_sz = IPA_DMA_SYS_DESC_MAX_FIFO_SZ;
-	sys_in.skip_ep_cfg = false;
-	sys_in.ipa_ep_cfg.mode.mode = IPA_BASIC;
-	sys_in.notify = ipa_dma_async_memcpy_notify_cb;
-	sys_in.priv = NULL;
-	if (ipa_setup_sys_pipe(&sys_in,
-		&ipa_dma_ctx_t->ipa_dma_async_cons_hdl)) {
-		IPADMA_ERR(":setup async cons pipe failed.\n");
-		res = -EPERM;
-		goto fail_async_cons;
-	}
-	ipa_dma_debugfs_init();
-	ipa_dma_ctx = ipa_dma_ctx_t;
-	IPADMA_DBG("ASYNC MEMCPY pipes are connected\n");
-
-	IPADMA_FUNC_EXIT();
-	return res;
-fail_async_cons:
-	ipa_teardown_sys_pipe(ipa_dma_ctx_t->ipa_dma_async_prod_hdl);
-fail_async_prod:
-	ipa_teardown_sys_pipe(ipa_dma_ctx_t->ipa_dma_sync_cons_hdl);
-fail_sync_cons:
-	ipa_teardown_sys_pipe(ipa_dma_ctx_t->ipa_dma_sync_prod_hdl);
-fail_sync_prod:
-	kmem_cache_destroy(ipa_dma_ctx_t->ipa_dma_xfer_wrapper_cache);
-fail_mem_ctrl:
-	kfree(ipa_dma_ctx_t);
-	ipa_dma_ctx = NULL;
-	return res;
-
-}
-EXPORT_SYMBOL(ipa_dma_init);
-
-/**
- * ipa_dma_enable() -Vote for IPA clocks.
- *
- *Return codes: 0: success
- *		-EINVAL: IPADMA is not initialized
- *		-EPERM: Operation not permitted as ipa_dma is already
- *		 enabled
- */
-int ipa_dma_enable(void)
-{
-	IPADMA_FUNC_ENTRY();
-	IS_INIT("can't enable");
-	mutex_lock(&ipa_dma_ctx->enable_lock);
-	if (ipa_dma_ctx->is_enabled) {
-		IPADMA_DBG("Already enabled.\n");
-		mutex_unlock(&ipa_dma_ctx->enable_lock);
-		return -EPERM;
-	}
-	ipa_inc_client_enable_clks();
-	ipa_dma_ctx->is_enabled = true;
-	mutex_unlock(&ipa_dma_ctx->enable_lock);
-
-	IPADMA_FUNC_EXIT();
-	return 0;
-}
-EXPORT_SYMBOL(ipa_dma_enable);
-
-static bool ipa_dma_work_pending(void)
-{
-	if (atomic_read(&ipa_dma_ctx->sync_memcpy_pending_cnt)) {
-		IPADMA_DBG("pending sync\n");
-		return true;
-	}
-	if (atomic_read(&ipa_dma_ctx->async_memcpy_pending_cnt)) {
-		IPADMA_DBG("pending async\n");
-		return true;
-	}
-	if (atomic_read(&ipa_dma_ctx->uc_memcpy_pending_cnt)) {
-		IPADMA_DBG("pending uc\n");
-		return true;
-	}
-	IPADMA_DBG("no pending work\n");
-	return false;
-}
-
-/**
- * ipa_dma_disable()- Unvote for IPA clocks.
- *
- * enter to power save mode.
- *
- * Return codes: 0: success
- *		-EINVAL: IPADMA is not initialized
- *		-EPERM: Operation not permitted as ipa_dma is already
- *			diabled
- *		-EFAULT: can not disable ipa_dma as there are pending
- *			memcopy works
- */
-int ipa_dma_disable(void)
-{
-	unsigned long flags;
-	IPADMA_FUNC_ENTRY();
-
-	IS_INIT("can't disable");
-	mutex_lock(&ipa_dma_ctx->enable_lock);
-	spin_lock_irqsave(&ipa_dma_ctx->pending_lock, flags);
-	if (!ipa_dma_ctx->is_enabled) {
-		IPADMA_DBG("Already disabled.\n");
-		spin_unlock_irqrestore(&ipa_dma_ctx->pending_lock, flags);
-		mutex_unlock(&ipa_dma_ctx->enable_lock);
-		return -EPERM;
-	}
-	if (ipa_dma_work_pending()) {
-		IPADMA_ERR("There is pending work, can't disable.\n");
-		spin_unlock_irqrestore(&ipa_dma_ctx->pending_lock, flags);
-		mutex_unlock(&ipa_dma_ctx->enable_lock);
-		return -EFAULT;
-	}
-	ipa_dma_ctx->is_enabled = false;
-	spin_unlock_irqrestore(&ipa_dma_ctx->pending_lock, flags);
-	ipa_dec_client_disable_clks();
-	mutex_unlock(&ipa_dma_ctx->enable_lock);
-	IPADMA_FUNC_EXIT();
-	return 0;
-}
-EXPORT_SYMBOL(ipa_dma_disable);
-
-/**
- * ipa_dma_sync_memcpy()- Perform synchronous memcpy using IPA.
- *
- * @dest: physical address to store the copied data.
- * @src: physical address of the source data to copy.
- * @len: number of bytes to copy.
- *
- * Return codes: 0: success
- *		-EINVAL: invalid params
- *		-EPERM: operation not permitted as ipa_dma isn't enable or
- *			initialized
- *		-SPS_ERROR: on sps faliures
- *		-EFAULT: other
- */
-int ipa_dma_sync_memcpy(phys_addr_t dest, phys_addr_t src, int len)
-{
-	int ep_idx;
-	int res;
-	int i = 0;
-	struct ipa_sys_context *cons_sys;
-	struct ipa_sys_context *prod_sys;
-	struct sps_iovec iov;
-	struct ipa_dma_xfer_wrapper *xfer_descr = NULL;
-	struct ipa_dma_xfer_wrapper *head_descr = NULL;
-	unsigned long flags;
-
-	IPADMA_FUNC_ENTRY();
-
-	IS_INIT("can't memcpy");
-	OVERLAPPING_CHECK(src, dest, len);
-	LEN_CHECK(len);
-	spin_lock_irqsave(&ipa_dma_ctx->pending_lock, flags);
-	if (!ipa_dma_ctx->is_enabled) {
-		IPADMA_ERR("can't memcpy, IPADMA isn't enabled\n");
-		spin_unlock_irqrestore(&ipa_dma_ctx->pending_lock, flags);
-		return -EPERM;
-	}
-	atomic_inc(&ipa_dma_ctx->sync_memcpy_pending_cnt);
-	spin_unlock_irqrestore(&ipa_dma_ctx->pending_lock, flags);
-	if (atomic_read(&ipa_dma_ctx->sync_memcpy_pending_cnt) >=
-		IPA_DMA_MAX_PENDING_SYNC) {
-		atomic_dec(&ipa_dma_ctx->sync_memcpy_pending_cnt);
-		IPADMA_DBG("Reached pending requests limit\n");
-		return -EFAULT;
-	}
-
-	ep_idx = ipa_get_ep_mapping(IPA_CLIENT_MEMCPY_DMA_SYNC_CONS);
-	if (-1 == ep_idx) {
-		IPADMA_ERR("Client %u is not mapped\n",
-			IPA_CLIENT_MEMCPY_DMA_SYNC_CONS);
-		return -EFAULT;
-	}
-	cons_sys = ipa_ctx->ep[ep_idx].sys;
-
-	ep_idx = ipa_get_ep_mapping(IPA_CLIENT_MEMCPY_DMA_SYNC_PROD);
-	if (-1 == ep_idx) {
-		IPADMA_ERR("Client %u is not mapped\n",
-			IPA_CLIENT_MEMCPY_DMA_SYNC_PROD);
-		return -EFAULT;
-	}
-	prod_sys = ipa_ctx->ep[ep_idx].sys;
-
-	xfer_descr = kmem_cache_zalloc(ipa_dma_ctx->ipa_dma_xfer_wrapper_cache,
-					GFP_KERNEL);
-	if (!xfer_descr) {
-		IPADMA_ERR("failed to alloc xfer descr wrapper\n");
-		res = -ENOMEM;
-		goto fail_mem_alloc;
-	}
-	xfer_descr->phys_addr_dest = dest;
-	xfer_descr->phys_addr_src = src;
-	xfer_descr->len = len;
-	init_completion(&xfer_descr->xfer_done);
-
-	mutex_lock(&ipa_dma_ctx->sync_lock);
-	list_add_tail(&xfer_descr->link, &cons_sys->head_desc_list);
-	cons_sys->len++;
-	res = sps_transfer_one(cons_sys->ep->ep_hdl, dest, len, NULL, 0);
-	if (res) {
-		IPADMA_ERR("Failed: sps_transfer_one on dest descr\n");
-		goto fail_sps_send;
-	}
-	res = sps_transfer_one(prod_sys->ep->ep_hdl, src, len,
-		NULL, SPS_IOVEC_FLAG_EOT);
-	if (res) {
-		IPADMA_ERR("Failed: sps_transfer_one on src descr\n");
-		BUG();
-	}
-	head_descr = list_first_entry(&cons_sys->head_desc_list,
-				struct ipa_dma_xfer_wrapper, link);
-
-	/* in case we are not the head of the list, wait for head to wake us */
-	if (xfer_descr != head_descr) {
-		mutex_unlock(&ipa_dma_ctx->sync_lock);
-		wait_for_completion(&xfer_descr->xfer_done);
-		mutex_lock(&ipa_dma_ctx->sync_lock);
-		head_descr = list_first_entry(&cons_sys->head_desc_list,
-					struct ipa_dma_xfer_wrapper, link);
-		BUG_ON(xfer_descr != head_descr);
-	}
-	mutex_unlock(&ipa_dma_ctx->sync_lock);
-
-	do {
-		/* wait for transfer to complete */
-		res = sps_get_iovec(cons_sys->ep->ep_hdl, &iov);
-		if (res)
-			IPADMA_ERR("Failed: get_iovec, returned %d loop#:%d\n"
-			, res, i);
-
-		usleep_range(IPA_DMA_POLLING_MIN_SLEEP_RX,
-			IPA_DMA_POLLING_MAX_SLEEP_RX);
-		i++;
-	} while (iov.addr == 0);
-
-	mutex_lock(&ipa_dma_ctx->sync_lock);
-	list_del(&head_descr->link);
-	cons_sys->len--;
-	kmem_cache_free(ipa_dma_ctx->ipa_dma_xfer_wrapper_cache, xfer_descr);
-	/* wake the head of the list */
-	if (!list_empty(&cons_sys->head_desc_list)) {
-		head_descr = list_first_entry(&cons_sys->head_desc_list,
-				struct ipa_dma_xfer_wrapper, link);
-		complete(&head_descr->xfer_done);
-	}
-	mutex_unlock(&ipa_dma_ctx->sync_lock);
-
-	BUG_ON(dest != iov.addr);
-	BUG_ON(len != iov.size);
-	atomic_inc(&ipa_dma_ctx->total_sync_memcpy);
-	atomic_dec(&ipa_dma_ctx->sync_memcpy_pending_cnt);
-	if (ipa_dma_ctx->destroy_pending && !ipa_dma_work_pending())
-			complete(&ipa_dma_ctx->done);
-
-	IPADMA_FUNC_EXIT();
-	return res;
-
-fail_sps_send:
-	list_del(&xfer_descr->link);
-	cons_sys->len--;
-	mutex_unlock(&ipa_dma_ctx->sync_lock);
-	kmem_cache_free(ipa_dma_ctx->ipa_dma_xfer_wrapper_cache, xfer_descr);
-fail_mem_alloc:
-	atomic_dec(&ipa_dma_ctx->sync_memcpy_pending_cnt);
-	if (ipa_dma_ctx->destroy_pending && !ipa_dma_work_pending())
-			complete(&ipa_dma_ctx->done);
-	return res;
-}
-EXPORT_SYMBOL(ipa_dma_sync_memcpy);
-
-/**
- * ipa_dma_async_memcpy()- Perform asynchronous memcpy using IPA.
- *
- * @dest: physical address to store the copied data.
- * @src: physical address of the source data to copy.
- * @len: number of bytes to copy.
- * @user_cb: callback function to notify the client when the copy was done.
- * @user_param: cookie for user_cb.
- *
- * Return codes: 0: success
- *		-EINVAL: invalid params
- *		-EPERM: operation not permitted as ipa_dma isn't enable or
- *			initialized
- *		-SPS_ERROR: on sps faliures
- *		-EFAULT: descr fifo is full.
- */
-int ipa_dma_async_memcpy(phys_addr_t dest, phys_addr_t src, int len,
-		void (*user_cb)(void *user1), void *user_param)
-{
-	int ep_idx;
-	int res = 0;
-	struct ipa_dma_xfer_wrapper *xfer_descr = NULL;
-	struct ipa_sys_context *prod_sys;
-	struct ipa_sys_context *cons_sys;
-	unsigned long flags;
-
-	IPADMA_FUNC_ENTRY();
-	IS_INIT("can't memcpy");
-	OVERLAPPING_CHECK(src, dest, len);
-	LEN_CHECK(len);
-	if (!user_cb) {
-		IPADMA_ERR("null pointer: user_cb\n");
-		return -EINVAL;
-	}
-	spin_lock_irqsave(&ipa_dma_ctx->pending_lock, flags);
-	if (!ipa_dma_ctx->is_enabled) {
-		IPADMA_ERR("can't memcpy, IPA_DMA isn't enabled\n");
-		spin_unlock_irqrestore(&ipa_dma_ctx->pending_lock, flags);
-		return -EPERM;
-	}
-	atomic_inc(&ipa_dma_ctx->async_memcpy_pending_cnt);
-	spin_unlock_irqrestore(&ipa_dma_ctx->pending_lock, flags);
-	if (atomic_read(&ipa_dma_ctx->async_memcpy_pending_cnt) >=
-		IPA_DMA_MAX_PENDING_ASYNC) {
-		atomic_dec(&ipa_dma_ctx->async_memcpy_pending_cnt);
-		IPADMA_DBG("Reached pending requests limit\n");
-		return -EFAULT;
-	}
-
-	ep_idx = ipa_get_ep_mapping(IPA_CLIENT_MEMCPY_DMA_ASYNC_CONS);
-	if (-1 == ep_idx) {
-		IPADMA_ERR("Client %u is not mapped\n",
-			IPA_CLIENT_MEMCPY_DMA_ASYNC_CONS);
-		return -EFAULT;
-	}
-	cons_sys = ipa_ctx->ep[ep_idx].sys;
-
-	ep_idx = ipa_get_ep_mapping(IPA_CLIENT_MEMCPY_DMA_ASYNC_PROD);
-	if (-1 == ep_idx) {
-		IPADMA_ERR("Client %u is not mapped\n",
-			IPA_CLIENT_MEMCPY_DMA_SYNC_PROD);
-		return -EFAULT;
-	}
-	prod_sys = ipa_ctx->ep[ep_idx].sys;
-
-	xfer_descr = kmem_cache_zalloc(ipa_dma_ctx->ipa_dma_xfer_wrapper_cache,
-					GFP_KERNEL);
-	if (!xfer_descr) {
-		IPADMA_ERR("failed to alloc xfrer descr wrapper\n");
-		res = -ENOMEM;
-		goto fail_mem_alloc;
-	}
-	xfer_descr->phys_addr_dest = dest;
-	xfer_descr->phys_addr_src = src;
-	xfer_descr->len = len;
-	xfer_descr->callback = user_cb;
-	xfer_descr->user1 = user_param;
-
-	spin_lock_irqsave(&ipa_dma_ctx->async_lock, flags);
-	list_add_tail(&xfer_descr->link, &cons_sys->head_desc_list);
-	cons_sys->len++;
-	res = sps_transfer_one(cons_sys->ep->ep_hdl, dest, len, xfer_descr, 0);
-	if (res) {
-		IPADMA_ERR("Failed: sps_transfer_one on dest descr\n");
-		goto fail_sps_send;
-	}
-	res = sps_transfer_one(prod_sys->ep->ep_hdl, src, len,
-		NULL, SPS_IOVEC_FLAG_EOT);
-	if (res) {
-		IPADMA_ERR("Failed: sps_transfer_one on src descr\n");
-		BUG();
-		goto fail_sps_send;
-	}
-	spin_unlock_irqrestore(&ipa_dma_ctx->async_lock, flags);
-	IPADMA_FUNC_EXIT();
-	return res;
-
-fail_sps_send:
-	list_del(&xfer_descr->link);
-	spin_unlock_irqrestore(&ipa_dma_ctx->async_lock, flags);
-	kmem_cache_free(ipa_dma_ctx->ipa_dma_xfer_wrapper_cache, xfer_descr);
-fail_mem_alloc:
-	atomic_dec(&ipa_dma_ctx->async_memcpy_pending_cnt);
-	if (ipa_dma_ctx->destroy_pending && !ipa_dma_work_pending())
-			complete(&ipa_dma_ctx->done);
-	return res;
-}
-EXPORT_SYMBOL(ipa_dma_async_memcpy);
-
-/**
- * ipa_dma_uc_memcpy() - Perform a memcpy action using IPA uC
- * @dest: physical address to store the copied data.
- * @src: physical address of the source data to copy.
- * @len: number of bytes to copy.
- *
- * Return codes: 0: success
- *		-EINVAL: invalid params
- *		-EPERM: operation not permitted as ipa_dma isn't enable or
- *			initialized
- *		-EBADF: IPA uC is not loaded
- */
-int ipa_dma_uc_memcpy(phys_addr_t dest, phys_addr_t src, int len)
-{
-	int res;
-	unsigned long flags;
-
-	IPADMA_FUNC_ENTRY();
-
-	IS_INIT("can't memcpy");
-	OVERLAPPING_CHECK(src, dest, len);
-	LEN_CHECK(len);
-
-	spin_lock_irqsave(&ipa_dma_ctx->pending_lock, flags);
-	if (!ipa_dma_ctx->is_enabled) {
-		IPADMA_ERR("can't memcpy, IPADMA isn't enabled\n");
-		spin_unlock_irqrestore(&ipa_dma_ctx->pending_lock, flags);
-		return -EPERM;
-	}
-	atomic_inc(&ipa_dma_ctx->uc_memcpy_pending_cnt);
-	spin_unlock_irqrestore(&ipa_dma_ctx->pending_lock, flags);
-
-	res = ipa_uc_memcpy(dest, src, len);
-	if (res) {
-		IPADMA_ERR("ipa_uc_memcpy failed %d\n", res);
-		goto dec_and_exit;
-	}
-
-	atomic_inc(&ipa_dma_ctx->total_uc_memcpy);
-	res = 0;
-dec_and_exit:
-	atomic_dec(&ipa_dma_ctx->uc_memcpy_pending_cnt);
-	if (ipa_dma_ctx->destroy_pending && !ipa_dma_work_pending())
-		complete(&ipa_dma_ctx->done);
-	IPADMA_FUNC_EXIT();
-	return res;
-}
-EXPORT_SYMBOL(ipa_dma_uc_memcpy);
-
-/**
- * ipa_dma_destroy() -teardown IPADMA pipes and release ipadma.
- *
- * this is a blocking function, returns just after destroying IPADMA.
- */
-void ipa_dma_destroy(void)
-{
-	int res = 0;
-	IPADMA_FUNC_ENTRY();
-
-	if (!ipa_dma_ctx) {
-		IPADMA_DBG("IPADMA isn't initialized\n");
-		return;
-	}
-
-	if (ipa_dma_work_pending()) {
-		ipa_dma_ctx->destroy_pending = true;
-		IPADMA_DBG("There are pending memcpy, wait for completion\n");
-		wait_for_completion(&ipa_dma_ctx->done);
-	}
-
-	res = ipa_teardown_sys_pipe(ipa_dma_ctx->ipa_dma_async_cons_hdl);
-	if (res)
-		IPADMA_ERR("teardown IPADMA ASYNC CONS failed\n");
-	ipa_dma_ctx->ipa_dma_async_cons_hdl = 0;
-	res = ipa_teardown_sys_pipe(ipa_dma_ctx->ipa_dma_sync_cons_hdl);
-	if (res)
-		IPADMA_ERR("teardown IPADMA SYNC CONS failed\n");
-	ipa_dma_ctx->ipa_dma_sync_cons_hdl = 0;
-	res = ipa_teardown_sys_pipe(ipa_dma_ctx->ipa_dma_async_prod_hdl);
-	if (res)
-		IPADMA_ERR("teardown IPADMA ASYNC PROD failed\n");
-	ipa_dma_ctx->ipa_dma_async_prod_hdl = 0;
-	res = ipa_teardown_sys_pipe(ipa_dma_ctx->ipa_dma_sync_prod_hdl);
-	if (res)
-		IPADMA_ERR("teardown IPADMA SYNC PROD failed\n");
-	ipa_dma_ctx->ipa_dma_sync_prod_hdl = 0;
-
-	ipa_dma_debugfs_destroy();
-	kmem_cache_destroy(ipa_dma_ctx->ipa_dma_xfer_wrapper_cache);
-	kfree(ipa_dma_ctx);
-	ipa_dma_ctx = NULL;
-
-	IPADMA_FUNC_EXIT();
-	return;
-}
-EXPORT_SYMBOL(ipa_dma_destroy);
-
-/**
- * ipa_dma_async_memcpy_notify_cb() -Callback function which will be called by
- * IPA driver after getting notify from SPS driver or poll mode on Rx operation
- * is completed (data was written to dest descriptor on async_cons ep).
- *
- * @priv -not in use.
- * @evt - event name - IPA_RECIVE.
- * @data -the iovec.
- */
-void ipa_dma_async_memcpy_notify_cb(void *priv
-			, enum ipa_dp_evt_type evt, unsigned long data)
-{
-	int ep_idx = 0;
-	struct sps_iovec *iov = (struct sps_iovec *) data;
-	struct ipa_dma_xfer_wrapper *xfer_descr_expected;
-	struct ipa_sys_context *sys;
-	unsigned long flags;
-
-	IPADMA_FUNC_ENTRY();
-
-	ep_idx = ipa_get_ep_mapping(IPA_CLIENT_MEMCPY_DMA_ASYNC_CONS);
-	sys = ipa_ctx->ep[ep_idx].sys;
-
-	spin_lock_irqsave(&ipa_dma_ctx->async_lock, flags);
-	xfer_descr_expected = list_first_entry(&sys->head_desc_list,
-				 struct ipa_dma_xfer_wrapper, link);
-	list_del(&xfer_descr_expected->link);
-	sys->len--;
-	spin_unlock_irqrestore(&ipa_dma_ctx->async_lock, flags);
-
-	BUG_ON(xfer_descr_expected->phys_addr_dest != iov->addr);
-	BUG_ON(xfer_descr_expected->len != iov->size);
-
-	atomic_inc(&ipa_dma_ctx->total_async_memcpy);
-	atomic_dec(&ipa_dma_ctx->async_memcpy_pending_cnt);
-	xfer_descr_expected->callback(xfer_descr_expected->user1);
-
-	kmem_cache_free(ipa_dma_ctx->ipa_dma_xfer_wrapper_cache,
-		xfer_descr_expected);
-
-	if (ipa_dma_ctx->destroy_pending && !ipa_dma_work_pending())
-			complete(&ipa_dma_ctx->done);
-
-	IPADMA_FUNC_EXIT();
-	return;
-}
-
-#ifdef CONFIG_DEBUG_FS
-static struct dentry *dent;
-static struct dentry *dfile_info;
-
-static ssize_t ipa_dma_debugfs_read(struct file *file, char __user *ubuf,
-				 size_t count, loff_t *ppos)
-{
-	int nbytes = 0;
-
-	if (!ipa_dma_ctx) {
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPADMA_MAX_MSG_LEN - nbytes,
-			"Not initialized\n");
-	} else {
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPADMA_MAX_MSG_LEN - nbytes,
-			"Status:\n	IPADMA is %s\n",
-			(ipa_dma_ctx->is_enabled) ? "Enabled" : "Disabled");
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPADMA_MAX_MSG_LEN - nbytes,
-			"Statistics:\n	total sync memcpy: %d\n	",
-			atomic_read(&ipa_dma_ctx->total_sync_memcpy));
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPADMA_MAX_MSG_LEN - nbytes,
-			"total async memcpy: %d\n	",
-			atomic_read(&ipa_dma_ctx->total_async_memcpy));
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPADMA_MAX_MSG_LEN - nbytes,
-			"pending sync memcpy jobs: %d\n	",
-			atomic_read(&ipa_dma_ctx->sync_memcpy_pending_cnt));
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPADMA_MAX_MSG_LEN - nbytes,
-			"pending async memcpy jobs: %d\n",
-			atomic_read(&ipa_dma_ctx->async_memcpy_pending_cnt));
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPADMA_MAX_MSG_LEN - nbytes,
-			"pending uc memcpy jobs: %d\n",
-			atomic_read(&ipa_dma_ctx->uc_memcpy_pending_cnt));
-	}
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, nbytes);
-}
-
-static ssize_t ipa_dma_debugfs_reset_statistics(struct file *file,
-					const char __user *ubuf,
-					size_t count,
-					loff_t *ppos)
-{
-	unsigned long missing;
-	s8 in_num = 0;
-
-	if (sizeof(dbg_buff) < count + 1)
-		return -EFAULT;
-
-	missing = copy_from_user(dbg_buff, ubuf, count);
-	if (missing)
-		return -EFAULT;
-
-	dbg_buff[count] = '\0';
-	if (kstrtos8(dbg_buff, 0, &in_num))
-		return -EFAULT;
-	switch (in_num) {
-	case 0:
-		if (ipa_dma_work_pending())
-			IPADMA_DBG("Note, there are pending memcpy\n");
-
-		atomic_set(&ipa_dma_ctx->total_async_memcpy, 0);
-		atomic_set(&ipa_dma_ctx->total_sync_memcpy, 0);
-		break;
-	default:
-		IPADMA_ERR("invalid argument: To reset statistics echo 0\n");
-		break;
-	}
-	return count;
-}
-
-const struct file_operations ipadma_stats_ops = {
-	.read = ipa_dma_debugfs_read,
-	.write = ipa_dma_debugfs_reset_statistics,
-};
-
-static void ipa_dma_debugfs_init(void)
-{
-	const mode_t read_write_mode = S_IRUSR | S_IRGRP | S_IROTH |
-			S_IWUSR | S_IWGRP | S_IWOTH;
-
-	dent = debugfs_create_dir("ipa_dma", 0);
-	if (IS_ERR(dent)) {
-		IPADMA_ERR("fail to create folder ipa_dma\n");
-		return;
-	}
-
-	dfile_info =
-		debugfs_create_file("info", read_write_mode, dent,
-				 0, &ipadma_stats_ops);
-	if (!dfile_info || IS_ERR(dfile_info)) {
-		IPADMA_ERR("fail to create file stats\n");
-		goto fail;
-	}
-	return;
-fail:
-	debugfs_remove_recursive(dent);
-}
-
-static void ipa_dma_debugfs_destroy(void)
-{
-	debugfs_remove_recursive(dent);
-}
-
-#endif /* !CONFIG_DEBUG_FS */
diff --git a/drivers/platform/msm/ipa/ipa_dp.c b/drivers/platform/msm/ipa/ipa_dp.c
deleted file mode 100644
index b34e83a..00000000
--- a/drivers/platform/msm/ipa/ipa_dp.c
+++ /dev/null
@@ -1,3307 +0,0 @@
-/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/delay.h>
-#include <linux/device.h>
-#include <linux/dmapool.h>
-#include <linux/list.h>
-#include <linux/netdevice.h>
-#include "ipa_i.h"
-
-
-#define IPA_LAST_DESC_CNT 0xFFFF
-#define POLLING_INACTIVITY_RX 40
-#define POLLING_MIN_SLEEP_RX 1010
-#define POLLING_MAX_SLEEP_RX 1050
-#define POLLING_INACTIVITY_TX 40
-#define POLLING_MIN_SLEEP_TX 400
-#define POLLING_MAX_SLEEP_TX 500
-/* 8K less 1 nominal MTU (1500 bytes) rounded to units of KB */
-#define IPA_MTU 1500
-#define IPA_GENERIC_AGGR_BYTE_LIMIT 6
-#define IPA_GENERIC_AGGR_TIME_LIMIT 1
-#define IPA_GENERIC_AGGR_PKT_LIMIT 0
-
-#define IPA_GENERIC_RX_BUFF_BASE_SZ 8192
-#define IPA_REAL_GENERIC_RX_BUFF_SZ(X) (SKB_DATA_ALIGN(\
-		(X) + NET_SKB_PAD) +\
-		SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
-#define IPA_GENERIC_RX_BUFF_SZ(X) ((X) -\
-		(IPA_REAL_GENERIC_RX_BUFF_SZ(X) - (X)))
-#define IPA_GENERIC_RX_BUFF_LIMIT (\
-		IPA_REAL_GENERIC_RX_BUFF_SZ(\
-		IPA_GENERIC_RX_BUFF_BASE_SZ) -\
-		IPA_GENERIC_RX_BUFF_BASE_SZ)
-
-#define IPA_RX_BUFF_CLIENT_HEADROOM 256
-
-/* less 1 nominal MTU (1500 bytes) rounded to units of KB */
-#define IPA_ADJUST_AGGR_BYTE_LIMIT(X) (((X) - IPA_MTU)/1000)
-
-#define IPA_WLAN_RX_POOL_SZ 100
-#define IPA_WLAN_RX_POOL_SZ_LOW_WM 5
-#define IPA_WLAN_RX_BUFF_SZ 2048
-#define IPA_WLAN_COMM_RX_POOL_LOW 100
-#define IPA_WLAN_COMM_RX_POOL_HIGH 900
-
-#define IPA_ODU_RX_BUFF_SZ 2048
-#define IPA_ODU_RX_POOL_SZ 64
-#define IPA_SIZE_DL_CSUM_META_TRAILER 8
-
-#define IPA_HEADROOM 128
-
-static struct sk_buff *ipa_get_skb_ipa_rx(unsigned int len, gfp_t flags);
-static void ipa_replenish_wlan_rx_cache(struct ipa_sys_context *sys);
-static void ipa_replenish_rx_cache(struct ipa_sys_context *sys);
-static void replenish_rx_work_func(struct work_struct *work);
-static void ipa_wq_handle_rx(struct work_struct *work);
-static void ipa_wq_handle_tx(struct work_struct *work);
-static void ipa_wq_rx_common(struct ipa_sys_context *sys, u32 size);
-static void ipa_wlan_wq_rx_common(struct ipa_sys_context *sys,
-				u32 size);
-static int ipa_assign_policy(struct ipa_sys_connect_params *in,
-		struct ipa_sys_context *sys);
-static void ipa_cleanup_rx(struct ipa_sys_context *sys);
-static void ipa_wq_rx_avail(struct work_struct *work);
-static void ipa_alloc_wlan_rx_common_cache(u32 size);
-static void ipa_cleanup_wlan_rx_common_cache(void);
-static void ipa_wq_repl_rx(struct work_struct *work);
-static void ipa_dma_memcpy_notify(struct ipa_sys_context *sys,
-		struct sps_iovec *iovec);
-
-static u32 ipa_adjust_ra_buff_base_sz(u32 aggr_byte_limit);
-
-static void ipa_wq_write_done_common(struct ipa_sys_context *sys, u32 cnt)
-{
-	struct ipa_tx_pkt_wrapper *tx_pkt_expected;
-	int i;
-
-	for (i = 0; i < cnt; i++) {
-		spin_lock_bh(&sys->spinlock);
-		if (unlikely(list_empty(&sys->head_desc_list))) {
-			spin_unlock_bh(&sys->spinlock);
-			return;
-		}
-		tx_pkt_expected = list_first_entry(&sys->head_desc_list,
-						   struct ipa_tx_pkt_wrapper,
-						   link);
-		list_del(&tx_pkt_expected->link);
-		sys->len--;
-		spin_unlock_bh(&sys->spinlock);
-		if (!tx_pkt_expected->no_unmap_dma)
-			dma_unmap_single(ipa_ctx->pdev,
-					tx_pkt_expected->mem.phys_base,
-					tx_pkt_expected->mem.size,
-					DMA_TO_DEVICE);
-		if (tx_pkt_expected->callback)
-			tx_pkt_expected->callback(tx_pkt_expected->user1,
-					tx_pkt_expected->user2);
-		if (tx_pkt_expected->cnt > 1 &&
-				tx_pkt_expected->cnt != IPA_LAST_DESC_CNT) {
-			if (tx_pkt_expected->cnt == IPA_NUM_DESC_PER_SW_TX) {
-				dma_pool_free(ipa_ctx->dma_pool,
-					tx_pkt_expected->mult.base,
-					tx_pkt_expected->mult.phys_base);
-			} else {
-				dma_unmap_single(ipa_ctx->pdev,
-					tx_pkt_expected->mult.phys_base,
-					tx_pkt_expected->mult.size,
-					DMA_TO_DEVICE);
-				kfree(tx_pkt_expected->mult.base);
-			}
-		}
-		kmem_cache_free(ipa_ctx->tx_pkt_wrapper_cache, tx_pkt_expected);
-	}
-}
-
-static void ipa_wq_write_done_status(int src_pipe)
-{
-	struct ipa_tx_pkt_wrapper *tx_pkt_expected;
-	struct ipa_sys_context *sys;
-	u32 cnt;
-
-	WARN_ON(src_pipe >= ipa_ctx->ipa_num_pipes);
-
-	if (!ipa_ctx->ep[src_pipe].status.status_en)
-		return;
-
-	sys = ipa_ctx->ep[src_pipe].sys;
-	if (!sys)
-		return;
-
-	spin_lock_bh(&sys->spinlock);
-	if (unlikely(list_empty(&sys->head_desc_list))) {
-		spin_unlock_bh(&sys->spinlock);
-		return;
-	}
-	tx_pkt_expected = list_first_entry(&sys->head_desc_list,
-					   struct ipa_tx_pkt_wrapper,
-					   link);
-	cnt = tx_pkt_expected->cnt;
-	spin_unlock_bh(&sys->spinlock);
-	ipa_wq_write_done_common(sys, cnt);
-}
-
-/**
- * ipa_write_done() - this function will be (eventually) called when a Tx
- * operation is complete
- * * @work:	work_struct used by the work queue
- *
- * Will be called in deferred context.
- * - invoke the callback supplied by the client who sent this command
- * - iterate over all packets and validate that
- *   the order for sent packet is the same as expected
- * - delete all the tx packet descriptors from the system
- *   pipe context (not needed anymore)
- * - return the tx buffer back to dma_pool
- */
-static void ipa_wq_write_done(struct work_struct *work)
-{
-	struct ipa_tx_pkt_wrapper *tx_pkt;
-	u32 cnt;
-	struct ipa_sys_context *sys;
-
-	tx_pkt = container_of(work, struct ipa_tx_pkt_wrapper, work);
-	cnt = tx_pkt->cnt;
-	sys = tx_pkt->sys;
-
-	ipa_wq_write_done_common(sys, cnt);
-}
-
-static int ipa_handle_tx_core(struct ipa_sys_context *sys, bool process_all,
-		bool in_poll_state)
-{
-	struct sps_iovec iov;
-	int ret;
-	int cnt = 0;
-
-	while ((in_poll_state ? atomic_read(&sys->curr_polling_state) :
-				!atomic_read(&sys->curr_polling_state))) {
-		if (cnt && !process_all)
-			break;
-		ret = sps_get_iovec(sys->ep->ep_hdl, &iov);
-		if (ret) {
-			IPAERR("sps_get_iovec failed %d\n", ret);
-			break;
-		}
-
-		if (iov.addr == 0)
-			break;
-
-		ipa_wq_write_done_common(sys, 1);
-		cnt++;
-	};
-
-	return cnt;
-}
-
-/**
- * ipa_tx_switch_to_intr_mode() - Operate the Tx data path in interrupt mode
- */
-static void ipa_tx_switch_to_intr_mode(struct ipa_sys_context *sys)
-{
-	int ret;
-
-	if (!atomic_read(&sys->curr_polling_state)) {
-		IPAERR("already in intr mode\n");
-		goto fail;
-	}
-
-	ret = sps_get_config(sys->ep->ep_hdl, &sys->ep->connect);
-	if (ret) {
-		IPAERR("sps_get_config() failed %d\n", ret);
-		goto fail;
-	}
-	sys->event.options = SPS_O_EOT;
-	ret = sps_register_event(sys->ep->ep_hdl, &sys->event);
-	if (ret) {
-		IPAERR("sps_register_event() failed %d\n", ret);
-		goto fail;
-	}
-	sys->ep->connect.options =
-		SPS_O_AUTO_ENABLE | SPS_O_ACK_TRANSFERS | SPS_O_EOT;
-	ret = sps_set_config(sys->ep->ep_hdl, &sys->ep->connect);
-	if (ret) {
-		IPAERR("sps_set_config() failed %d\n", ret);
-		goto fail;
-	}
-	atomic_set(&sys->curr_polling_state, 0);
-	ipa_handle_tx_core(sys, true, false);
-	ipa_dec_release_wakelock();
-	return;
-
-fail:
-	queue_delayed_work(sys->wq, &sys->switch_to_intr_work,
-			msecs_to_jiffies(1));
-	return;
-}
-
-static void ipa_handle_tx(struct ipa_sys_context *sys)
-{
-	int inactive_cycles = 0;
-	int cnt;
-
-	ipa_inc_client_enable_clks();
-	do {
-		cnt = ipa_handle_tx_core(sys, true, true);
-		if (cnt == 0) {
-			inactive_cycles++;
-			usleep_range(POLLING_MIN_SLEEP_TX,
-					POLLING_MAX_SLEEP_TX);
-		} else {
-			inactive_cycles = 0;
-		}
-	} while (inactive_cycles <= POLLING_INACTIVITY_TX);
-
-	ipa_tx_switch_to_intr_mode(sys);
-	ipa_dec_client_disable_clks();
-}
-
-static void ipa_wq_handle_tx(struct work_struct *work)
-{
-	struct ipa_sys_context *sys;
-	sys = container_of(work, struct ipa_sys_context, work);
-	ipa_handle_tx(sys);
-}
-
-/**
- * ipa_send_one() - Send a single descriptor
- * @sys:	system pipe context
- * @desc:	descriptor to send
- * @in_atomic:  whether caller is in atomic context
- *
- * - Allocate tx_packet wrapper
- * - transfer data to the IPA
- * - after the transfer was done the SPS will
- *   notify the sending user via ipa_sps_irq_comp_tx()
- *
- * Return codes: 0: success, -EFAULT: failure
- */
-int ipa_send_one(struct ipa_sys_context *sys, struct ipa_desc *desc,
-		bool in_atomic)
-{
-	struct ipa_tx_pkt_wrapper *tx_pkt;
-	int result;
-	u16 sps_flags = SPS_IOVEC_FLAG_EOT;
-	dma_addr_t dma_address;
-	u16 len;
-	u32 mem_flag = GFP_ATOMIC;
-
-	if (unlikely(!in_atomic))
-		mem_flag = GFP_KERNEL;
-
-	tx_pkt = kmem_cache_zalloc(ipa_ctx->tx_pkt_wrapper_cache, mem_flag);
-	if (!tx_pkt) {
-		IPAERR("failed to alloc tx wrapper\n");
-		goto fail_mem_alloc;
-	}
-
-	if (!desc->dma_address_valid) {
-		dma_address = dma_map_single(ipa_ctx->pdev, desc->pyld,
-			desc->len, DMA_TO_DEVICE);
-	} else {
-		dma_address = desc->dma_address;
-		tx_pkt->no_unmap_dma = true;
-	}
-	if (!dma_address) {
-		IPAERR("failed to DMA wrap\n");
-		goto fail_dma_map;
-	}
-
-	INIT_LIST_HEAD(&tx_pkt->link);
-	tx_pkt->type = desc->type;
-	tx_pkt->cnt = 1;    /* only 1 desc in this "set" */
-
-	tx_pkt->mem.phys_base = dma_address;
-	tx_pkt->mem.base = desc->pyld;
-	tx_pkt->mem.size = desc->len;
-	tx_pkt->sys = sys;
-	tx_pkt->callback = desc->callback;
-	tx_pkt->user1 = desc->user1;
-	tx_pkt->user2 = desc->user2;
-
-	/*
-	 * Special treatment for immediate commands, where the structure of the
-	 * descriptor is different
-	 */
-	if (desc->type == IPA_IMM_CMD_DESC) {
-		sps_flags |= SPS_IOVEC_FLAG_IMME;
-		len = desc->opcode;
-		IPADBG("sending cmd=%d pyld_len=%d sps_flags=%x\n",
-				desc->opcode, desc->len, sps_flags);
-		IPA_DUMP_BUFF(desc->pyld, dma_address, desc->len);
-	} else {
-		len = desc->len;
-	}
-
-	INIT_WORK(&tx_pkt->work, ipa_wq_write_done);
-
-	spin_lock_bh(&sys->spinlock);
-	list_add_tail(&tx_pkt->link, &sys->head_desc_list);
-	result = sps_transfer_one(sys->ep->ep_hdl, dma_address, len, tx_pkt,
-			sps_flags);
-	if (result) {
-		IPAERR("sps_transfer_one failed rc=%d\n", result);
-		goto fail_sps_send;
-	}
-
-	spin_unlock_bh(&sys->spinlock);
-
-	return 0;
-
-fail_sps_send:
-	list_del(&tx_pkt->link);
-	spin_unlock_bh(&sys->spinlock);
-	dma_unmap_single(ipa_ctx->pdev, dma_address, desc->len, DMA_TO_DEVICE);
-fail_dma_map:
-	kmem_cache_free(ipa_ctx->tx_pkt_wrapper_cache, tx_pkt);
-fail_mem_alloc:
-	return -EFAULT;
-}
-
-/**
- * ipa_send() - Send multiple descriptors in one HW transaction
- * @sys: system pipe context
- * @num_desc: number of packets
- * @desc: packets to send (may be immediate command or data)
- * @in_atomic:  whether caller is in atomic context
- *
- * This function is used for system-to-bam connection.
- * - SPS driver expect struct sps_transfer which will contain all the data
- *   for a transaction
- * - ipa_tx_pkt_wrapper will be used for each ipa
- *   descriptor (allocated from wrappers cache)
- * - The wrapper struct will be configured for each ipa-desc payload and will
- *   contain information which will be later used by the user callbacks
- * - each transfer will be made by calling to sps_transfer()
- * - Each packet (command or data) that will be sent will also be saved in
- *   ipa_sys_context for later check that all data was sent
- *
- * Return codes: 0: success, -EFAULT: failure
- */
-int ipa_send(struct ipa_sys_context *sys, u32 num_desc, struct ipa_desc *desc,
-		bool in_atomic)
-{
-	struct ipa_tx_pkt_wrapper *tx_pkt;
-	struct ipa_tx_pkt_wrapper *next_pkt;
-	struct sps_transfer transfer = { 0 };
-	struct sps_iovec *iovec;
-	dma_addr_t dma_addr;
-	int i = 0;
-	int j;
-	int result;
-	int fail_dma_wrap = 0;
-	uint size = num_desc * sizeof(struct sps_iovec);
-	u32 mem_flag = GFP_ATOMIC;
-
-	if (unlikely(!in_atomic))
-		mem_flag = GFP_KERNEL;
-
-	if (num_desc == IPA_NUM_DESC_PER_SW_TX) {
-		transfer.iovec = dma_pool_alloc(ipa_ctx->dma_pool, mem_flag,
-				&dma_addr);
-		if (!transfer.iovec) {
-			IPAERR("fail to alloc dma mem for sps xfr buff\n");
-			return -EFAULT;
-		}
-	} else {
-		transfer.iovec = kmalloc(size, mem_flag);
-		if (!transfer.iovec) {
-			IPAERR("fail to alloc mem for sps xfr buff ");
-			IPAERR("num_desc = %d size = %d\n", num_desc, size);
-			return -EFAULT;
-		}
-		dma_addr  = dma_map_single(ipa_ctx->pdev,
-				transfer.iovec, size, DMA_TO_DEVICE);
-		if (!dma_addr) {
-			IPAERR("dma_map_single failed for sps xfr buff\n");
-			kfree(transfer.iovec);
-			return -EFAULT;
-		}
-	}
-
-	transfer.iovec_phys = dma_addr;
-	transfer.iovec_count = num_desc;
-	spin_lock_bh(&sys->spinlock);
-
-	for (i = 0; i < num_desc; i++) {
-		fail_dma_wrap = 0;
-		tx_pkt = kmem_cache_zalloc(ipa_ctx->tx_pkt_wrapper_cache,
-					   mem_flag);
-		if (!tx_pkt) {
-			IPAERR("failed to alloc tx wrapper\n");
-			goto failure;
-		}
-		/*
-		 * first desc of set is "special" as it holds the count and
-		 * other info
-		 */
-		if (i == 0) {
-			transfer.user = tx_pkt;
-			tx_pkt->mult.phys_base = dma_addr;
-			tx_pkt->mult.base = transfer.iovec;
-			tx_pkt->mult.size = size;
-			tx_pkt->cnt = num_desc;
-			INIT_WORK(&tx_pkt->work, ipa_wq_write_done);
-		}
-
-		iovec = &transfer.iovec[i];
-		iovec->flags = 0;
-
-		INIT_LIST_HEAD(&tx_pkt->link);
-		tx_pkt->type = desc[i].type;
-
-		tx_pkt->mem.base = desc[i].pyld;
-		tx_pkt->mem.size = desc[i].len;
-
-		if (!desc->dma_address_valid) {
-			tx_pkt->mem.phys_base =
-				dma_map_single(ipa_ctx->pdev,
-				tx_pkt->mem.base,
-				tx_pkt->mem.size,
-				DMA_TO_DEVICE);
-		} else {
-			tx_pkt->mem.phys_base = desc->dma_address;
-			tx_pkt->no_unmap_dma = true;
-		}
-
-		if (!tx_pkt->mem.phys_base) {
-			IPAERR("failed to alloc tx wrapper\n");
-			fail_dma_wrap = 1;
-			goto failure;
-		}
-
-		tx_pkt->sys = sys;
-		tx_pkt->callback = desc[i].callback;
-		tx_pkt->user1 = desc[i].user1;
-		tx_pkt->user2 = desc[i].user2;
-
-		/*
-		 * Point the iovec to the buffer and
-		 * add this packet to system pipe context.
-		 */
-		iovec->addr = tx_pkt->mem.phys_base;
-		list_add_tail(&tx_pkt->link, &sys->head_desc_list);
-
-		/*
-		 * Special treatment for immediate commands, where the structure
-		 * of the descriptor is different
-		 */
-		if (desc[i].type == IPA_IMM_CMD_DESC) {
-			iovec->size = desc[i].opcode;
-			iovec->flags |= SPS_IOVEC_FLAG_IMME;
-			IPA_DUMP_BUFF(desc[i].pyld,
-					tx_pkt->mem.phys_base, desc[i].len);
-		} else {
-			iovec->size = desc[i].len;
-		}
-
-		if (i == (num_desc - 1)) {
-			iovec->flags |= SPS_IOVEC_FLAG_EOT;
-			/* "mark" the last desc */
-			tx_pkt->cnt = IPA_LAST_DESC_CNT;
-		}
-	}
-
-	result = sps_transfer(sys->ep->ep_hdl, &transfer);
-	if (result) {
-		IPAERR("sps_transfer failed rc=%d\n", result);
-		goto failure;
-	}
-
-	spin_unlock_bh(&sys->spinlock);
-	return 0;
-
-failure:
-	tx_pkt = transfer.user;
-	for (j = 0; j < i; j++) {
-		next_pkt = list_next_entry(tx_pkt, link);
-		list_del(&tx_pkt->link);
-		dma_unmap_single(ipa_ctx->pdev, tx_pkt->mem.phys_base,
-				tx_pkt->mem.size,
-				DMA_TO_DEVICE);
-		kmem_cache_free(ipa_ctx->tx_pkt_wrapper_cache, tx_pkt);
-		tx_pkt = next_pkt;
-	}
-	if (i < num_desc)
-		/* last desc failed */
-		if (fail_dma_wrap)
-			kmem_cache_free(ipa_ctx->tx_pkt_wrapper_cache, tx_pkt);
-	if (transfer.iovec_phys) {
-		if (num_desc == IPA_NUM_DESC_PER_SW_TX) {
-			dma_pool_free(ipa_ctx->dma_pool, transfer.iovec,
-					transfer.iovec_phys);
-		} else {
-			dma_unmap_single(ipa_ctx->pdev, transfer.iovec_phys,
-					size, DMA_TO_DEVICE);
-			kfree(transfer.iovec);
-		}
-	}
-	spin_unlock_bh(&sys->spinlock);
-	return -EFAULT;
-}
-
-/**
- * ipa_sps_irq_cmd_ack - callback function which will be called by SPS driver after an
- * immediate command is complete.
- * @user1:	pointer to the descriptor of the transfer
- * @user2:
- *
- * Complete the immediate commands completion object, this will release the
- * thread which waits on this completion object (ipa_send_cmd())
- */
-static void ipa_sps_irq_cmd_ack(void *user1, int user2)
-{
-	struct ipa_desc *desc = (struct ipa_desc *)user1;
-
-	if (!desc) {
-		IPAERR("desc is NULL\n");
-		WARN_ON(1);
-		return;
-	}
-	IPADBG("got ack for cmd=%d\n", desc->opcode);
-	complete(&desc->xfer_done);
-}
-
-/**
- * ipa_send_cmd - send immediate commands
- * @num_desc:	number of descriptors within the desc struct
- * @descr:	descriptor structure
- *
- * Function will block till command gets ACK from IPA HW, caller needs
- * to free any resources it allocated after function returns
- * The callback in ipa_desc should not be set by the caller
- * for this function.
- */
-int ipa_send_cmd(u16 num_desc, struct ipa_desc *descr)
-{
-	struct ipa_desc *desc;
-	int result = 0;
-	struct ipa_sys_context *sys;
-	int ep_idx;
-
-	IPADBG("sending command\n");
-
-	ep_idx = ipa_get_ep_mapping(IPA_CLIENT_APPS_CMD_PROD);
-	if (-1 == ep_idx) {
-		IPAERR("Client %u is not mapped\n",
-			IPA_CLIENT_APPS_CMD_PROD);
-		return -EFAULT;
-	}
-	sys = ipa_ctx->ep[ep_idx].sys;
-
-	ipa_inc_client_enable_clks();
-
-	if (num_desc == 1) {
-		init_completion(&descr->xfer_done);
-
-		if (descr->callback || descr->user1)
-			WARN_ON(1);
-
-		descr->callback = ipa_sps_irq_cmd_ack;
-		descr->user1 = descr;
-		if (ipa_send_one(sys, descr, true)) {
-			IPAERR("fail to send immediate command\n");
-			result = -EFAULT;
-			goto bail;
-		}
-		wait_for_completion(&descr->xfer_done);
-	} else {
-		desc = &descr[num_desc - 1];
-		init_completion(&desc->xfer_done);
-
-		if (desc->callback || desc->user1)
-			WARN_ON(1);
-
-		desc->callback = ipa_sps_irq_cmd_ack;
-		desc->user1 = desc;
-		if (ipa_send(sys, num_desc, descr, true)) {
-			IPAERR("fail to send multiple immediate command set\n");
-			result = -EFAULT;
-			goto bail;
-		}
-		wait_for_completion(&desc->xfer_done);
-	}
-
-bail:
-	ipa_dec_client_disable_clks();
-	return result;
-}
-
-/**
- * ipa_sps_irq_tx_notify() - Callback function which will be called by
- * the SPS driver to start a Tx poll operation.
- * Called in an interrupt context.
- * @notify:	SPS driver supplied notification struct
- *
- * This function defer the work for this event to the tx workqueue.
- */
-static void ipa_sps_irq_tx_notify(struct sps_event_notify *notify)
-{
-	struct ipa_sys_context *sys = (struct ipa_sys_context *)notify->user;
-	int ret;
-
-	IPADBG("event %d notified\n", notify->event_id);
-
-	switch (notify->event_id) {
-	case SPS_EVENT_EOT:
-		if (IPA_CLIENT_IS_APPS_CONS(sys->ep->client))
-			atomic_set(&ipa_ctx->sps_pm.eot_activity, 1);
-		if (!atomic_read(&sys->curr_polling_state)) {
-			ret = sps_get_config(sys->ep->ep_hdl,
-					&sys->ep->connect);
-			if (ret) {
-				IPAERR("sps_get_config() failed %d\n", ret);
-				break;
-			}
-			sys->ep->connect.options = SPS_O_AUTO_ENABLE |
-				SPS_O_ACK_TRANSFERS | SPS_O_POLL;
-			ret = sps_set_config(sys->ep->ep_hdl,
-					&sys->ep->connect);
-			if (ret) {
-				IPAERR("sps_set_config() failed %d\n", ret);
-				break;
-			}
-			ipa_inc_acquire_wakelock();
-			atomic_set(&sys->curr_polling_state, 1);
-			queue_work(sys->wq, &sys->work);
-		}
-		break;
-	default:
-		IPAERR("recieved unexpected event id %d\n", notify->event_id);
-	}
-}
-
-/**
- * ipa_sps_irq_tx_no_aggr_notify() - Callback function which will be called by
- * the SPS driver after a Tx operation is complete.
- * Called in an interrupt context.
- * @notify:	SPS driver supplied notification struct
- *
- * This function defer the work for this event to the tx workqueue.
- * This event will be later handled by ipa_write_done.
- */
-static void ipa_sps_irq_tx_no_aggr_notify(struct sps_event_notify *notify)
-{
-	struct ipa_tx_pkt_wrapper *tx_pkt;
-
-	IPADBG("event %d notified\n", notify->event_id);
-
-	switch (notify->event_id) {
-	case SPS_EVENT_EOT:
-		tx_pkt = notify->data.transfer.user;
-		if (IPA_CLIENT_IS_APPS_CONS(tx_pkt->sys->ep->client))
-			atomic_set(&ipa_ctx->sps_pm.eot_activity, 1);
-		queue_work(tx_pkt->sys->wq, &tx_pkt->work);
-		break;
-	default:
-		IPAERR("recieved unexpected event id %d\n", notify->event_id);
-	}
-}
-
-/**
- * ipa_handle_rx_core() - The core functionality of packet reception. This
- * function is read from multiple code paths.
- *
- * All the packets on the Rx data path are received on the IPA_A5_LAN_WAN_IN
- * endpoint. The function runs as long as there are packets in the pipe.
- * For each packet:
- *  - Disconnect the packet from the system pipe linked list
- *  - Unmap the packets skb, make it non DMAable
- *  - Free the packet from the cache
- *  - Prepare a proper skb
- *  - Call the endpoints notify function, passing the skb in the parameters
- *  - Replenish the rx cache
- */
-static int ipa_handle_rx_core(struct ipa_sys_context *sys, bool process_all,
-		bool in_poll_state)
-{
-	struct sps_iovec iov;
-	int ret;
-	int cnt = 0;
-
-	while ((in_poll_state ? atomic_read(&sys->curr_polling_state) :
-				!atomic_read(&sys->curr_polling_state))) {
-		if (cnt && !process_all)
-			break;
-
-		ret = sps_get_iovec(sys->ep->ep_hdl, &iov);
-		if (ret) {
-			IPAERR("sps_get_iovec failed %d\n", ret);
-			break;
-		}
-
-		if (iov.addr == 0)
-			break;
-		if (IPA_CLIENT_IS_MEMCPY_DMA_CONS(sys->ep->client))
-			ipa_dma_memcpy_notify(sys, &iov);
-		else if (IPA_CLIENT_IS_WLAN_CONS(sys->ep->client))
-			ipa_wlan_wq_rx_common(sys, iov.size);
-		else
-			ipa_wq_rx_common(sys, iov.size);
-
-		cnt++;
-	};
-
-	return cnt;
-}
-
-/**
- * ipa_rx_switch_to_intr_mode() - Operate the Rx data path in interrupt mode
- */
-static void ipa_rx_switch_to_intr_mode(struct ipa_sys_context *sys)
-{
-	int ret;
-
-	if (!atomic_read(&sys->curr_polling_state)) {
-		IPAERR("already in intr mode\n");
-		goto fail;
-	}
-
-	ret = sps_get_config(sys->ep->ep_hdl, &sys->ep->connect);
-	if (ret) {
-		IPAERR("sps_get_config() failed %d\n", ret);
-		goto fail;
-	}
-	sys->event.options = SPS_O_EOT;
-	ret = sps_register_event(sys->ep->ep_hdl, &sys->event);
-	if (ret) {
-		IPAERR("sps_register_event() failed %d\n", ret);
-		goto fail;
-	}
-	sys->ep->connect.options =
-		SPS_O_AUTO_ENABLE | SPS_O_ACK_TRANSFERS | SPS_O_EOT;
-	ret = sps_set_config(sys->ep->ep_hdl, &sys->ep->connect);
-	if (ret) {
-		IPAERR("sps_set_config() failed %d\n", ret);
-		goto fail;
-	}
-	atomic_set(&sys->curr_polling_state, 0);
-	ipa_handle_rx_core(sys, true, false);
-	ipa_dec_release_wakelock();
-	return;
-
-fail:
-	queue_delayed_work(sys->wq, &sys->switch_to_intr_work,
-			msecs_to_jiffies(1));
-}
-
-
-/**
- * ipa_sps_irq_control() - Function to enable or disable BAM IRQ.
- */
-static void ipa_sps_irq_control(struct ipa_sys_context *sys, bool enable)
-{
-	int ret;
-
-	/*
-	 * Do not change sps config in case we are in polling mode as this
-	 * indicates that sps driver already notified EOT event and sps config
-	 * should not change until ipa driver processes the packet.
-	 */
-	if (atomic_read(&sys->curr_polling_state)) {
-		IPADBG("in polling mode, do not change config\n");
-		return;
-	}
-
-	if (enable) {
-		ret = sps_get_config(sys->ep->ep_hdl, &sys->ep->connect);
-		if (ret) {
-			IPAERR("sps_get_config() failed %d\n", ret);
-			return;
-		}
-		sys->event.options = SPS_O_EOT;
-		ret = sps_register_event(sys->ep->ep_hdl, &sys->event);
-		if (ret) {
-			IPAERR("sps_register_event() failed %d\n", ret);
-			return;
-		}
-		sys->ep->connect.options =
-			SPS_O_AUTO_ENABLE | SPS_O_ACK_TRANSFERS | SPS_O_EOT;
-		ret = sps_set_config(sys->ep->ep_hdl, &sys->ep->connect);
-		if (ret) {
-			IPAERR("sps_set_config() failed %d\n", ret);
-			return;
-		}
-	} else {
-		ret = sps_get_config(sys->ep->ep_hdl,
-				&sys->ep->connect);
-		if (ret) {
-			IPAERR("sps_get_config() failed %d\n", ret);
-			return;
-		}
-		sys->ep->connect.options = SPS_O_AUTO_ENABLE |
-			SPS_O_ACK_TRANSFERS | SPS_O_POLL;
-		ret = sps_set_config(sys->ep->ep_hdl,
-				&sys->ep->connect);
-		if (ret) {
-			IPAERR("sps_set_config() failed %d\n", ret);
-			return;
-		}
-	}
-}
-
-void ipa_sps_irq_control_all(bool enable)
-{
-	struct ipa_ep_context *ep;
-	int ipa_ep_idx, client_num;
-
-	IPADBG("\n");
-
-	for (client_num = IPA_CLIENT_CONS;
-		client_num < IPA_CLIENT_MAX; client_num++) {
-		if (!IPA_CLIENT_IS_APPS_CONS(client_num))
-			continue;
-
-		ipa_ep_idx = ipa_get_ep_mapping(client_num);
-		if (ipa_ep_idx == -1) {
-			IPAERR("Invalid client.\n");
-			continue;
-		}
-		ep = &ipa_ctx->ep[ipa_ep_idx];
-		if (!ep->valid) {
-			IPAERR("EP (%d) not allocated.\n", ipa_ep_idx);
-			continue;
-		}
-		ipa_sps_irq_control(ep->sys, enable);
-	}
-}
-
-
-
-/**
- * ipa_rx_notify() - Callback function which is called by the SPS driver when a
- * a packet is received
- * @notify:	SPS driver supplied notification information
- *
- * Called in an interrupt context, therefore the majority of the work is
- * deffered using a work queue.
- *
- * After receiving a packet, the driver goes to polling mode and keeps pulling
- * packets until the rx buffer is empty, then it goes back to interrupt mode.
- * This comes to prevent the CPU from handling too many interrupts when the
- * throughput is high.
- */
-static void ipa_sps_irq_rx_notify(struct sps_event_notify *notify)
-{
-	struct ipa_sys_context *sys = (struct ipa_sys_context *)notify->user;
-	int ret;
-
-	IPADBG("event %d notified\n", notify->event_id);
-
-	switch (notify->event_id) {
-	case SPS_EVENT_EOT:
-		if (IPA_CLIENT_IS_APPS_CONS(sys->ep->client))
-			atomic_set(&ipa_ctx->sps_pm.eot_activity, 1);
-		if (!atomic_read(&sys->curr_polling_state)) {
-			ret = sps_get_config(sys->ep->ep_hdl,
-					&sys->ep->connect);
-			if (ret) {
-				IPAERR("sps_get_config() failed %d\n", ret);
-				break;
-			}
-			sys->ep->connect.options = SPS_O_AUTO_ENABLE |
-				SPS_O_ACK_TRANSFERS | SPS_O_POLL;
-			ret = sps_set_config(sys->ep->ep_hdl,
-					&sys->ep->connect);
-			if (ret) {
-				IPAERR("sps_set_config() failed %d\n", ret);
-				break;
-			}
-			ipa_inc_acquire_wakelock();
-			atomic_set(&sys->curr_polling_state, 1);
-			queue_work(sys->wq, &sys->work);
-		}
-		break;
-	default:
-		IPAERR("recieved unexpected event id %d\n", notify->event_id);
-	}
-}
-
-static void switch_to_intr_tx_work_func(struct work_struct *work)
-{
-	struct delayed_work *dwork;
-	struct ipa_sys_context *sys;
-	dwork = container_of(work, struct delayed_work, work);
-	sys = container_of(dwork, struct ipa_sys_context, switch_to_intr_work);
-	ipa_handle_tx(sys);
-}
-
-/**
- * ipa_handle_rx() - handle packet reception. This function is executed in the
- * context of a work queue.
- * @work: work struct needed by the work queue
- *
- * ipa_handle_rx_core() is run in polling mode. After all packets has been
- * received, the driver switches back to interrupt mode.
- */
-static void ipa_handle_rx(struct ipa_sys_context *sys)
-{
-	int inactive_cycles = 0;
-	int cnt;
-
-	ipa_inc_client_enable_clks();
-	do {
-		cnt = ipa_handle_rx_core(sys, true, true);
-		if (cnt == 0) {
-			inactive_cycles++;
-			usleep_range(POLLING_MIN_SLEEP_RX,
-					POLLING_MAX_SLEEP_RX);
-		} else {
-			inactive_cycles = 0;
-		}
-
-		/* if pipe is out of buffers there is no point polling for
-		 * completed descs; release the worker so delayed work can
-		 * run in a timely manner
-		 */
-		if (sys->len == 0)
-			break;
-
-	} while (inactive_cycles <= POLLING_INACTIVITY_RX);
-
-	ipa_rx_switch_to_intr_mode(sys);
-	ipa_dec_client_disable_clks();
-}
-
-static void switch_to_intr_rx_work_func(struct work_struct *work)
-{
-	struct delayed_work *dwork;
-	struct ipa_sys_context *sys;
-	dwork = container_of(work, struct delayed_work, work);
-	sys = container_of(dwork, struct ipa_sys_context, switch_to_intr_work);
-	ipa_handle_rx(sys);
-}
-
-/**
- * ipa_update_repl_threshold()- Update the repl_threshold for the client.
- *
- * Return value: None.
- */
-void ipa_update_repl_threshold(enum ipa_client_type ipa_client)
-{
-	int ep_idx;
-	struct ipa_ep_context *ep;
-
-	/* Check if ep is valid. */
-	ep_idx = ipa_get_ep_mapping(ipa_client);
-	if (ep_idx == -1) {
-		IPADBG("Invalid IPA client\n");
-		return;
-	}
-
-	ep = &ipa_ctx->ep[ep_idx];
-	if (!ep->valid) {
-		IPADBG("EP not valid/Not applicable for client.\n");
-		return;
-	}
-	/*
-	 * Determine how many buffers/descriptors remaining will
-	 * cause to drop below the yellow WM bar.
-	 */
-	ep->rx_replenish_threshold = ipa_get_sys_yellow_wm()
-					/ ep->sys->rx_buff_sz;
-}
-
-/**
- * ipa_setup_sys_pipe() - Setup an IPA end-point in system-BAM mode and perform
- * IPA EP configuration
- * @sys_in:	[in] input needed to setup BAM pipe and configure EP
- * @clnt_hdl:	[out] client handle
- *
- *  - configure the end-point registers with the supplied
- *    parameters from the user.
- *  - call SPS APIs to create a system-to-bam connection with IPA.
- *  - allocate descriptor FIFO
- *  - register callback function(ipa_sps_irq_rx_notify or
- *    ipa_sps_irq_tx_notify - depends on client type) in case the driver is
- *    not configured to pulling mode
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_setup_sys_pipe(struct ipa_sys_connect_params *sys_in, u32 *clnt_hdl)
-{
-	struct ipa_ep_context *ep;
-	int ipa_ep_idx;
-	int result = -EINVAL;
-	dma_addr_t dma_addr;
-	char buff[IPA_RESOURCE_NAME_MAX];
-	struct iommu_domain *smmu_domain;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (sys_in == NULL || clnt_hdl == NULL) {
-		IPAERR("NULL args\n");
-		goto fail_gen;
-	}
-
-	if (sys_in->client >= IPA_CLIENT_MAX || sys_in->desc_fifo_sz == 0) {
-		IPAERR("bad parm client:%d fifo_sz:%d\n",
-			sys_in->client, sys_in->desc_fifo_sz);
-		goto fail_gen;
-	}
-
-	ipa_ep_idx = ipa_get_ep_mapping(sys_in->client);
-	if (ipa_ep_idx == -1) {
-		IPAERR("Invalid client.\n");
-		goto fail_gen;
-	}
-
-	ep = &ipa_ctx->ep[ipa_ep_idx];
-
-	ipa_inc_client_enable_clks();
-
-	if (ep->valid == 1) {
-		if (sys_in->client != IPA_CLIENT_APPS_LAN_WAN_PROD) {
-			IPAERR("EP already allocated.\n");
-			goto fail_and_disable_clocks;
-		} else {
-			if (ipa_cfg_ep_hdr(ipa_ep_idx,
-						&sys_in->ipa_ep_cfg.hdr)) {
-				IPAERR("fail to configure hdr prop of EP.\n");
-				result = -EFAULT;
-				goto fail_and_disable_clocks;
-			}
-			if (ipa_cfg_ep_cfg(ipa_ep_idx,
-						&sys_in->ipa_ep_cfg.cfg)) {
-				IPAERR("fail to configure cfg prop of EP.\n");
-				result = -EFAULT;
-				goto fail_and_disable_clocks;
-			}
-			IPADBG("client %d (ep: %d) overlay ok sys=%p\n",
-					sys_in->client, ipa_ep_idx, ep->sys);
-			ep->client_notify = sys_in->notify;
-			ep->priv = sys_in->priv;
-			*clnt_hdl = ipa_ep_idx;
-			if (!ep->keep_ipa_awake)
-				ipa_dec_client_disable_clks();
-
-			return 0;
-		}
-	}
-
-	memset(ep, 0, offsetof(struct ipa_ep_context, sys));
-
-	if (!ep->sys) {
-		ep->sys = kzalloc(sizeof(struct ipa_sys_context), GFP_KERNEL);
-		if (!ep->sys) {
-			IPAERR("failed to sys ctx for client %d\n",
-					sys_in->client);
-			result = -ENOMEM;
-			goto fail_and_disable_clocks;
-		}
-
-		ep->sys->ep = ep;
-		snprintf(buff, IPA_RESOURCE_NAME_MAX, "ipawq%d",
-				sys_in->client);
-		ep->sys->wq = alloc_workqueue(buff,
-				WQ_HIGHPRI | WQ_MEM_RECLAIM | WQ_UNBOUND, 1);
-		if (!ep->sys->wq) {
-			IPAERR("failed to create wq for client %d\n",
-					sys_in->client);
-			result = -EFAULT;
-			goto fail_wq;
-		}
-
-		snprintf(buff, IPA_RESOURCE_NAME_MAX, "iparepwq%d",
-				sys_in->client);
-		ep->sys->repl_wq = alloc_workqueue(buff,
-				WQ_HIGHPRI | WQ_MEM_RECLAIM | WQ_UNBOUND, 1);
-		if (!ep->sys->repl_wq) {
-			IPAERR("failed to create rep wq for client %d\n",
-					sys_in->client);
-			result = -EFAULT;
-			goto fail_wq2;
-		}
-
-		INIT_LIST_HEAD(&ep->sys->head_desc_list);
-		spin_lock_init(&ep->sys->spinlock);
-	} else {
-		memset(ep->sys, 0, offsetof(struct ipa_sys_context, ep));
-	}
-
-	ep->skip_ep_cfg = sys_in->skip_ep_cfg;
-	if (ipa_assign_policy(sys_in, ep->sys)) {
-		IPAERR("failed to sys ctx for client %d\n", sys_in->client);
-		result = -ENOMEM;
-		goto fail_gen2;
-	}
-
-	ep->valid = 1;
-	ep->client = sys_in->client;
-	ep->client_notify = sys_in->notify;
-	ep->priv = sys_in->priv;
-	ep->keep_ipa_awake = sys_in->keep_ipa_awake;
-	atomic_set(&ep->avail_fifo_desc,
-		((sys_in->desc_fifo_sz/sizeof(struct sps_iovec))-1));
-
-	result = ipa_enable_data_path(ipa_ep_idx);
-	if (result) {
-		IPAERR("enable data path failed res=%d clnt=%d.\n", result,
-				ipa_ep_idx);
-		goto fail_gen2;
-	}
-
-	if (!ep->skip_ep_cfg) {
-		if (ipa_cfg_ep(ipa_ep_idx, &sys_in->ipa_ep_cfg)) {
-			IPAERR("fail to configure EP.\n");
-			goto fail_gen2;
-		}
-		if (ipa_cfg_ep_status(ipa_ep_idx, &ep->status)) {
-			IPAERR("fail to configure status of EP.\n");
-			goto fail_gen2;
-		}
-		IPADBG("ep configuration successful\n");
-	} else {
-		IPADBG("skipping ep configuration\n");
-	}
-
-	/* Default Config */
-	ep->ep_hdl = sps_alloc_endpoint();
-	if (ep->ep_hdl == NULL) {
-		IPAERR("SPS EP allocation failed.\n");
-		goto fail_gen2;
-	}
-
-	result = sps_get_config(ep->ep_hdl, &ep->connect);
-	if (result) {
-		IPAERR("fail to get config.\n");
-		goto fail_sps_cfg;
-	}
-
-	/* Specific Config */
-	if (IPA_CLIENT_IS_CONS(sys_in->client)) {
-		ep->connect.mode = SPS_MODE_SRC;
-		ep->connect.destination = SPS_DEV_HANDLE_MEM;
-		ep->connect.source = ipa_ctx->bam_handle;
-		ep->connect.dest_pipe_index = ipa_ctx->a5_pipe_index++;
-		ep->connect.src_pipe_index = ipa_ep_idx;
-		/*
-		 * Determine how many buffers/descriptors remaining will
-		 * cause to drop below the yellow WM bar.
-		 */
-		ep->rx_replenish_threshold = ipa_get_sys_yellow_wm()
-						/ ep->sys->rx_buff_sz;
-		/* Only when the WAN pipes are setup, actual threshold will
-		 * be read from the register. So update LAN_CONS ep again with
-		 * right value.
-		 */
-		if (sys_in->client == IPA_CLIENT_APPS_WAN_CONS)
-			ipa_update_repl_threshold(IPA_CLIENT_APPS_LAN_CONS);
-	} else {
-		ep->connect.mode = SPS_MODE_DEST;
-		ep->connect.source = SPS_DEV_HANDLE_MEM;
-		ep->connect.destination = ipa_ctx->bam_handle;
-		ep->connect.src_pipe_index = ipa_ctx->a5_pipe_index++;
-		ep->connect.dest_pipe_index = ipa_ep_idx;
-	}
-
-	IPADBG("client:%d ep:%d",
-		sys_in->client, ipa_ep_idx);
-
-	IPADBG("dest_pipe_index:%d src_pipe_index:%d\n",
-		ep->connect.dest_pipe_index,
-		ep->connect.src_pipe_index);
-
-	ep->connect.options = ep->sys->sps_option;
-	ep->connect.desc.size = sys_in->desc_fifo_sz;
-	ep->connect.desc.base = dma_alloc_coherent(ipa_ctx->pdev,
-			ep->connect.desc.size, &dma_addr, 0);
-	if (!ipa_ctx->smmu_present) {
-		ep->connect.desc.phys_base = dma_addr;
-	} else {
-		ep->connect.desc.iova = dma_addr;
-		smmu_domain = ipa_get_smmu_domain();
-		if (smmu_domain != NULL) {
-			ep->connect.desc.phys_base =
-				iommu_iova_to_phys(smmu_domain, dma_addr);
-		}
-	}
-	if (ep->connect.desc.base == NULL) {
-		IPAERR("fail to get DMA desc memory.\n");
-		goto fail_sps_cfg;
-	}
-
-	ep->connect.event_thresh = IPA_EVENT_THRESHOLD;
-
-	result = ipa_sps_connect_safe(ep->ep_hdl, &ep->connect, sys_in->client);
-	if (result) {
-		IPAERR("sps_connect fails.\n");
-		goto fail_sps_connect;
-	}
-
-	ep->sys->event.options = SPS_O_EOT;
-	ep->sys->event.mode = SPS_TRIGGER_CALLBACK;
-	ep->sys->event.xfer_done = NULL;
-	ep->sys->event.user = ep->sys;
-	ep->sys->event.callback = ep->sys->sps_callback;
-	result = sps_register_event(ep->ep_hdl, &ep->sys->event);
-	if (result < 0) {
-		IPAERR("register event error %d\n", result);
-		goto fail_register_event;
-	}
-
-	*clnt_hdl = ipa_ep_idx;
-
-	if (IPA_CLIENT_IS_CONS(sys_in->client))
-		ipa_replenish_rx_cache(ep->sys);
-
-	if (IPA_CLIENT_IS_WLAN_CONS(sys_in->client)) {
-		ipa_alloc_wlan_rx_common_cache(IPA_WLAN_COMM_RX_POOL_LOW);
-		atomic_inc(&ipa_ctx->wc_memb.active_clnt_cnt);
-	}
-
-	if (nr_cpu_ids > 1 &&
-		(sys_in->client == IPA_CLIENT_APPS_LAN_CONS ||
-		 sys_in->client == IPA_CLIENT_APPS_WAN_CONS)) {
-		ep->sys->repl.capacity = ep->sys->rx_pool_sz + 1;
-		ep->sys->repl.cache = kzalloc(ep->sys->repl.capacity *
-				sizeof(void *), GFP_KERNEL);
-		if (!ep->sys->repl.cache) {
-			IPAERR("ep=%d fail to alloc repl cache\n", ipa_ep_idx);
-			ep->sys->repl_hdlr = ipa_replenish_rx_cache;
-			ep->sys->repl.capacity = 0;
-		} else {
-			atomic_set(&ep->sys->repl.head_idx, 0);
-			atomic_set(&ep->sys->repl.tail_idx, 0);
-			ipa_wq_repl_rx(&ep->sys->repl_work);
-		}
-	}
-
-	ipa_ctx->skip_ep_cfg_shadow[ipa_ep_idx] = ep->skip_ep_cfg;
-	if (!ep->skip_ep_cfg && IPA_CLIENT_IS_PROD(sys_in->client)) {
-		if (ipa_ctx->modem_cfg_emb_pipe_flt &&
-			sys_in->client == IPA_CLIENT_APPS_LAN_WAN_PROD)
-			IPADBG("modem cfg emb pipe flt\n");
-		else
-			ipa_install_dflt_flt_rules(ipa_ep_idx);
-	}
-
-	if (!ep->keep_ipa_awake)
-		ipa_dec_client_disable_clks();
-
-	IPADBG("client %d (ep: %d) connected sys=%p\n", sys_in->client,
-			ipa_ep_idx, ep->sys);
-
-	return 0;
-
-fail_register_event:
-	sps_disconnect(ep->ep_hdl);
-fail_sps_connect:
-	dma_free_coherent(ipa_ctx->pdev, ep->connect.desc.size,
-			  ep->connect.desc.base,
-			  ipa_ctx->smmu_present ? ep->connect.desc.iova :
-			  ep->connect.desc.phys_base);
-fail_sps_cfg:
-	sps_free_endpoint(ep->ep_hdl);
-fail_gen2:
-	destroy_workqueue(ep->sys->repl_wq);
-fail_wq2:
-	destroy_workqueue(ep->sys->wq);
-fail_wq:
-	kfree(ep->sys);
-	memset(&ipa_ctx->ep[ipa_ep_idx], 0, sizeof(struct ipa_ep_context));
-fail_and_disable_clocks:
-	ipa_dec_client_disable_clks();
-fail_gen:
-	return result;
-}
-EXPORT_SYMBOL(ipa_setup_sys_pipe);
-
-/**
- * ipa_teardown_sys_pipe() - Teardown the system-BAM pipe and cleanup IPA EP
- * @clnt_hdl:	[in] the handle obtained from ipa_setup_sys_pipe
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_teardown_sys_pipe(u32 clnt_hdl)
-{
-	struct ipa_ep_context *ep;
-	int empty;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (!ep->keep_ipa_awake)
-		ipa_inc_client_enable_clks();
-
-	ipa_disable_data_path(clnt_hdl);
-	ep->valid = 0;
-
-	if (IPA_CLIENT_IS_PROD(ep->client)) {
-		do {
-			spin_lock_bh(&ep->sys->spinlock);
-			empty = list_empty(&ep->sys->head_desc_list);
-			spin_unlock_bh(&ep->sys->spinlock);
-			if (!empty)
-				usleep(100);
-			else
-				break;
-		} while (1);
-	}
-
-	flush_workqueue(ep->sys->wq);
-	sps_disconnect(ep->ep_hdl);
-	dma_free_coherent(ipa_ctx->pdev, ep->connect.desc.size,
-			  ep->connect.desc.base,
-			  ipa_ctx->smmu_present ? ep->connect.desc.iova :
-			  ep->connect.desc.phys_base);
-	sps_free_endpoint(ep->ep_hdl);
-	if (ep->sys->repl_wq)
-		flush_workqueue(ep->sys->repl_wq);
-	if (IPA_CLIENT_IS_CONS(ep->client))
-		ipa_cleanup_rx(ep->sys);
-
-	if (!ep->skip_ep_cfg && IPA_CLIENT_IS_PROD(ep->client)) {
-		if (ipa_ctx->modem_cfg_emb_pipe_flt &&
-			ep->client == IPA_CLIENT_APPS_LAN_WAN_PROD)
-			IPADBG("modem cfg emb pipe flt\n");
-		else
-			ipa_delete_dflt_flt_rules(clnt_hdl);
-	}
-
-	if (IPA_CLIENT_IS_WLAN_CONS(ep->client))
-		atomic_dec(&ipa_ctx->wc_memb.active_clnt_cnt);
-
-	memset(&ep->wstats, 0, sizeof(struct ipa_wlan_stats));
-
-	if (!atomic_read(&ipa_ctx->wc_memb.active_clnt_cnt))
-		ipa_cleanup_wlan_rx_common_cache();
-
-	ipa_dec_client_disable_clks();
-
-	IPADBG("client (ep: %d) disconnected\n", clnt_hdl);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_teardown_sys_pipe);
-
-/**
- * ipa_tx_comp_usr_notify_release() - Callback function which will call the
- * user supplied callback function to release the skb, or release it on
- * its own if no callback function was supplied.
- * @user1
- * @user2
- *
- * This notified callback is for the destination client.
- * This function is supplied in ipa_connect.
- */
-static void ipa_tx_comp_usr_notify_release(void *user1, int user2)
-{
-	struct sk_buff *skb = (struct sk_buff *)user1;
-	int ep_idx = user2;
-
-	IPADBG("skb=%p ep=%d\n", skb, ep_idx);
-
-	IPA_STATS_INC_CNT(ipa_ctx->stats.tx_pkts_compl);
-
-	if (ipa_ctx->ep[ep_idx].client_notify)
-		ipa_ctx->ep[ep_idx].client_notify(ipa_ctx->ep[ep_idx].priv,
-				IPA_WRITE_DONE, (unsigned long)skb);
-	else
-		dev_kfree_skb_any(skb);
-}
-
-static void ipa_tx_cmd_comp(void *user1, int user2)
-{
-	kfree(user1);
-}
-
-/**
- * ipa_tx_dp() - Data-path tx handler
- * @dst:	[in] which IPA destination to route tx packets to
- * @skb:	[in] the packet to send
- * @metadata:	[in] TX packet meta-data
- *
- * Data-path tx handler, this is used for both SW data-path which by-passes most
- * IPA HW blocks AND the regular HW data-path for WLAN AMPDU traffic only. If
- * dst is a "valid" CONS type, then SW data-path is used. If dst is the
- * WLAN_AMPDU PROD type, then HW data-path for WLAN AMPDU is used. Anything else
- * is an error. For errors, client needs to free the skb as needed. For success,
- * IPA driver will later invoke client callback if one was supplied. That
- * callback should free the skb. If no callback supplied, IPA driver will free
- * the skb internally
- *
- * The function will use two descriptors for this send command
- * (for A5_WLAN_AMPDU_PROD only one desciprtor will be sent),
- * the first descriptor will be used to inform the IPA hardware that
- * apps need to push data into the IPA (IP_PACKET_INIT immediate command).
- * Once this send was done from SPS point-of-view the IPA driver will
- * get notified by the supplied callback - ipa_sps_irq_tx_comp()
- *
- * ipa_sps_irq_tx_comp will call to the user supplied
- * callback (from ipa_connect)
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_tx_dp(enum ipa_client_type dst, struct sk_buff *skb,
-		struct ipa_tx_meta *meta)
-{
-	struct ipa_desc desc[2];
-	int dst_ep_idx;
-	struct ipa_ip_packet_init *cmd;
-	struct ipa_sys_context *sys;
-	int src_ep_idx;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	memset(desc, 0, 2 * sizeof(struct ipa_desc));
-
-	if (skb->len == 0) {
-		IPAERR("packet size is 0\n");
-		return -EINVAL;
-	}
-
-	/*
-	 * USB_CONS: PKT_INIT ep_idx = dst pipe
-	 * Q6_CONS: PKT_INIT ep_idx = sender pipe
-	 * A5_LAN_WAN_PROD: HW path ep_idx = sender pipe
-	 *
-	 * LAN TX: all PKT_INIT
-	 * WAN TX: PKT_INIT (cmd) + HW (data)
-	 *
-	 */
-	if (IPA_CLIENT_IS_CONS(dst)) {
-		src_ep_idx = ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_WAN_PROD);
-		if (-1 == src_ep_idx) {
-			IPAERR("Client %u is not mapped\n",
-				IPA_CLIENT_APPS_LAN_WAN_PROD);
-			return -EFAULT;
-		}
-		dst_ep_idx = ipa_get_ep_mapping(dst);
-	} else {
-		src_ep_idx = ipa_get_ep_mapping(dst);
-		if (-1 == src_ep_idx) {
-			IPAERR("Client %u is not mapped\n", dst);
-			return -EFAULT;
-		}
-		if (meta && meta->pkt_init_dst_ep_valid)
-			dst_ep_idx = meta->pkt_init_dst_ep;
-		else
-			dst_ep_idx = -1;
-	}
-
-	sys = ipa_ctx->ep[src_ep_idx].sys;
-
-	if (!sys->ep->valid) {
-		IPAERR("pipe not valid\n");
-		goto fail_gen;
-	}
-
-	if (dst_ep_idx != -1) {
-		/* SW data path */
-		cmd = kzalloc(sizeof(struct ipa_ip_packet_init), GFP_ATOMIC);
-		if (!cmd) {
-			IPAERR("failed to alloc immediate command object\n");
-			goto fail_gen;
-		}
-
-		cmd->destination_pipe_index = dst_ep_idx;
-		if (meta && meta->mbim_stream_id_valid)
-			cmd->metadata = meta->mbim_stream_id;
-		desc[0].opcode = IPA_IP_PACKET_INIT;
-		desc[0].pyld = cmd;
-		desc[0].len = sizeof(struct ipa_ip_packet_init);
-		desc[0].type = IPA_IMM_CMD_DESC;
-		desc[0].callback = ipa_tx_cmd_comp;
-		desc[0].user1 = cmd;
-		desc[1].pyld = skb->data;
-		desc[1].len = skb->len;
-		desc[1].type = IPA_DATA_DESC_SKB;
-		desc[1].callback = ipa_tx_comp_usr_notify_release;
-		desc[1].user1 = skb;
-		desc[1].user2 = (meta && meta->pkt_init_dst_ep_valid &&
-				meta->pkt_init_dst_ep_remote) ?
-				src_ep_idx :
-				dst_ep_idx;
-		if (meta && meta->dma_address_valid) {
-			desc[1].dma_address_valid = true;
-			desc[1].dma_address = meta->dma_address;
-		}
-
-		if (ipa_send(sys, 2, desc, true)) {
-			IPAERR("fail to send immediate command\n");
-			goto fail_send;
-		}
-		IPA_STATS_INC_CNT(ipa_ctx->stats.tx_sw_pkts);
-	} else {
-		/* HW data path */
-		desc[0].pyld = skb->data;
-		desc[0].len = skb->len;
-		desc[0].type = IPA_DATA_DESC_SKB;
-		desc[0].callback = ipa_tx_comp_usr_notify_release;
-		desc[0].user1 = skb;
-		desc[0].user2 = src_ep_idx;
-
-		if (meta && meta->dma_address_valid) {
-			desc[0].dma_address_valid = true;
-			desc[0].dma_address = meta->dma_address;
-		}
-
-		if (ipa_send_one(sys, &desc[0], true)) {
-			IPAERR("fail to send skb\n");
-			goto fail_gen;
-		}
-		IPA_STATS_INC_CNT(ipa_ctx->stats.tx_hw_pkts);
-	}
-
-	return 0;
-
-fail_send:
-	kfree(cmd);
-fail_gen:
-	return -EFAULT;
-}
-EXPORT_SYMBOL(ipa_tx_dp);
-
-static void ipa_wq_handle_rx(struct work_struct *work)
-{
-	struct ipa_sys_context *sys;
-	sys = container_of(work, struct ipa_sys_context, work);
-	ipa_handle_rx(sys);
-}
-
-static void ipa_wq_repl_rx(struct work_struct *work)
-{
-	struct ipa_sys_context *sys;
-	void *ptr;
-	struct ipa_rx_pkt_wrapper *rx_pkt;
-	gfp_t flag = GFP_KERNEL;
-	u32 next;
-	u32 curr;
-
-	sys = container_of(work, struct ipa_sys_context, repl_work);
-	curr = atomic_read(&sys->repl.tail_idx);
-
-begin:
-	while (1) {
-		next = (curr + 1) % sys->repl.capacity;
-		if (next == atomic_read(&sys->repl.head_idx))
-			goto fail_kmem_cache_alloc;
-
-		rx_pkt = kmem_cache_zalloc(ipa_ctx->rx_pkt_wrapper_cache,
-					   flag);
-		if (!rx_pkt) {
-			pr_err_ratelimited("%s fail alloc rx wrapper sys=%p\n",
-					__func__, sys);
-			goto fail_kmem_cache_alloc;
-		}
-
-		INIT_LIST_HEAD(&rx_pkt->link);
-		INIT_WORK(&rx_pkt->work, ipa_wq_rx_avail);
-		rx_pkt->sys = sys;
-
-		rx_pkt->data.skb = sys->get_skb(sys->rx_buff_sz, flag);
-		if (rx_pkt->data.skb == NULL) {
-			pr_err_ratelimited("%s fail alloc skb sys=%p\n",
-					__func__, sys);
-			goto fail_skb_alloc;
-		}
-		ptr = skb_put(rx_pkt->data.skb, sys->rx_buff_sz);
-		rx_pkt->data.dma_addr = dma_map_single(ipa_ctx->pdev, ptr,
-						     sys->rx_buff_sz,
-						     DMA_FROM_DEVICE);
-		if (rx_pkt->data.dma_addr == 0 ||
-				rx_pkt->data.dma_addr == ~0) {
-			pr_err_ratelimited("%s dma map fail %p for %p sys=%p\n",
-			       __func__, (void *)rx_pkt->data.dma_addr,
-			       ptr, sys);
-			goto fail_dma_mapping;
-		}
-
-		sys->repl.cache[curr] = rx_pkt;
-		curr = next;
-		/* ensure write is done before setting tail index */
-		mb();
-		atomic_set(&sys->repl.tail_idx, next);
-	}
-
-	return;
-
-fail_dma_mapping:
-	sys->free_skb(rx_pkt->data.skb);
-fail_skb_alloc:
-	kmem_cache_free(ipa_ctx->rx_pkt_wrapper_cache, rx_pkt);
-fail_kmem_cache_alloc:
-	if (atomic_read(&sys->repl.tail_idx) ==
-			atomic_read(&sys->repl.head_idx)) {
-		if (sys->ep->client == IPA_CLIENT_APPS_WAN_CONS)
-			IPA_STATS_INC_CNT(ipa_ctx->stats.wan_repl_rx_empty);
-		else if (sys->ep->client == IPA_CLIENT_APPS_LAN_CONS)
-			IPA_STATS_INC_CNT(ipa_ctx->stats.lan_repl_rx_empty);
-		else
-			WARN_ON(1);
-		pr_err_ratelimited("%s sys=%p repl ring empty\n",
-				__func__, sys);
-		goto begin;
-	}
-}
-
-static void ipa_replenish_wlan_rx_cache(struct ipa_sys_context *sys)
-{
-	struct ipa_rx_pkt_wrapper *rx_pkt = NULL;
-	struct ipa_rx_pkt_wrapper *tmp;
-	int ret;
-	u32 rx_len_cached = 0;
-
-	IPADBG("\n");
-
-	spin_lock_bh(&ipa_ctx->wc_memb.wlan_spinlock);
-	rx_len_cached = sys->len;
-
-	if (rx_len_cached < sys->rx_pool_sz) {
-		list_for_each_entry_safe(rx_pkt, tmp,
-			&ipa_ctx->wc_memb.wlan_comm_desc_list, link)
-		{
-			list_del(&rx_pkt->link);
-
-			if (ipa_ctx->wc_memb.wlan_comm_free_cnt > 0)
-				ipa_ctx->wc_memb.wlan_comm_free_cnt--;
-
-			INIT_LIST_HEAD(&rx_pkt->link);
-			rx_pkt->len = 0;
-			rx_pkt->sys = sys;
-
-			ret = sps_transfer_one(sys->ep->ep_hdl,
-				rx_pkt->data.dma_addr,
-				IPA_WLAN_RX_BUFF_SZ, rx_pkt, 0);
-
-			if (ret) {
-				IPAERR("sps_transfer_one failed %d\n", ret);
-				goto fail_sps_transfer;
-			}
-
-			list_add_tail(&rx_pkt->link, &sys->head_desc_list);
-			rx_len_cached = ++sys->len;
-
-			if (rx_len_cached >= sys->rx_pool_sz) {
-				spin_unlock_bh(&ipa_ctx->wc_memb.wlan_spinlock);
-				return;
-			}
-		}
-	}
-	spin_unlock_bh(&ipa_ctx->wc_memb.wlan_spinlock);
-
-	if (rx_len_cached < sys->rx_pool_sz &&
-			ipa_ctx->wc_memb.wlan_comm_total_cnt <
-			 IPA_WLAN_COMM_RX_POOL_HIGH) {
-		ipa_replenish_rx_cache(sys);
-		ipa_ctx->wc_memb.wlan_comm_total_cnt +=
-			(sys->rx_pool_sz - rx_len_cached);
-	}
-
-	return;
-
-fail_sps_transfer:
-	list_del(&rx_pkt->link);
-	spin_unlock_bh(&ipa_ctx->wc_memb.wlan_spinlock);
-
-	return;
-}
-
-static void ipa_cleanup_wlan_rx_common_cache(void)
-{
-	struct ipa_rx_pkt_wrapper *rx_pkt;
-	struct ipa_rx_pkt_wrapper *tmp;
-
-	list_for_each_entry_safe(rx_pkt, tmp,
-		&ipa_ctx->wc_memb.wlan_comm_desc_list, link) {
-		list_del(&rx_pkt->link);
-		dma_unmap_single(ipa_ctx->pdev, rx_pkt->data.dma_addr,
-			IPA_WLAN_COMM_RX_POOL_LOW, DMA_FROM_DEVICE);
-		dev_kfree_skb_any(rx_pkt->data.skb);
-		kmem_cache_free(ipa_ctx->rx_pkt_wrapper_cache, rx_pkt);
-		ipa_ctx->wc_memb.wlan_comm_free_cnt--;
-		ipa_ctx->wc_memb.wlan_comm_total_cnt--;
-	}
-	ipa_ctx->wc_memb.total_tx_pkts_freed = 0;
-
-	if (ipa_ctx->wc_memb.wlan_comm_free_cnt != 0)
-		IPAERR("wlan comm buff free cnt: %d\n",
-			ipa_ctx->wc_memb.wlan_comm_free_cnt);
-
-	if (ipa_ctx->wc_memb.wlan_comm_total_cnt != 0)
-		IPAERR("wlan comm buff total cnt: %d\n",
-			ipa_ctx->wc_memb.wlan_comm_total_cnt);
-
-}
-
-static void ipa_alloc_wlan_rx_common_cache(u32 size)
-{
-	void *ptr;
-	struct ipa_rx_pkt_wrapper *rx_pkt;
-	int rx_len_cached = 0;
-	gfp_t flag = GFP_NOWAIT | __GFP_NOWARN;
-
-	rx_len_cached = ipa_ctx->wc_memb.wlan_comm_total_cnt;
-	while (rx_len_cached < size) {
-		rx_pkt = kmem_cache_zalloc(ipa_ctx->rx_pkt_wrapper_cache,
-					   flag);
-		if (!rx_pkt) {
-			IPAERR("failed to alloc rx wrapper\n");
-			goto fail_kmem_cache_alloc;
-		}
-
-		INIT_LIST_HEAD(&rx_pkt->link);
-		INIT_WORK(&rx_pkt->work, ipa_wq_rx_avail);
-
-		rx_pkt->data.skb =
-			ipa_get_skb_ipa_rx(IPA_WLAN_RX_BUFF_SZ,
-						flag);
-		if (rx_pkt->data.skb == NULL) {
-			IPAERR("failed to alloc skb\n");
-			goto fail_skb_alloc;
-		}
-		ptr = skb_put(rx_pkt->data.skb, IPA_WLAN_RX_BUFF_SZ);
-		rx_pkt->data.dma_addr = dma_map_single(ipa_ctx->pdev, ptr,
-				IPA_WLAN_RX_BUFF_SZ, DMA_FROM_DEVICE);
-		if (rx_pkt->data.dma_addr == 0 ||
-				rx_pkt->data.dma_addr == ~0) {
-			IPAERR("dma_map_single failure %p for %p\n",
-			       (void *)rx_pkt->data.dma_addr, ptr);
-			goto fail_dma_mapping;
-		}
-
-		list_add_tail(&rx_pkt->link,
-			&ipa_ctx->wc_memb.wlan_comm_desc_list);
-		rx_len_cached = ++ipa_ctx->wc_memb.wlan_comm_total_cnt;
-
-		ipa_ctx->wc_memb.wlan_comm_free_cnt++;
-
-	}
-
-	return;
-
-fail_dma_mapping:
-	dev_kfree_skb_any(rx_pkt->data.skb);
-fail_skb_alloc:
-	kmem_cache_free(ipa_ctx->rx_pkt_wrapper_cache, rx_pkt);
-fail_kmem_cache_alloc:
-	return;
-}
-
-
-/**
- * ipa_replenish_rx_cache() - Replenish the Rx packets cache.
- *
- * The function allocates buffers in the rx_pkt_wrapper_cache cache until there
- * are IPA_RX_POOL_CEIL buffers in the cache.
- *   - Allocate a buffer in the cache
- *   - Initialized the packets link
- *   - Initialize the packets work struct
- *   - Allocate the packets socket buffer (skb)
- *   - Fill the packets skb with data
- *   - Make the packet DMAable
- *   - Add the packet to the system pipe linked list
- *   - Initiate a SPS transfer so that SPS driver will use this packet later.
- */
-static void ipa_replenish_rx_cache(struct ipa_sys_context *sys)
-{
-	void *ptr;
-	struct ipa_rx_pkt_wrapper *rx_pkt;
-	int ret;
-	int rx_len_cached = 0;
-	gfp_t flag = GFP_NOWAIT | __GFP_NOWARN;
-
-	rx_len_cached = sys->len;
-
-	while (rx_len_cached < sys->rx_pool_sz) {
-		rx_pkt = kmem_cache_zalloc(ipa_ctx->rx_pkt_wrapper_cache,
-					   flag);
-		if (!rx_pkt) {
-			IPAERR("failed to alloc rx wrapper\n");
-			goto fail_kmem_cache_alloc;
-		}
-
-		INIT_LIST_HEAD(&rx_pkt->link);
-		INIT_WORK(&rx_pkt->work, ipa_wq_rx_avail);
-		rx_pkt->sys = sys;
-
-		rx_pkt->data.skb = sys->get_skb(sys->rx_buff_sz, flag);
-		if (rx_pkt->data.skb == NULL) {
-			IPAERR("failed to alloc skb\n");
-			goto fail_skb_alloc;
-		}
-		ptr = skb_put(rx_pkt->data.skb, sys->rx_buff_sz);
-		rx_pkt->data.dma_addr = dma_map_single(ipa_ctx->pdev, ptr,
-						     sys->rx_buff_sz,
-						     DMA_FROM_DEVICE);
-		if (rx_pkt->data.dma_addr == 0 ||
-				rx_pkt->data.dma_addr == ~0) {
-			IPAERR("dma_map_single failure %p for %p\n",
-			       (void *)rx_pkt->data.dma_addr, ptr);
-			goto fail_dma_mapping;
-		}
-
-		list_add_tail(&rx_pkt->link, &sys->head_desc_list);
-		rx_len_cached = ++sys->len;
-
-		ret = sps_transfer_one(sys->ep->ep_hdl,
-			rx_pkt->data.dma_addr, sys->rx_buff_sz, rx_pkt, 0);
-
-		if (ret) {
-			IPAERR("sps_transfer_one failed %d\n", ret);
-			goto fail_sps_transfer;
-		}
-	}
-
-	return;
-
-fail_sps_transfer:
-	list_del(&rx_pkt->link);
-	rx_len_cached = --sys->len;
-	dma_unmap_single(ipa_ctx->pdev, rx_pkt->data.dma_addr,
-			sys->rx_buff_sz, DMA_FROM_DEVICE);
-fail_dma_mapping:
-	sys->free_skb(rx_pkt->data.skb);
-fail_skb_alloc:
-	kmem_cache_free(ipa_ctx->rx_pkt_wrapper_cache, rx_pkt);
-fail_kmem_cache_alloc:
-	if (rx_len_cached == 0)
-		queue_delayed_work(sys->wq, &sys->replenish_rx_work,
-				msecs_to_jiffies(1));
-}
-
-static void ipa_fast_replenish_rx_cache(struct ipa_sys_context *sys)
-{
-	struct ipa_rx_pkt_wrapper *rx_pkt;
-	int ret;
-	int rx_len_cached = 0;
-	u32 curr;
-
-	rx_len_cached = sys->len;
-	curr = atomic_read(&sys->repl.head_idx);
-
-	while (rx_len_cached < sys->rx_pool_sz) {
-		if (curr == atomic_read(&sys->repl.tail_idx))
-			break;
-
-		rx_pkt = sys->repl.cache[curr];
-		list_add_tail(&rx_pkt->link, &sys->head_desc_list);
-
-		ret = sps_transfer_one(sys->ep->ep_hdl,
-			rx_pkt->data.dma_addr, sys->rx_buff_sz, rx_pkt, 0);
-
-		if (ret) {
-			IPAERR("sps_transfer_one failed %d\n", ret);
-			list_del(&rx_pkt->link);
-			break;
-		}
-		rx_len_cached = ++sys->len;
-		sys->repl_trig_cnt++;
-		curr = (curr + 1) % sys->repl.capacity;
-		/* ensure write is done before setting head index */
-		mb();
-		atomic_set(&sys->repl.head_idx, curr);
-	}
-
-	if (sys->repl_trig_cnt % sys->repl_trig_thresh == 0)
-		queue_work(sys->repl_wq, &sys->repl_work);
-
-	if (rx_len_cached <= sys->ep->rx_replenish_threshold) {
-		if (rx_len_cached == 0) {
-			if (sys->ep->client == IPA_CLIENT_APPS_WAN_CONS)
-				IPA_STATS_INC_CNT(ipa_ctx->stats.wan_rx_empty);
-			else if (sys->ep->client == IPA_CLIENT_APPS_LAN_CONS)
-				IPA_STATS_INC_CNT(ipa_ctx->stats.lan_rx_empty);
-			else
-				WARN_ON(1);
-		}
-		sys->repl_trig_cnt = 0;
-		queue_delayed_work(sys->wq, &sys->replenish_rx_work,
-			msecs_to_jiffies(1));
-	}
-
-	return;
-}
-
-static void replenish_rx_work_func(struct work_struct *work)
-{
-	struct delayed_work *dwork;
-	struct ipa_sys_context *sys;
-	dwork = container_of(work, struct delayed_work, work);
-	sys = container_of(dwork, struct ipa_sys_context, replenish_rx_work);
-	ipa_inc_client_enable_clks();
-	sys->repl_hdlr(sys);
-	ipa_dec_client_disable_clks();
-}
-
-/**
- * ipa_cleanup_rx() - release RX queue resources
- *
- */
-static void ipa_cleanup_rx(struct ipa_sys_context *sys)
-{
-	struct ipa_rx_pkt_wrapper *rx_pkt;
-	struct ipa_rx_pkt_wrapper *r;
-	u32 head;
-	u32 tail;
-
-	list_for_each_entry_safe(rx_pkt, r,
-				 &sys->head_desc_list, link) {
-		list_del(&rx_pkt->link);
-		dma_unmap_single(ipa_ctx->pdev, rx_pkt->data.dma_addr,
-			sys->rx_buff_sz, DMA_FROM_DEVICE);
-		sys->free_skb(rx_pkt->data.skb);
-		kmem_cache_free(ipa_ctx->rx_pkt_wrapper_cache, rx_pkt);
-	}
-
-	if (sys->repl.cache) {
-		head = atomic_read(&sys->repl.head_idx);
-		tail = atomic_read(&sys->repl.tail_idx);
-		while (head != tail) {
-			rx_pkt = sys->repl.cache[head];
-			dma_unmap_single(ipa_ctx->pdev, rx_pkt->data.dma_addr,
-					sys->rx_buff_sz, DMA_FROM_DEVICE);
-			sys->free_skb(rx_pkt->data.skb);
-			kmem_cache_free(ipa_ctx->rx_pkt_wrapper_cache, rx_pkt);
-			head = (head + 1) % sys->repl.capacity;
-		}
-		kfree(sys->repl.cache);
-	}
-}
-
-static struct sk_buff *ipa_skb_copy_for_client(struct sk_buff *skb, int len)
-{
-	struct sk_buff *skb2 = NULL;
-
-	skb2 = __dev_alloc_skb(len + IPA_RX_BUFF_CLIENT_HEADROOM, GFP_KERNEL);
-	if (likely(skb2)) {
-		/* Set the data pointer */
-		skb_reserve(skb2, IPA_RX_BUFF_CLIENT_HEADROOM);
-		memcpy(skb2->data, skb->data, len);
-		skb2->len = len;
-		skb_set_tail_pointer(skb2, len);
-	}
-
-	return skb2;
-}
-
-static int ipa_lan_rx_pyld_hdlr(struct sk_buff *skb,
-		struct ipa_sys_context *sys)
-{
-	int rc = 0;
-	struct ipa_hw_pkt_status *status;
-	struct sk_buff *skb2;
-	int pad_len_byte;
-	int len;
-	unsigned char *buf;
-	bool drop_packet;
-	int src_pipe;
-	unsigned int used = *(unsigned int *)skb->cb;
-	unsigned int used_align = ALIGN(used, 32);
-	unsigned long unused = IPA_GENERIC_RX_BUFF_BASE_SZ - used;
-
-	IPA_DUMP_BUFF(skb->data, 0, skb->len);
-
-	if (skb->len == 0) {
-		IPAERR("ZLT\n");
-		sys->free_skb(skb);
-		return rc;
-	}
-
-	if (sys->len_partial) {
-		IPADBG("len_partial %d\n", sys->len_partial);
-		buf = skb_push(skb, sys->len_partial);
-		memcpy(buf, sys->prev_skb->data, sys->len_partial);
-		sys->len_partial = 0;
-		sys->free_skb(sys->prev_skb);
-		goto begin;
-	}
-
-	/* this pipe has TX comp (status only) + mux-ed LAN RX data
-	 * (status+data) */
-	if (sys->len_rem) {
-		IPADBG("rem %d skb %d pad %d\n", sys->len_rem, skb->len,
-				sys->len_pad);
-		if (sys->len_rem <= skb->len) {
-			if (sys->prev_skb) {
-				skb2 = skb_copy_expand(sys->prev_skb, 0,
-						sys->len_rem, GFP_KERNEL);
-				if (likely(skb2)) {
-					memcpy(skb_put(skb2, sys->len_rem),
-						skb->data, sys->len_rem);
-					skb_trim(skb2,
-						skb2->len - sys->len_pad);
-					skb2->truesize = skb2->len +
-						sizeof(struct sk_buff);
-					sys->ep->client_notify(sys->ep->priv,
-						IPA_RECEIVE,
-						(unsigned long)(skb2));
-				} else {
-					IPAERR("copy expand failed\n");
-				}
-				dev_kfree_skb_any(sys->prev_skb);
-			}
-			skb_pull(skb, sys->len_rem);
-			sys->prev_skb = NULL;
-			sys->len_rem = 0;
-			sys->len_pad = 0;
-		} else {
-			if (sys->prev_skb) {
-				skb2 = skb_copy_expand(sys->prev_skb, 0,
-					skb->len, GFP_KERNEL);
-				if (likely(skb2)) {
-					memcpy(skb_put(skb2, skb->len),
-						skb->data, skb->len);
-				} else {
-					IPAERR("copy expand failed\n");
-				}
-				dev_kfree_skb_any(sys->prev_skb);
-				sys->prev_skb = skb2;
-			}
-			sys->len_rem -= skb->len;
-			sys->free_skb(skb);
-			return rc;
-		}
-	}
-
-begin:
-	while (skb->len) {
-		drop_packet = false;
-		IPADBG("LEN_REM %d\n", skb->len);
-
-		if (skb->len < IPA_PKT_STATUS_SIZE) {
-			WARN_ON(sys->prev_skb != NULL);
-			IPADBG("status straddles buffer\n");
-			sys->prev_skb = skb;
-			sys->len_partial = skb->len;
-			return rc;
-		}
-
-		status = (struct ipa_hw_pkt_status *)skb->data;
-		IPADBG("STATUS opcode=%d src=%d dst=%d len=%d\n",
-				status->status_opcode, status->endp_src_idx,
-				status->endp_dest_idx, status->pkt_len);
-
-		if (status->status_opcode !=
-			IPA_HW_STATUS_OPCODE_DROPPED_PACKET &&
-			status->status_opcode !=
-			IPA_HW_STATUS_OPCODE_PACKET &&
-			status->status_opcode !=
-			IPA_HW_STATUS_OPCODE_SUSPENDED_PACKET &&
-			status->status_opcode !=
-			IPA_HW_STATUS_OPCODE_XLAT_PACKET) {
-			IPAERR("unsupported opcode(%d)\n",
-				status->status_opcode);
-			skb_pull(skb, IPA_PKT_STATUS_SIZE);
-			continue;
-		}
-		IPA_STATS_EXCP_CNT(status->exception,
-				ipa_ctx->stats.rx_excp_pkts);
-		if (status->endp_dest_idx >= ipa_ctx->ipa_num_pipes ||
-			status->endp_src_idx >= ipa_ctx->ipa_num_pipes ||
-			status->pkt_len > IPA_GENERIC_AGGR_BYTE_LIMIT * 1024) {
-			IPAERR("status fields invalid\n");
-			WARN_ON(1);
-			BUG();
-		}
-		if (status->status_mask & IPA_HW_PKT_STATUS_MASK_TAG_VALID) {
-			struct ipa_tag_completion *comp;
-			IPADBG("TAG packet arrived\n");
-			if (status->tag_f_2 == IPA_COOKIE) {
-				skb_pull(skb, IPA_PKT_STATUS_SIZE);
-				if (skb->len < sizeof(comp)) {
-					IPAERR("TAG arrived without packet\n");
-					return rc;
-				}
-				memcpy(&comp, skb->data, sizeof(comp));
-				skb_pull(skb, sizeof(comp) +
-						IPA_SIZE_DL_CSUM_META_TRAILER);
-				complete(&comp->comp);
-				if (atomic_dec_return(&comp->cnt) == 0)
-					kfree(comp);
-				continue;
-			} else {
-				IPADBG("ignoring TAG with wrong cookie\n");
-			}
-		}
-		if (status->pkt_len == 0) {
-			IPADBG("Skip aggr close status\n");
-			skb_pull(skb, IPA_PKT_STATUS_SIZE);
-			IPA_STATS_INC_CNT(ipa_ctx->stats.aggr_close);
-			IPA_STATS_DEC_CNT(
-				ipa_ctx->stats.rx_excp_pkts[MAX_NUM_EXCP - 1]);
-			continue;
-		}
-		if (status->endp_dest_idx == (sys->ep - ipa_ctx->ep)) {
-			/* RX data */
-			src_pipe = status->endp_src_idx;
-
-			/*
-			 * A packet which is received back to the AP after
-			 * there was no route match.
-			 */
-			if (!status->exception && !status->route_match)
-				drop_packet = true;
-
-			if (skb->len == IPA_PKT_STATUS_SIZE &&
-					!status->exception) {
-				WARN_ON(sys->prev_skb != NULL);
-				IPADBG("Ins header in next buffer\n");
-				sys->prev_skb = skb;
-				sys->len_partial =	 skb->len;
-				return rc;
-			}
-
-			pad_len_byte = ((status->pkt_len + 3) & ~3) -
-					status->pkt_len;
-
-			len = status->pkt_len + pad_len_byte +
-				IPA_SIZE_DL_CSUM_META_TRAILER;
-			IPADBG("pad %d pkt_len %d len %d\n", pad_len_byte,
-					status->pkt_len, len);
-
-			if (status->exception ==
-					IPA_HW_PKT_STATUS_EXCEPTION_DEAGGR) {
-				IPADBG("Dropping packet on DeAggr Exception\n");
-				skb_pull(skb, len + IPA_PKT_STATUS_SIZE);
-				continue;
-			}
-
-			skb2 = ipa_skb_copy_for_client(skb,
-				status->pkt_len + IPA_PKT_STATUS_SIZE);
-			if (likely(skb2)) {
-				if (skb->len < len + IPA_PKT_STATUS_SIZE) {
-					IPADBG("SPL skb len %d len %d\n",
-							skb->len, len);
-					sys->prev_skb = skb2;
-					sys->len_rem = len - skb->len +
-						IPA_PKT_STATUS_SIZE;
-					sys->len_pad = pad_len_byte;
-					skb_pull(skb, skb->len);
-				} else {
-					skb_trim(skb2, status->pkt_len +
-							IPA_PKT_STATUS_SIZE);
-					IPADBG("rx avail for %d\n",
-							status->endp_dest_idx);
-					if (drop_packet)
-						dev_kfree_skb_any(skb2);
-					else {
-					skb2->truesize = skb2->len +
-						sizeof(struct sk_buff) +
-						(ALIGN(len +
-						IPA_PKT_STATUS_SIZE, 32) *
-						unused / used_align);
-						sys->ep->client_notify(
-							sys->ep->priv,
-							IPA_RECEIVE,
-							(unsigned long)(skb2));
-					}
-					skb_pull(skb, len +
-						IPA_PKT_STATUS_SIZE);
-				}
-			} else {
-				IPAERR("fail to alloc skb\n");
-				if (skb->len < len) {
-					sys->prev_skb = NULL;
-					sys->len_rem = len - skb->len +
-						IPA_PKT_STATUS_SIZE;
-					sys->len_pad = pad_len_byte;
-					skb_pull(skb, skb->len);
-				} else {
-					skb_pull(skb, len +
-						IPA_PKT_STATUS_SIZE);
-				}
-			}
-			/* TX comp */
-			ipa_wq_write_done_status(src_pipe);
-			IPADBG("tx comp imp for %d\n", src_pipe);
-		} else {
-			/* TX comp */
-			ipa_wq_write_done_status(status->endp_src_idx);
-			IPADBG("tx comp exp for %d\n", status->endp_src_idx);
-			skb_pull(skb, IPA_PKT_STATUS_SIZE);
-			IPA_STATS_INC_CNT(ipa_ctx->stats.stat_compl);
-			IPA_STATS_DEC_CNT(
-				ipa_ctx->stats.rx_excp_pkts[MAX_NUM_EXCP - 1]);
-		}
-	};
-
-	sys->free_skb(skb);
-	return rc;
-}
-
-static struct sk_buff *join_prev_skb(struct sk_buff *prev_skb,
-		struct sk_buff *skb, unsigned int len)
-{
-	struct sk_buff *skb2;
-
-	skb2 = skb_copy_expand(prev_skb, 0,
-			len, GFP_KERNEL);
-	if (likely(skb2)) {
-		memcpy(skb_put(skb2, len),
-			skb->data, len);
-	} else {
-		IPAERR("copy expand failed\n");
-		skb2 = NULL;
-	}
-	dev_kfree_skb_any(prev_skb);
-
-	return skb2;
-}
-
-static void wan_rx_handle_splt_pyld(struct sk_buff *skb,
-		struct ipa_sys_context *sys)
-{
-	struct sk_buff *skb2;
-
-	IPADBG("rem %d skb %d\n", sys->len_rem, skb->len);
-	if (sys->len_rem <= skb->len) {
-		if (sys->prev_skb) {
-			skb2 = join_prev_skb(sys->prev_skb, skb,
-					sys->len_rem);
-			if (likely(skb2)) {
-				IPADBG(
-					"removing Status element from skb and sending to WAN client");
-				skb_pull(skb2, IPA_PKT_STATUS_SIZE);
-				skb2->truesize = skb2->len +
-					sizeof(struct sk_buff);
-				sys->ep->client_notify(sys->ep->priv,
-					IPA_RECEIVE,
-					(unsigned long)(skb2));
-			}
-		}
-		skb_pull(skb, sys->len_rem);
-		sys->prev_skb = NULL;
-		sys->len_rem = 0;
-	} else {
-		if (sys->prev_skb) {
-			skb2 = join_prev_skb(sys->prev_skb, skb,
-					skb->len);
-			sys->prev_skb = skb2;
-		}
-		sys->len_rem -= skb->len;
-		skb_pull(skb, skb->len);
-	}
-}
-
-static int ipa_wan_rx_pyld_hdlr(struct sk_buff *skb,
-		struct ipa_sys_context *sys)
-{
-	int rc = 0;
-	struct ipa_hw_pkt_status *status;
-	struct sk_buff *skb2;
-	u16 pkt_len_with_pad;
-	u32 qmap_hdr;
-	int checksum_trailer_exists;
-	int frame_len;
-	int ep_idx;
-	unsigned int used = *(unsigned int *)skb->cb;
-	unsigned int used_align = ALIGN(used, 32);
-	unsigned long unused = IPA_GENERIC_RX_BUFF_BASE_SZ - used;
-
-	IPA_DUMP_BUFF(skb->data, 0, skb->len);
-	if (skb->len == 0) {
-		IPAERR("ZLT\n");
-		goto bail;
-	}
-
-	if (ipa_ctx->ipa_client_apps_wan_cons_agg_gro) {
-		sys->ep->client_notify(sys->ep->priv,
-					IPA_RECEIVE, (unsigned long)(skb));
-		return rc;
-	}
-	/*
-	 * payload splits across 2 buff or more,
-	 * take the start of the payload from prev_skb
-	 */
-	if (sys->len_rem)
-		wan_rx_handle_splt_pyld(skb, sys);
-
-
-	while (skb->len) {
-		IPADBG("LEN_REM %d\n", skb->len);
-		if (skb->len < IPA_PKT_STATUS_SIZE) {
-			IPAERR("status straddles buffer\n");
-			WARN_ON(1);
-			goto bail;
-		}
-		status = (struct ipa_hw_pkt_status *)skb->data;
-		IPADBG("STATUS opcode=%d src=%d dst=%d len=%d\n",
-				status->status_opcode, status->endp_src_idx,
-				status->endp_dest_idx, status->pkt_len);
-		if (status->status_opcode !=
-			IPA_HW_STATUS_OPCODE_DROPPED_PACKET &&
-			status->status_opcode !=
-			IPA_HW_STATUS_OPCODE_PACKET &&
-			status->status_opcode !=
-			IPA_HW_STATUS_OPCODE_XLAT_PACKET) {
-			IPAERR("unsupported opcode\n");
-			skb_pull(skb, IPA_PKT_STATUS_SIZE);
-			continue;
-		}
-		IPA_STATS_INC_CNT(ipa_ctx->stats.rx_pkts);
-		if (status->endp_dest_idx >= ipa_ctx->ipa_num_pipes ||
-			status->endp_src_idx >= ipa_ctx->ipa_num_pipes ||
-			status->pkt_len > IPA_GENERIC_AGGR_BYTE_LIMIT * 1024) {
-			IPAERR("status fields invalid\n");
-			WARN_ON(1);
-			goto bail;
-		}
-		if (status->pkt_len == 0) {
-			IPADBG("Skip aggr close status\n");
-			skb_pull(skb, IPA_PKT_STATUS_SIZE);
-			IPA_STATS_DEC_CNT(ipa_ctx->stats.rx_pkts);
-			IPA_STATS_INC_CNT(ipa_ctx->stats.wan_aggr_close);
-			continue;
-		}
-		ep_idx = ipa_get_ep_mapping(IPA_CLIENT_APPS_WAN_CONS);
-		if (status->endp_dest_idx != ep_idx) {
-			IPAERR("expected endp_dest_idx %d received %d\n",
-					ep_idx, status->endp_dest_idx);
-			WARN_ON(1);
-			goto bail;
-		}
-		/* RX data */
-		if (skb->len == IPA_PKT_STATUS_SIZE) {
-			IPAERR("Ins header in next buffer\n");
-			WARN_ON(1);
-			goto bail;
-		}
-		qmap_hdr = *(u32 *)(status+1);
-		/*
-		 * Take the pkt_len_with_pad from the last 2 bytes of the QMAP
-		 * header
-		 */
-
-		/*QMAP is BE: convert the pkt_len field from BE to LE*/
-		pkt_len_with_pad = ntohs((qmap_hdr>>16) & 0xffff);
-		IPADBG("pkt_len with pad %d\n", pkt_len_with_pad);
-		/*get the CHECKSUM_PROCESS bit*/
-		checksum_trailer_exists = status->status_mask &
-				IPA_HW_PKT_STATUS_MASK_CKSUM_PROCESS;
-		IPADBG("checksum_trailer_exists %d\n",
-				checksum_trailer_exists);
-
-		frame_len = IPA_PKT_STATUS_SIZE +
-			    IPA_QMAP_HEADER_LENGTH +
-			    pkt_len_with_pad;
-		if (checksum_trailer_exists)
-			frame_len += IPA_DL_CHECKSUM_LENGTH;
-		IPADBG("frame_len %d\n", frame_len);
-
-		skb2 = skb_clone(skb, GFP_KERNEL);
-		if (likely(skb2)) {
-			/*
-			 * the len of actual data is smaller than expected
-			 * payload split across 2 buff
-			 */
-			if (skb->len < frame_len) {
-				IPADBG("SPL skb len %d len %d\n",
-						skb->len, frame_len);
-				sys->prev_skb = skb2;
-				sys->len_rem = frame_len - skb->len;
-				skb_pull(skb, skb->len);
-			} else {
-				skb_trim(skb2, frame_len);
-				IPADBG("rx avail for %d\n",
-						status->endp_dest_idx);
-				IPADBG(
-					"removing Status element from skb and sending to WAN client");
-				skb_pull(skb2, IPA_PKT_STATUS_SIZE);
-				skb2->truesize = skb2->len +
-					sizeof(struct sk_buff) +
-					(ALIGN(frame_len, 32) *
-					 unused / used_align);
-				sys->ep->client_notify(sys->ep->priv,
-					IPA_RECEIVE, (unsigned long)(skb2));
-				skb_pull(skb, frame_len);
-			}
-		} else {
-			IPAERR("fail to clone\n");
-			if (skb->len < frame_len) {
-				sys->prev_skb = NULL;
-				sys->len_rem = frame_len - skb->len;
-				skb_pull(skb, skb->len);
-			} else {
-				skb_pull(skb, frame_len);
-			}
-		}
-	};
-bail:
-	sys->free_skb(skb);
-	return rc;
-}
-
-static int ipa_rx_pyld_hdlr(struct sk_buff *rx_skb, struct ipa_sys_context *sys)
-{
-	struct ipa_a5_mux_hdr *mux_hdr;
-	unsigned int pull_len;
-	unsigned int padding;
-	struct ipa_ep_context *ep;
-	unsigned int src_pipe;
-
-	mux_hdr = (struct ipa_a5_mux_hdr *)rx_skb->data;
-
-	src_pipe = mux_hdr->src_pipe_index;
-
-	IPADBG("RX pkt len=%d IID=0x%x src=%d, flags=0x%x, meta=0x%x\n",
-		rx_skb->len, ntohs(mux_hdr->interface_id),
-		src_pipe, mux_hdr->flags, ntohl(mux_hdr->metadata));
-
-	IPA_DUMP_BUFF(rx_skb->data, 0, rx_skb->len);
-
-	IPA_STATS_INC_CNT(ipa_ctx->stats.rx_pkts);
-	IPA_STATS_EXCP_CNT(mux_hdr->flags, ipa_ctx->stats.rx_excp_pkts);
-
-	/*
-	 * Any packets arriving over AMPDU_TX should be dispatched
-	 * to the regular WLAN RX data-path.
-	 */
-	if (unlikely(src_pipe == WLAN_AMPDU_TX_EP))
-		src_pipe = WLAN_PROD_TX_EP;
-
-	ep = &ipa_ctx->ep[src_pipe];
-	spin_lock(&ipa_ctx->disconnect_lock);
-	if (unlikely(src_pipe >= ipa_ctx->ipa_num_pipes ||
-		!ep->valid || !ep->client_notify)) {
-		IPAERR("drop pipe=%d ep_valid=%d client_notify=%p\n",
-		  src_pipe, ep->valid, ep->client_notify);
-		dev_kfree_skb_any(rx_skb);
-		spin_unlock(&ipa_ctx->disconnect_lock);
-		return 0;
-	}
-
-	pull_len = sizeof(struct ipa_a5_mux_hdr);
-
-	/*
-	 * IP packet starts on word boundary
-	 * remove the MUX header and any padding and pass the frame to
-	 * the client which registered a rx callback on the "src pipe"
-	 */
-	padding = ep->cfg.hdr.hdr_len & 0x3;
-	if (padding)
-		pull_len += 4 - padding;
-
-	IPADBG("pulling %d bytes from skb\n", pull_len);
-	skb_pull(rx_skb, pull_len);
-	ep->client_notify(ep->priv, IPA_RECEIVE,
-			(unsigned long)(rx_skb));
-	spin_unlock(&ipa_ctx->disconnect_lock);
-	return 0;
-}
-
-static struct sk_buff *ipa_get_skb_ipa_rx(unsigned int len, gfp_t flags)
-{
-	return __dev_alloc_skb(len, flags);
-}
-
-static struct sk_buff *ipa_get_skb_ipa_rx_headroom(unsigned int len,
-		gfp_t flags)
-{
-	struct sk_buff *skb;
-
-	skb = __dev_alloc_skb(len + IPA_HEADROOM, flags);
-	if (skb)
-		skb_reserve(skb, IPA_HEADROOM);
-
-	return skb;
-}
-
-static void ipa_free_skb_rx(struct sk_buff *skb)
-{
-	dev_kfree_skb_any(skb);
-}
-
-void ipa_lan_rx_cb(void *priv, enum ipa_dp_evt_type evt, unsigned long data)
-{
-	struct sk_buff *rx_skb = (struct sk_buff *)data;
-	struct ipa_hw_pkt_status *status;
-	struct ipa_ep_context *ep;
-	unsigned int src_pipe;
-	u32 metadata;
-
-	status = (struct ipa_hw_pkt_status *)rx_skb->data;
-	src_pipe = status->endp_src_idx;
-	metadata = status->metadata;
-	ep = &ipa_ctx->ep[src_pipe];
-	if (unlikely(src_pipe >= ipa_ctx->ipa_num_pipes ||
-		!ep->valid ||
-		!ep->client_notify)) {
-		IPAERR("drop pipe=%d ep_valid=%d client_notify=%p\n",
-		  src_pipe, ep->valid, ep->client_notify);
-		dev_kfree_skb_any(rx_skb);
-		return;
-	}
-	if (!status->exception)
-		skb_pull(rx_skb, IPA_PKT_STATUS_SIZE +
-				IPA_LAN_RX_HEADER_LENGTH);
-	else
-		skb_pull(rx_skb, IPA_PKT_STATUS_SIZE);
-
-	/* Metadata Info
-	   ------------------------------------------
-	   |   3     |   2     |    1        |  0   |
-	   | fw_desc | vdev_id | qmap mux id | Resv |
-	   ------------------------------------------
-	 */
-	*(u16 *)rx_skb->cb = ((metadata >> 16) & 0xFFFF);
-	IPADBG("meta_data: 0x%x cb: 0x%x\n",
-			metadata, *(u32 *)rx_skb->cb);
-
-	ep->client_notify(ep->priv, IPA_RECEIVE, (unsigned long)(rx_skb));
-}
-
-static void ipa_wq_rx_common(struct ipa_sys_context *sys, u32 size)
-{
-	struct ipa_rx_pkt_wrapper *rx_pkt_expected;
-	struct sk_buff *rx_skb;
-
-	if (unlikely(list_empty(&sys->head_desc_list))) {
-		WARN_ON(1);
-		return;
-	}
-	rx_pkt_expected = list_first_entry(&sys->head_desc_list,
-					   struct ipa_rx_pkt_wrapper,
-					   link);
-	list_del(&rx_pkt_expected->link);
-	sys->len--;
-	if (size)
-		rx_pkt_expected->len = size;
-	rx_skb = rx_pkt_expected->data.skb;
-	dma_unmap_single(ipa_ctx->pdev, rx_pkt_expected->data.dma_addr,
-			sys->rx_buff_sz, DMA_FROM_DEVICE);
-	skb_set_tail_pointer(rx_skb, rx_pkt_expected->len);
-	rx_skb->len = rx_pkt_expected->len;
-	*(unsigned int *)rx_skb->cb = rx_skb->len;
-	rx_skb->truesize = rx_pkt_expected->len + sizeof(struct sk_buff);
-	sys->pyld_hdlr(rx_skb, sys);
-	sys->repl_hdlr(sys);
-	kmem_cache_free(ipa_ctx->rx_pkt_wrapper_cache, rx_pkt_expected);
-
-}
-
-static void ipa_wlan_wq_rx_common(struct ipa_sys_context *sys, u32 size)
-{
-	struct ipa_rx_pkt_wrapper *rx_pkt_expected;
-	struct sk_buff *rx_skb;
-
-	if (unlikely(list_empty(&sys->head_desc_list))) {
-		WARN_ON(1);
-		return;
-	}
-	rx_pkt_expected = list_first_entry(&sys->head_desc_list,
-					   struct ipa_rx_pkt_wrapper,
-					   link);
-	list_del(&rx_pkt_expected->link);
-	sys->len--;
-
-	if (size)
-		rx_pkt_expected->len = size;
-
-	rx_skb = rx_pkt_expected->data.skb;
-	skb_set_tail_pointer(rx_skb, rx_pkt_expected->len);
-	rx_skb->len = rx_pkt_expected->len;
-	rx_skb->truesize = rx_pkt_expected->len + sizeof(struct sk_buff);
-	sys->ep->wstats.tx_pkts_rcvd++;
-	if (sys->len <= IPA_WLAN_RX_POOL_SZ_LOW_WM) {
-		ipa_free_skb(&rx_pkt_expected->data);
-		sys->ep->wstats.tx_pkts_dropped++;
-	} else {
-		sys->ep->wstats.tx_pkts_sent++;
-		sys->ep->client_notify(sys->ep->priv, IPA_RECEIVE,
-				(unsigned long)(&rx_pkt_expected->data));
-	}
-	ipa_replenish_wlan_rx_cache(sys);
-}
-
-static void ipa_dma_memcpy_notify(struct ipa_sys_context *sys,
-	struct sps_iovec *iovec)
-{
-	IPADBG("ENTER.\n");
-	if (unlikely(list_empty(&sys->head_desc_list))) {
-		IPAERR("descriptor list is empty!\n");
-		WARN_ON(1);
-		return;
-	}
-	if (!(iovec->flags & SPS_IOVEC_FLAG_EOT)) {
-		IPAERR("recieved unexpected event. sps flag is 0x%x\n"
-			, iovec->flags);
-		WARN_ON(1);
-		return;
-	}
-	sys->ep->client_notify(sys->ep->priv, IPA_RECEIVE,
-				(unsigned long)(iovec));
-	IPADBG("EXIT\n");
-}
-
-static void ipa_wq_rx_avail(struct work_struct *work)
-{
-	struct ipa_rx_pkt_wrapper *rx_pkt;
-	struct ipa_sys_context *sys;
-
-	rx_pkt = container_of(work, struct ipa_rx_pkt_wrapper, work);
-	if (unlikely(rx_pkt == NULL))
-		WARN_ON(1);
-	sys = rx_pkt->sys;
-	ipa_wq_rx_common(sys, 0);
-}
-
-/**
- * ipa_sps_irq_rx_no_aggr_notify() - Callback function which will be called by
- * the SPS driver after a Rx operation is complete.
- * Called in an interrupt context.
- * @notify:	SPS driver supplied notification struct
- *
- * This function defer the work for this event to a workqueue.
- */
-void ipa_sps_irq_rx_no_aggr_notify(struct sps_event_notify *notify)
-{
-	struct ipa_rx_pkt_wrapper *rx_pkt;
-
-	switch (notify->event_id) {
-	case SPS_EVENT_EOT:
-		rx_pkt = notify->data.transfer.user;
-		if (IPA_CLIENT_IS_APPS_CONS(rx_pkt->sys->ep->client))
-			atomic_set(&ipa_ctx->sps_pm.eot_activity, 1);
-		rx_pkt->len = notify->data.transfer.iovec.size;
-		IPADBG("event %d notified sys=%p len=%u\n", notify->event_id,
-				notify->user, rx_pkt->len);
-		queue_work(rx_pkt->sys->wq, &rx_pkt->work);
-		break;
-	default:
-		IPAERR("recieved unexpected event id %d sys=%p\n",
-				notify->event_id, notify->user);
-	}
-}
-
-static int ipa_odu_rx_pyld_hdlr(struct sk_buff *rx_skb,
-	struct ipa_sys_context *sys)
-{
-	if (sys->ep->client_notify) {
-		sys->ep->client_notify(sys->ep->priv, IPA_RECEIVE,
-			(unsigned long)(rx_skb));
-	} else {
-		dev_kfree_skb_any(rx_skb);
-		WARN_ON(1);
-	}
-
-	return 0;
-}
-
-static int ipa_assign_policy(struct ipa_sys_connect_params *in,
-		struct ipa_sys_context *sys)
-{
-	if (in->client == IPA_CLIENT_APPS_CMD_PROD) {
-		sys->policy = IPA_POLICY_INTR_MODE;
-		sys->sps_option = (SPS_O_AUTO_ENABLE | SPS_O_EOT);
-		sys->sps_callback = ipa_sps_irq_tx_no_aggr_notify;
-		return 0;
-	}
-
-	if (ipa_ctx->ipa_hw_type == IPA_HW_v1_1) {
-		if (in->client == IPA_CLIENT_APPS_LAN_WAN_PROD) {
-			sys->policy = IPA_POLICY_INTR_POLL_MODE;
-			sys->sps_option = (SPS_O_AUTO_ENABLE | SPS_O_EOT |
-					SPS_O_ACK_TRANSFERS);
-			sys->sps_callback = ipa_sps_irq_tx_notify;
-			INIT_WORK(&sys->work, ipa_wq_handle_tx);
-			INIT_DELAYED_WORK(&sys->switch_to_intr_work,
-				switch_to_intr_tx_work_func);
-			atomic_set(&sys->curr_polling_state, 0);
-		} else if (in->client == IPA_CLIENT_APPS_LAN_CONS) {
-			sys->policy = IPA_POLICY_INTR_POLL_MODE;
-			sys->sps_option = (SPS_O_AUTO_ENABLE | SPS_O_EOT |
-					SPS_O_ACK_TRANSFERS);
-			sys->sps_callback = ipa_sps_irq_rx_notify;
-			INIT_WORK(&sys->work, ipa_wq_handle_rx);
-			INIT_DELAYED_WORK(&sys->switch_to_intr_work,
-				switch_to_intr_rx_work_func);
-			INIT_DELAYED_WORK(&sys->replenish_rx_work,
-					replenish_rx_work_func);
-			atomic_set(&sys->curr_polling_state, 0);
-			sys->rx_buff_sz = IPA_RX_SKB_SIZE;
-			sys->rx_pool_sz = IPA_RX_POOL_CEIL;
-			sys->pyld_hdlr = ipa_rx_pyld_hdlr;
-			sys->get_skb = ipa_get_skb_ipa_rx;
-			sys->free_skb = ipa_free_skb_rx;
-			sys->repl_hdlr = ipa_replenish_rx_cache;
-		} else if (IPA_CLIENT_IS_PROD(in->client)) {
-			sys->policy = IPA_POLICY_INTR_MODE;
-			sys->sps_option = (SPS_O_AUTO_ENABLE | SPS_O_EOT);
-			sys->sps_callback = ipa_sps_irq_tx_no_aggr_notify;
-		} else {
-			IPAERR("Need to install a RX pipe hdlr\n");
-			WARN_ON(1);
-			return -EINVAL;
-		}
-	} else if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_0) {
-		sys->ep->status.status_en = true;
-		if (IPA_CLIENT_IS_PROD(in->client)) {
-			if (!sys->ep->skip_ep_cfg) {
-				sys->policy = IPA_POLICY_NOINTR_MODE;
-				sys->sps_option = SPS_O_AUTO_ENABLE;
-				sys->sps_callback = NULL;
-				sys->ep->status.status_ep = ipa_get_ep_mapping(
-						IPA_CLIENT_APPS_LAN_CONS);
-				if (IPA_CLIENT_IS_MEMCPY_DMA_PROD(in->client))
-					sys->ep->status.status_en = false;
-			} else {
-				sys->policy = IPA_POLICY_INTR_MODE;
-				sys->sps_option = (SPS_O_AUTO_ENABLE |
-						SPS_O_EOT);
-				sys->sps_callback =
-					ipa_sps_irq_tx_no_aggr_notify;
-			}
-		} else {
-			if (in->client == IPA_CLIENT_APPS_LAN_CONS ||
-			    in->client == IPA_CLIENT_APPS_WAN_CONS) {
-				sys->policy = IPA_POLICY_INTR_POLL_MODE;
-				sys->sps_option = (SPS_O_AUTO_ENABLE | SPS_O_EOT
-						| SPS_O_ACK_TRANSFERS);
-				sys->sps_callback = ipa_sps_irq_rx_notify;
-				INIT_WORK(&sys->work, ipa_wq_handle_rx);
-				INIT_DELAYED_WORK(&sys->switch_to_intr_work,
-					switch_to_intr_rx_work_func);
-				INIT_DELAYED_WORK(&sys->replenish_rx_work,
-						replenish_rx_work_func);
-				INIT_WORK(&sys->repl_work, ipa_wq_repl_rx);
-				atomic_set(&sys->curr_polling_state, 0);
-				sys->rx_buff_sz = IPA_GENERIC_RX_BUFF_SZ(
-					IPA_GENERIC_RX_BUFF_BASE_SZ) -
-					IPA_HEADROOM;
-				sys->get_skb = ipa_get_skb_ipa_rx_headroom;
-				sys->free_skb = ipa_free_skb_rx;
-				in->ipa_ep_cfg.aggr.aggr_en = IPA_ENABLE_AGGR;
-				in->ipa_ep_cfg.aggr.aggr = IPA_GENERIC;
-				in->ipa_ep_cfg.aggr.aggr_time_limit =
-					IPA_GENERIC_AGGR_TIME_LIMIT;
-				if (in->client == IPA_CLIENT_APPS_LAN_CONS) {
-					sys->pyld_hdlr = ipa_lan_rx_pyld_hdlr;
-					sys->rx_pool_sz =
-						IPA_GENERIC_RX_POOL_SZ;
-					in->ipa_ep_cfg.aggr.aggr_byte_limit =
-					IPA_GENERIC_AGGR_BYTE_LIMIT;
-					in->ipa_ep_cfg.aggr.aggr_pkt_limit =
-					IPA_GENERIC_AGGR_PKT_LIMIT;
-				} else if (in->client ==
-						IPA_CLIENT_APPS_WAN_CONS) {
-					sys->pyld_hdlr = ipa_wan_rx_pyld_hdlr;
-					sys->rx_pool_sz =
-						ipa_ctx->wan_rx_ring_size;
-					if (ipa_ctx->
-					ipa_client_apps_wan_cons_agg_gro) {
-						IPAERR("get close-by %u\n",
-						ipa_adjust_ra_buff_base_sz(
-						in->ipa_ep_cfg.aggr.
-						aggr_byte_limit));
-						IPAERR("set rx_buff_sz %lu\n",
-						(unsigned long int)
-						IPA_GENERIC_RX_BUFF_SZ(
-						ipa_adjust_ra_buff_base_sz(
-						in->ipa_ep_cfg.
-							aggr.aggr_byte_limit)));
-						/* disable ipa_status */
-						sys->ep->status.
-							status_en = false;
-						sys->rx_buff_sz =
-						IPA_GENERIC_RX_BUFF_SZ(
-						ipa_adjust_ra_buff_base_sz(
-						in->ipa_ep_cfg.aggr.
-							aggr_byte_limit));
-						in->ipa_ep_cfg.aggr.
-							aggr_byte_limit =
-						sys->rx_buff_sz < in->
-						ipa_ep_cfg.aggr.
-						aggr_byte_limit ?
-						IPA_ADJUST_AGGR_BYTE_LIMIT(
-						sys->rx_buff_sz) :
-						IPA_ADJUST_AGGR_BYTE_LIMIT(
-						in->ipa_ep_cfg.
-						aggr.aggr_byte_limit);
-						IPAERR("set aggr_limit %lu\n",
-						(unsigned long int)
-						in->ipa_ep_cfg.aggr.
-						aggr_byte_limit);
-					} else {
-						in->ipa_ep_cfg.aggr.
-							aggr_byte_limit =
-						IPA_GENERIC_AGGR_BYTE_LIMIT;
-						in->ipa_ep_cfg.aggr.
-							aggr_pkt_limit =
-						IPA_GENERIC_AGGR_PKT_LIMIT;
-					}
-				}
-				/*
-				 * When HOLB mitigation is enabled, there is
-				 * a need to replenish the buffers quickly.
-				 * Otherwise we may run into issues with Q6
-				 * ZIP requests.
-				 */
-				if (ipa_ctx->ipa_hw_type == IPA_HW_v2_6L)
-					sys->repl_trig_thresh =
-						 sys->rx_pool_sz / 16;
-				else
-					sys->repl_trig_thresh =
-						 sys->rx_pool_sz / 8;
-				if (nr_cpu_ids > 1)
-					sys->repl_hdlr =
-						ipa_fast_replenish_rx_cache;
-				else
-					sys->repl_hdlr = ipa_replenish_rx_cache;
-			} else if (IPA_CLIENT_IS_WLAN_CONS(in->client)) {
-				IPADBG("assigning policy to client:%d",
-					in->client);
-
-				sys->ep->status.status_en = false;
-				sys->policy = IPA_POLICY_INTR_POLL_MODE;
-				sys->sps_option = (SPS_O_AUTO_ENABLE | SPS_O_EOT
-						| SPS_O_ACK_TRANSFERS);
-				sys->sps_callback = ipa_sps_irq_rx_notify;
-				INIT_WORK(&sys->work, ipa_wq_handle_rx);
-				INIT_DELAYED_WORK(&sys->switch_to_intr_work,
-					switch_to_intr_rx_work_func);
-				INIT_DELAYED_WORK(&sys->replenish_rx_work,
-					replenish_rx_work_func);
-				atomic_set(&sys->curr_polling_state, 0);
-				sys->rx_buff_sz = IPA_WLAN_RX_BUFF_SZ;
-				sys->rx_pool_sz = in->desc_fifo_sz/
-					sizeof(struct sps_iovec) - 1;
-				if (sys->rx_pool_sz > IPA_WLAN_RX_POOL_SZ)
-					sys->rx_pool_sz = IPA_WLAN_RX_POOL_SZ;
-				sys->pyld_hdlr = NULL;
-				sys->repl_hdlr = ipa_replenish_wlan_rx_cache;
-				sys->get_skb = ipa_get_skb_ipa_rx;
-				sys->free_skb = ipa_free_skb_rx;
-				in->ipa_ep_cfg.aggr.aggr_en = IPA_BYPASS_AGGR;
-			} else if (IPA_CLIENT_IS_ODU_CONS(in->client)) {
-				IPADBG("assigning policy to client:%d",
-					in->client);
-
-				sys->ep->status.status_en = false;
-				sys->policy = IPA_POLICY_INTR_POLL_MODE;
-				sys->sps_option = (SPS_O_AUTO_ENABLE | SPS_O_EOT
-					| SPS_O_ACK_TRANSFERS);
-				sys->sps_callback = ipa_sps_irq_rx_notify;
-				INIT_WORK(&sys->work, ipa_wq_handle_rx);
-				INIT_DELAYED_WORK(&sys->switch_to_intr_work,
-					switch_to_intr_rx_work_func);
-				INIT_DELAYED_WORK(&sys->replenish_rx_work,
-					replenish_rx_work_func);
-				atomic_set(&sys->curr_polling_state, 0);
-				sys->rx_buff_sz = IPA_ODU_RX_BUFF_SZ;
-				sys->rx_pool_sz = in->desc_fifo_sz /
-					sizeof(struct sps_iovec) - 1;
-				if (sys->rx_pool_sz > IPA_ODU_RX_POOL_SZ)
-					sys->rx_pool_sz = IPA_ODU_RX_POOL_SZ;
-				sys->pyld_hdlr = ipa_odu_rx_pyld_hdlr;
-				sys->get_skb = ipa_get_skb_ipa_rx;
-				sys->free_skb = ipa_free_skb_rx;
-				sys->repl_hdlr = ipa_replenish_rx_cache;
-			} else if (in->client ==
-					IPA_CLIENT_MEMCPY_DMA_ASYNC_CONS) {
-				IPADBG("assigning policy to client:%d",
-					in->client);
-				sys->ep->status.status_en = false;
-				sys->policy = IPA_POLICY_INTR_POLL_MODE;
-				sys->sps_option = (SPS_O_AUTO_ENABLE | SPS_O_EOT
-					| SPS_O_ACK_TRANSFERS);
-				sys->sps_callback = ipa_sps_irq_rx_notify;
-				INIT_WORK(&sys->work, ipa_wq_handle_rx);
-				INIT_DELAYED_WORK(&sys->switch_to_intr_work,
-					switch_to_intr_rx_work_func);
-			} else if (in->client ==
-					IPA_CLIENT_MEMCPY_DMA_SYNC_CONS) {
-				IPADBG("assigning policy to client:%d",
-					in->client);
-				sys->ep->status.status_en = false;
-				sys->policy = IPA_POLICY_NOINTR_MODE;
-				sys->sps_option = SPS_O_AUTO_ENABLE |
-				SPS_O_ACK_TRANSFERS | SPS_O_POLL;
-			} else {
-				IPAERR("Need to install a RX pipe hdlr\n");
-				WARN_ON(1);
-				return -EINVAL;
-			}
-		}
-	} else {
-		IPAERR("Unsupported HW type %d\n", ipa_ctx->ipa_hw_type);
-		WARN_ON(1);
-		return -EINVAL;
-
-	}
-
-	return 0;
-}
-
-/**
- * ipa_tx_client_rx_notify_release() - Callback function
- * which will call the user supplied callback function to
- * release the skb, or release it on its own if no callback
- * function was supplied
- *
- * @user1: [in] - Data Descriptor
- * @user2: [in] - endpoint idx
- *
- * This notified callback is for the destination client
- * This function is supplied in ipa_tx_dp_mul
- */
-static void ipa_tx_client_rx_notify_release(void *user1, int user2)
-{
-	struct ipa_tx_data_desc *dd = (struct ipa_tx_data_desc *)user1;
-	int ep_idx = user2;
-
-	IPADBG("Received data desc anchor:%p\n", dd);
-
-	atomic_inc(&ipa_ctx->ep[ep_idx].avail_fifo_desc);
-	ipa_ctx->ep[ep_idx].wstats.rx_pkts_status_rcvd++;
-
-  /* wlan host driver waits till tx complete before unload */
-	IPADBG("ep=%d fifo_desc_free_count=%d\n",
-		ep_idx, atomic_read(&ipa_ctx->ep[ep_idx].avail_fifo_desc));
-	IPADBG("calling client notify callback with priv:%p\n",
-		ipa_ctx->ep[ep_idx].priv);
-
-	if (ipa_ctx->ep[ep_idx].client_notify) {
-		ipa_ctx->ep[ep_idx].client_notify(ipa_ctx->ep[ep_idx].priv,
-				IPA_WRITE_DONE, (unsigned long)user1);
-		ipa_ctx->ep[ep_idx].wstats.rx_hd_reply++;
-	}
-}
-/**
- * ipa_tx_client_rx_pkt_status() - Callback function
- * which will call the user supplied callback function to
- * increase the available fifo descriptor
- *
- * @user1: [in] - Data Descriptor
- * @user2: [in] - endpoint idx
- *
- * This notified callback is for the destination client
- * This function is supplied in ipa_tx_dp_mul
- */
-static void ipa_tx_client_rx_pkt_status(void *user1, int user2)
-{
-	int ep_idx = user2;
-
-	atomic_inc(&ipa_ctx->ep[ep_idx].avail_fifo_desc);
-	ipa_ctx->ep[ep_idx].wstats.rx_pkts_status_rcvd++;
-}
-
-
-/**
- * ipa_tx_dp_mul() - Data-path tx handler for multiple packets
- * @src: [in] - Client that is sending data
- * @ipa_tx_data_desc:	[in] data descriptors from wlan
- *
- * this is used for to transfer data descriptors that received
- * from WLAN1_PROD pipe to IPA HW
- *
- * The function will send data descriptors from WLAN1_PROD (one
- * at a time) using sps_transfer_one. Will set EOT flag for last
- * descriptor Once this send was done from SPS point-of-view the
- * IPA driver will get notified by the supplied callback -
- * ipa_sps_irq_tx_no_aggr_notify()
- *
- * ipa_sps_irq_tx_no_aggr_notify will call to the user supplied
- * callback (from ipa_connect)
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_tx_dp_mul(enum ipa_client_type src,
-			struct ipa_tx_data_desc *data_desc)
-{
-	/* The second byte in wlan header holds qmap id */
-#define IPA_WLAN_HDR_QMAP_ID_OFFSET 1
-	struct ipa_tx_data_desc *entry;
-	struct ipa_sys_context *sys;
-	struct ipa_desc desc = { 0 };
-	u32 num_desc, cnt;
-	int ep_idx;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	IPADBG("Received data desc anchor:%p\n", data_desc);
-
-	spin_lock_bh(&ipa_ctx->wc_memb.ipa_tx_mul_spinlock);
-
-	ep_idx = ipa_get_ep_mapping(src);
-	if (unlikely(ep_idx == -1)) {
-		IPAERR("dest EP does not exist.\n");
-		goto fail_send;
-	}
-	IPADBG("ep idx:%d\n", ep_idx);
-	sys = ipa_ctx->ep[ep_idx].sys;
-
-	if (unlikely(ipa_ctx->ep[ep_idx].valid == 0)) {
-		IPAERR("dest EP not valid.\n");
-		goto fail_send;
-	}
-	sys->ep->wstats.rx_hd_rcvd++;
-
-	/* Calculate the number of descriptors */
-	num_desc = 0;
-	list_for_each_entry(entry, &data_desc->link, link) {
-		num_desc++;
-	}
-	IPADBG("Number of Data Descriptors:%d", num_desc);
-
-	if (atomic_read(&sys->ep->avail_fifo_desc) < num_desc) {
-		IPAERR("Insufficient data descriptors available\n");
-		goto fail_send;
-	}
-
-	/* Assign callback only for last data descriptor */
-	cnt = 0;
-	list_for_each_entry(entry, &data_desc->link, link) {
-		IPADBG("Parsing data desc :%d\n", cnt);
-		cnt++;
-		((u8 *)entry->pyld_buffer)[IPA_WLAN_HDR_QMAP_ID_OFFSET] =
-			(u8)sys->ep->cfg.meta.qmap_id;
-		desc.pyld = entry->pyld_buffer;
-		desc.len = entry->pyld_len;
-		desc.type = IPA_DATA_DESC_SKB;
-		desc.user1 = data_desc;
-		desc.user2 = ep_idx;
-		IPADBG("priv:%p pyld_buf:0x%p pyld_len:%d\n",
-			entry->priv, desc.pyld, desc.len);
-
-		/* In case of last descriptor populate callback */
-		if (cnt == num_desc) {
-			IPADBG("data desc:%p\n", data_desc);
-			desc.callback = ipa_tx_client_rx_notify_release;
-		} else {
-			desc.callback = ipa_tx_client_rx_pkt_status;
-		}
-
-		IPADBG("calling ipa_send_one()\n");
-		if (ipa_send_one(sys, &desc, true)) {
-			IPAERR("fail to send skb\n");
-			sys->ep->wstats.rx_pkt_leak += (cnt-1);
-			sys->ep->wstats.rx_dp_fail++;
-			goto fail_send;
-		}
-
-		if (atomic_read(&sys->ep->avail_fifo_desc) >= 0)
-			atomic_dec(&sys->ep->avail_fifo_desc);
-
-		sys->ep->wstats.rx_pkts_rcvd++;
-		IPADBG("ep=%d fifo desc=%d\n",
-			ep_idx, atomic_read(&sys->ep->avail_fifo_desc));
-	}
-
-	sys->ep->wstats.rx_hd_processed++;
-	spin_unlock_bh(&ipa_ctx->wc_memb.ipa_tx_mul_spinlock);
-	return 0;
-
-fail_send:
-	spin_unlock_bh(&ipa_ctx->wc_memb.ipa_tx_mul_spinlock);
-	return -EFAULT;
-
-}
-EXPORT_SYMBOL(ipa_tx_dp_mul);
-
-void ipa_free_skb(struct ipa_rx_data *data)
-{
-	struct ipa_rx_pkt_wrapper *rx_pkt;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return;
-	}
-
-	spin_lock_bh(&ipa_ctx->wc_memb.wlan_spinlock);
-
-	ipa_ctx->wc_memb.total_tx_pkts_freed++;
-	rx_pkt = container_of(data, struct ipa_rx_pkt_wrapper, data);
-
-	ipa_skb_recycle(rx_pkt->data.skb);
-	(void)skb_put(rx_pkt->data.skb, IPA_WLAN_RX_BUFF_SZ);
-
-	list_add_tail(&rx_pkt->link,
-		&ipa_ctx->wc_memb.wlan_comm_desc_list);
-	ipa_ctx->wc_memb.wlan_comm_free_cnt++;
-
-	spin_unlock_bh(&ipa_ctx->wc_memb.wlan_spinlock);
-}
-EXPORT_SYMBOL(ipa_free_skb);
-
-/* Functions added to support kernel tests */
-
-int ipa_sys_setup(struct ipa_sys_connect_params *sys_in,
-			unsigned long *ipa_bam_hdl,
-			u32 *ipa_pipe_num, u32 *clnt_hdl)
-{
-	struct ipa_ep_context *ep;
-	int ipa_ep_idx;
-	int result = -EINVAL;
-
-	if (sys_in == NULL || clnt_hdl == NULL) {
-		IPAERR("NULL args\n");
-		goto fail_gen;
-	}
-
-	if (ipa_bam_hdl == NULL || ipa_pipe_num == NULL) {
-		IPAERR("NULL args\n");
-		goto fail_gen;
-	}
-	if (sys_in->client >= IPA_CLIENT_MAX) {
-		IPAERR("bad parm client:%d\n", sys_in->client);
-		goto fail_gen;
-	}
-
-	ipa_ep_idx = ipa_get_ep_mapping(sys_in->client);
-	if (ipa_ep_idx == -1) {
-		IPAERR("Invalid client :%d\n", sys_in->client);
-		goto fail_gen;
-	}
-
-	ep = &ipa_ctx->ep[ipa_ep_idx];
-
-	ipa_inc_client_enable_clks();
-
-	if (ep->valid == 1) {
-		if (sys_in->client != IPA_CLIENT_APPS_LAN_WAN_PROD) {
-			IPAERR("EP %d already allocated\n", ipa_ep_idx);
-			goto fail_and_disable_clocks;
-		} else {
-			if (ipa_cfg_ep_hdr(ipa_ep_idx,
-						&sys_in->ipa_ep_cfg.hdr)) {
-				IPAERR("fail to configure hdr prop of EP %d\n",
-						ipa_ep_idx);
-				result = -EFAULT;
-				goto fail_and_disable_clocks;
-			}
-			if (ipa_cfg_ep_cfg(ipa_ep_idx,
-						&sys_in->ipa_ep_cfg.cfg)) {
-				IPAERR("fail to configure cfg prop of EP %d\n",
-						ipa_ep_idx);
-				result = -EFAULT;
-				goto fail_and_disable_clocks;
-			}
-			IPAERR("client %d (ep: %d) overlay ok sys=%p\n",
-					sys_in->client, ipa_ep_idx, ep->sys);
-			ep->client_notify = sys_in->notify;
-			ep->priv = sys_in->priv;
-			*clnt_hdl = ipa_ep_idx;
-			if (!ep->keep_ipa_awake)
-				ipa_dec_client_disable_clks();
-
-			return 0;
-		}
-	}
-
-	memset(ep, 0, offsetof(struct ipa_ep_context, sys));
-
-	ep->valid = 1;
-	ep->client = sys_in->client;
-	ep->client_notify = sys_in->notify;
-	ep->priv = sys_in->priv;
-	ep->keep_ipa_awake = true;
-
-	result = ipa_enable_data_path(ipa_ep_idx);
-	if (result) {
-		IPAERR("enable data path failed res=%d clnt=%d.\n",
-				 result, ipa_ep_idx);
-		goto fail_gen2;
-	}
-
-	if (!ep->skip_ep_cfg) {
-		if (ipa_cfg_ep(ipa_ep_idx, &sys_in->ipa_ep_cfg)) {
-			IPAERR("fail to configure EP.\n");
-			goto fail_gen2;
-		}
-		if (ipa_cfg_ep_status(ipa_ep_idx, &ep->status)) {
-			IPAERR("fail to configure status of EP.\n");
-			goto fail_gen2;
-		}
-		IPADBG("ep configuration successful\n");
-	} else {
-		IPADBG("skipping ep configuration\n");
-	}
-
-	*clnt_hdl = ipa_ep_idx;
-
-	*ipa_pipe_num = ipa_ep_idx;
-	*ipa_bam_hdl = ipa_ctx->bam_handle;
-
-	if (!ep->keep_ipa_awake)
-		ipa_dec_client_disable_clks();
-
-	ipa_ctx->skip_ep_cfg_shadow[ipa_ep_idx] = ep->skip_ep_cfg;
-	IPADBG("client %d (ep: %d) connected sys=%p\n", sys_in->client,
-			ipa_ep_idx, ep->sys);
-
-	return 0;
-
-fail_gen2:
-fail_and_disable_clocks:
-	ipa_dec_client_disable_clks();
-fail_gen:
-	return result;
-}
-EXPORT_SYMBOL(ipa_sys_setup);
-
-int ipa_sys_teardown(u32 clnt_hdl)
-{
-	struct ipa_ep_context *ep;
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm(Either endpoint or client hdl invalid)\n");
-		return -EINVAL;
-	}
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (!ep->keep_ipa_awake)
-		ipa_inc_client_enable_clks();
-
-	ipa_disable_data_path(clnt_hdl);
-	ep->valid = 0;
-
-	ipa_dec_client_disable_clks();
-
-	IPADBG("client (ep: %d) disconnected\n", clnt_hdl);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_sys_teardown);
-
-/**
- * ipa_adjust_ra_buff_base_sz()
- *
- * Return value: the largest power of two which is smaller
- * than the input value
- */
-static u32 ipa_adjust_ra_buff_base_sz(u32 aggr_byte_limit)
-{
-	aggr_byte_limit += IPA_MTU;
-	aggr_byte_limit += IPA_GENERIC_RX_BUFF_LIMIT;
-	aggr_byte_limit--;
-	aggr_byte_limit |= aggr_byte_limit >> 1;
-	aggr_byte_limit |= aggr_byte_limit >> 2;
-	aggr_byte_limit |= aggr_byte_limit >> 4;
-	aggr_byte_limit |= aggr_byte_limit >> 8;
-	aggr_byte_limit |= aggr_byte_limit >> 16;
-	aggr_byte_limit++;
-	return aggr_byte_limit >> 1;
-}
diff --git a/drivers/platform/msm/ipa/ipa_flt.c b/drivers/platform/msm/ipa/ipa_flt.c
deleted file mode 100644
index 4e2119b9..00000000
--- a/drivers/platform/msm/ipa/ipa_flt.c
+++ /dev/null
@@ -1,1508 +0,0 @@
-/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include "ipa_i.h"
-
-#define IPA_FLT_TABLE_WORD_SIZE			(4)
-#define IPA_FLT_ENTRY_MEMORY_ALLIGNMENT		(0x3)
-#define IPA_FLT_BIT_MASK			(0x1)
-#define IPA_FLT_TABLE_INDEX_NOT_FOUND		(-1)
-#define IPA_FLT_STATUS_OF_ADD_FAILED		(-1)
-#define IPA_FLT_STATUS_OF_DEL_FAILED		(-1)
-#define IPA_FLT_STATUS_OF_MDFY_FAILED		(-1)
-
-static int ipa_generate_hw_rule_from_eq(
-		const struct ipa_ipfltri_rule_eq *attrib, u8 **buf)
-{
-	int num_offset_meq_32 = attrib->num_offset_meq_32;
-	int num_ihl_offset_range_16 = attrib->num_ihl_offset_range_16;
-	int num_ihl_offset_meq_32 = attrib->num_ihl_offset_meq_32;
-	int num_offset_meq_128 = attrib->num_offset_meq_128;
-	int i;
-
-	if (attrib->tos_eq_present) {
-		*buf = ipa_write_8(attrib->tos_eq, *buf);
-		*buf = ipa_pad_to_32(*buf);
-	}
-
-	if (attrib->protocol_eq_present) {
-		*buf = ipa_write_8(attrib->protocol_eq, *buf);
-		*buf = ipa_pad_to_32(*buf);
-	}
-
-	if (num_offset_meq_32) {
-		*buf = ipa_write_8(attrib->offset_meq_32[0].offset, *buf);
-		*buf = ipa_write_32(attrib->offset_meq_32[0].mask, *buf);
-		*buf = ipa_write_32(attrib->offset_meq_32[0].value, *buf);
-		*buf = ipa_pad_to_32(*buf);
-		num_offset_meq_32--;
-	}
-
-	if (num_offset_meq_32) {
-		*buf = ipa_write_8(attrib->offset_meq_32[1].offset, *buf);
-		*buf = ipa_write_32(attrib->offset_meq_32[1].mask, *buf);
-		*buf = ipa_write_32(attrib->offset_meq_32[1].value, *buf);
-		*buf = ipa_pad_to_32(*buf);
-		num_offset_meq_32--;
-	}
-
-	if (num_ihl_offset_range_16) {
-		*buf = ipa_write_8(attrib->ihl_offset_range_16[0].offset, *buf);
-		*buf = ipa_write_16(attrib->ihl_offset_range_16[0].range_high,
-				*buf);
-		*buf = ipa_write_16(attrib->ihl_offset_range_16[0].range_low,
-				*buf);
-		*buf = ipa_pad_to_32(*buf);
-		num_ihl_offset_range_16--;
-	}
-
-	if (num_ihl_offset_range_16) {
-		*buf = ipa_write_8(attrib->ihl_offset_range_16[1].offset, *buf);
-		*buf = ipa_write_16(attrib->ihl_offset_range_16[1].range_high,
-				*buf);
-		*buf = ipa_write_16(attrib->ihl_offset_range_16[1].range_low,
-				*buf);
-		*buf = ipa_pad_to_32(*buf);
-		num_ihl_offset_range_16--;
-	}
-
-	if (attrib->ihl_offset_eq_16_present) {
-		*buf = ipa_write_8(attrib->ihl_offset_eq_16.offset, *buf);
-		*buf = ipa_write_16(attrib->ihl_offset_eq_16.value, *buf);
-		*buf = ipa_pad_to_32(*buf);
-	}
-
-	if (attrib->ihl_offset_eq_32_present) {
-		*buf = ipa_write_8(attrib->ihl_offset_eq_32.offset, *buf);
-		*buf = ipa_write_32(attrib->ihl_offset_eq_32.value, *buf);
-		*buf = ipa_pad_to_32(*buf);
-	}
-
-	if (num_ihl_offset_meq_32) {
-		*buf = ipa_write_8(attrib->ihl_offset_meq_32[0].offset, *buf);
-		*buf = ipa_write_32(attrib->ihl_offset_meq_32[0].mask, *buf);
-		*buf = ipa_write_32(attrib->ihl_offset_meq_32[0].value, *buf);
-		*buf = ipa_pad_to_32(*buf);
-		num_ihl_offset_meq_32--;
-	}
-
-	/* TODO check layout of 16 byte mask and value */
-	if (num_offset_meq_128) {
-		*buf = ipa_write_8(attrib->offset_meq_128[0].offset, *buf);
-		for (i = 0; i < 16; i++)
-			*buf = ipa_write_8(attrib->offset_meq_128[0].mask[i],
-					*buf);
-		for (i = 0; i < 16; i++)
-			*buf = ipa_write_8(attrib->offset_meq_128[0].value[i],
-					*buf);
-		*buf = ipa_pad_to_32(*buf);
-		num_offset_meq_128--;
-	}
-
-	if (num_offset_meq_128) {
-		*buf = ipa_write_8(attrib->offset_meq_128[1].offset, *buf);
-		for (i = 0; i < 16; i++)
-			*buf = ipa_write_8(attrib->offset_meq_128[1].mask[i],
-					*buf);
-		for (i = 0; i < 16; i++)
-			*buf = ipa_write_8(attrib->offset_meq_128[1].value[i],
-					*buf);
-		*buf = ipa_pad_to_32(*buf);
-		num_offset_meq_128--;
-	}
-
-	if (attrib->tc_eq_present) {
-		*buf = ipa_write_8(attrib->tc_eq, *buf);
-		*buf = ipa_pad_to_32(*buf);
-	}
-
-	if (attrib->fl_eq_present) {
-		*buf = ipa_write_32(attrib->fl_eq, *buf);
-		*buf = ipa_pad_to_32(*buf);
-	}
-
-	if (num_ihl_offset_meq_32) {
-		*buf = ipa_write_8(attrib->ihl_offset_meq_32[1].offset, *buf);
-		*buf = ipa_write_32(attrib->ihl_offset_meq_32[1].mask, *buf);
-		*buf = ipa_write_32(attrib->ihl_offset_meq_32[1].value, *buf);
-		*buf = ipa_pad_to_32(*buf);
-		num_ihl_offset_meq_32--;
-	}
-
-	if (attrib->metadata_meq32_present) {
-		*buf = ipa_write_8(attrib->metadata_meq32.offset, *buf);
-		*buf = ipa_write_32(attrib->metadata_meq32.mask, *buf);
-		*buf = ipa_write_32(attrib->metadata_meq32.value, *buf);
-		*buf = ipa_pad_to_32(*buf);
-	}
-
-	if (attrib->ipv4_frag_eq_present)
-		*buf = ipa_pad_to_32(*buf);
-
-	return 0;
-}
-
-/**
- * ipa_generate_flt_hw_rule() - generates the filtering hardware rule
- * @ip: the ip address family type
- * @entry: routing entry
- * @buf: output buffer, buf == NULL means
- *		caller wants to know the size of the rule as seen
- *		by HW so they did not pass a valid buffer, we will use a
- *		scratch buffer instead.
- *		With this scheme we are going to
- *		generate the rule twice, once to know size using scratch
- *		buffer and second to write the rule to the actual caller
- *		supplied buffer which is of required size
- *
- * Returns:	0 on success, negative on failure
- *
- * caller needs to hold any needed locks to ensure integrity
- *
- */
-static int ipa_generate_flt_hw_rule(enum ipa_ip_type ip,
-		struct ipa_flt_entry *entry, u8 *buf)
-{
-	struct ipa_flt_rule_hw_hdr *hdr;
-	const struct ipa_flt_rule *rule =
-		(const struct ipa_flt_rule *)&entry->rule;
-	u16 en_rule = 0;
-	u32 tmp[IPA_RT_FLT_HW_RULE_BUF_SIZE/4];
-	u8 *start;
-
-	if (buf == NULL) {
-		memset(tmp, 0, IPA_RT_FLT_HW_RULE_BUF_SIZE);
-		buf = (u8 *)tmp;
-	}
-
-	start = buf;
-	hdr = (struct ipa_flt_rule_hw_hdr *)buf;
-	hdr->u.hdr.action = entry->rule.action;
-	hdr->u.hdr.retain_hdr =  entry->rule.retain_hdr;
-	hdr->u.hdr.to_uc = entry->rule.to_uc;
-	if (entry->rt_tbl)
-		hdr->u.hdr.rt_tbl_idx = entry->rt_tbl->idx;
-	else
-		hdr->u.hdr.rt_tbl_idx = entry->rule.rt_tbl_idx;
-	hdr->u.hdr.rsvd = 0;
-	buf += sizeof(struct ipa_flt_rule_hw_hdr);
-
-	if (rule->eq_attrib_type) {
-		if (ipa_generate_hw_rule_from_eq(&rule->eq_attrib, &buf)) {
-			IPAERR("fail to generate hw rule\n");
-			return -EPERM;
-		}
-		en_rule = rule->eq_attrib.rule_eq_bitmap;
-	} else {
-		if (ipa_generate_hw_rule(ip, &rule->attrib, &buf, &en_rule)) {
-			IPAERR("fail to generate hw rule\n");
-			return -EPERM;
-		}
-	}
-
-	IPADBG("en_rule 0x%x, action=%d, rt_idx=%d, uc=%d, retain_hdr=%d\n",
-			en_rule,
-			hdr->u.hdr.action,
-			hdr->u.hdr.rt_tbl_idx,
-			hdr->u.hdr.to_uc,
-			hdr->u.hdr.retain_hdr);
-
-	hdr->u.hdr.en_rule = en_rule;
-	ipa_write_32(hdr->u.word, (u8 *)hdr);
-
-	if (entry->hw_len == 0) {
-		entry->hw_len = buf - start;
-	} else if (entry->hw_len != (buf - start)) {
-		IPAERR("hw_len differs b/w passes passed=%x calc=%td\n",
-		       entry->hw_len, (buf - start));
-		return -EPERM;
-	}
-
-	return 0;
-}
-
-/**
- * ipa_get_flt_hw_tbl_size() - returns the size of HW filtering table
- * @ip: the ip address family type
- * @hdr_sz: header size
- *
- * Returns:	size on success, negative on failure
- *
- * caller needs to hold any needed locks to ensure integrity
- *
- */
-static int ipa_get_flt_hw_tbl_size(enum ipa_ip_type ip, u32 *hdr_sz)
-{
-	struct ipa_flt_tbl *tbl;
-	struct ipa_flt_entry *entry;
-	u32 total_sz = 0;
-	u32 rule_set_sz;
-	int i;
-
-	*hdr_sz = 0;
-	tbl = &ipa_ctx->glob_flt_tbl[ip];
-	rule_set_sz = 0;
-	list_for_each_entry(entry, &tbl->head_flt_rule_list, link) {
-		if (ipa_generate_flt_hw_rule(ip, entry, NULL)) {
-			IPAERR("failed to find HW FLT rule size\n");
-			return -EPERM;
-		}
-		IPADBG("glob ip %d len %d\n", ip, entry->hw_len);
-		rule_set_sz += entry->hw_len;
-	}
-
-	if (rule_set_sz) {
-		tbl->sz = rule_set_sz + IPA_FLT_TABLE_WORD_SIZE;
-		/* this rule-set uses a word in header block */
-		*hdr_sz += IPA_FLT_TABLE_WORD_SIZE;
-		if (!tbl->in_sys) {
-			/* add the terminator */
-			total_sz += (rule_set_sz + IPA_FLT_TABLE_WORD_SIZE);
-			total_sz = (total_sz +
-					IPA_FLT_ENTRY_MEMORY_ALLIGNMENT) &
-					~IPA_FLT_ENTRY_MEMORY_ALLIGNMENT;
-		}
-	}
-
-	for (i = 0; i < ipa_ctx->ipa_num_pipes; i++) {
-		tbl = &ipa_ctx->flt_tbl[i][ip];
-		rule_set_sz = 0;
-		list_for_each_entry(entry, &tbl->head_flt_rule_list, link) {
-			if (ipa_generate_flt_hw_rule(ip, entry, NULL)) {
-				IPAERR("failed to find HW FLT rule size\n");
-				return -EPERM;
-			}
-			IPADBG("pipe %d len %d\n", i, entry->hw_len);
-			rule_set_sz += entry->hw_len;
-		}
-
-		if (rule_set_sz) {
-			tbl->sz = rule_set_sz + IPA_FLT_TABLE_WORD_SIZE;
-			/* this rule-set uses a word in header block */
-			*hdr_sz += IPA_FLT_TABLE_WORD_SIZE;
-			if (!tbl->in_sys) {
-				/* add the terminator */
-				total_sz += (rule_set_sz +
-					    IPA_FLT_TABLE_WORD_SIZE);
-				total_sz = (total_sz +
-					IPA_FLT_ENTRY_MEMORY_ALLIGNMENT) &
-					~IPA_FLT_ENTRY_MEMORY_ALLIGNMENT;
-			}
-		}
-	}
-
-	*hdr_sz += IPA_FLT_TABLE_WORD_SIZE;
-	total_sz += *hdr_sz;
-	IPADBG("FLT HW TBL SZ %d HDR SZ %d IP %d\n", total_sz, *hdr_sz, ip);
-
-	return total_sz;
-}
-
-static int ipa_generate_flt_hw_tbl_common(enum ipa_ip_type ip, u8 *base,
-		u8 *hdr, u32 body_start_offset, u8 *hdr2, u32 *hdr_top)
-{
-	struct ipa_flt_tbl *tbl;
-	struct ipa_flt_entry *entry;
-	int i;
-	u32 offset;
-	u8 *body;
-	struct ipa_mem_buffer flt_tbl_mem;
-	u8 *ftbl_membody;
-
-	*hdr_top = 0;
-	body = base;
-
-#define IPA_WRITE_FLT_HDR(idx, val) {			\
-	if (idx <= 5) {					\
-		*((u32 *)hdr + 1 + idx) = val;		\
-	} else if (idx >= 6 && idx <= 10) {		\
-		WARN_ON(1);				\
-	} else if (idx >= 11 && idx <= 19) {		\
-		*((u32 *)hdr2 + idx - 11) = val;	\
-	} else {					\
-		WARN_ON(1);				\
-	}						\
-}
-
-	tbl = &ipa_ctx->glob_flt_tbl[ip];
-
-	if (!list_empty(&tbl->head_flt_rule_list)) {
-		*hdr_top |= IPA_FLT_BIT_MASK;
-
-		if (!tbl->in_sys) {
-			offset = body - base + body_start_offset;
-			if (offset & IPA_FLT_ENTRY_MEMORY_ALLIGNMENT) {
-				IPAERR("offset is not word multiple %d\n",
-						offset);
-				goto proc_err;
-			}
-
-			offset &= ~IPA_FLT_ENTRY_MEMORY_ALLIGNMENT;
-			/* rule is at an offset from base */
-			offset |= IPA_FLT_BIT_MASK;
-
-			if (hdr2)
-				*(u32 *)hdr = offset;
-			else
-				hdr = ipa_write_32(offset, hdr);
-
-			/* generate the rule-set */
-			list_for_each_entry(entry, &tbl->head_flt_rule_list,
-					link) {
-				if (ipa_generate_flt_hw_rule(ip, entry, body)) {
-					IPAERR("failed to gen HW FLT rule\n");
-					goto proc_err;
-				}
-				body += entry->hw_len;
-			}
-
-			/* write the rule-set terminator */
-			body = ipa_write_32(0, body);
-			if ((long)body & IPA_FLT_ENTRY_MEMORY_ALLIGNMENT)
-				/* advance body to next word boundary */
-				body = body + (IPA_FLT_TABLE_WORD_SIZE -
-					((long)body &
-					IPA_FLT_ENTRY_MEMORY_ALLIGNMENT));
-		} else {
-			if (tbl->sz == 0) {
-				IPAERR("tbl size is 0\n");
-				WARN_ON(1);
-				goto proc_err;
-			}
-
-			/* allocate memory for the flt tbl */
-			flt_tbl_mem.size = tbl->sz;
-			flt_tbl_mem.base =
-			   dma_alloc_coherent(ipa_ctx->pdev, flt_tbl_mem.size,
-					   &flt_tbl_mem.phys_base, GFP_KERNEL);
-			if (!flt_tbl_mem.base) {
-				IPAERR("fail to alloc DMA buff of size %d\n",
-						flt_tbl_mem.size);
-				WARN_ON(1);
-				goto proc_err;
-			}
-
-			WARN_ON(flt_tbl_mem.phys_base &
-				IPA_FLT_ENTRY_MEMORY_ALLIGNMENT);
-			ftbl_membody = flt_tbl_mem.base;
-			memset(flt_tbl_mem.base, 0, flt_tbl_mem.size);
-
-			if (hdr2)
-				*(u32 *)hdr = flt_tbl_mem.phys_base;
-			else
-				hdr = ipa_write_32(flt_tbl_mem.phys_base, hdr);
-
-			/* generate the rule-set */
-			list_for_each_entry(entry, &tbl->head_flt_rule_list,
-					link) {
-				if (ipa_generate_flt_hw_rule(ip, entry,
-							ftbl_membody)) {
-					IPAERR("failed to gen HW FLT rule\n");
-					WARN_ON(1);
-				}
-				ftbl_membody += entry->hw_len;
-			}
-
-			/* write the rule-set terminator */
-			ftbl_membody = ipa_write_32(0, ftbl_membody);
-			if (tbl->curr_mem.phys_base) {
-				WARN_ON(tbl->prev_mem.phys_base);
-				tbl->prev_mem = tbl->curr_mem;
-			}
-			tbl->curr_mem = flt_tbl_mem;
-		}
-	}
-
-	for (i = 0; i < ipa_ctx->ipa_num_pipes; i++) {
-		tbl = &ipa_ctx->flt_tbl[i][ip];
-		if (!list_empty(&tbl->head_flt_rule_list)) {
-			/* pipe "i" is at bit "i+1" */
-			*hdr_top |= (1 << (i + 1));
-
-			if (!tbl->in_sys) {
-				offset = body - base + body_start_offset;
-				if (offset & IPA_FLT_ENTRY_MEMORY_ALLIGNMENT) {
-					IPAERR("ofst is not word multiple %d\n",
-					       offset);
-					goto proc_err;
-				}
-				offset &= ~IPA_FLT_ENTRY_MEMORY_ALLIGNMENT;
-				/* rule is at an offset from base */
-				offset |= IPA_FLT_BIT_MASK;
-
-				if (hdr2)
-					IPA_WRITE_FLT_HDR(i, offset)
-				else
-					hdr = ipa_write_32(offset, hdr);
-
-				/* generate the rule-set */
-				list_for_each_entry(entry,
-						&tbl->head_flt_rule_list,
-						link) {
-					if (ipa_generate_flt_hw_rule(ip, entry,
-								body)) {
-						IPAERR("fail gen FLT rule\n");
-						goto proc_err;
-					}
-					body += entry->hw_len;
-				}
-
-				/* write the rule-set terminator */
-				body = ipa_write_32(0, body);
-				if ((long)body &
-					IPA_FLT_ENTRY_MEMORY_ALLIGNMENT)
-					/* advance body to next word boundary */
-					body = body + (IPA_FLT_TABLE_WORD_SIZE -
-						((long)body &
-					IPA_FLT_ENTRY_MEMORY_ALLIGNMENT));
-			} else {
-				if (tbl->sz == 0) {
-					IPAERR("tbl size is 0\n");
-					WARN_ON(1);
-					goto proc_err;
-				}
-
-				/* allocate memory for the flt tbl */
-				flt_tbl_mem.size = tbl->sz;
-				flt_tbl_mem.base =
-				   dma_alloc_coherent(ipa_ctx->pdev,
-						   flt_tbl_mem.size,
-						   &flt_tbl_mem.phys_base,
-						   GFP_KERNEL);
-				if (!flt_tbl_mem.base) {
-					IPAERR("fail alloc DMA buff size %d\n",
-							flt_tbl_mem.size);
-					WARN_ON(1);
-					goto proc_err;
-				}
-
-				WARN_ON(flt_tbl_mem.phys_base &
-				IPA_FLT_ENTRY_MEMORY_ALLIGNMENT);
-
-				ftbl_membody = flt_tbl_mem.base;
-				memset(flt_tbl_mem.base, 0, flt_tbl_mem.size);
-
-				if (hdr2)
-					IPA_WRITE_FLT_HDR(i,
-						flt_tbl_mem.phys_base)
-				else
-					hdr = ipa_write_32(
-						flt_tbl_mem.phys_base, hdr);
-
-				/* generate the rule-set */
-				list_for_each_entry(entry,
-						&tbl->head_flt_rule_list,
-						link) {
-					if (ipa_generate_flt_hw_rule(ip, entry,
-							ftbl_membody)) {
-						IPAERR("fail gen FLT rule\n");
-						WARN_ON(1);
-					}
-					ftbl_membody += entry->hw_len;
-				}
-
-				/* write the rule-set terminator */
-				ftbl_membody =
-					ipa_write_32(0, ftbl_membody);
-				if (tbl->curr_mem.phys_base) {
-					WARN_ON(tbl->prev_mem.phys_base);
-					tbl->prev_mem = tbl->curr_mem;
-				}
-				tbl->curr_mem = flt_tbl_mem;
-			}
-		}
-	}
-
-	return 0;
-
-proc_err:
-	return -EPERM;
-}
-
-
-/**
- * ipa_generate_flt_hw_tbl() - generates the filtering hardware table
- * @ip:	[in] the ip address family type
- * @mem:	[out] buffer to put the filtering table
- *
- * Returns:	0 on success, negative on failure
- */
-static int ipa_generate_flt_hw_tbl_v1_1(enum ipa_ip_type ip,
-		struct ipa_mem_buffer *mem)
-{
-	u32 hdr_top = 0;
-	u32 hdr_sz;
-	u8 *hdr;
-	u8 *body;
-	u8 *base;
-	int res;
-
-	res = ipa_get_flt_hw_tbl_size(ip, &hdr_sz);
-	if (res < 0) {
-		IPAERR("ipa_get_flt_hw_tbl_size failed %d\n", res);
-		return res;
-	}
-
-	mem->size = res;
-	mem->size = IPA_HW_TABLE_ALIGNMENT(mem->size);
-
-	if (mem->size == 0) {
-		IPAERR("flt tbl empty ip=%d\n", ip);
-		goto error;
-	}
-	mem->base = dma_alloc_coherent(ipa_ctx->pdev, mem->size,
-			&mem->phys_base, GFP_KERNEL);
-	if (!mem->base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem->size);
-		goto error;
-	}
-
-	memset(mem->base, 0, mem->size);
-
-	/* build the flt tbl in the DMA buffer to submit to IPA HW */
-	base = hdr = (u8 *)mem->base;
-	body = base + hdr_sz;
-
-	/* write a dummy header to move cursor */
-	hdr = ipa_write_32(hdr_top, hdr);
-
-	if (ipa_generate_flt_hw_tbl_common(ip, body, hdr, hdr_sz, 0,
-				&hdr_top)) {
-		IPAERR("fail to generate FLT HW table\n");
-		goto proc_err;
-	}
-
-	/* now write the hdr_top */
-	ipa_write_32(hdr_top, base);
-
-	IPA_DUMP_BUFF(mem->base, mem->phys_base, mem->size);
-
-	return 0;
-
-proc_err:
-	dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base, mem->phys_base);
-error:
-	return -EPERM;
-}
-
-static void __ipa_reap_sys_flt_tbls(enum ipa_ip_type ip)
-{
-	struct ipa_flt_tbl *tbl;
-	int i;
-
-	tbl = &ipa_ctx->glob_flt_tbl[ip];
-	if (tbl->prev_mem.phys_base) {
-		IPADBG("reaping glob flt tbl (prev) ip=%d\n", ip);
-		dma_free_coherent(ipa_ctx->pdev, tbl->prev_mem.size,
-				tbl->prev_mem.base, tbl->prev_mem.phys_base);
-		memset(&tbl->prev_mem, 0, sizeof(tbl->prev_mem));
-	}
-
-	if (list_empty(&tbl->head_flt_rule_list)) {
-		if (tbl->curr_mem.phys_base) {
-			IPADBG("reaping glob flt tbl (curr) ip=%d\n", ip);
-			dma_free_coherent(ipa_ctx->pdev, tbl->curr_mem.size,
-					tbl->curr_mem.base,
-					tbl->curr_mem.phys_base);
-			memset(&tbl->curr_mem, 0, sizeof(tbl->curr_mem));
-		}
-	}
-
-	for (i = 0; i < ipa_ctx->ipa_num_pipes; i++) {
-		tbl = &ipa_ctx->flt_tbl[i][ip];
-		if (tbl->prev_mem.phys_base) {
-			IPADBG("reaping flt tbl (prev) pipe=%d ip=%d\n", i, ip);
-			dma_free_coherent(ipa_ctx->pdev, tbl->prev_mem.size,
-					tbl->prev_mem.base,
-					tbl->prev_mem.phys_base);
-			memset(&tbl->prev_mem, 0, sizeof(tbl->prev_mem));
-		}
-
-		if (list_empty(&tbl->head_flt_rule_list)) {
-			if (tbl->curr_mem.phys_base) {
-				IPADBG("reaping flt tbl (curr) pipe=%d ip=%d\n",
-						i, ip);
-				dma_free_coherent(ipa_ctx->pdev,
-						tbl->curr_mem.size,
-						tbl->curr_mem.base,
-						tbl->curr_mem.phys_base);
-				memset(&tbl->curr_mem, 0,
-						sizeof(tbl->curr_mem));
-			}
-		}
-	}
-}
-
-int __ipa_commit_flt_v1_1(enum ipa_ip_type ip)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer *mem;
-	void *cmd;
-	struct ipa_ip_v4_filter_init *v4;
-	struct ipa_ip_v6_filter_init *v6;
-	u16 avail;
-	u16 size;
-
-	mem = kmalloc(sizeof(struct ipa_mem_buffer), GFP_KERNEL);
-	if (!mem) {
-		IPAERR("failed to alloc memory object\n");
-		goto fail_alloc_mem;
-	}
-
-	if (ip == IPA_IP_v4) {
-		avail = ipa_ctx->ip4_flt_tbl_lcl ? IPA_MEM_v1_RAM_V4_FLT_SIZE :
-			IPA_MEM_PART(v4_flt_size_ddr);
-		size = sizeof(struct ipa_ip_v4_filter_init);
-	} else {
-		avail = ipa_ctx->ip6_flt_tbl_lcl ? IPA_MEM_v1_RAM_V6_FLT_SIZE :
-			IPA_MEM_PART(v6_flt_size_ddr);
-		size = sizeof(struct ipa_ip_v6_filter_init);
-	}
-	cmd = kmalloc(size, GFP_KERNEL);
-	if (!cmd) {
-		IPAERR("failed to alloc immediate command object\n");
-		goto fail_alloc_cmd;
-	}
-
-	if (ipa_generate_flt_hw_tbl_v1_1(ip, mem)) {
-		IPAERR("fail to generate FLT HW TBL ip %d\n", ip);
-		goto fail_hw_tbl_gen;
-	}
-
-	if (mem->size > avail) {
-		IPAERR("tbl too big, needed %d avail %d\n", mem->size, avail);
-		goto fail_send_cmd;
-	}
-
-	if (ip == IPA_IP_v4) {
-		v4 = (struct ipa_ip_v4_filter_init *)cmd;
-		desc.opcode = IPA_IP_V4_FILTER_INIT;
-		v4->ipv4_rules_addr = mem->phys_base;
-		v4->size_ipv4_rules = mem->size;
-		v4->ipv4_addr = IPA_MEM_v1_RAM_V4_FLT_OFST;
-	} else {
-		v6 = (struct ipa_ip_v6_filter_init *)cmd;
-		desc.opcode = IPA_IP_V6_FILTER_INIT;
-		v6->ipv6_rules_addr = mem->phys_base;
-		v6->size_ipv6_rules = mem->size;
-		v6->ipv6_addr = IPA_MEM_v1_RAM_V6_FLT_OFST;
-	}
-
-	desc.pyld = cmd;
-	desc.len = size;
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem->base, mem->phys_base, mem->size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		goto fail_send_cmd;
-	}
-
-	__ipa_reap_sys_flt_tbls(ip);
-	dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base, mem->phys_base);
-	kfree(cmd);
-	kfree(mem);
-
-	return 0;
-
-fail_send_cmd:
-	if (mem->phys_base)
-		dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base,
-				mem->phys_base);
-fail_hw_tbl_gen:
-	kfree(cmd);
-fail_alloc_cmd:
-	kfree(mem);
-fail_alloc_mem:
-
-	return -EPERM;
-}
-
-static int ipa_generate_flt_hw_tbl_v2(enum ipa_ip_type ip,
-		struct ipa_mem_buffer *mem, struct ipa_mem_buffer *head1,
-		struct ipa_mem_buffer *head2)
-{
-	int i;
-	u32 hdr_sz;
-	int num_words;
-	u32 *entr;
-	u32 body_start_offset;
-	u32 hdr_top;
-	int res;
-
-	if (ip == IPA_IP_v4)
-		body_start_offset = IPA_MEM_PART(apps_v4_flt_ofst) -
-			IPA_MEM_PART(v4_flt_ofst);
-	else
-		body_start_offset = IPA_MEM_PART(apps_v6_flt_ofst) -
-			IPA_MEM_PART(v6_flt_ofst);
-
-	num_words = 7;
-	head1->size = num_words * 4;
-	head1->base = dma_alloc_coherent(ipa_ctx->pdev, head1->size,
-			&head1->phys_base, GFP_KERNEL);
-	if (!head1->base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", head1->size);
-		goto err;
-	}
-	entr = (u32 *)head1->base;
-	for (i = 0; i < num_words; i++) {
-		*entr = ipa_ctx->empty_rt_tbl_mem.phys_base;
-		entr++;
-	}
-
-	num_words = 9;
-	head2->size = num_words * 4;
-	head2->base = dma_alloc_coherent(ipa_ctx->pdev, head2->size,
-			&head2->phys_base, GFP_KERNEL);
-	if (!head2->base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", head2->size);
-		goto head_err;
-	}
-	entr = (u32 *)head2->base;
-	for (i = 0; i < num_words; i++) {
-		*entr = ipa_ctx->empty_rt_tbl_mem.phys_base;
-		entr++;
-	}
-
-	res = ipa_get_flt_hw_tbl_size(ip, &hdr_sz);
-	if (res < 0) {
-		IPAERR("ipa_get_flt_hw_tbl_size failed %d\n", res);
-		goto body_err;
-	}
-
-	mem->size = res;
-	mem->size -= hdr_sz;
-	mem->size = IPA_HW_TABLE_ALIGNMENT(mem->size);
-
-	if (mem->size) {
-		mem->base = dma_alloc_coherent(ipa_ctx->pdev, mem->size,
-				&mem->phys_base, GFP_KERNEL);
-		if (!mem->base) {
-			IPAERR("fail to alloc DMA buff of size %d\n",
-					mem->size);
-			goto body_err;
-		}
-		memset(mem->base, 0, mem->size);
-	}
-
-	if (ipa_generate_flt_hw_tbl_common(ip, mem->base, head1->base,
-				body_start_offset, head2->base, &hdr_top)) {
-		IPAERR("fail to generate FLT HW table\n");
-		goto proc_err;
-	}
-
-	IPADBG("HEAD1\n");
-	IPA_DUMP_BUFF(head1->base, head1->phys_base, head1->size);
-	IPADBG("HEAD2\n");
-	IPA_DUMP_BUFF(head2->base, head2->phys_base, head2->size);
-	if (mem->size) {
-		IPADBG("BODY\n");
-		IPA_DUMP_BUFF(mem->base, mem->phys_base, mem->size);
-	}
-
-	return 0;
-
-proc_err:
-	if (mem->size)
-		dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base,
-				mem->phys_base);
-body_err:
-	dma_free_coherent(ipa_ctx->pdev, head2->size, head2->base,
-			head2->phys_base);
-head_err:
-	dma_free_coherent(ipa_ctx->pdev, head1->size, head1->base,
-			head1->phys_base);
-err:
-	return -EPERM;
-}
-
-int __ipa_commit_flt_v2(enum ipa_ip_type ip)
-{
-	struct ipa_desc *desc;
-	struct ipa_hw_imm_cmd_dma_shared_mem *cmd;
-	struct ipa_mem_buffer body;
-	struct ipa_mem_buffer head1;
-	struct ipa_mem_buffer head2;
-	int rc = 0;
-	u32 local_addrb;
-	u32 local_addrh;
-	bool lcl;
-	int num_desc = 0;
-	int i;
-	u16 avail;
-
-	desc = kzalloc(16 * sizeof(*desc), GFP_ATOMIC);
-	if (desc == NULL) {
-		IPAERR("fail to alloc desc blob ip %d\n", ip);
-		rc = -ENOMEM;
-		goto fail_desc;
-	}
-
-	cmd = kzalloc(16 * sizeof(*cmd), GFP_ATOMIC);
-	if (cmd == NULL) {
-		IPAERR("fail to alloc cmd blob ip %d\n", ip);
-		rc = -ENOMEM;
-		goto fail_imm;
-	}
-
-	if (ip == IPA_IP_v4) {
-		avail = ipa_ctx->ip4_flt_tbl_lcl ?
-			IPA_MEM_PART(apps_v4_flt_size) :
-			IPA_MEM_PART(v4_flt_size_ddr);
-		local_addrh = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(v4_flt_ofst) + 4;
-		local_addrb = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(apps_v4_flt_ofst);
-		lcl = ipa_ctx->ip4_flt_tbl_lcl;
-	} else {
-		avail = ipa_ctx->ip6_flt_tbl_lcl ?
-			IPA_MEM_PART(apps_v6_flt_size) :
-			IPA_MEM_PART(v6_flt_size_ddr);
-		local_addrh = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(v6_flt_ofst) + 4;
-		local_addrb = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(apps_v6_flt_ofst);
-		lcl = ipa_ctx->ip6_flt_tbl_lcl;
-	}
-
-	if (ipa_generate_flt_hw_tbl_v2(ip, &body, &head1, &head2)) {
-		IPAERR("fail to generate FLT HW TBL ip %d\n", ip);
-		rc = -EFAULT;
-		goto fail_gen;
-	}
-
-	if (body.size > avail) {
-		IPAERR("tbl too big, needed %d avail %d\n", body.size, avail);
-		goto fail_send_cmd;
-	}
-
-	cmd[num_desc].size = 4;
-	cmd[num_desc].system_addr = head1.phys_base;
-	cmd[num_desc].local_addr = local_addrh;
-
-	desc[num_desc].opcode = IPA_DMA_SHARED_MEM;
-	desc[num_desc].pyld = &cmd[num_desc];
-	desc[num_desc].len = sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-	desc[num_desc++].type = IPA_IMM_CMD_DESC;
-
-	for (i = 0; i < 6; i++) {
-		if (ipa_ctx->skip_ep_cfg_shadow[i]) {
-			IPADBG("skip %d\n", i);
-			continue;
-		}
-
-		if (ipa_get_ep_mapping(IPA_CLIENT_APPS_WAN_CONS) == i ||
-			ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_CONS) == i ||
-			ipa_get_ep_mapping(IPA_CLIENT_APPS_CMD_PROD) == i ||
-			(ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_WAN_PROD) == i
-			&& ipa_ctx->modem_cfg_emb_pipe_flt)) {
-			IPADBG("skip %d\n", i);
-			continue;
-		}
-
-		if (ip == IPA_IP_v4) {
-			local_addrh = ipa_ctx->smem_restricted_bytes +
-				IPA_MEM_PART(v4_flt_ofst) +
-				8 + i * 4;
-		} else {
-			local_addrh = ipa_ctx->smem_restricted_bytes +
-				IPA_MEM_PART(v6_flt_ofst) +
-				8 + i * 4;
-		}
-		cmd[num_desc].size = 4;
-		cmd[num_desc].system_addr = head1.phys_base + 4 + i * 4;
-		cmd[num_desc].local_addr = local_addrh;
-
-		desc[num_desc].opcode = IPA_DMA_SHARED_MEM;
-		desc[num_desc].pyld = &cmd[num_desc];
-		desc[num_desc].len =
-			sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-		desc[num_desc++].type = IPA_IMM_CMD_DESC;
-	}
-
-	for (i = 11; i < ipa_ctx->ipa_num_pipes; i++) {
-		if (ipa_ctx->skip_ep_cfg_shadow[i]) {
-			IPADBG("skip %d\n", i);
-			continue;
-		}
-		if (ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_WAN_PROD) == i &&
-			ipa_ctx->modem_cfg_emb_pipe_flt) {
-			IPADBG("skip %d\n", i);
-			continue;
-		}
-		if (ip == IPA_IP_v4) {
-			local_addrh = ipa_ctx->smem_restricted_bytes +
-				IPA_MEM_PART(v4_flt_ofst) +
-				13 * 4 + (i - 11) * 4;
-		} else {
-			local_addrh = ipa_ctx->smem_restricted_bytes +
-				IPA_MEM_PART(v6_flt_ofst) +
-				13 * 4 + (i - 11) * 4;
-		}
-		cmd[num_desc].size = 4;
-		cmd[num_desc].system_addr = head2.phys_base + (i - 11) * 4;
-		cmd[num_desc].local_addr = local_addrh;
-
-		desc[num_desc].opcode = IPA_DMA_SHARED_MEM;
-		desc[num_desc].pyld = &cmd[num_desc];
-		desc[num_desc].len =
-			sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-		desc[num_desc++].type = IPA_IMM_CMD_DESC;
-	}
-
-	if (lcl) {
-		cmd[num_desc].size = body.size;
-		cmd[num_desc].system_addr = body.phys_base;
-		cmd[num_desc].local_addr = local_addrb;
-
-		desc[num_desc].opcode = IPA_DMA_SHARED_MEM;
-		desc[num_desc].pyld = &cmd[num_desc];
-		desc[num_desc].len =
-			sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-		desc[num_desc++].type = IPA_IMM_CMD_DESC;
-
-		if (ipa_send_cmd(num_desc, desc)) {
-			IPAERR("fail to send immediate command\n");
-			rc = -EFAULT;
-			goto fail_send_cmd;
-		}
-	} else {
-		if (ipa_send_cmd(num_desc, desc)) {
-			IPAERR("fail to send immediate command\n");
-			rc = -EFAULT;
-			goto fail_send_cmd;
-		}
-	}
-
-	__ipa_reap_sys_flt_tbls(ip);
-
-fail_send_cmd:
-	if (body.size)
-		dma_free_coherent(ipa_ctx->pdev, body.size, body.base,
-				body.phys_base);
-	dma_free_coherent(ipa_ctx->pdev, head1.size, head1.base,
-			head1.phys_base);
-	dma_free_coherent(ipa_ctx->pdev, head2.size, head2.base,
-			head2.phys_base);
-fail_gen:
-	kfree(cmd);
-fail_imm:
-	kfree(desc);
-fail_desc:
-	return rc;
-}
-
-static int __ipa_add_flt_rule(struct ipa_flt_tbl *tbl, enum ipa_ip_type ip,
-			      const struct ipa_flt_rule *rule, u8 add_rear,
-			      u32 *rule_hdl)
-{
-	struct ipa_flt_entry *entry;
-	struct ipa_rt_tbl *rt_tbl = NULL;
-	int id;
-
-	if (rule->action != IPA_PASS_TO_EXCEPTION) {
-		if (!rule->eq_attrib_type) {
-			if (!rule->rt_tbl_hdl) {
-				IPAERR("invalid RT tbl\n");
-				goto error;
-			}
-
-			rt_tbl = ipa_id_find(rule->rt_tbl_hdl);
-			if (rt_tbl == NULL) {
-				IPAERR("RT tbl not found\n");
-				goto error;
-			}
-
-			if (rt_tbl->cookie != IPA_RT_TBL_COOKIE) {
-				IPAERR("RT table cookie is invalid\n");
-				goto error;
-			}
-		} else {
-			if (rule->rt_tbl_idx > ((ip == IPA_IP_v4) ?
-				IPA_MEM_PART(v4_modem_rt_index_hi) :
-				IPA_MEM_PART(v6_modem_rt_index_hi))) {
-				IPAERR("invalid RT tbl\n");
-				goto error;
-			}
-		}
-	}
-
-	entry = kmem_cache_zalloc(ipa_ctx->flt_rule_cache, GFP_KERNEL);
-	if (!entry) {
-		IPAERR("failed to alloc FLT rule object\n");
-		goto error;
-	}
-	INIT_LIST_HEAD(&entry->link);
-	entry->rule = *rule;
-	entry->cookie = IPA_FLT_COOKIE;
-	entry->rt_tbl = rt_tbl;
-	entry->tbl = tbl;
-	if (add_rear) {
-		if (tbl->sticky_rear)
-			list_add_tail(&entry->link,
-					tbl->head_flt_rule_list.prev);
-		else
-			list_add_tail(&entry->link, &tbl->head_flt_rule_list);
-	} else {
-		list_add(&entry->link, &tbl->head_flt_rule_list);
-	}
-	tbl->rule_cnt++;
-	if (entry->rt_tbl)
-		entry->rt_tbl->ref_cnt++;
-	id = ipa_id_alloc(entry);
-	if (id < 0) {
-		IPAERR("failed to add to tree\n");
-		WARN_ON(1);
-		goto ipa_insert_failed;
-	}
-	*rule_hdl = id;
-	entry->id = id;
-	IPADBG("add flt rule rule_cnt=%d\n", tbl->rule_cnt);
-
-	return 0;
-ipa_insert_failed:
-	tbl->rule_cnt--;
-	if (entry->rt_tbl)
-		entry->rt_tbl->ref_cnt--;
-	list_del(&entry->link);
-	kmem_cache_free(ipa_ctx->flt_rule_cache, entry);
-error:
-	return -EPERM;
-}
-
-static int __ipa_del_flt_rule(u32 rule_hdl)
-{
-	struct ipa_flt_entry *entry;
-	int id;
-
-	entry = ipa_id_find(rule_hdl);
-	if (entry == NULL) {
-		IPAERR("lookup failed\n");
-		return -EINVAL;
-	}
-
-	if (entry->cookie != IPA_FLT_COOKIE) {
-		IPAERR("bad params\n");
-		return -EINVAL;
-	}
-	id = entry->id;
-
-	list_del(&entry->link);
-	entry->tbl->rule_cnt--;
-	if (entry->rt_tbl)
-		entry->rt_tbl->ref_cnt--;
-	IPADBG("del flt rule rule_cnt=%d\n", entry->tbl->rule_cnt);
-	entry->cookie = 0;
-	kmem_cache_free(ipa_ctx->flt_rule_cache, entry);
-
-	/* remove the handle from the database */
-	ipa_id_remove(id);
-
-	return 0;
-}
-
-static int __ipa_mdfy_flt_rule(struct ipa_flt_rule_mdfy *frule,
-		enum ipa_ip_type ip)
-{
-	struct ipa_flt_entry *entry;
-	struct ipa_rt_tbl *rt_tbl = NULL;
-
-	entry = ipa_id_find(frule->rule_hdl);
-	if (entry == NULL) {
-		IPAERR("lookup failed\n");
-		goto error;
-	}
-
-	if (entry->cookie != IPA_FLT_COOKIE) {
-		IPAERR("bad params\n");
-		goto error;
-	}
-
-	if (entry->rt_tbl)
-		entry->rt_tbl->ref_cnt--;
-
-	if (frule->rule.action != IPA_PASS_TO_EXCEPTION) {
-		if (!frule->rule.eq_attrib_type) {
-			if (!frule->rule.rt_tbl_hdl) {
-				IPAERR("invalid RT tbl\n");
-				goto error;
-			}
-
-			rt_tbl = ipa_id_find(frule->rule.rt_tbl_hdl);
-			if (rt_tbl == NULL) {
-				IPAERR("RT tbl not found\n");
-				goto error;
-			}
-
-			if (rt_tbl->cookie != IPA_RT_TBL_COOKIE) {
-				IPAERR("RT table cookie is invalid\n");
-				goto error;
-			}
-		} else {
-			if (frule->rule.rt_tbl_idx > ((ip == IPA_IP_v4) ?
-				IPA_MEM_PART(v4_modem_rt_index_hi) :
-				IPA_MEM_PART(v6_modem_rt_index_hi))) {
-				IPAERR("invalid RT tbl\n");
-				goto error;
-			}
-		}
-	}
-
-	entry->rule = frule->rule;
-	entry->rt_tbl = rt_tbl;
-	if (entry->rt_tbl)
-		entry->rt_tbl->ref_cnt++;
-	entry->hw_len = 0;
-
-	return 0;
-
-error:
-	return -EPERM;
-}
-
-static int __ipa_add_global_flt_rule(enum ipa_ip_type ip,
-		const struct ipa_flt_rule *rule, u8 add_rear, u32 *rule_hdl)
-{
-	struct ipa_flt_tbl *tbl;
-
-	if (rule == NULL || rule_hdl == NULL) {
-		IPAERR("bad parms rule=%p rule_hdl=%p\n", rule, rule_hdl);
-
-		return -EINVAL;
-	}
-
-	tbl = &ipa_ctx->glob_flt_tbl[ip];
-	IPADBG("add global flt rule ip=%d\n", ip);
-
-	return __ipa_add_flt_rule(tbl, ip, rule, add_rear, rule_hdl);
-}
-
-static int __ipa_add_ep_flt_rule(enum ipa_ip_type ip, enum ipa_client_type ep,
-				 const struct ipa_flt_rule *rule, u8 add_rear,
-				 u32 *rule_hdl)
-{
-	struct ipa_flt_tbl *tbl;
-	int ipa_ep_idx;
-
-	if (rule == NULL || rule_hdl == NULL || ep >= IPA_CLIENT_MAX) {
-		IPAERR("bad parms rule=%p rule_hdl=%p ep=%d\n", rule,
-				rule_hdl, ep);
-
-		return -EINVAL;
-	}
-	ipa_ep_idx = ipa_get_ep_mapping(ep);
-	if (ipa_ep_idx == IPA_FLT_TABLE_INDEX_NOT_FOUND) {
-		IPAERR("ep not valid ep=%d\n", ep);
-		return -EINVAL;
-	}
-	if (ipa_ctx->ep[ipa_ep_idx].valid == 0)
-		IPADBG("ep not connected ep_idx=%d\n", ipa_ep_idx);
-
-	tbl = &ipa_ctx->flt_tbl[ipa_ep_idx][ip];
-	IPADBG("add ep flt rule ip=%d ep=%d\n", ip, ep);
-
-	return __ipa_add_flt_rule(tbl, ip, rule, add_rear, rule_hdl);
-}
-
-/**
- * ipa_add_flt_rule() - Add the specified filtering rules to SW and optionally
- * commit to IPA HW
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_add_flt_rule(struct ipa_ioc_add_flt_rule *rules)
-{
-	int i;
-	int result;
-
-	if (rules == NULL || rules->num_rules == 0 ||
-			rules->ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	for (i = 0; i < rules->num_rules; i++) {
-		if (rules->global)
-			result = __ipa_add_global_flt_rule(rules->ip,
-					&rules->rules[i].rule,
-					rules->rules[i].at_rear,
-					&rules->rules[i].flt_rule_hdl);
-		else
-			result = __ipa_add_ep_flt_rule(rules->ip, rules->ep,
-					&rules->rules[i].rule,
-					rules->rules[i].at_rear,
-					&rules->rules[i].flt_rule_hdl);
-		if (result) {
-			IPAERR("failed to add flt rule %d\n", i);
-			rules->rules[i].status = IPA_FLT_STATUS_OF_ADD_FAILED;
-		} else {
-			rules->rules[i].status = 0;
-		}
-	}
-
-	if (rules->commit)
-		if (ipa_ctx->ctrl->ipa_commit_flt(rules->ip)) {
-			result = -EPERM;
-			goto bail;
-		}
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_add_flt_rule);
-
-/**
- * ipa_del_flt_rule() - Remove the specified filtering rules from SW and
- * optionally commit to IPA HW
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_del_flt_rule(struct ipa_ioc_del_flt_rule *hdls)
-{
-	int i;
-	int result;
-
-	if (hdls == NULL || hdls->num_hdls == 0 || hdls->ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	for (i = 0; i < hdls->num_hdls; i++) {
-		if (__ipa_del_flt_rule(hdls->hdl[i].hdl)) {
-			IPAERR("failed to del rt rule %i\n", i);
-			hdls->hdl[i].status = IPA_FLT_STATUS_OF_DEL_FAILED;
-		} else {
-			hdls->hdl[i].status = 0;
-		}
-	}
-
-	if (hdls->commit)
-		if (ipa_ctx->ctrl->ipa_commit_flt(hdls->ip)) {
-			result = -EPERM;
-			goto bail;
-		}
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_del_flt_rule);
-
-/**
- * ipa_mdfy_flt_rule() - Modify the specified filtering rules in SW and optionally
- * commit to IPA HW
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_mdfy_flt_rule(struct ipa_ioc_mdfy_flt_rule *hdls)
-{
-	int i;
-	int result;
-
-	if (hdls == NULL || hdls->num_rules == 0 || hdls->ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	for (i = 0; i < hdls->num_rules; i++) {
-		if (__ipa_mdfy_flt_rule(&hdls->rules[i], hdls->ip)) {
-			IPAERR("failed to mdfy rt rule %i\n", i);
-			hdls->rules[i].status = IPA_FLT_STATUS_OF_MDFY_FAILED;
-		} else {
-			hdls->rules[i].status = 0;
-		}
-	}
-
-	if (hdls->commit)
-		if (ipa_ctx->ctrl->ipa_commit_flt(hdls->ip)) {
-			result = -EPERM;
-			goto bail;
-		}
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_mdfy_flt_rule);
-
-
-/**
- * ipa_commit_flt() - Commit the current SW filtering table of specified type to
- * IPA HW
- * @ip:	[in] the family of routing tables
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_commit_flt(enum ipa_ip_type ip)
-{
-	int result;
-
-	if (ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-
-	if (ipa_ctx->ctrl->ipa_commit_flt(ip)) {
-		result = -EPERM;
-		goto bail;
-	}
-	result = 0;
-
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_commit_flt);
-
-/**
- * ipa_reset_flt() - Reset the current SW filtering table of specified type
- * (does not commit to HW)
- * @ip:	[in] the family of routing tables
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_reset_flt(enum ipa_ip_type ip)
-{
-	struct ipa_flt_tbl *tbl;
-	struct ipa_flt_entry *entry;
-	struct ipa_flt_entry *next;
-	int i;
-	int id;
-
-	if (ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	tbl = &ipa_ctx->glob_flt_tbl[ip];
-	mutex_lock(&ipa_ctx->lock);
-	IPADBG("reset flt ip=%d\n", ip);
-	list_for_each_entry_safe(entry, next, &tbl->head_flt_rule_list, link) {
-		if (ipa_id_find(entry->id) == NULL) {
-			WARN_ON(1);
-			mutex_unlock(&ipa_ctx->lock);
-			return -EFAULT;
-		}
-
-		if ((ip == IPA_IP_v4 &&
-		     entry->rule.attrib.attrib_mask == IPA_FLT_PROTOCOL &&
-		     entry->rule.attrib.u.v4.protocol ==
-		      IPA_INVALID_L4_PROTOCOL) ||
-		    (ip == IPA_IP_v6 &&
-		     entry->rule.attrib.attrib_mask == IPA_FLT_NEXT_HDR &&
-		     entry->rule.attrib.u.v6.next_hdr ==
-		      IPA_INVALID_L4_PROTOCOL))
-			continue;
-
-		list_del(&entry->link);
-		entry->tbl->rule_cnt--;
-		if (entry->rt_tbl)
-			entry->rt_tbl->ref_cnt--;
-		entry->cookie = 0;
-		id = entry->id;
-		kmem_cache_free(ipa_ctx->flt_rule_cache, entry);
-
-		/* remove the handle from the database */
-		ipa_id_remove(id);
-	}
-
-	for (i = 0; i < ipa_ctx->ipa_num_pipes; i++) {
-		tbl = &ipa_ctx->flt_tbl[i][ip];
-		list_for_each_entry_safe(entry, next, &tbl->head_flt_rule_list,
-				link) {
-			if (ipa_id_find(entry->id) == NULL) {
-				WARN_ON(1);
-				mutex_unlock(&ipa_ctx->lock);
-				return -EFAULT;
-			}
-			list_del(&entry->link);
-			entry->tbl->rule_cnt--;
-			if (entry->rt_tbl)
-				entry->rt_tbl->ref_cnt--;
-			entry->cookie = 0;
-			id = entry->id;
-			kmem_cache_free(ipa_ctx->flt_rule_cache, entry);
-
-			/* remove the handle from the database */
-			ipa_id_remove(id);
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_reset_flt);
-
-void ipa_install_dflt_flt_rules(u32 ipa_ep_idx)
-{
-	struct ipa_flt_tbl *tbl;
-	struct ipa_ep_context *ep = &ipa_ctx->ep[ipa_ep_idx];
-	struct ipa_flt_rule rule;
-
-	memset(&rule, 0, sizeof(rule));
-
-	mutex_lock(&ipa_ctx->lock);
-	tbl = &ipa_ctx->flt_tbl[ipa_ep_idx][IPA_IP_v4];
-	tbl->sticky_rear = true;
-	rule.action = IPA_PASS_TO_EXCEPTION;
-	__ipa_add_flt_rule(tbl, IPA_IP_v4, &rule, false,
-			&ep->dflt_flt4_rule_hdl);
-	ipa_ctx->ctrl->ipa_commit_flt(IPA_IP_v4);
-
-	tbl = &ipa_ctx->flt_tbl[ipa_ep_idx][IPA_IP_v6];
-	tbl->sticky_rear = true;
-	rule.action = IPA_PASS_TO_EXCEPTION;
-	__ipa_add_flt_rule(tbl, IPA_IP_v6, &rule, false,
-			&ep->dflt_flt6_rule_hdl);
-	ipa_ctx->ctrl->ipa_commit_flt(IPA_IP_v6);
-	mutex_unlock(&ipa_ctx->lock);
-}
-
-void ipa_delete_dflt_flt_rules(u32 ipa_ep_idx)
-{
-	struct ipa_ep_context *ep = &ipa_ctx->ep[ipa_ep_idx];
-
-	mutex_lock(&ipa_ctx->lock);
-	if (ep->dflt_flt4_rule_hdl) {
-		__ipa_del_flt_rule(ep->dflt_flt4_rule_hdl);
-		ipa_ctx->ctrl->ipa_commit_flt(IPA_IP_v4);
-		ep->dflt_flt4_rule_hdl = 0;
-	}
-	if (ep->dflt_flt6_rule_hdl) {
-		__ipa_del_flt_rule(ep->dflt_flt6_rule_hdl);
-		ipa_ctx->ctrl->ipa_commit_flt(IPA_IP_v6);
-		ep->dflt_flt6_rule_hdl = 0;
-	}
-	mutex_unlock(&ipa_ctx->lock);
-}
diff --git a/drivers/platform/msm/ipa/ipa_hdr.c b/drivers/platform/msm/ipa/ipa_hdr.c
deleted file mode 100644
index f0614e32..00000000
--- a/drivers/platform/msm/ipa/ipa_hdr.c
+++ /dev/null
@@ -1,1423 +0,0 @@
-/* Copyright (c) 2012-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include "ipa_i.h"
-
-static const u32 ipa_hdr_bin_sz[IPA_HDR_BIN_MAX] = { 8, 16, 24, 36, 60};
-static const u32 ipa_hdr_proc_ctx_bin_sz[IPA_HDR_PROC_CTX_BIN_MAX] = { 32, 64};
-
-#define HDR_TYPE_IS_VALID(type) \
-	((type) >= 0 && (type) < IPA_HDR_L2_MAX)
-
-#define HDR_PROC_TYPE_IS_VALID(type) \
-	((type) >= 0 && (type) < IPA_HDR_PROC_MAX)
-
-/* uCP command numbers */
-#define IPA_HDR_UCP_802_3_TO_802_3 6
-#define IPA_HDR_UCP_802_3_TO_ETHII 7
-#define IPA_HDR_UCP_ETHII_TO_802_3 8
-#define IPA_HDR_UCP_ETHII_TO_ETHII 9
-
-/**
- * ipa_generate_hdr_hw_tbl() - generates the headers table
- * @mem:	[out] buffer to put the header table
- *
- * Returns:	0 on success, negative on failure
- */
-static int ipa_generate_hdr_hw_tbl(struct ipa_mem_buffer *mem)
-{
-	struct ipa_hdr_entry *entry;
-
-	mem->size = ipa_ctx->hdr_tbl.end;
-
-	if (mem->size == 0) {
-		IPAERR("hdr tbl empty\n");
-		return -EPERM;
-	}
-	IPADBG("tbl_sz=%d\n", ipa_ctx->hdr_tbl.end);
-
-	mem->base = dma_alloc_coherent(ipa_ctx->pdev, mem->size,
-			&mem->phys_base, GFP_KERNEL);
-	if (!mem->base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem->size);
-		return -ENOMEM;
-	}
-
-	memset(mem->base, 0, mem->size);
-	list_for_each_entry(entry, &ipa_ctx->hdr_tbl.head_hdr_entry_list,
-			link) {
-		if (entry->is_hdr_proc_ctx)
-			continue;
-		IPADBG("hdr of len %d ofst=%d\n", entry->hdr_len,
-				entry->offset_entry->offset);
-		memcpy(mem->base + entry->offset_entry->offset, entry->hdr,
-				entry->hdr_len);
-	}
-
-	return 0;
-}
-
-static void ipa_hdr_proc_ctx_to_hw_format(struct ipa_mem_buffer *mem,
-	u32 hdr_base_addr)
-{
-	struct ipa_hdr_proc_ctx_entry *entry;
-
-	list_for_each_entry(entry,
-			&ipa_ctx->hdr_proc_ctx_tbl.head_proc_ctx_entry_list,
-			link) {
-		IPADBG("processing type %d ofst=%d\n",
-			entry->type, entry->offset_entry->offset);
-		if (entry->type == IPA_HDR_PROC_NONE) {
-			struct ipa_hdr_proc_ctx_add_hdr_seq *ctx;
-
-			ctx = (struct ipa_hdr_proc_ctx_add_hdr_seq *)
-				(mem->base + entry->offset_entry->offset);
-			ctx->hdr_add.tlv.type = IPA_PROC_CTX_TLV_TYPE_HDR_ADD;
-			ctx->hdr_add.tlv.length = 1;
-			ctx->hdr_add.tlv.value = entry->hdr->hdr_len;
-			ctx->hdr_add.hdr_addr = (entry->hdr->is_hdr_proc_ctx) ?
-				entry->hdr->phys_base :
-				hdr_base_addr +
-				entry->hdr->offset_entry->offset;
-			IPADBG("header address 0x%x\n",
-				ctx->hdr_add.hdr_addr);
-			ctx->end.type = IPA_PROC_CTX_TLV_TYPE_END;
-			ctx->end.length = 0;
-			ctx->end.value = 0;
-		} else {
-			struct ipa_hdr_proc_ctx_add_hdr_cmd_seq *ctx;
-
-			ctx = (struct ipa_hdr_proc_ctx_add_hdr_cmd_seq *)
-				(mem->base + entry->offset_entry->offset);
-			ctx->hdr_add.tlv.type = IPA_PROC_CTX_TLV_TYPE_HDR_ADD;
-			ctx->hdr_add.tlv.length = 1;
-			ctx->hdr_add.tlv.value = entry->hdr->hdr_len;
-			ctx->hdr_add.hdr_addr = (entry->hdr->is_hdr_proc_ctx) ?
-				entry->hdr->phys_base :
-				hdr_base_addr +
-				entry->hdr->offset_entry->offset;
-			IPADBG("header address 0x%x\n",
-				ctx->hdr_add.hdr_addr);
-			ctx->cmd.type = IPA_PROC_CTX_TLV_TYPE_PROC_CMD;
-			ctx->cmd.length = 0;
-			if (entry->type == IPA_HDR_PROC_ETHII_TO_ETHII)
-				ctx->cmd.value = IPA_HDR_UCP_ETHII_TO_ETHII;
-			else if (entry->type == IPA_HDR_PROC_ETHII_TO_802_3)
-				ctx->cmd.value = IPA_HDR_UCP_ETHII_TO_802_3;
-			else if (entry->type == IPA_HDR_PROC_802_3_TO_ETHII)
-				ctx->cmd.value = IPA_HDR_UCP_802_3_TO_ETHII;
-			else if (entry->type == IPA_HDR_PROC_802_3_TO_802_3)
-				ctx->cmd.value = IPA_HDR_UCP_802_3_TO_802_3;
-			IPADBG("command id %d\n", ctx->cmd.value);
-			ctx->end.type = IPA_PROC_CTX_TLV_TYPE_END;
-			ctx->end.length = 0;
-			ctx->end.value = 0;
-		}
-	}
-}
-
-/**
- * ipa_generate_hdr_proc_ctx_hw_tbl() -
- * generates the headers processing context table.
- * @mem:		[out] buffer to put the processing context table
- * @aligned_mem:	[out] actual processing context table (with alignment).
- *			Processing context table needs to be 8 Bytes aligned.
- *
- * Returns:	0 on success, negative on failure
- */
-static int ipa_generate_hdr_proc_ctx_hw_tbl(u32 hdr_sys_addr,
-	struct ipa_mem_buffer *mem, struct ipa_mem_buffer *aligned_mem)
-{
-	u32 hdr_base_addr;
-
-	mem->size = (ipa_ctx->hdr_proc_ctx_tbl.end) ? : 4;
-
-	/* make sure table is aligned */
-	mem->size += IPA_HDR_PROC_CTX_TABLE_ALIGNMENT_BYTE;
-
-	IPADBG("tbl_sz=%d\n", ipa_ctx->hdr_proc_ctx_tbl.end);
-
-	mem->base = dma_alloc_coherent(ipa_ctx->pdev, mem->size,
-			&mem->phys_base, GFP_KERNEL);
-	if (!mem->base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem->size);
-		return -ENOMEM;
-	}
-
-	aligned_mem->phys_base =
-		IPA_HDR_PROC_CTX_TABLE_ALIGNMENT(mem->phys_base);
-	aligned_mem->base = mem->base +
-		(aligned_mem->phys_base - mem->phys_base);
-	aligned_mem->size = mem->size - IPA_HDR_PROC_CTX_TABLE_ALIGNMENT_BYTE;
-	memset(aligned_mem->base, 0, aligned_mem->size);
-	hdr_base_addr = (ipa_ctx->hdr_tbl_lcl) ? IPA_MEM_PART(apps_hdr_ofst) :
-		hdr_sys_addr;
-	ipa_hdr_proc_ctx_to_hw_format(aligned_mem, hdr_base_addr);
-
-	return 0;
-}
-
-/*
- * __ipa_commit_hdr() commits hdr to hardware
- * This function needs to be called with a locked mutex.
- */
-int __ipa_commit_hdr_v1_1(void)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer *mem;
-	struct ipa_hdr_init_local *cmd;
-	u16 len;
-
-	mem = kmalloc(sizeof(struct ipa_mem_buffer), GFP_KERNEL);
-	if (!mem) {
-		IPAERR("failed to alloc memory object\n");
-		goto fail_alloc_mem;
-	}
-
-	/* the immediate command param size is same for both local and system */
-	len = sizeof(struct ipa_hdr_init_local);
-
-	/*
-	 * we can use init_local ptr for init_system due to layout of the
-	 * struct
-	 */
-	cmd = kmalloc(len, GFP_KERNEL);
-	if (!cmd) {
-		IPAERR("failed to alloc immediate command object\n");
-		goto fail_alloc_cmd;
-	}
-
-	if (ipa_generate_hdr_hw_tbl(mem)) {
-		IPAERR("fail to generate HDR HW TBL\n");
-		goto fail_hw_tbl_gen;
-	}
-
-	if (ipa_ctx->hdr_tbl_lcl) {
-		if (mem->size > IPA_MEM_v1_RAM_HDR_SIZE) {
-			IPAERR("tbl too big, needed %d avail %d\n", mem->size,
-				IPA_MEM_v1_RAM_HDR_SIZE);
-			goto fail_send_cmd;
-		}
-	} else {
-		if (mem->size > IPA_MEM_PART(apps_hdr_size_ddr)) {
-			IPAERR("tbl too big, needed %d avail %d\n", mem->size,
-				IPA_MEM_PART(apps_hdr_size_ddr));
-			goto fail_send_cmd;
-		}
-	}
-
-	cmd->hdr_table_src_addr = mem->phys_base;
-	if (ipa_ctx->hdr_tbl_lcl) {
-		cmd->size_hdr_table = mem->size;
-		cmd->hdr_table_dst_addr = IPA_MEM_v1_RAM_HDR_OFST;
-		desc.opcode = IPA_HDR_INIT_LOCAL;
-	} else {
-		desc.opcode = IPA_HDR_INIT_SYSTEM;
-	}
-	desc.pyld = cmd;
-	desc.len = sizeof(struct ipa_hdr_init_local);
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem->base, mem->phys_base, mem->size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		goto fail_send_cmd;
-	}
-
-	if (ipa_ctx->hdr_tbl_lcl) {
-		dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base,
-				mem->phys_base);
-	} else {
-		if (ipa_ctx->hdr_mem.phys_base) {
-			dma_free_coherent(ipa_ctx->pdev, ipa_ctx->hdr_mem.size,
-					  ipa_ctx->hdr_mem.base,
-					  ipa_ctx->hdr_mem.phys_base);
-		}
-		ipa_ctx->hdr_mem = *mem;
-	}
-	kfree(cmd);
-	kfree(mem);
-
-	return 0;
-
-fail_send_cmd:
-	if (mem->base)
-		dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base,
-				mem->phys_base);
-fail_hw_tbl_gen:
-	kfree(cmd);
-fail_alloc_cmd:
-	kfree(mem);
-fail_alloc_mem:
-
-	return -EPERM;
-}
-
-int __ipa_commit_hdr_v2(void)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer mem;
-	struct ipa_hdr_init_system cmd;
-	struct ipa_hw_imm_cmd_dma_shared_mem dma_cmd;
-	int rc = -EFAULT;
-
-	if (ipa_generate_hdr_hw_tbl(&mem)) {
-		IPAERR("fail to generate HDR HW TBL\n");
-		goto end;
-	}
-
-	if (ipa_ctx->hdr_tbl_lcl) {
-		if (mem.size > IPA_MEM_PART(apps_hdr_size)) {
-			IPAERR("tbl too big, needed %d avail %d\n", mem.size,
-				IPA_MEM_PART(apps_hdr_size));
-			goto end;
-		} else {
-			dma_cmd.system_addr = mem.phys_base;
-			dma_cmd.size = mem.size;
-			dma_cmd.local_addr = ipa_ctx->smem_restricted_bytes +
-				IPA_MEM_PART(apps_hdr_ofst);
-			desc.opcode = IPA_DMA_SHARED_MEM;
-			desc.pyld = &dma_cmd;
-			desc.len =
-				sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-		}
-	} else {
-		if (mem.size > IPA_MEM_PART(apps_hdr_size_ddr)) {
-			IPAERR("tbl too big, needed %d avail %d\n", mem.size,
-				IPA_MEM_PART(apps_hdr_size_ddr));
-			goto end;
-		} else {
-			cmd.hdr_table_addr = mem.phys_base;
-			desc.opcode = IPA_HDR_INIT_SYSTEM;
-			desc.pyld = &cmd;
-			desc.len = sizeof(struct ipa_hdr_init_system);
-		}
-	}
-
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem.base, mem.phys_base, mem.size);
-
-	if (ipa_send_cmd(1, &desc))
-		IPAERR("fail to send immediate command\n");
-	else
-		rc = 0;
-
-	if (ipa_ctx->hdr_tbl_lcl) {
-		dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base,
-				mem.phys_base);
-	} else {
-		if (!rc) {
-			if (ipa_ctx->hdr_mem.phys_base)
-				dma_free_coherent(ipa_ctx->pdev,
-						ipa_ctx->hdr_mem.size,
-						ipa_ctx->hdr_mem.base,
-						ipa_ctx->hdr_mem.phys_base);
-			ipa_ctx->hdr_mem = mem;
-		}
-	}
-
-end:
-	return rc;
-}
-
-int __ipa_commit_hdr_v2_5(void)
-{
-	struct ipa_desc desc[2];
-	struct ipa_mem_buffer hdr_mem;
-	struct ipa_mem_buffer ctx_mem;
-	struct ipa_mem_buffer aligned_ctx_mem;
-	struct ipa_hdr_init_system hdr_init_cmd = {0};
-	struct ipa_hw_imm_cmd_dma_shared_mem dma_cmd_hdr = {0};
-	struct ipa_hw_imm_cmd_dma_shared_mem dma_cmd_ctx = {0};
-	struct ipa_register_write reg_write_cmd = {0};
-	int rc = -EFAULT;
-	u32 proc_ctx_size;
-	u32 proc_ctx_ofst;
-	u32 proc_ctx_size_ddr;
-
-	memset(desc, 0, 2 * sizeof(struct ipa_desc));
-
-	if (ipa_generate_hdr_hw_tbl(&hdr_mem)) {
-		IPAERR("fail to generate HDR HW TBL\n");
-		goto end;
-	}
-
-	if (ipa_generate_hdr_proc_ctx_hw_tbl(hdr_mem.phys_base, &ctx_mem,
-	    &aligned_ctx_mem)) {
-		IPAERR("fail to generate HDR PROC CTX HW TBL\n");
-		goto end;
-	}
-
-	if (ipa_ctx->hdr_tbl_lcl) {
-		if (hdr_mem.size > IPA_MEM_PART(apps_hdr_size)) {
-			IPAERR("tbl too big needed %d avail %d\n", hdr_mem.size,
-				IPA_MEM_PART(apps_hdr_size));
-			goto end;
-		} else {
-			dma_cmd_hdr.system_addr = hdr_mem.phys_base;
-			dma_cmd_hdr.size = hdr_mem.size;
-			dma_cmd_hdr.local_addr =
-				ipa_ctx->smem_restricted_bytes +
-				IPA_MEM_PART(apps_hdr_ofst);
-			desc[0].opcode = IPA_DMA_SHARED_MEM;
-			desc[0].pyld = &dma_cmd_hdr;
-			desc[0].len =
-				sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-		}
-	} else {
-		if (hdr_mem.size > IPA_MEM_PART(apps_hdr_size_ddr)) {
-			IPAERR("tbl too big needed %d avail %d\n", hdr_mem.size,
-				IPA_MEM_PART(apps_hdr_size_ddr));
-			goto end;
-		} else {
-			hdr_init_cmd.hdr_table_addr = hdr_mem.phys_base;
-			desc[0].opcode = IPA_HDR_INIT_SYSTEM;
-			desc[0].pyld = &hdr_init_cmd;
-			desc[0].len = sizeof(struct ipa_hdr_init_system);
-		}
-	}
-	desc[0].type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(hdr_mem.base, hdr_mem.phys_base, hdr_mem.size);
-
-	proc_ctx_size = IPA_MEM_PART(apps_hdr_proc_ctx_size);
-	proc_ctx_ofst = IPA_MEM_PART(apps_hdr_proc_ctx_ofst);
-	if (ipa_ctx->hdr_proc_ctx_tbl_lcl) {
-		if (aligned_ctx_mem.size > proc_ctx_size) {
-			IPAERR("tbl too big needed %d avail %d\n",
-				aligned_ctx_mem.size,
-				proc_ctx_size);
-			goto end;
-		} else {
-			dma_cmd_ctx.system_addr = aligned_ctx_mem.phys_base;
-			dma_cmd_ctx.size = aligned_ctx_mem.size;
-			dma_cmd_ctx.local_addr =
-				ipa_ctx->smem_restricted_bytes +
-				proc_ctx_ofst;
-			desc[1].opcode = IPA_DMA_SHARED_MEM;
-			desc[1].pyld = &dma_cmd_ctx;
-			desc[1].len =
-				sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-		}
-	} else {
-		proc_ctx_size_ddr = IPA_MEM_PART(apps_hdr_proc_ctx_size_ddr);
-		if (aligned_ctx_mem.size > proc_ctx_size_ddr) {
-			IPAERR("tbl too big, needed %d avail %d\n",
-				aligned_ctx_mem.size,
-				proc_ctx_size_ddr);
-			goto end;
-		} else {
-			reg_write_cmd.offset = IPA_SYS_PKT_PROC_CNTXT_BASE_OFST;
-			reg_write_cmd.value = aligned_ctx_mem.phys_base;
-			reg_write_cmd.value_mask =
-				~(IPA_HDR_PROC_CTX_TABLE_ALIGNMENT_BYTE - 1);
-			desc[1].pyld = &reg_write_cmd;
-			desc[1].opcode = IPA_REGISTER_WRITE;
-			desc[1].len = sizeof(reg_write_cmd);
-		}
-	}
-	desc[1].type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(ctx_mem.base, ctx_mem.phys_base, ctx_mem.size);
-
-	if (ipa_send_cmd(2, desc))
-		IPAERR("fail to send immediate command\n");
-	else
-		rc = 0;
-
-	if (ipa_ctx->hdr_tbl_lcl) {
-		dma_free_coherent(ipa_ctx->pdev, hdr_mem.size, hdr_mem.base,
-			hdr_mem.phys_base);
-	} else {
-		if (!rc) {
-			if (ipa_ctx->hdr_mem.phys_base)
-				dma_free_coherent(ipa_ctx->pdev,
-				ipa_ctx->hdr_mem.size,
-				ipa_ctx->hdr_mem.base,
-				ipa_ctx->hdr_mem.phys_base);
-			ipa_ctx->hdr_mem = hdr_mem;
-		}
-	}
-
-	if (ipa_ctx->hdr_proc_ctx_tbl_lcl) {
-		dma_free_coherent(ipa_ctx->pdev, ctx_mem.size, ctx_mem.base,
-			ctx_mem.phys_base);
-	} else {
-		if (!rc) {
-			if (ipa_ctx->hdr_proc_ctx_mem.phys_base)
-				dma_free_coherent(ipa_ctx->pdev,
-					ipa_ctx->hdr_proc_ctx_mem.size,
-					ipa_ctx->hdr_proc_ctx_mem.base,
-					ipa_ctx->hdr_proc_ctx_mem.phys_base);
-			ipa_ctx->hdr_proc_ctx_mem = ctx_mem;
-		}
-	}
-
-end:
-	return rc;
-}
-
-/**
- * __ipa_commit_hdr_v2_6L() - Commits a header to the IPA HW.
- *
- * This function needs to be called with a locked mutex.
- */
-int __ipa_commit_hdr_v2_6L(void)
-{
-	/* Same implementation as IPAv2 */
-	return __ipa_commit_hdr_v2();
-}
-
-static int __ipa_add_hdr_proc_ctx(struct ipa_hdr_proc_ctx_add *proc_ctx,
-	bool add_ref_hdr)
-{
-	struct ipa_hdr_entry *hdr_entry;
-	struct ipa_hdr_proc_ctx_entry *entry;
-	struct ipa_hdr_proc_ctx_offset_entry *offset = NULL;
-	u32 bin;
-	struct ipa_hdr_proc_ctx_tbl *htbl = &ipa_ctx->hdr_proc_ctx_tbl;
-	int id;
-	int needed_len;
-
-	IPADBG("processing type %d hdr_hdl %d\n",
-		proc_ctx->type, proc_ctx->hdr_hdl);
-
-	if (!HDR_PROC_TYPE_IS_VALID(proc_ctx->type)) {
-		IPAERR("invalid processing type %d\n", proc_ctx->type);
-		return -EINVAL;
-	}
-
-	hdr_entry = ipa_id_find(proc_ctx->hdr_hdl);
-	if (!hdr_entry || (hdr_entry->cookie != IPA_HDR_COOKIE)) {
-		IPAERR("hdr_hdl is invalid\n");
-		return -EINVAL;
-	}
-
-	entry = kmem_cache_zalloc(ipa_ctx->hdr_proc_ctx_cache, GFP_KERNEL);
-	if (!entry) {
-		IPAERR("failed to alloc proc_ctx object\n");
-		return -ENOMEM;
-	}
-
-	INIT_LIST_HEAD(&entry->link);
-
-	entry->type = proc_ctx->type;
-	entry->hdr = hdr_entry;
-	if (add_ref_hdr)
-		hdr_entry->ref_cnt++;
-	entry->cookie = IPA_PROC_HDR_COOKIE;
-
-	needed_len = (proc_ctx->type == IPA_HDR_PROC_NONE) ?
-			sizeof(struct ipa_hdr_proc_ctx_add_hdr_seq) :
-			sizeof(struct ipa_hdr_proc_ctx_add_hdr_cmd_seq);
-
-	if (needed_len <= ipa_hdr_proc_ctx_bin_sz[IPA_HDR_PROC_CTX_BIN0]) {
-		bin = IPA_HDR_PROC_CTX_BIN0;
-	} else if (needed_len <=
-			ipa_hdr_proc_ctx_bin_sz[IPA_HDR_PROC_CTX_BIN1]) {
-		bin = IPA_HDR_PROC_CTX_BIN1;
-	} else {
-		IPAERR("unexpected needed len %d\n", needed_len);
-		WARN_ON(1);
-		goto bad_len;
-	}
-
-	if (list_empty(&htbl->head_free_offset_list[bin])) {
-		offset = kmem_cache_zalloc(ipa_ctx->hdr_proc_ctx_offset_cache,
-					   GFP_KERNEL);
-		if (!offset) {
-			IPAERR("failed to alloc offset object\n");
-			goto bad_len;
-		}
-		INIT_LIST_HEAD(&offset->link);
-		/*
-		 * for a first item grow, set the bin and offset which are set
-		 * in stone
-		 */
-		offset->offset = htbl->end;
-		offset->bin = bin;
-		htbl->end += ipa_hdr_proc_ctx_bin_sz[bin];
-		list_add(&offset->link,
-				&htbl->head_offset_list[bin]);
-	} else {
-		/* get the first free slot */
-		offset =
-		    list_first_entry(&htbl->head_free_offset_list[bin],
-				    struct ipa_hdr_proc_ctx_offset_entry, link);
-		list_move(&offset->link, &htbl->head_offset_list[bin]);
-	}
-
-	entry->offset_entry = offset;
-	list_add(&entry->link, &htbl->head_proc_ctx_entry_list);
-	htbl->proc_ctx_cnt++;
-	IPADBG("add proc ctx of sz=%d cnt=%d ofst=%d\n", needed_len,
-			htbl->proc_ctx_cnt, offset->offset);
-
-	id = ipa_id_alloc(entry);
-	if (id < 0) {
-		IPAERR("failed to alloc id\n");
-		WARN_ON(1);
-		goto ipa_insert_failed;
-	}
-	entry->id = id;
-	proc_ctx->proc_ctx_hdl = id;
-	entry->ref_cnt++;
-
-	return 0;
-
-ipa_insert_failed:
-	if (offset)
-		list_move(&offset->link,
-		&htbl->head_free_offset_list[offset->bin]);
-	entry->offset_entry = NULL;
-	list_del(&entry->link);
-	htbl->proc_ctx_cnt--;
-bad_len:
-	hdr_entry->ref_cnt--;
-	entry->cookie = 0;
-	kmem_cache_free(ipa_ctx->hdr_proc_ctx_cache, entry);
-	return -EPERM;
-}
-
-
-static int __ipa_add_hdr(struct ipa_hdr_add *hdr)
-{
-	struct ipa_hdr_entry *entry;
-	struct ipa_hdr_offset_entry *offset = NULL;
-	u32 bin;
-	struct ipa_hdr_tbl *htbl = &ipa_ctx->hdr_tbl;
-	int id;
-	int mem_size;
-
-	if (hdr->hdr_len == 0 || hdr->hdr_len > IPA_HDR_MAX_SIZE) {
-		IPAERR("bad parm\n");
-		goto error;
-	}
-
-	if (!HDR_TYPE_IS_VALID(hdr->type)) {
-		IPAERR("invalid hdr type %d\n", hdr->type);
-		goto error;
-	}
-
-	entry = kmem_cache_zalloc(ipa_ctx->hdr_cache, GFP_KERNEL);
-	if (!entry) {
-		IPAERR("failed to alloc hdr object\n");
-		goto error;
-	}
-
-	INIT_LIST_HEAD(&entry->link);
-
-	memcpy(entry->hdr, hdr->hdr, hdr->hdr_len);
-	entry->hdr_len = hdr->hdr_len;
-	strlcpy(entry->name, hdr->name, IPA_RESOURCE_NAME_MAX);
-	entry->is_partial = hdr->is_partial;
-	entry->type = hdr->type;
-	entry->is_eth2_ofst_valid = hdr->is_eth2_ofst_valid;
-	entry->eth2_ofst = hdr->eth2_ofst;
-	entry->cookie = IPA_HDR_COOKIE;
-
-	if (hdr->hdr_len <= ipa_hdr_bin_sz[IPA_HDR_BIN0])
-		bin = IPA_HDR_BIN0;
-	else if (hdr->hdr_len <= ipa_hdr_bin_sz[IPA_HDR_BIN1])
-		bin = IPA_HDR_BIN1;
-	else if (hdr->hdr_len <= ipa_hdr_bin_sz[IPA_HDR_BIN2])
-		bin = IPA_HDR_BIN2;
-	else if (hdr->hdr_len <= ipa_hdr_bin_sz[IPA_HDR_BIN3])
-		bin = IPA_HDR_BIN3;
-	else if (hdr->hdr_len <= ipa_hdr_bin_sz[IPA_HDR_BIN4])
-		bin = IPA_HDR_BIN4;
-	else {
-		IPAERR("unexpected hdr len %d\n", hdr->hdr_len);
-		goto bad_hdr_len;
-	}
-
-	mem_size = (ipa_ctx->hdr_tbl_lcl) ? IPA_MEM_PART(apps_hdr_size) :
-		IPA_MEM_PART(apps_hdr_size_ddr);
-
-	/*
-	 * if header does not fit to table, place it in DDR
-	 * This is valid for IPA 2.5 and on,
-	 * with the exception of IPA2.6L.
-	 */
-	if (htbl->end + ipa_hdr_bin_sz[bin] > mem_size) {
-		if (ipa_ctx->ipa_hw_type != IPA_HW_v2_5) {
-			IPAERR("not enough room for header\n");
-			goto bad_hdr_len;
-		} else {
-			entry->is_hdr_proc_ctx = true;
-			entry->phys_base = dma_map_single(ipa_ctx->pdev,
-				entry->hdr,
-				entry->hdr_len,
-				DMA_TO_DEVICE);
-		}
-	} else {
-		entry->is_hdr_proc_ctx = false;
-		if (list_empty(&htbl->head_free_offset_list[bin])) {
-			offset = kmem_cache_zalloc(ipa_ctx->hdr_offset_cache,
-						   GFP_KERNEL);
-			if (!offset) {
-				IPAERR("failed to alloc hdr offset object\n");
-				goto bad_hdr_len;
-			}
-			INIT_LIST_HEAD(&offset->link);
-			/*
-			 * for a first item grow, set the bin and offset which
-			 * are set in stone
-			 */
-			offset->offset = htbl->end;
-			offset->bin = bin;
-			htbl->end += ipa_hdr_bin_sz[bin];
-			list_add(&offset->link,
-					&htbl->head_offset_list[bin]);
-		} else {
-			/* get the first free slot */
-			offset =
-			list_first_entry(&htbl->head_free_offset_list[bin],
-					struct ipa_hdr_offset_entry, link);
-			list_move(&offset->link, &htbl->head_offset_list[bin]);
-		}
-
-		entry->offset_entry = offset;
-	}
-
-	list_add(&entry->link, &htbl->head_hdr_entry_list);
-	htbl->hdr_cnt++;
-	if (entry->is_hdr_proc_ctx)
-		IPADBG("add hdr of sz=%d hdr_cnt=%d phys_base=%pa\n",
-			hdr->hdr_len,
-			htbl->hdr_cnt,
-			&entry->phys_base);
-	else
-		IPADBG("add hdr of sz=%d hdr_cnt=%d ofst=%d\n",
-			hdr->hdr_len,
-			htbl->hdr_cnt,
-			entry->offset_entry->offset);
-
-	id = ipa_id_alloc(entry);
-	if (id < 0) {
-		IPAERR("failed to alloc id\n");
-		WARN_ON(1);
-		 goto ipa_insert_failed;
-	}
-	entry->id = id;
-	hdr->hdr_hdl = id;
-	entry->ref_cnt++;
-
-	if (entry->is_hdr_proc_ctx) {
-		struct ipa_hdr_proc_ctx_add proc_ctx;
-
-		IPADBG("adding processing context for header %s\n", hdr->name);
-		proc_ctx.type = IPA_HDR_PROC_NONE;
-		proc_ctx.hdr_hdl = id;
-		if (__ipa_add_hdr_proc_ctx(&proc_ctx, false)) {
-			IPAERR("failed to add hdr proc ctx\n");
-			goto fail_add_proc_ctx;
-		}
-		entry->proc_ctx = ipa_id_find(proc_ctx.proc_ctx_hdl);
-	}
-
-	return 0;
-
-fail_add_proc_ctx:
-	entry->ref_cnt--;
-	hdr->hdr_hdl = 0;
-	ipa_id_remove(id);
-ipa_insert_failed:
-	 if (entry->is_hdr_proc_ctx) {
-		dma_unmap_single(ipa_ctx->pdev, entry->phys_base,
-			entry->hdr_len, DMA_TO_DEVICE);
-	} else {
-		if (offset)
-			list_move(&offset->link,
-			&htbl->head_free_offset_list[offset->bin]);
-		entry->offset_entry = NULL;
-	}
-
-	htbl->hdr_cnt--;
-	list_del(&entry->link);
-bad_hdr_len:
-	entry->cookie = 0;
-	kmem_cache_free(ipa_ctx->hdr_cache, entry);
-error:
-	return -EPERM;
-}
-
-static int __ipa_del_hdr_proc_ctx(u32 proc_ctx_hdl,
-	bool release_hdr, bool by_user)
-{
-	struct ipa_hdr_proc_ctx_entry *entry;
-	struct ipa_hdr_proc_ctx_tbl *htbl = &ipa_ctx->hdr_proc_ctx_tbl;
-
-	entry = ipa_id_find(proc_ctx_hdl);
-	if (!entry || (entry->cookie != IPA_PROC_HDR_COOKIE)) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	IPADBG("del ctx proc cnt=%d ofst=%d\n",
-		htbl->proc_ctx_cnt, entry->offset_entry->offset);
-
-	if (by_user && entry->user_deleted) {
-		IPAERR("proc_ctx already deleted by user\n");
-		return -EINVAL;
-	}
-
-	if (by_user)
-		entry->user_deleted = true;
-
-	if (--entry->ref_cnt) {
-		IPADBG("proc_ctx_hdl %x ref_cnt %d\n",
-			proc_ctx_hdl, entry->ref_cnt);
-		return 0;
-	}
-
-	if (release_hdr)
-		__ipa_del_hdr(entry->hdr->id, false);
-
-	/* move the offset entry to appropriate free list */
-	list_move(&entry->offset_entry->link,
-		&htbl->head_free_offset_list[entry->offset_entry->bin]);
-	list_del(&entry->link);
-	htbl->proc_ctx_cnt--;
-	entry->cookie = 0;
-	kmem_cache_free(ipa_ctx->hdr_proc_ctx_cache, entry);
-
-	/* remove the handle from the database */
-	ipa_id_remove(proc_ctx_hdl);
-
-	return 0;
-}
-
-
-int __ipa_del_hdr(u32 hdr_hdl, bool by_user)
-{
-	struct ipa_hdr_entry *entry;
-	struct ipa_hdr_tbl *htbl = &ipa_ctx->hdr_tbl;
-
-	entry = ipa_id_find(hdr_hdl);
-	if (entry == NULL) {
-		IPAERR("lookup failed\n");
-		return -EINVAL;
-	}
-
-	if (!entry || (entry->cookie != IPA_HDR_COOKIE)) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	IPADBG("del hdr of sz=%d hdr_cnt=%d ofst=%d\n", entry->hdr_len,
-			htbl->hdr_cnt, entry->offset_entry->offset);
-
-	if (by_user && entry->user_deleted) {
-		IPAERR("hdr already deleted by user\n");
-		return -EINVAL;
-	}
-
-	if (by_user)
-		entry->user_deleted = true;
-
-	if (--entry->ref_cnt) {
-		IPADBG("hdr_hdl %x ref_cnt %d\n", hdr_hdl, entry->ref_cnt);
-		return 0;
-	}
-
-	if (entry->is_hdr_proc_ctx) {
-		dma_unmap_single(ipa_ctx->pdev,
-			entry->phys_base,
-			entry->hdr_len,
-			DMA_TO_DEVICE);
-		__ipa_del_hdr_proc_ctx(entry->proc_ctx->id, false, false);
-	} else {
-		/* move the offset entry to appropriate free list */
-		list_move(&entry->offset_entry->link,
-			&htbl->head_free_offset_list[entry->offset_entry->bin]);
-	}
-	list_del(&entry->link);
-	htbl->hdr_cnt--;
-	entry->cookie = 0;
-	kmem_cache_free(ipa_ctx->hdr_cache, entry);
-
-	/* remove the handle from the database */
-	ipa_id_remove(hdr_hdl);
-
-	return 0;
-}
-
-/**
- * ipa_add_hdr() - add the specified headers to SW and optionally commit them to
- * IPA HW
- * @hdrs:	[inout] set of headers to add
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_add_hdr(struct ipa_ioc_add_hdr *hdrs)
-{
-	int i;
-	int result = -EFAULT;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (hdrs == NULL || hdrs->num_hdrs == 0) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	IPADBG("adding %d headers to IPA driver internal data struct\n",
-			hdrs->num_hdrs);
-	for (i = 0; i < hdrs->num_hdrs; i++) {
-		if (__ipa_add_hdr(&hdrs->hdr[i])) {
-			IPAERR("failed to add hdr %d\n", i);
-			hdrs->hdr[i].status = -1;
-		} else {
-			hdrs->hdr[i].status = 0;
-		}
-	}
-
-	if (hdrs->commit) {
-		IPADBG("committing all headers to IPA core");
-		if (ipa_ctx->ctrl->ipa_commit_hdr()) {
-			result = -EPERM;
-			goto bail;
-		}
-	}
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-EXPORT_SYMBOL(ipa_add_hdr);
-
-/**
- * ipa_del_hdr_by_user() - Remove the specified headers
- * from SW and optionally commit them to IPA HW
- * @hdls:	[inout] set of headers to delete
- * @by_user:	Operation requested by user?
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_del_hdr_by_user(struct ipa_ioc_del_hdr *hdls, bool by_user)
-{
-	int i;
-	int result = -EFAULT;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (hdls == NULL || hdls->num_hdls == 0) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	for (i = 0; i < hdls->num_hdls; i++) {
-		if (__ipa_del_hdr(hdls->hdl[i].hdl, by_user)) {
-			IPAERR("failed to del hdr %i\n", i);
-			hdls->hdl[i].status = -1;
-		} else {
-			hdls->hdl[i].status = 0;
-		}
-	}
-
-	if (hdls->commit) {
-		if (ipa_ctx->ctrl->ipa_commit_hdr()) {
-			result = -EPERM;
-			goto bail;
-		}
-	}
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-
-/**
- * ipa_del_hdr() - Remove the specified headers from SW and optionally commit them
- * to IPA HW
- * @hdls:	[inout] set of headers to delete
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_del_hdr(struct ipa_ioc_del_hdr *hdls)
-{
-	return ipa_del_hdr_by_user(hdls, false);
-}
-EXPORT_SYMBOL(ipa_del_hdr);
-
-/**
- * ipa_add_hdr_proc_ctx() - add the specified headers to SW
- * and optionally commit them to IPA HW
- * @proc_ctxs:	[inout] set of processing context headers to add
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_add_hdr_proc_ctx(struct ipa_ioc_add_hdr_proc_ctx *proc_ctxs)
-{
-	int i;
-	int result = -EFAULT;
-
-	if (ipa_ctx->ipa_hw_type <= IPA_HW_v2_0 ||
-	    ipa_ctx->ipa_hw_type == IPA_HW_v2_6L) {
-		IPAERR("Processing context not supported on IPA HW %d\n",
-			ipa_ctx->ipa_hw_type);
-		return -EFAULT;
-	}
-
-	if (proc_ctxs == NULL || proc_ctxs->num_proc_ctxs == 0) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	IPADBG("adding %d header processing contextes to IPA driver\n",
-			proc_ctxs->num_proc_ctxs);
-	for (i = 0; i < proc_ctxs->num_proc_ctxs; i++) {
-		if (__ipa_add_hdr_proc_ctx(&proc_ctxs->proc_ctx[i], true)) {
-			IPAERR("failed to add hdr pric ctx %d\n", i);
-			proc_ctxs->proc_ctx[i].status = -1;
-		} else {
-			proc_ctxs->proc_ctx[i].status = 0;
-		}
-	}
-
-	if (proc_ctxs->commit) {
-		IPADBG("committing all headers to IPA core");
-		if (ipa_ctx->ctrl->ipa_commit_hdr()) {
-			result = -EPERM;
-			goto bail;
-		}
-	}
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-EXPORT_SYMBOL(ipa_add_hdr_proc_ctx);
-
-/**
- * ipa_del_hdr_proc_ctx_by_user() -
- * Remove the specified processing context headers from SW and
- * optionally commit them to IPA HW.
- * @hdls:	[inout] set of processing context headers to delete
- * @by_user:	Operation requested by user?
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_del_hdr_proc_ctx_by_user(struct ipa_ioc_del_hdr_proc_ctx *hdls,
-	bool by_user)
-{
-	int i;
-	int result;
-
-	if (ipa_ctx->ipa_hw_type <= IPA_HW_v2_0 ||
-	    ipa_ctx->ipa_hw_type == IPA_HW_v2_6L) {
-		IPAERR("Processing context not supported on IPA HW %d\n",
-			ipa_ctx->ipa_hw_type);
-		return -EFAULT;
-	}
-
-	if (hdls == NULL || hdls->num_hdls == 0) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	for (i = 0; i < hdls->num_hdls; i++) {
-		if (__ipa_del_hdr_proc_ctx(hdls->hdl[i].hdl, true, by_user)) {
-			IPAERR("failed to del hdr %i\n", i);
-			hdls->hdl[i].status = -1;
-		} else {
-			hdls->hdl[i].status = 0;
-		}
-	}
-
-	if (hdls->commit) {
-		if (ipa_ctx->ctrl->ipa_commit_hdr()) {
-			result = -EPERM;
-			goto bail;
-		}
-	}
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-
-/**
- * ipa_del_hdr_proc_ctx() -
- * Remove the specified processing context headers from SW and
- * optionally commit them to IPA HW.
- * @hdls:	[inout] set of processing context headers to delete
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_del_hdr_proc_ctx(struct ipa_ioc_del_hdr_proc_ctx *hdls)
-{
-	return ipa_del_hdr_proc_ctx_by_user(hdls, false);
-}
-EXPORT_SYMBOL(ipa_del_hdr_proc_ctx);
-
-/**
- * ipa_commit_hdr() - commit to IPA HW the current header table in SW
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_commit_hdr(void)
-{
-	int result = -EFAULT;
-
-	/*
-	 * issue a commit on the routing module since routing rules point to
-	 * header table entries
-	 */
-	if (ipa_commit_rt(IPA_IP_v4))
-		return -EPERM;
-	if (ipa_commit_rt(IPA_IP_v6))
-		return -EPERM;
-
-	mutex_lock(&ipa_ctx->lock);
-	if (ipa_ctx->ctrl->ipa_commit_hdr()) {
-		result = -EPERM;
-		goto bail;
-	}
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-EXPORT_SYMBOL(ipa_commit_hdr);
-
-/**
- * ipa_reset_hdr() - reset the current header table in SW (does not commit to
- * HW)
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_reset_hdr(void)
-{
-	struct ipa_hdr_entry *entry;
-	struct ipa_hdr_entry *next;
-	struct ipa_hdr_proc_ctx_entry *ctx_entry;
-	struct ipa_hdr_proc_ctx_entry *ctx_next;
-	struct ipa_hdr_offset_entry *off_entry;
-	struct ipa_hdr_offset_entry *off_next;
-	struct ipa_hdr_proc_ctx_offset_entry *ctx_off_entry;
-	struct ipa_hdr_proc_ctx_offset_entry *ctx_off_next;
-	int i;
-
-	/*
-	 * issue a reset on the routing module since routing rules point to
-	 * header table entries
-	 */
-	if (ipa_reset_rt(IPA_IP_v4))
-		IPAERR("fail to reset v4 rt\n");
-	if (ipa_reset_rt(IPA_IP_v6))
-		IPAERR("fail to reset v4 rt\n");
-
-	mutex_lock(&ipa_ctx->lock);
-	IPADBG("reset hdr\n");
-	list_for_each_entry_safe(entry, next,
-			&ipa_ctx->hdr_tbl.head_hdr_entry_list, link) {
-
-		/* do not remove the default header */
-		if (!strcmp(entry->name, IPA_LAN_RX_HDR_NAME))
-			continue;
-
-		if (ipa_id_find(entry->id) == NULL) {
-			WARN_ON(1);
-			mutex_unlock(&ipa_ctx->lock);
-			return -EFAULT;
-		}
-		if (entry->is_hdr_proc_ctx) {
-			dma_unmap_single(ipa_ctx->pdev,
-				entry->phys_base,
-				entry->hdr_len,
-				DMA_TO_DEVICE);
-			entry->proc_ctx = NULL;
-		}
-		list_del(&entry->link);
-		entry->ref_cnt = 0;
-		entry->cookie = 0;
-
-		/* remove the handle from the database */
-		ipa_id_remove(entry->id);
-		kmem_cache_free(ipa_ctx->hdr_cache, entry);
-
-	}
-	for (i = 0; i < IPA_HDR_BIN_MAX; i++) {
-		list_for_each_entry_safe(off_entry, off_next,
-					 &ipa_ctx->hdr_tbl.head_offset_list[i],
-					 link) {
-
-			/*
-			 * do not remove the default exception header which is
-			 * at offset 0
-			 */
-			if (off_entry->offset == 0)
-				continue;
-
-			list_del(&off_entry->link);
-			kmem_cache_free(ipa_ctx->hdr_offset_cache, off_entry);
-		}
-		list_for_each_entry_safe(off_entry, off_next,
-				&ipa_ctx->hdr_tbl.head_free_offset_list[i],
-				link) {
-			list_del(&off_entry->link);
-			kmem_cache_free(ipa_ctx->hdr_offset_cache, off_entry);
-		}
-	}
-	/* there is one header of size 8 */
-	ipa_ctx->hdr_tbl.end = 8;
-	ipa_ctx->hdr_tbl.hdr_cnt = 1;
-
-	IPADBG("reset hdr proc ctx\n");
-	list_for_each_entry_safe(
-		ctx_entry,
-		ctx_next,
-		&ipa_ctx->hdr_proc_ctx_tbl.head_proc_ctx_entry_list,
-		link) {
-
-		if (ipa_id_find(ctx_entry->id) == NULL) {
-			WARN_ON(1);
-			mutex_unlock(&ipa_ctx->lock);
-			return -EFAULT;
-		}
-		list_del(&ctx_entry->link);
-		ctx_entry->ref_cnt = 0;
-		ctx_entry->cookie = 0;
-
-		/* remove the handle from the database */
-		ipa_id_remove(ctx_entry->id);
-		kmem_cache_free(ipa_ctx->hdr_proc_ctx_cache, ctx_entry);
-
-	}
-	for (i = 0; i < IPA_HDR_PROC_CTX_BIN_MAX; i++) {
-		list_for_each_entry_safe(ctx_off_entry, ctx_off_next,
-				&ipa_ctx->hdr_proc_ctx_tbl.head_offset_list[i],
-				link) {
-
-			list_del(&ctx_off_entry->link);
-			kmem_cache_free(ipa_ctx->hdr_proc_ctx_offset_cache,
-					ctx_off_entry);
-		}
-		list_for_each_entry_safe(ctx_off_entry, ctx_off_next,
-			    &ipa_ctx->hdr_proc_ctx_tbl.head_free_offset_list[i],
-			    link) {
-			list_del(&ctx_off_entry->link);
-			kmem_cache_free(ipa_ctx->hdr_proc_ctx_offset_cache,
-				ctx_off_entry);
-		}
-	}
-	ipa_ctx->hdr_proc_ctx_tbl.end = 0;
-	ipa_ctx->hdr_proc_ctx_tbl.proc_ctx_cnt = 0;
-	mutex_unlock(&ipa_ctx->lock);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_reset_hdr);
-
-static struct ipa_hdr_entry *__ipa_find_hdr(const char *name)
-{
-	struct ipa_hdr_entry *entry;
-
-	list_for_each_entry(entry, &ipa_ctx->hdr_tbl.head_hdr_entry_list,
-			link) {
-		if (!strncmp(name, entry->name, IPA_RESOURCE_NAME_MAX))
-			return entry;
-	}
-
-	return NULL;
-}
-
-/**
- * ipa_get_hdr() - Lookup the specified header resource
- * @lookup:	[inout] header to lookup and its handle
- *
- * lookup the specified header resource and return handle if it exists
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- *		Caller should call ipa_put_hdr later if this function succeeds
- */
-int ipa_get_hdr(struct ipa_ioc_get_hdr *lookup)
-{
-	struct ipa_hdr_entry *entry;
-	int result = -1;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (lookup == NULL) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-	mutex_lock(&ipa_ctx->lock);
-	entry = __ipa_find_hdr(lookup->name);
-	if (entry) {
-		lookup->hdl = entry->id;
-		result = 0;
-	}
-	mutex_unlock(&ipa_ctx->lock);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_get_hdr);
-
-/**
- * __ipa_release_hdr() - drop reference to header and cause
- * deletion if reference count permits
- * @hdr_hdl:	[in] handle of header to be released
- *
- * Returns:	0 on success, negative on failure
- */
-int __ipa_release_hdr(u32 hdr_hdl)
-{
-	int result = 0;
-
-	if (__ipa_del_hdr(hdr_hdl, false)) {
-		IPADBG("fail to del hdr %x\n", hdr_hdl);
-		result = -EFAULT;
-		goto bail;
-	}
-
-	/* commit for put */
-	if (ipa_ctx->ctrl->ipa_commit_hdr()) {
-		IPAERR("fail to commit hdr\n");
-		result = -EFAULT;
-		goto bail;
-	}
-
-bail:
-	return result;
-}
-
-/**
- * __ipa_release_hdr_proc_ctx() - drop reference to processing context
- *  and cause deletion if reference count permits
- * @proc_ctx_hdl:	[in] handle of processing context to be released
- *
- * Returns:	0 on success, negative on failure
- */
-int __ipa_release_hdr_proc_ctx(u32 proc_ctx_hdl)
-{
-	int result = 0;
-
-	if (__ipa_del_hdr_proc_ctx(proc_ctx_hdl, true, false)) {
-		IPADBG("fail to del hdr %x\n", proc_ctx_hdl);
-		result = -EFAULT;
-		goto bail;
-	}
-
-	/* commit for put */
-	if (ipa_ctx->ctrl->ipa_commit_hdr()) {
-		IPAERR("fail to commit hdr\n");
-		result = -EFAULT;
-		goto bail;
-	}
-
-bail:
-	return result;
-}
-
-/**
- * ipa_put_hdr() - Release the specified header handle
- * @hdr_hdl:	[in] the header handle to release
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_put_hdr(u32 hdr_hdl)
-{
-	struct ipa_hdr_entry *entry;
-	int result = -EFAULT;
-
-	mutex_lock(&ipa_ctx->lock);
-
-	entry = ipa_id_find(hdr_hdl);
-	if (entry == NULL) {
-		IPAERR("lookup failed\n");
-		result = -EINVAL;
-		goto bail;
-	}
-
-	if (entry == NULL || entry->cookie != IPA_HDR_COOKIE) {
-		IPAERR("bad params\n");
-		result = -EINVAL;
-		goto bail;
-	}
-
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-EXPORT_SYMBOL(ipa_put_hdr);
-
-/**
- * ipa_copy_hdr() - Lookup the specified header resource and return a copy of it
- * @copy:	[inout] header to lookup and its copy
- *
- * lookup the specified header resource and return a copy of it (along with its
- * attributes) if it exists, this would be called for partial headers
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_copy_hdr(struct ipa_ioc_copy_hdr *copy)
-{
-	struct ipa_hdr_entry *entry;
-	int result = -EFAULT;
-
-	if (copy == NULL) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-	mutex_lock(&ipa_ctx->lock);
-	entry = __ipa_find_hdr(copy->name);
-	if (entry) {
-		memcpy(copy->hdr, entry->hdr, entry->hdr_len);
-		copy->hdr_len = entry->hdr_len;
-		copy->type = entry->type;
-		copy->is_partial = entry->is_partial;
-		copy->is_eth2_ofst_valid = entry->is_eth2_ofst_valid;
-		copy->eth2_ofst = entry->eth2_ofst;
-		result = 0;
-	}
-	mutex_unlock(&ipa_ctx->lock);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_copy_hdr);
diff --git a/drivers/platform/msm/ipa/ipa_hw_defs.h b/drivers/platform/msm/ipa/ipa_hw_defs.h
deleted file mode 100644
index f12a3c6cc8..00000000
--- a/drivers/platform/msm/ipa/ipa_hw_defs.h
+++ /dev/null
@@ -1,450 +0,0 @@
-/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef _IPA_HW_DEFS_H
-#define _IPA_HW_DEFS_H
-#include <linux/bitops.h>
-
-/* This header defines various HW related data types */
-
-/* immediate command op-codes */
-#define IPA_DECIPH_INIT           (1)
-#define IPA_PPP_FRM_INIT          (2)
-#define IPA_IP_V4_FILTER_INIT     (3)
-#define IPA_IP_V6_FILTER_INIT     (4)
-#define IPA_IP_V4_NAT_INIT        (5)
-#define IPA_IP_V6_NAT_INIT        (6)
-#define IPA_IP_V4_ROUTING_INIT    (7)
-#define IPA_IP_V6_ROUTING_INIT    (8)
-#define IPA_HDR_INIT_LOCAL        (9)
-#define IPA_HDR_INIT_SYSTEM      (10)
-#define IPA_DECIPH_SETUP         (11)
-#define IPA_REGISTER_WRITE       (12)
-#define IPA_NAT_DMA              (14)
-#define IPA_IP_PACKET_TAG        (15)
-#define IPA_IP_PACKET_INIT       (16)
-#define IPA_DMA_SHARED_MEM       (19)
-#define IPA_IP_PACKET_TAG_STATUS (20)
-
-/* Processing context TLV type */
-#define IPA_PROC_CTX_TLV_TYPE_END 0
-#define IPA_PROC_CTX_TLV_TYPE_HDR_ADD 1
-#define IPA_PROC_CTX_TLV_TYPE_PROC_CMD 3
-
-
-/**
- * struct ipa_flt_rule_hw_hdr - HW header of IPA filter rule
- * @word: filtering rule properties
- * @en_rule: enable rule
- * @action: post routing action
- * @rt_tbl_idx: index in routing table
- * @retain_hdr: added to add back to the packet the header removed
- *  as part of header removal. This will be done as part of
- *  header insertion block.
- * @to_uc: direct IPA to sent the packet to uc instead of
- *  the intended destination. This will be performed just after
- *  routing block processing, so routing will have determined
- *  destination end point and uc will receive this information
- *  together with the packet as part of the HW packet TX commands
- * @rsvd: reserved bits
- */
-struct ipa_flt_rule_hw_hdr {
-	union {
-		u32 word;
-		struct {
-			u32 en_rule:16;
-			u32 action:5;
-			u32 rt_tbl_idx:5;
-			u32 retain_hdr:1;
-			u32 to_uc:1;
-			u32 rsvd:4;
-		} hdr;
-	} u;
-};
-
-/**
- * struct ipa_rt_rule_hw_hdr - HW header of IPA routing rule
- * @word: filtering rule properties
- * @en_rule: enable rule
- * @pipe_dest_idx: destination pipe index
- * @system: changed from local to system due to HW change
- * @hdr_offset: header offset
- * @proc_ctx: whether hdr_offset points to header table or to
- *	header processing context table
- */
-struct ipa_rt_rule_hw_hdr {
-	union {
-		u32 word;
-		struct {
-			u32 en_rule:16;
-			u32 pipe_dest_idx:5;
-			u32 system:1;
-			u32 hdr_offset:10;
-		} hdr;
-		struct {
-			u32 en_rule:16;
-			u32 pipe_dest_idx:5;
-			u32 system:1;
-			u32 hdr_offset:9;
-			u32 proc_ctx:1;
-		} hdr_v2_5;
-	} u;
-};
-
-/**
- * struct ipa_ip_v4_filter_init - IPA_IP_V4_FILTER_INIT command payload
- * @ipv4_rules_addr: address of ipv4 rules
- * @size_ipv4_rules: size of the above
- * @ipv4_addr: ipv4 address
- * @rsvd: reserved
- */
-struct ipa_ip_v4_filter_init {
-	u64 ipv4_rules_addr:32;
-	u64 size_ipv4_rules:12;
-	u64 ipv4_addr:16;
-	u64 rsvd:4;
-};
-
-/**
- * struct ipa_ip_v6_filter_init - IPA_IP_V6_FILTER_INIT command payload
- * @ipv6_rules_addr: address of ipv6 rules
- * @size_ipv6_rules: size of the above
- * @ipv6_addr: ipv6 address
- */
-struct ipa_ip_v6_filter_init {
-	u64 ipv6_rules_addr:32;
-	u64 size_ipv6_rules:16;
-	u64 ipv6_addr:16;
-};
-
-/**
- * struct ipa_ip_v4_routing_init - IPA_IP_V4_ROUTING_INIT command payload
- * @ipv4_rules_addr: address of ipv4 rules
- * @size_ipv4_rules: size of the above
- * @ipv4_addr: ipv4 address
- * @rsvd: reserved
- */
-struct ipa_ip_v4_routing_init {
-	u64 ipv4_rules_addr:32;
-	u64 size_ipv4_rules:12;
-	u64 ipv4_addr:16;
-	u64 rsvd:4;
-};
-
-/**
- * struct ipa_ip_v6_routing_init - IPA_IP_V6_ROUTING_INIT command payload
- * @ipv6_rules_addr: address of ipv6 rules
- * @size_ipv6_rules: size of the above
- * @ipv6_addr: ipv6 address
- */
-struct ipa_ip_v6_routing_init {
-	u64 ipv6_rules_addr:32;
-	u64 size_ipv6_rules:16;
-	u64 ipv6_addr:16;
-};
-
-/**
- * struct ipa_hdr_init_local - IPA_HDR_INIT_LOCAL command payload
- * @hdr_table_src_addr: word address of header table in system memory where the
- *  table starts (use as source for memory copying)
- * @size_hdr_table: size of the above (in bytes)
- * @hdr_table_dst_addr: header address in IPA sram (used as dst for memory copy)
- * @rsvd: reserved
- */
-struct ipa_hdr_init_local {
-	u64 hdr_table_src_addr:32;
-	u64 size_hdr_table:12;
-	u64 hdr_table_dst_addr:16;
-	u64 rsvd:4;
-};
-
-/**
- * struct ipa_hdr_init_system - IPA_HDR_INIT_SYSTEM command payload
- * @hdr_table_addr: word address of header table in system memory where the
- *  table starts (use as source for memory copying)
- * @rsvd: reserved
- */
-struct ipa_hdr_init_system {
-	u64 hdr_table_addr:32;
-	u64 rsvd:32;
-};
-
-/**
- * struct ipa_hdr_proc_ctx_tlv -
- * HW structure of IPA processing context header - TLV part
- * @type: 0 - end type
- *        1 - header addition type
- *        3 - processing command type
- * @length: number of bytes after tlv
- *        for type:
- *        0 - needs to be 0
- *        1 - header addition length
- *        3 - number of 32B including type and length.
- * @value: specific value for type
- *        for type:
- *        0 - needs to be 0
- *        1 - header length
- *        3 - command ID (see IPA_HDR_UCP_* definitions)
- */
-struct ipa_hdr_proc_ctx_tlv {
-	u32 type:8;
-	u32 length:8;
-	u32 value:16;
-};
-
-/**
- * struct ipa_hdr_proc_ctx_hdr_add -
- * HW structure of IPA processing context - add header tlv
- * @tlv: IPA processing context TLV
- * @hdr_addr: processing context header address
- */
-struct ipa_hdr_proc_ctx_hdr_add {
-	struct ipa_hdr_proc_ctx_tlv tlv;
-	u32 hdr_addr;
-};
-
-#define IPA_A5_MUX_HDR_EXCP_FLAG_IP		BIT(7)
-#define IPA_A5_MUX_HDR_EXCP_FLAG_NAT		BIT(6)
-#define IPA_A5_MUX_HDR_EXCP_FLAG_SW_FLT	BIT(5)
-#define IPA_A5_MUX_HDR_EXCP_FLAG_TAG		BIT(4)
-#define IPA_A5_MUX_HDR_EXCP_FLAG_REPLICATED	BIT(3)
-#define IPA_A5_MUX_HDR_EXCP_FLAG_IHL		BIT(2)
-
-/**
- * struct ipa_a5_mux_hdr - A5 MUX header definition
- * @interface_id: interface ID
- * @src_pipe_index: source pipe index
- * @flags: flags
- * @metadata: metadata
- *
- * A5 MUX header is in BE, A5 runs in LE. This struct definition
- * allows A5 SW to correctly parse the header
- */
-struct ipa_a5_mux_hdr {
-	u16 interface_id;
-	u8 src_pipe_index;
-	u8 flags;
-	u32 metadata;
-};
-
-/**
- * struct ipa_register_write - IPA_REGISTER_WRITE command payload
- * @rsvd: reserved
- * @skip_pipeline_clear: 0 to wait until IPA pipeline is clear
- * @offset: offset from IPA base address
- * @value: value to write to register
- * @value_mask: mask specifying which value bits to write to the register
- */
-struct ipa_register_write {
-	u32 rsvd:15;
-	u32 skip_pipeline_clear:1;
-	u32 offset:16;
-	u32 value:32;
-	u32 value_mask:32;
-};
-
-/**
- * struct ipa_nat_dma - IPA_NAT_DMA command payload
- * @table_index: NAT table index
- * @rsvd1: reserved
- * @base_addr: base address
- * @rsvd2: reserved
- * @offset: offset
- * @data: metadata
- * @rsvd3: reserved
- */
-struct ipa_nat_dma {
-	u64 table_index:3;
-	u64 rsvd1:1;
-	u64 base_addr:2;
-	u64 rsvd2:2;
-	u64 offset:32;
-	u64 data:16;
-	u64 rsvd3:8;
-};
-
-/**
- * struct ipa_nat_dma - IPA_IP_PACKET_INIT command payload
- * @destination_pipe_index: destination pipe index
- * @rsvd1: reserved
- * @metadata: metadata
- * @rsvd2: reserved
- */
-struct ipa_ip_packet_init {
-	u64 destination_pipe_index:5;
-	u64 rsvd1:3;
-	u64 metadata:32;
-	u64 rsvd2:24;
-};
-
-/**
- * struct ipa_nat_dma - IPA_IP_V4_NAT_INIT command payload
- * @ipv4_rules_addr: ipv4 rules address
- * @ipv4_expansion_rules_addr: ipv4 expansion rules address
- * @index_table_addr: index tables address
- * @index_table_expansion_addr: index expansion table address
- * @table_index: index in table
- * @ipv4_rules_addr_type: ipv4 address type
- * @ipv4_expansion_rules_addr_type: ipv4 expansion address type
- * @index_table_addr_type: index table address type
- * @index_table_expansion_addr_type: index expansion table type
- * @size_base_tables: size of base tables
- * @size_expansion_tables: size of expansion tables
- * @rsvd2: reserved
- * @public_ip_addr: public IP address
- */
-struct ipa_ip_v4_nat_init {
-	u64 ipv4_rules_addr:32;
-	u64 ipv4_expansion_rules_addr:32;
-	u64 index_table_addr:32;
-	u64 index_table_expansion_addr:32;
-	u64 table_index:3;
-	u64 rsvd1:1;
-	u64 ipv4_rules_addr_type:1;
-	u64 ipv4_expansion_rules_addr_type:1;
-	u64 index_table_addr_type:1;
-	u64 index_table_expansion_addr_type:1;
-	u64 size_base_tables:12;
-	u64 size_expansion_tables:10;
-	u64 rsvd2:2;
-	u64 public_ip_addr:32;
-};
-
-/**
- * struct ipa_ip_packet_tag - IPA_IP_PACKET_TAG command payload
- * @tag: tag value returned with response
- */
-struct ipa_ip_packet_tag {
-	u32 tag;
-};
-
-/**
- * struct ipa_ip_packet_tag_status - IPA_IP_PACKET_TAG_STATUS command payload
- * @rsvd: reserved
- * @tag_f_1: tag value returned within status
- * @tag_f_2: tag value returned within status
- */
-struct ipa_ip_packet_tag_status {
-	u32 rsvd:16;
-	u32 tag_f_1:16;
-	u32 tag_f_2:32;
-};
-
-/*! @brief Struct for the IPAv2.0 and IPAv2.5 UL packet status header */
-struct ipa_hw_pkt_status {
-	u32 status_opcode:8;
-	u32 exception:8;
-	u32 status_mask:16;
-	u32 pkt_len:16;
-	u32 endp_src_idx:5;
-	u32 reserved_1:3;
-	u32 endp_dest_idx:5;
-	u32 reserved_2:3;
-	u32 metadata:32;
-	union {
-		struct {
-			u32 filt_local:1;
-			u32 filt_global:1;
-			u32 filt_pipe_idx:5;
-			u32 filt_match:1;
-			u32 filt_rule_idx:6;
-			u32 ret_hdr:1;
-			u32 reserved_3:1;
-			u32 tag_f_1:16;
-
-		} ipa_hw_v2_0_pkt_status;
-		struct {
-			u32 filt_local:1;
-			u32 filt_global:1;
-			u32 filt_pipe_idx:5;
-			u32 ret_hdr:1;
-			u32 filt_rule_idx:8;
-			u32 tag_f_1:16;
-
-		} ipa_hw_v2_5_pkt_status;
-	};
-
-	u32 tag_f_2:32;
-	u32 time_day_ctr:32;
-	u32 nat_hit:1;
-	u32 nat_tbl_idx:13;
-	u32 nat_type:2;
-	u32 route_local:1;
-	u32 route_tbl_idx:5;
-	u32 route_match:1;
-	u32 ucp:1;
-	u32 route_rule_idx:8;
-	u32 hdr_local:1;
-	u32 hdr_offset:10;
-	u32 frag_hit:1;
-	u32 frag_rule:4;
-	u32 reserved_4:16;
-};
-
-#define IPA_PKT_STATUS_SIZE 32
-
-/*! @brief Status header opcodes */
-enum ipa_hw_status_opcode {
-	IPA_HW_STATUS_OPCODE_MIN,
-	IPA_HW_STATUS_OPCODE_PACKET = IPA_HW_STATUS_OPCODE_MIN,
-	IPA_HW_STATUS_OPCODE_NEW_FRAG_RULE,
-	IPA_HW_STATUS_OPCODE_DROPPED_PACKET,
-	IPA_HW_STATUS_OPCODE_SUSPENDED_PACKET,
-	IPA_HW_STATUS_OPCODE_XLAT_PACKET = 6,
-	IPA_HW_STATUS_OPCODE_MAX
-};
-
-/*! @brief Possible Masks received in status */
-enum ipa_hw_pkt_status_mask {
-	IPA_HW_PKT_STATUS_MASK_FRAG_PROCESS      = 0x1,
-	IPA_HW_PKT_STATUS_MASK_FILT_PROCESS      = 0x2,
-	IPA_HW_PKT_STATUS_MASK_NAT_PROCESS       = 0x4,
-	IPA_HW_PKT_STATUS_MASK_ROUTE_PROCESS     = 0x8,
-	IPA_HW_PKT_STATUS_MASK_TAG_VALID         = 0x10,
-	IPA_HW_PKT_STATUS_MASK_FRAGMENT          = 0x20,
-	IPA_HW_PKT_STATUS_MASK_FIRST_FRAGMENT    = 0x40,
-	IPA_HW_PKT_STATUS_MASK_V4                = 0x80,
-	IPA_HW_PKT_STATUS_MASK_CKSUM_PROCESS     = 0x100,
-	IPA_HW_PKT_STATUS_MASK_AGGR_PROCESS      = 0x200,
-	IPA_HW_PKT_STATUS_MASK_DEST_EOT          = 0x400,
-	IPA_HW_PKT_STATUS_MASK_DEAGGR_PROCESS    = 0x800,
-	IPA_HW_PKT_STATUS_MASK_DEAGG_FIRST       = 0x1000,
-	IPA_HW_PKT_STATUS_MASK_SRC_EOT           = 0x2000
-};
-
-/*! @brief Possible Exceptions received in status */
-enum ipa_hw_pkt_status_exception {
-	IPA_HW_PKT_STATUS_EXCEPTION_NONE           = 0x0,
-	IPA_HW_PKT_STATUS_EXCEPTION_DEAGGR         = 0x1,
-	IPA_HW_PKT_STATUS_EXCEPTION_REPL           = 0x2,
-	IPA_HW_PKT_STATUS_EXCEPTION_IPTYPE         = 0x4,
-	IPA_HW_PKT_STATUS_EXCEPTION_IHL            = 0x8,
-	IPA_HW_PKT_STATUS_EXCEPTION_FRAG_RULE_MISS = 0x10,
-	IPA_HW_PKT_STATUS_EXCEPTION_SW_FILT        = 0x20,
-	IPA_HW_PKT_STATUS_EXCEPTION_NAT            = 0x40,
-	IPA_HW_PKT_STATUS_EXCEPTION_ACTUAL_MAX,
-	IPA_HW_PKT_STATUS_EXCEPTION_MAX            = 0xFF
-};
-
-/*! @brief IPA_HW_IMM_CMD_DMA_SHARED_MEM Immediate Command Parameters */
-struct ipa_hw_imm_cmd_dma_shared_mem {
-	u32 reserved_1:16;
-	u32 size:16;
-	u32 system_addr:32;
-	u32 local_addr:16;
-	u32 direction:1;
-	u32 skip_pipeline_clear:1;
-	u32 reserved_2:14;
-	u32 padding:32;
-};
-
-#endif /* _IPA_HW_DEFS_H */
diff --git a/drivers/platform/msm/ipa/ipa_i.h b/drivers/platform/msm/ipa/ipa_i.h
deleted file mode 100644
index fec806f..00000000
--- a/drivers/platform/msm/ipa/ipa_i.h
+++ /dev/null
@@ -1,1660 +0,0 @@
-/* Copyright (c) 2012-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef _IPA_I_H_
-#define _IPA_I_H_
-
-#include <linux/bitops.h>
-#include <linux/cdev.h>
-#include <linux/export.h>
-#include <linux/idr.h>
-#include <linux/list.h>
-#include <linux/mutex.h>
-#include <linux/skbuff.h>
-#include <linux/slab.h>
-#include <linux/ipa.h>
-#include <linux/msm-sps.h>
-#include <asm/dma-iommu.h>
-#include <linux/iommu.h>
-#include "ipa_hw_defs.h"
-#include "ipa_ram_mmap.h"
-#include "ipa_reg.h"
-#include "ipa_qmi_service.h"
-
-#define DRV_NAME "ipa"
-#define NAT_DEV_NAME "ipaNatTable"
-#define IPA_COOKIE 0x57831603
-#define IPA_RT_RULE_COOKIE 0x57831604
-#define IPA_RT_TBL_COOKIE 0x57831605
-#define IPA_FLT_COOKIE 0x57831606
-#define IPA_HDR_COOKIE 0x57831607
-#define IPA_PROC_HDR_COOKIE 0x57831608
-
-#define MTU_BYTE 1500
-
-#define IPA_MAX_NUM_PIPES 0x14
-#define IPA_SYS_DESC_FIFO_SZ 0x800
-#define IPA_SYS_TX_DATA_DESC_FIFO_SZ 0x1000
-#define IPA_LAN_RX_HEADER_LENGTH (2)
-#define IPA_QMAP_HEADER_LENGTH (4)
-#define IPA_DL_CHECKSUM_LENGTH (8)
-#define IPA_NUM_DESC_PER_SW_TX (2)
-#define IPA_GENERIC_RX_POOL_SZ 192
-
-#define IPADBG(fmt, args...) \
-	pr_debug(DRV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-#define IPAERR(fmt, args...) \
-	pr_err(DRV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-
-#define WLAN_AMPDU_TX_EP 15
-#define WLAN_PROD_TX_EP  19
-#define WLAN1_CONS_RX_EP  14
-#define WLAN2_CONS_RX_EP  16
-#define WLAN3_CONS_RX_EP  17
-#define WLAN4_CONS_RX_EP  18
-
-#define MAX_NUM_EXCP     8
-
-#define IPA_STATS
-
-#ifdef IPA_STATS
-#define IPA_STATS_INC_CNT(val) (++val)
-#define IPA_STATS_DEC_CNT(val) (--val)
-#define IPA_STATS_EXCP_CNT(flags, base) do {			\
-			int i;					\
-			for (i = 0; i < MAX_NUM_EXCP; i++)	\
-				if (flags & BIT(i))		\
-					++base[i];		\
-			if (flags == 0)				\
-				++base[MAX_NUM_EXCP - 1];	\
-			} while (0)
-#else
-#define IPA_STATS_INC_CNT(x) do { } while (0)
-#define IPA_STATS_DEC_CNT(x)
-#define IPA_STATS_EXCP_CNT(flags, base) do { } while (0)
-#endif
-
-#define IPA_TOS_EQ			BIT(0)
-#define IPA_PROTOCOL_EQ			BIT(1)
-#define IPA_OFFSET_MEQ32_0		BIT(2)
-#define IPA_OFFSET_MEQ32_1		BIT(3)
-#define IPA_IHL_OFFSET_RANGE16_0	BIT(4)
-#define IPA_IHL_OFFSET_RANGE16_1	BIT(5)
-#define IPA_IHL_OFFSET_EQ_16		BIT(6)
-#define IPA_IHL_OFFSET_EQ_32		BIT(7)
-#define IPA_IHL_OFFSET_MEQ32_0		BIT(8)
-#define IPA_OFFSET_MEQ128_0		BIT(9)
-#define IPA_OFFSET_MEQ128_1		BIT(10)
-#define IPA_TC_EQ			BIT(11)
-#define IPA_FL_EQ			BIT(12)
-#define IPA_IHL_OFFSET_MEQ32_1		BIT(13)
-#define IPA_METADATA_COMPARE		BIT(14)
-#define IPA_IS_FRAG			BIT(15)
-
-#define IPA_HDR_BIN0 0
-#define IPA_HDR_BIN1 1
-#define IPA_HDR_BIN2 2
-#define IPA_HDR_BIN3 3
-#define IPA_HDR_BIN4 4
-#define IPA_HDR_BIN_MAX 5
-
-#define IPA_HDR_PROC_CTX_BIN0 0
-#define IPA_HDR_PROC_CTX_BIN1 1
-#define IPA_HDR_PROC_CTX_BIN_MAX 2
-
-#define IPA_EVENT_THRESHOLD 0x10
-
-/*
- * Due to ZLT issue with USB 3.0 core, IPA BAM threashold need to be set
- * to max packet size + 1. After setting the threshold, USB core
- * will not be notified on ZLTs
- */
-#define IPA_USB_EVENT_THRESHOLD 0x4001
-
-#define IPA_RX_POOL_CEIL 32
-#define IPA_RX_SKB_SIZE 1792
-
-#define IPA_A5_MUX_HDR_NAME "ipa_excp_hdr"
-#define IPA_LAN_RX_HDR_NAME "ipa_lan_hdr"
-#define IPA_INVALID_L4_PROTOCOL 0xFF
-
-#define IPA_CLIENT_IS_PROD(x) (x >= IPA_CLIENT_PROD && x < IPA_CLIENT_CONS)
-#define IPA_CLIENT_IS_CONS(x) (x >= IPA_CLIENT_CONS && x < IPA_CLIENT_MAX)
-#define IPA_SETFIELD(val, shift, mask) (((val) << (shift)) & (mask))
-#define IPA_SETFIELD_IN_REG(reg, val, shift, mask) \
-			(reg |= ((val) << (shift)) & (mask))
-
-#define IPA_HW_TABLE_ALIGNMENT(start_ofst) \
-	(((start_ofst) + 127) & ~127)
-#define IPA_RT_FLT_HW_RULE_BUF_SIZE	(256)
-
-#define IPA_HDR_PROC_CTX_TABLE_ALIGNMENT_BYTE 8
-#define IPA_HDR_PROC_CTX_TABLE_ALIGNMENT(start_ofst) \
-	(((start_ofst) + IPA_HDR_PROC_CTX_TABLE_ALIGNMENT_BYTE - 1) & \
-	~(IPA_HDR_PROC_CTX_TABLE_ALIGNMENT_BYTE - 1))
-
-#define MAX_RESOURCE_TO_CLIENTS (IPA_CLIENT_MAX)
-#define IPA_MEM_PART(x_) (ipa_ctx->ctrl->mem_partition.x_)
-
-#define IPA_SMMU_AP_VA_START 0x1000
-#define IPA_SMMU_AP_VA_SIZE 0x40000000
-#define IPA_SMMU_AP_VA_END (IPA_SMMU_AP_VA_START +  IPA_SMMU_AP_VA_SIZE)
-#define IPA_SMMU_UC_VA_START 0x40000000
-#define IPA_SMMU_UC_VA_SIZE 0x20000000
-#define IPA_SMMU_UC_VA_END (IPA_SMMU_UC_VA_START +  IPA_SMMU_UC_VA_SIZE)
-
-
-struct ipa_client_names {
-	enum ipa_client_type names[MAX_RESOURCE_TO_CLIENTS];
-	int length;
-};
-
-struct ipa_smmu_cb_ctx {
-	bool valid;
-	struct device *dev;
-	struct dma_iommu_mapping *mapping;
-	struct iommu_domain *iommu;
-	unsigned long next_addr;
-};
-
-/**
- * struct ipa_mem_buffer - IPA memory buffer
- * @base: base
- * @phys_base: physical base address
- * @size: size of memory buffer
- */
-struct ipa_mem_buffer {
-	void *base;
-	dma_addr_t phys_base;
-	u32 size;
-};
-
-/**
- * struct ipa_flt_entry - IPA filtering table entry
- * @link: entry's link in global filtering enrties list
- * @rule: filter rule
- * @cookie: cookie used for validity check
- * @tbl: filter table
- * @rt_tbl: routing table
- * @hw_len: entry's size
- */
-struct ipa_flt_entry {
-	struct list_head link;
-	u32 cookie;
-	struct ipa_flt_rule rule;
-	struct ipa_flt_tbl *tbl;
-	struct ipa_rt_tbl *rt_tbl;
-	u32 hw_len;
-	int id;
-};
-
-/**
- * struct ipa_rt_tbl - IPA routing table
- * @link: table's link in global routing tables list
- * @head_rt_rule_list: head of routing rules list
- * @name: routing table name
- * @idx: routing table index
- * @rule_cnt: number of rules in routing table
- * @ref_cnt: reference counter of routing table
- * @set: collection of routing tables
- * @cookie: cookie used for validity check
- * @in_sys: flag indicating if the table is located in system memory
- * @sz: the size of the routing table
- * @curr_mem: current routing tables block in sys memory
- * @prev_mem: previous routing table block in sys memory
- * @id: routing table id
- */
-struct ipa_rt_tbl {
-	struct list_head link;
-	u32 cookie;
-	struct list_head head_rt_rule_list;
-	char name[IPA_RESOURCE_NAME_MAX];
-	u32 idx;
-	u32 rule_cnt;
-	u32 ref_cnt;
-	struct ipa_rt_tbl_set *set;
-	bool in_sys;
-	u32 sz;
-	struct ipa_mem_buffer curr_mem;
-	struct ipa_mem_buffer prev_mem;
-	int id;
-};
-
-/**
- * struct ipa_hdr_entry - IPA header table entry
- * @link: entry's link in global header table entries list
- * @hdr: the header
- * @hdr_len: header length
- * @name: name of header table entry
- * @type: l2 header type
- * @is_partial: flag indicating if header table entry is partial
- * @is_hdr_proc_ctx: false - hdr entry resides in hdr table,
- * true - hdr entry resides in DDR and pointed to by proc ctx
- * @phys_base: physical address of entry in SRAM when is_hdr_proc_ctx is true,
- * else 0
- * @proc_ctx: processing context header
- * @offset_entry: entry's offset
- * @cookie: cookie used for validity check
- * @ref_cnt: reference counter of routing table
- * @id: header entry id
- * @is_eth2_ofst_valid: is eth2_ofst field valid?
- * @eth2_ofst: offset to start of Ethernet-II/802.3 header
- * @user_deleted: is the header deleted by the user?
- */
-struct ipa_hdr_entry {
-	struct list_head link;
-	u32 cookie;
-	u8 hdr[IPA_HDR_MAX_SIZE];
-	u32 hdr_len;
-	char name[IPA_RESOURCE_NAME_MAX];
-	enum ipa_hdr_l2_type type;
-	u8 is_partial;
-	bool is_hdr_proc_ctx;
-	dma_addr_t phys_base;
-	struct ipa_hdr_proc_ctx_entry *proc_ctx;
-	struct ipa_hdr_offset_entry *offset_entry;
-	u32 ref_cnt;
-	int id;
-	u8 is_eth2_ofst_valid;
-	u16 eth2_ofst;
-	bool user_deleted;
-};
-
-/**
- * struct ipa_hdr_offset_entry - IPA header offset entry
- * @link: entry's link in global header offset entries list
- * @offset: the offset
- * @bin: bin
- */
-struct ipa_hdr_offset_entry {
-	struct list_head link;
-	u32 offset;
-	u32 bin;
-};
-
-/**
- * struct ipa_hdr_tbl - IPA header table
- * @head_hdr_entry_list: header entries list
- * @head_offset_list: header offset list
- * @head_free_offset_list: header free offset list
- * @hdr_cnt: number of headers
- * @end: the last header index
- */
-struct ipa_hdr_tbl {
-	struct list_head head_hdr_entry_list;
-	struct list_head head_offset_list[IPA_HDR_BIN_MAX];
-	struct list_head head_free_offset_list[IPA_HDR_BIN_MAX];
-	u32 hdr_cnt;
-	u32 end;
-};
-
-/**
- * struct ipa_hdr_offset_entry - IPA header offset entry
- * @link: entry's link in global processing context header offset entries list
- * @offset: the offset
- * @bin: bin
- */
-struct ipa_hdr_proc_ctx_offset_entry {
-	struct list_head link;
-	u32 offset;
-	u32 bin;
-};
-
-/**
- * struct ipa_hdr_proc_ctx_add_hdr_seq -
- * IPA processing context header - add header sequence
- * @hdr_add: add header command
- * @end: tlv end command (cmd.type must be 0)
- */
-struct ipa_hdr_proc_ctx_add_hdr_seq {
-	struct ipa_hdr_proc_ctx_hdr_add hdr_add;
-	struct ipa_hdr_proc_ctx_tlv end;
-};
-
-/**
- * struct ipa_hdr_proc_ctx_add_hdr_cmd_seq -
- * IPA processing context header - process command sequence
- * @hdr_add: add header command
- * @cmd: tlv processing command (cmd.type must be 3)
- * @end: tlv end command (cmd.type must be 0)
- */
-struct ipa_hdr_proc_ctx_add_hdr_cmd_seq {
-	struct ipa_hdr_proc_ctx_hdr_add hdr_add;
-	struct ipa_hdr_proc_ctx_tlv cmd;
-	struct ipa_hdr_proc_ctx_tlv end;
-};
-
-/**
- struct ipa_hdr_proc_ctx_entry - IPA processing context header table entry
- * @link: entry's link in global header table entries list
- * @type:
- * @offset_entry: entry's offset
- * @hdr: the header
- * @cookie: cookie used for validity check
- * @ref_cnt: reference counter of routing table
- * @id: processing context header entry id
- * @user_deleted: is the hdr processing context deleted by the user?
- */
-struct ipa_hdr_proc_ctx_entry {
-	struct list_head link;
-	u32 cookie;
-	enum ipa_hdr_proc_type type;
-	struct ipa_hdr_proc_ctx_offset_entry *offset_entry;
-	struct ipa_hdr_entry *hdr;
-	u32 ref_cnt;
-	int id;
-	bool user_deleted;
-};
-
-/**
- * struct ipa_hdr_proc_ctx_tbl - IPA processing context header table
- * @head_proc_ctx_entry_list: header entries list
- * @head_offset_list: header offset list
- * @head_free_offset_list: header free offset list
- * @proc_ctx_cnt: number of processing context headers
- * @end: the last processing context header index
- * @start_offset: offset in words of processing context header table
- */
-struct ipa_hdr_proc_ctx_tbl {
-	struct list_head head_proc_ctx_entry_list;
-	struct list_head head_offset_list[IPA_HDR_PROC_CTX_BIN_MAX];
-	struct list_head head_free_offset_list[IPA_HDR_PROC_CTX_BIN_MAX];
-	u32 proc_ctx_cnt;
-	u32 end;
-	u32 start_offset;
-};
-
-/**
- * struct ipa_flt_tbl - IPA filter table
- * @head_flt_rule_list: filter rules list
- * @rule_cnt: number of filter rules
- * @in_sys: flag indicating if filter table is located in system memory
- * @sz: the size of the filter table
- * @end: the last header index
- * @curr_mem: current filter tables block in sys memory
- * @prev_mem: previous filter table block in sys memory
- */
-struct ipa_flt_tbl {
-	struct list_head head_flt_rule_list;
-	u32 rule_cnt;
-	bool in_sys;
-	u32 sz;
-	struct ipa_mem_buffer curr_mem;
-	struct ipa_mem_buffer prev_mem;
-	bool sticky_rear;
-};
-
-/**
- * struct ipa_rt_entry - IPA routing table entry
- * @link: entry's link in global routing table entries list
- * @rule: routing rule
- * @cookie: cookie used for validity check
- * @tbl: routing table
- * @hdr: header table
- * @proc_ctx: processing context table
- * @hw_len: the length of the table
- */
-struct ipa_rt_entry {
-	struct list_head link;
-	u32 cookie;
-	struct ipa_rt_rule rule;
-	struct ipa_rt_tbl *tbl;
-	struct ipa_hdr_entry *hdr;
-	struct ipa_hdr_proc_ctx_entry *proc_ctx;
-	u32 hw_len;
-	int id;
-};
-
-/**
- * struct ipa_rt_tbl_set - collection of routing tables
- * @head_rt_tbl_list: collection of routing tables
- * @tbl_cnt: number of routing tables
- */
-struct ipa_rt_tbl_set {
-	struct list_head head_rt_tbl_list;
-	u32 tbl_cnt;
-};
-
-/**
- * struct ipa_ep_cfg_status - status configuration in IPA end-point
- * @status_en: Determines if end point supports Status Indications. SW should
- *	set this bit in order to enable Statuses. Output Pipe - send
- *	Status indications only if bit is set. Input Pipe - forward Status
- *	indication to STATUS_ENDP only if bit is set. Valid for Input
- *	and Output Pipes (IPA Consumer and Producer)
- * @status_ep: Statuses generated for this endpoint will be forwarded to the
- *	specifed Status End Point. Status endpoint needs to be
- *	configured with STATUS_EN=1 Valid only for Input Pipes (IPA
- *	Consumer)
- */
-struct ipa_ep_cfg_status {
-	bool status_en;
-	u8 status_ep;
-};
-
-/**
- * struct ipa_wlan_stats - Wlan stats for each wlan endpoint
- * @rx_pkts_rcvd: Packets sent by wlan driver
- * @rx_pkts_status_rcvd: Status packets received from ipa hw
- * @rx_hd_processed: Data Descriptors processed by IPA Driver
- * @rx_hd_reply: Data Descriptors recycled by wlan driver
- * @rx_hd_rcvd: Data Descriptors sent by wlan driver
- * @rx_pkt_leak: Packet count that are not recycled
- * @rx_dp_fail: Packets failed to transfer to IPA HW
- * @tx_pkts_rcvd: SKB Buffers received from ipa hw
- * @tx_pkts_sent: SKB Buffers sent to wlan driver
- * @tx_pkts_dropped: Dropped packets count
- */
-struct ipa_wlan_stats {
-	u32 rx_pkts_rcvd;
-	u32 rx_pkts_status_rcvd;
-	u32 rx_hd_processed;
-	u32 rx_hd_reply;
-	u32 rx_hd_rcvd;
-	u32 rx_pkt_leak;
-	u32 rx_dp_fail;
-	u32 tx_pkts_rcvd;
-	u32 tx_pkts_sent;
-	u32 tx_pkts_dropped;
-};
-
-/**
- * struct ipa_wlan_comm_memb - Wlan comm members
- * @wlan_spinlock: protects wlan comm buff list and its size
- * @ipa_tx_mul_spinlock: protects tx dp mul transfer
- * @wlan_comm_total_cnt: wlan common skb buffers allocated count
- * @wlan_comm_free_cnt: wlan common skb buffer free count
- * @total_tx_pkts_freed: Recycled Buffer count
- * @wlan_comm_desc_list: wlan common skb buffer list
- */
-struct ipa_wlan_comm_memb {
-	spinlock_t wlan_spinlock;
-	spinlock_t ipa_tx_mul_spinlock;
-	u32 wlan_comm_total_cnt;
-	u32 wlan_comm_free_cnt;
-	u32 total_tx_pkts_freed;
-	struct list_head wlan_comm_desc_list;
-	atomic_t active_clnt_cnt;
-};
-
-/**
- * struct ipa_ep_context - IPA end point context
- * @valid: flag indicating id EP context is valid
- * @client: EP client type
- * @ep_hdl: EP's client SPS handle
- * @cfg: EP cionfiguration
- * @dst_pipe_index: destination pipe index
- * @rt_tbl_idx: routing table index
- * @connect: SPS connect
- * @priv: user provided information which will forwarded once the user is
- *        notified for new data avail
- * @client_notify: user provided CB for EP events notification, the event is
- *                 data revived.
- * @desc_fifo_in_pipe_mem: flag indicating if descriptors FIFO uses pipe memory
- * @data_fifo_in_pipe_mem: flag indicating if data FIFO uses pipe memory
- * @desc_fifo_pipe_mem_ofst: descriptors FIFO pipe memory offset
- * @data_fifo_pipe_mem_ofst: data FIFO pipe memory offset
- * @desc_fifo_client_allocated: if descriptors FIFO was allocated by a client
- * @data_fifo_client_allocated: if data FIFO was allocated by a client
- * @skip_ep_cfg: boolean field that determines if EP should be configured
- *  by IPA driver
- * @keep_ipa_awake: when true, IPA will not be clock gated
- * @rx_replenish_threshold: Indicates the WM value which requires the RX
- *                          descriptors replenish function to be called to
- *                          avoid the RX pipe to run out of descriptors
- *                          and cause HOLB.
- * @disconnect_in_progress: Indicates client disconnect in progress.
- * @qmi_request_sent: Indicates whether QMI request to enable clear data path
- *					request is sent or not.
- */
-struct ipa_ep_context {
-	int valid;
-	enum ipa_client_type client;
-	struct sps_pipe *ep_hdl;
-	struct ipa_ep_cfg cfg;
-	struct ipa_ep_cfg_holb holb;
-	struct ipa_ep_cfg_status status;
-	u32 dst_pipe_index;
-	u32 rt_tbl_idx;
-	struct sps_connect connect;
-	void *priv;
-	void (*client_notify)(void *priv, enum ipa_dp_evt_type evt,
-		       unsigned long data);
-	bool desc_fifo_in_pipe_mem;
-	bool data_fifo_in_pipe_mem;
-	u32 desc_fifo_pipe_mem_ofst;
-	u32 data_fifo_pipe_mem_ofst;
-	bool desc_fifo_client_allocated;
-	bool data_fifo_client_allocated;
-	atomic_t avail_fifo_desc;
-	u32 dflt_flt4_rule_hdl;
-	u32 dflt_flt6_rule_hdl;
-	bool skip_ep_cfg;
-	bool keep_ipa_awake;
-	struct ipa_wlan_stats wstats;
-	u32 wdi_state;
-	u32 rx_replenish_threshold;
-	bool disconnect_in_progress;
-	u32 qmi_request_sent;
-
-	/* sys MUST be the last element of this struct */
-	struct ipa_sys_context *sys;
-};
-
-enum ipa_sys_pipe_policy {
-	IPA_POLICY_INTR_MODE,
-	IPA_POLICY_NOINTR_MODE,
-	IPA_POLICY_INTR_POLL_MODE,
-};
-
-struct ipa_repl_ctx {
-	struct ipa_rx_pkt_wrapper **cache;
-	atomic_t head_idx;
-	atomic_t tail_idx;
-	u32 capacity;
-};
-
-/**
- * struct ipa_sys_context - IPA endpoint context for system to BAM pipes
- * @head_desc_list: header descriptors list
- * @len: the size of the above list
- * @spinlock: protects the list and its size
- * @event: used to request CALLBACK mode from SPS driver
- * @ep: IPA EP context
- *
- * IPA context specific to the system-bam pipes a.k.a LAN IN/OUT and WAN
- */
-struct ipa_sys_context {
-	u32 len;
-	struct sps_register_event event;
-	atomic_t curr_polling_state;
-	struct delayed_work switch_to_intr_work;
-	enum ipa_sys_pipe_policy policy;
-	int (*pyld_hdlr)(struct sk_buff *skb, struct ipa_sys_context *sys);
-	struct sk_buff * (*get_skb)(unsigned int len, gfp_t flags);
-	void (*free_skb)(struct sk_buff *skb);
-	u32 rx_buff_sz;
-	u32 rx_pool_sz;
-	struct sk_buff *prev_skb;
-	unsigned int len_rem;
-	unsigned int len_pad;
-	unsigned int len_partial;
-	struct work_struct work;
-	void (*sps_callback)(struct sps_event_notify *notify);
-	enum sps_option sps_option;
-	struct delayed_work replenish_rx_work;
-	struct work_struct repl_work;
-	void (*repl_hdlr)(struct ipa_sys_context *sys);
-	struct ipa_repl_ctx repl;
-	unsigned int repl_trig_cnt;
-	unsigned int repl_trig_thresh;
-
-	/* ordering is important - mutable fields go above */
-	struct ipa_ep_context *ep;
-	struct list_head head_desc_list;
-	spinlock_t spinlock;
-	struct workqueue_struct *wq;
-	struct workqueue_struct *repl_wq;
-	/* ordering is important - other immutable fields go below */
-};
-
-/**
- * enum ipa_desc_type - IPA decriptors type
- *
- * IPA decriptors type, IPA supports DD and ICD but no CD
- */
-enum ipa_desc_type {
-	IPA_DATA_DESC,
-	IPA_DATA_DESC_SKB,
-	IPA_IMM_CMD_DESC
-};
-
-/**
- * struct ipa_tx_pkt_wrapper - IPA Tx packet wrapper
- * @type: specify if this packet is for the skb or immediate command
- * @mem: memory buffer used by this Tx packet
- * @work: work struct for current Tx packet
- * @link: linked to the wrappers on that pipe
- * @callback: IPA client provided callback
- * @user1: cookie1 for above callback
- * @user2: cookie2 for above callback
- * @sys: corresponding IPA sys context
- * @mult: valid only for first of a "multiple" transfer,
- * holds info for the "sps_transfer" buffer
- * @cnt: 1 for single transfers,
- * >1 and <0xFFFF for first of a "multiple" tranfer,
- * 0xFFFF for last desc, 0 for rest of "multiple' transfer
- * @bounce: va of bounce buffer
- * @unmap_dma: in case this is true, the buffer will not be dma unmapped
- *
- * This struct can wrap both data packet and immediate command packet.
- */
-struct ipa_tx_pkt_wrapper {
-	enum ipa_desc_type type;
-	struct ipa_mem_buffer mem;
-	struct work_struct work;
-	struct list_head link;
-	void (*callback)(void *user1, int user2);
-	void *user1;
-	int user2;
-	struct ipa_sys_context *sys;
-	struct ipa_mem_buffer mult;
-	u32 cnt;
-	void *bounce;
-	bool no_unmap_dma;
-};
-
-/**
- * struct ipa_desc - IPA descriptor
- * @type: skb or immediate command or plain old data
- * @pyld: points to skb
- * or kmalloc'ed immediate command parameters/plain old data
- * @dma_address: dma mapped address of pyld
- * @dma_address_valid: valid field for dma_address
- * @len: length of the pyld
- * @opcode: for immediate commands
- * @callback: IPA client provided completion callback
- * @user1: cookie1 for above callback
- * @user2: cookie2 for above callback
- * @xfer_done: completion object for sync completion
- */
-struct ipa_desc {
-	enum ipa_desc_type type;
-	void *pyld;
-	dma_addr_t dma_address;
-	bool dma_address_valid;
-	u16 len;
-	u16 opcode;
-	void (*callback)(void *user1, int user2);
-	void *user1;
-	int user2;
-	struct completion xfer_done;
-};
-
-/**
- * struct ipa_rx_pkt_wrapper - IPA Rx packet wrapper
- * @skb: skb
- * @dma_address: DMA address of this Rx packet
- * @link: linked to the Rx packets on that pipe
- * @len: how many bytes are copied into skb's flat buffer
- */
-struct ipa_rx_pkt_wrapper {
-	struct list_head link;
-	struct ipa_rx_data data;
-	u32 len;
-	struct work_struct work;
-	struct ipa_sys_context *sys;
-};
-
-/**
- * struct ipa_nat_mem - IPA NAT memory description
- * @class: pointer to the struct class
- * @dev: the dev_t of the device
- * @cdev: cdev of the device
- * @dev_num: device number
- * @vaddr: virtual address
- * @dma_handle: DMA handle
- * @size: NAT memory size
- * @is_mapped: flag indicating if NAT memory is mapped
- * @is_sys_mem: flag indicating if NAT memory is sys memory
- * @is_dev_init: flag indicating if NAT device is initialized
- * @lock: NAT memory mutex
- * @nat_base_address: nat table virutal address
- * @ipv4_rules_addr: base nat table address
- * @ipv4_expansion_rules_addr: expansion table address
- * @index_table_addr: index table address
- * @index_table_expansion_addr: index expansion table address
- * @size_base_tables: base table size
- * @size_expansion_tables: expansion table size
- * @public_ip_addr: ip address of nat table
- */
-struct ipa_nat_mem {
-	struct class *class;
-	struct device *dev;
-	struct cdev cdev;
-	dev_t dev_num;
-	void *vaddr;
-	dma_addr_t dma_handle;
-	size_t size;
-	bool is_mapped;
-	bool is_sys_mem;
-	bool is_dev_init;
-	bool is_dev;
-	struct mutex lock;
-	void *nat_base_address;
-	char *ipv4_rules_addr;
-	char *ipv4_expansion_rules_addr;
-	char *index_table_addr;
-	char *index_table_expansion_addr;
-	u32 size_base_tables;
-	u32 size_expansion_tables;
-	u32 public_ip_addr;
-	void *tmp_vaddr;
-	dma_addr_t tmp_dma_handle;
-	bool is_tmp_mem;
-};
-
-/**
- * enum ipa_hw_mode - IPA hardware mode
- * @IPA_HW_Normal: Regular IPA hardware
- * @IPA_HW_Virtual: IPA hardware supporting virtual memory allocation
- * @IPA_HW_PCIE: IPA hardware supporting memory allocation over PCIE Bridge
- */
-enum ipa_hw_mode {
-	IPA_HW_MODE_NORMAL  = 0,
-	IPA_HW_MODE_VIRTUAL = 1,
-	IPA_HW_MODE_PCIE    = 2
-};
-
-enum ipa_config_this_ep {
-	IPA_CONFIGURE_THIS_EP,
-	IPA_DO_NOT_CONFIGURE_THIS_EP,
-};
-
-struct ipa_stats {
-	u32 tx_sw_pkts;
-	u32 tx_hw_pkts;
-	u32 rx_pkts;
-	u32 rx_excp_pkts[MAX_NUM_EXCP];
-	u32 rx_repl_repost;
-	u32 tx_pkts_compl;
-	u32 rx_q_len;
-	u32 msg_w[IPA_EVENT_MAX_NUM];
-	u32 msg_r[IPA_EVENT_MAX_NUM];
-	u32 stat_compl;
-	u32 aggr_close;
-	u32 wan_aggr_close;
-	u32 wan_rx_empty;
-	u32 wan_repl_rx_empty;
-	u32 lan_rx_empty;
-	u32 lan_repl_rx_empty;
-	u32 flow_enable;
-	u32 flow_disable;
-};
-
-struct ipa_active_clients {
-	struct mutex mutex;
-	spinlock_t spinlock;
-	bool mutex_locked;
-	int cnt;
-};
-
-struct ipa_wakelock_ref_cnt {
-	spinlock_t spinlock;
-	int cnt;
-};
-
-struct ipa_tag_completion {
-	struct completion comp;
-	atomic_t cnt;
-};
-
-struct ipa_controller;
-
-/**
- *  @brief   Enum value determined based on the feature it
- *           corresponds to
- *  +----------------+----------------+
- *  |    3 bits      |     5 bits     |
- *  +----------------+----------------+
- *  |   HW_FEATURE   |     OPCODE     |
- *  +----------------+----------------+
- *
- */
-#define FEATURE_ENUM_VAL(feature, opcode) ((feature << 5) | opcode)
-#define EXTRACT_UC_FEATURE(value) (value >> 5)
-
-#define IPA_HW_NUM_FEATURES 0x8
-
-/**
- * enum ipa_hw_features - Values that represent the features supported in IPA HW
- * @IPA_HW_FEATURE_COMMON : Feature related to common operation of IPA HW
- * @IPA_HW_FEATURE_MHI : Feature related to MHI operation in IPA HW
- * @IPA_HW_FEATURE_WDI : Feature related to WDI operation in IPA HW
-*/
-enum ipa_hw_features {
-	IPA_HW_FEATURE_COMMON = 0x0,
-	IPA_HW_FEATURE_MHI    = 0x1,
-	IPA_HW_FEATURE_WDI    = 0x3,
-	IPA_HW_FEATURE_MAX    = IPA_HW_NUM_FEATURES
-};
-
-/**
- * struct IpaHwSharedMemCommonMapping_t - Structure referring to the common
- * section in 128B shared memory located in offset zero of SW Partition in IPA
- * SRAM.
- * @cmdOp : CPU->HW command opcode. See IPA_CPU_2_HW_COMMANDS
- * @cmdParams : CPU->HW command parameter. The parameter filed can hold 32 bits
- * of parameters (immediate parameters) and point on structure in system memory
- * (in such case the address must be accessible for HW)
- * @responseOp : HW->CPU response opcode. See IPA_HW_2_CPU_RESPONSES
- * @responseParams : HW->CPU response parameter. The parameter filed can hold 32
- * bits of parameters (immediate parameters) and point on structure in system
- * memory
- * @eventOp : HW->CPU event opcode. See IPA_HW_2_CPU_EVENTS
- * @eventParams : HW->CPU event parameter. The parameter filed can hold 32 bits of
- * parameters (immediate parameters) and point on structure in system memory
- * @firstErrorAddress : Contains the address of first error-source on SNOC
- * @hwState : State of HW. The state carries information regarding the error type.
- * @warningCounter : The warnings counter. The counter carries information regarding
- * non fatal errors in HW
- * @interfaceVersionCommon : The Common interface version as reported by HW
- *
- * The shared memory is used for communication between IPA HW and CPU.
- */
-struct IpaHwSharedMemCommonMapping_t {
-	u8  cmdOp;
-	u8  reserved_01;
-	u16 reserved_03_02;
-	u32 cmdParams;
-	u8  responseOp;
-	u8  reserved_09;
-	u16 reserved_0B_0A;
-	u32 responseParams;
-	u8  eventOp;
-	u8  reserved_11;
-	u16 reserved_13_12;
-	u32 eventParams;
-	u32 reserved_1B_18;
-	u32 firstErrorAddress;
-	u8  hwState;
-	u8  warningCounter;
-	u16 reserved_23_22;
-	u16 interfaceVersionCommon;
-	u16 reserved_27_26;
-} __packed;
-
-/**
- * union IpaHwFeatureInfoData_t - parameters for stats/config blob
- *
- * @offset : Location of a feature within the EventInfoData
- * @size : Size of the feature
- */
-union IpaHwFeatureInfoData_t {
-	struct IpaHwFeatureInfoParams_t {
-		u32 offset:16;
-		u32 size:16;
-	} __packed params;
-	u32 raw32b;
-} __packed;
-
-/**
- * struct IpaHwEventInfoData_t - Structure holding the parameters for
- * statistics and config info
- *
- * @baseAddrOffset : Base Address Offset of the statistics or config
- * structure from IPA_WRAPPER_BASE
- * @IpaHwFeatureInfoData_t : Location and size of each feature within
- * the statistics or config structure
- *
- * @note    Information about each feature in the featureInfo[]
- * array is populated at predefined indices per the IPA_HW_FEATURES
- * enum definition
- */
-struct IpaHwEventInfoData_t {
-	u32 baseAddrOffset;
-	union IpaHwFeatureInfoData_t featureInfo[IPA_HW_NUM_FEATURES];
-} __packed;
-
-/**
- * struct IpaHwEventLogInfoData_t - Structure holding the parameters for
- * IPA_HW_2_CPU_EVENT_LOG_INFO Event
- *
- * @featureMask : Mask indicating the features enabled in HW.
- * Refer IPA_HW_FEATURE_MASK
- * @circBuffBaseAddrOffset : Base Address Offset of the Circular Event
- * Log Buffer structure
- * @statsInfo : Statistics related information
- * @configInfo : Configuration related information
- *
- * @note    The offset location of this structure from IPA_WRAPPER_BASE
- * will be provided as Event Params for the IPA_HW_2_CPU_EVENT_LOG_INFO
- * Event
- */
-struct IpaHwEventLogInfoData_t {
-	u32 featureMask;
-	u32 circBuffBaseAddrOffset;
-	struct IpaHwEventInfoData_t statsInfo;
-	struct IpaHwEventInfoData_t configInfo;
-
-} __packed;
-
-/**
- * struct ipa_uc_hdlrs - IPA uC callback functions
- * @ipa_uc_loaded_hdlr: Function handler when uC is loaded
- * @ipa_uc_event_hdlr: Event handler function
- * @ipa_uc_response_hdlr: Response handler function
- * @ipa_uc_event_log_info_hdlr: Log event handler function
- */
-struct ipa_uc_hdlrs {
-	void (*ipa_uc_loaded_hdlr)(void);
-	void (*ipa_uc_event_hdlr)
-		(struct IpaHwSharedMemCommonMapping_t *uc_sram_mmio);
-	int (*ipa_uc_response_hdlr)
-		(struct IpaHwSharedMemCommonMapping_t *uc_sram_mmio,
-		u32 *uc_status);
-	void (*ipa_uc_event_log_info_hdlr)
-		(struct IpaHwEventLogInfoData_t *uc_event_top_mmio);
-};
-
-/**
- * enum ipa_hw_flags - flags which defines the behavior of HW
- *
- * @IPA_HW_FLAG_HALT_SYSTEM_ON_ASSERT_FAILURE: Halt system in case of assert
- *	failure.
- * @IPA_HW_FLAG_NO_REPORT_MHI_CHANNEL_ERORR: Channel error would be reported
- *	in the event ring only. No event to CPU.
- * @IPA_HW_FLAG_NO_REPORT_MHI_CHANNEL_WAKE_UP: No need to report event
- *	IPA_HW_2_CPU_EVENT_MHI_WAKE_UP_REQUEST
- * @IPA_HW_FLAG_WORK_OVER_DDR: Perform all transaction to external addresses by
- *	QMB (avoid memcpy)
- * @IPA_HW_FLAG_NO_REPORT_OOB: If set do not report that the device is OOB in
- *	IN Channel
- * @IPA_HW_FLAG_NO_REPORT_DB_MODE: If set, do not report that the device is
- *	entering a mode where it expects a doorbell to be rung for OUT Channel
- * @IPA_HW_FLAG_NO_START_OOB_TIMER
- */
-enum ipa_hw_flags {
-	IPA_HW_FLAG_HALT_SYSTEM_ON_ASSERT_FAILURE	= 0x01,
-	IPA_HW_FLAG_NO_REPORT_MHI_CHANNEL_ERORR		= 0x02,
-	IPA_HW_FLAG_NO_REPORT_MHI_CHANNEL_WAKE_UP	= 0x04,
-	IPA_HW_FLAG_WORK_OVER_DDR			= 0x08,
-	IPA_HW_FLAG_NO_REPORT_OOB			= 0x10,
-	IPA_HW_FLAG_NO_REPORT_DB_MODE			= 0x20,
-	IPA_HW_FLAG_NO_START_OOB_TIMER			= 0x40
-};
-
-/**
- * enum ipa_hw_mhi_channel_states - MHI channel state machine
- *
- * Values are according to MHI specification
- * @IPA_HW_MHI_CHANNEL_STATE_DISABLE: Channel is disabled and not processed by
- *	the host or device.
- * @IPA_HW_MHI_CHANNEL_STATE_ENABLE: A channel is enabled after being
- *	initialized and configured by host, including its channel context and
- *	associated transfer ring. While this state, the channel is not active
- *	and the device does not process transfer.
- * @IPA_HW_MHI_CHANNEL_STATE_RUN: The device processes transfers and doorbell
- *	for channels.
- * @IPA_HW_MHI_CHANNEL_STATE_SUSPEND: Used to halt operations on the channel.
- *	The device does not process transfers for the channel in this state.
- *	This state is typically used to synchronize the transition to low power
- *	modes.
- * @IPA_HW_MHI_CHANNEL_STATE_STOP: Used to halt operations on the channel.
- *	The device does not process transfers for the channel in this state.
- * @IPA_HW_MHI_CHANNEL_STATE_ERROR: The device detected an error in an element
- *	from the transfer ring associated with the channel.
- * @IPA_HW_MHI_CHANNEL_STATE_INVALID: Invalid state. Shall not be in use in
- *	operational scenario.
- */
-enum ipa_hw_mhi_channel_states {
-	IPA_HW_MHI_CHANNEL_STATE_DISABLE	= 0,
-	IPA_HW_MHI_CHANNEL_STATE_ENABLE		= 1,
-	IPA_HW_MHI_CHANNEL_STATE_RUN		= 2,
-	IPA_HW_MHI_CHANNEL_STATE_SUSPEND	= 3,
-	IPA_HW_MHI_CHANNEL_STATE_STOP		= 4,
-	IPA_HW_MHI_CHANNEL_STATE_ERROR		= 5,
-	IPA_HW_MHI_CHANNEL_STATE_INVALID	= 0xFF
-};
-
-/**
- * Structure holding the parameters for IPA_CPU_2_HW_CMD_MHI_DL_UL_SYNC_INFO
- * command. Parameters are sent as 32b immediate parameters.
- * @isDlUlSyncEnabled: Flag to indicate if DL UL Syncronization is enabled
- * @UlAccmVal: UL Timer Accumulation value (Period after which device will poll
- *	for UL data)
- * @ulMsiEventThreshold: Threshold at which HW fires MSI to host for UL events
- * @dlMsiEventThreshold: Threshold at which HW fires MSI to host for DL events
- */
-union IpaHwMhiDlUlSyncCmdData_t {
-	struct IpaHwMhiDlUlSyncCmdParams_t {
-		u32 isDlUlSyncEnabled:8;
-		u32 UlAccmVal:8;
-		u32 ulMsiEventThreshold:8;
-		u32 dlMsiEventThreshold:8;
-	} params;
-	u32 raw32b;
-};
-
-/**
- * struct ipa_uc_ctx - IPA uC context
- * @uc_inited: Indicates if uC interface has been initialized
- * @uc_loaded: Indicates if uC has loaded
- * @uc_failed: Indicates if uC has failed / returned an error
- * @uc_lock: uC interface lock to allow only one uC interaction at a time
- * @uc_completation: Completion mechanism to wait for uC commands
- * @uc_sram_mmio: Pointer to uC mapped memory
- * @pending_cmd: The last command sent waiting to be ACKed
- * @uc_status: The last status provided by the uC
- * @uc_zip_error: uC has notified the APPS upon a ZIP engine error
- * @uc_error_type: error type from uC error event
- */
-struct ipa_uc_ctx {
-	bool uc_inited;
-	bool uc_loaded;
-	bool uc_failed;
-	struct mutex uc_lock;
-	struct completion uc_completion;
-	struct IpaHwSharedMemCommonMapping_t *uc_sram_mmio;
-	struct IpaHwEventLogInfoData_t *uc_event_top_mmio;
-	u32 uc_event_top_ofst;
-	u32 pending_cmd;
-	u32 uc_status;
-	bool uc_zip_error;
-	u32 uc_error_type;
-};
-
-/**
- * struct ipa_uc_wdi_ctx
- * @wdi_uc_top_ofst:
- * @wdi_uc_top_mmio:
- * @wdi_uc_stats_ofst:
- * @wdi_uc_stats_mmio:
- */
-struct ipa_uc_wdi_ctx {
-	/* WDI specific fields */
-	u32 wdi_uc_stats_ofst;
-	struct IpaHwStatsWDIInfoData_t *wdi_uc_stats_mmio;
-	void *priv;
-	ipa_uc_ready_cb uc_ready_cb;
-};
-
-/**
- * struct ipa_sps_pm - SPS power management related members
- * @dec_clients: true if need to decrease active clients count
- * @eot_activity: represent EOT interrupt activity to determine to reset
- *  the inactivity timer
- */
-struct ipa_sps_pm {
-	atomic_t dec_clients;
-	atomic_t eot_activity;
-};
-
-/**
- * struct ipacm_client_info - the client-info indicated from IPACM
- * @ipacm_client_enum: the enum to indicate tether-client
- * @ipacm_client_uplink: the bool to indicate pipe for uplink
- */
-struct ipacm_client_info {
-	enum ipacm_client_enum client_enum;
-	bool uplink;
-};
-
-/**
- * struct ipa_context - IPA context
- * @class: pointer to the struct class
- * @dev_num: device number
- * @dev: the dev_t of the device
- * @cdev: cdev of the device
- * @bam_handle: IPA driver's BAM handle
- * @ep: list of all end points
- * @skip_ep_cfg_shadow: state to update filter table correctly across
-  power-save
- * @resume_on_connect: resume ep on ipa_connect
- * @flt_tbl: list of all IPA filter tables
- * @mode: IPA operating mode
- * @mmio: iomem
- * @ipa_wrapper_base: IPA wrapper base address
- * @glob_flt_tbl: global filter table
- * @hdr_tbl: IPA header table
- * @hdr_proc_ctx_tbl: IPA processing context table
- * @rt_tbl_set: list of routing tables each of which is a list of rules
- * @reap_rt_tbl_set: list of sys mem routing tables waiting to be reaped
- * @flt_rule_cache: filter rule cache
- * @rt_rule_cache: routing rule cache
- * @hdr_cache: header cache
- * @hdr_offset_cache: header offset cache
- * @hdr_proc_ctx_cache: processing context cache
- * @hdr_proc_ctx_offset_cache: processing context offset cache
- * @rt_tbl_cache: routing table cache
- * @tx_pkt_wrapper_cache: Tx packets cache
- * @rx_pkt_wrapper_cache: Rx packets cache
- * @rt_idx_bitmap: routing table index bitmap
- * @lock: this does NOT protect the linked lists within ipa_sys_context
- * @smem_sz: shared memory size available for SW use starting
- *  from non-restricted bytes
- * @smem_restricted_bytes: the bytes that SW should not use in the shared mem
- * @nat_mem: NAT memory
- * @excp_hdr_hdl: exception header handle
- * @dflt_v4_rt_rule_hdl: default v4 routing rule handle
- * @dflt_v6_rt_rule_hdl: default v6 routing rule handle
- * @aggregation_type: aggregation type used on USB client endpoint
- * @aggregation_byte_limit: aggregation byte limit used on USB client endpoint
- * @aggregation_time_limit: aggregation time limit used on USB client endpoint
- * @hdr_tbl_lcl: where hdr tbl resides 1-local, 0-system
- * @hdr_proc_ctx_tbl_lcl: where proc_ctx tbl resides true-local, false-system
- * @hdr_mem: header memory
- * @hdr_proc_ctx_mem: processing context memory
- * @ip4_rt_tbl_lcl: where ip4 rt tables reside 1-local; 0-system
- * @ip6_rt_tbl_lcl: where ip6 rt tables reside 1-local; 0-system
- * @ip4_flt_tbl_lcl: where ip4 flt tables reside 1-local; 0-system
- * @ip6_flt_tbl_lcl: where ip6 flt tables reside 1-local; 0-system
- * @empty_rt_tbl_mem: empty routing tables memory
- * @power_mgmt_wq: workqueue for power management
- * @sps_power_mgmt_wq: workqueue SPS related power management
- * @tag_process_before_gating: indicates whether to start tag process before
- *  gating IPA clocks
- * @sps_pm: sps power management related information
- * @disconnect_lock: protects LAN_CONS packet receive notification CB
- * @pipe_mem_pool: pipe memory pool
- * @dma_pool: special purpose DMA pool
- * @ipa_active_clients: structure for reference counting connected IPA clients
- * @ipa_hw_type: type of IPA HW type (e.g. IPA 1.0, IPA 1.1 etc')
- * @ipa_hw_mode: mode of IPA HW mode (e.g. Normal, Virtual or over PCIe)
- * @use_ipa_teth_bridge: use tethering bridge driver
- * @ipa_bam_remote_mode: ipa bam is in remote mode
- * @modem_cfg_emb_pipe_flt: modem configure embedded pipe filtering rules
- * @ipa_bus_hdl: msm driver handle for the data path bus
- * @ctrl: holds the core specific operations based on
- *  core version (vtable like)
- * @enable_clock_scaling: clock scaling is enabled ?
- * @curr_ipa_clk_rate: ipa_clk current rate
- * @wcstats: wlan common buffer stats
- * @uc_ctx: uC interface context
- * @uc_wdi_ctx: WDI specific fields for uC interface
- * @ipa_num_pipes: The number of pipes used by IPA HW
- * @skip_uc_pipe_reset: Indicates whether pipe reset via uC needs to be avoided
- * @ipa_client_apps_wan_cons_agg_gro: RMNET_IOCTL_INGRESS_FORMAT_AGG_DATA
- * @w_lock: Indicates the wakeup source.
- * @wakelock_ref_cnt: Indicates the number of times wakelock is acquired
-
- * IPA context - holds all relevant info about IPA driver and its state
- */
-struct ipa_context {
-	struct class *class;
-	dev_t dev_num;
-	struct device *dev;
-	struct cdev cdev;
-	unsigned long bam_handle;
-	struct ipa_ep_context ep[IPA_MAX_NUM_PIPES];
-	bool skip_ep_cfg_shadow[IPA_MAX_NUM_PIPES];
-	bool resume_on_connect[IPA_CLIENT_MAX];
-	struct ipa_flt_tbl flt_tbl[IPA_MAX_NUM_PIPES][IPA_IP_MAX];
-	void __iomem *mmio;
-	u32 ipa_wrapper_base;
-	struct ipa_flt_tbl glob_flt_tbl[IPA_IP_MAX];
-	struct ipa_hdr_tbl hdr_tbl;
-	struct ipa_hdr_proc_ctx_tbl hdr_proc_ctx_tbl;
-	struct ipa_rt_tbl_set rt_tbl_set[IPA_IP_MAX];
-	struct ipa_rt_tbl_set reap_rt_tbl_set[IPA_IP_MAX];
-	struct kmem_cache *flt_rule_cache;
-	struct kmem_cache *rt_rule_cache;
-	struct kmem_cache *hdr_cache;
-	struct kmem_cache *hdr_offset_cache;
-	struct kmem_cache *hdr_proc_ctx_cache;
-	struct kmem_cache *hdr_proc_ctx_offset_cache;
-	struct kmem_cache *rt_tbl_cache;
-	struct kmem_cache *tx_pkt_wrapper_cache;
-	struct kmem_cache *rx_pkt_wrapper_cache;
-	unsigned long rt_idx_bitmap[IPA_IP_MAX];
-	struct mutex lock;
-	u16 smem_sz;
-	u16 smem_restricted_bytes;
-	u16 smem_reqd_sz;
-	struct ipa_nat_mem nat_mem;
-	u32 excp_hdr_hdl;
-	u32 dflt_v4_rt_rule_hdl;
-	u32 dflt_v6_rt_rule_hdl;
-	uint aggregation_type;
-	uint aggregation_byte_limit;
-	uint aggregation_time_limit;
-	bool hdr_tbl_lcl;
-	bool hdr_proc_ctx_tbl_lcl;
-	struct ipa_mem_buffer hdr_mem;
-	struct ipa_mem_buffer hdr_proc_ctx_mem;
-	bool ip4_rt_tbl_lcl;
-	bool ip6_rt_tbl_lcl;
-	bool ip4_flt_tbl_lcl;
-	bool ip6_flt_tbl_lcl;
-	struct ipa_mem_buffer empty_rt_tbl_mem;
-	struct gen_pool *pipe_mem_pool;
-	struct dma_pool *dma_pool;
-	struct ipa_active_clients ipa_active_clients;
-	struct workqueue_struct *power_mgmt_wq;
-	struct workqueue_struct *sps_power_mgmt_wq;
-	bool tag_process_before_gating;
-	struct ipa_sps_pm sps_pm;
-	u32 clnt_hdl_cmd;
-	u32 clnt_hdl_data_in;
-	u32 clnt_hdl_data_out;
-	spinlock_t disconnect_lock;
-	u8 a5_pipe_index;
-	struct list_head intf_list;
-	struct list_head msg_list;
-	struct list_head pull_msg_list;
-	struct mutex msg_lock;
-	wait_queue_head_t msg_waitq;
-	enum ipa_hw_type ipa_hw_type;
-	enum ipa_hw_mode ipa_hw_mode;
-	bool use_ipa_teth_bridge;
-	bool ipa_bam_remote_mode;
-	bool modem_cfg_emb_pipe_flt;
-	/* featurize if memory footprint becomes a concern */
-	struct ipa_stats stats;
-	void *smem_pipe_mem;
-	u32 ipa_bus_hdl;
-	struct ipa_controller *ctrl;
-	struct idr ipa_idr;
-	struct device *pdev;
-	struct device *uc_pdev;
-	spinlock_t idr_lock;
-	u32 enable_clock_scaling;
-	u32 curr_ipa_clk_rate;
-	bool q6_proxy_clk_vote_valid;
-	u32 ipa_num_pipes;
-
-	struct ipa_wlan_comm_memb wc_memb;
-
-	struct ipa_uc_ctx uc_ctx;
-
-	struct ipa_uc_wdi_ctx uc_wdi_ctx;
-	u32 wan_rx_ring_size;
-	bool skip_uc_pipe_reset;
-	bool smmu_present;
-	unsigned long peer_bam_iova;
-	phys_addr_t peer_bam_pa;
-	u32 peer_bam_map_size;
-	unsigned long peer_bam_dev;
-	u32 peer_bam_map_cnt;
-	u32 wdi_map_cnt;
-	struct wakeup_source w_lock;
-	struct ipa_wakelock_ref_cnt wakelock_ref_cnt;
-
-	/* RMNET_IOCTL_INGRESS_FORMAT_AGG_DATA */
-	bool ipa_client_apps_wan_cons_agg_gro;
-	bool tethered_flow_control;
-	/* M-release support to know client pipes */
-	struct ipacm_client_info ipacm_client[IPA_MAX_NUM_PIPES];
-};
-
-/**
- * struct ipa_route - IPA route
- * @route_dis: route disable
- * @route_def_pipe: route default pipe
- * @route_def_hdr_table: route default header table
- * @route_def_hdr_ofst: route default header offset table
- * @route_frag_def_pipe: Default pipe to route fragmented exception
- *    packets and frag new rule statues, if source pipe does not have
- *    a notification status pipe defined.
- */
-struct ipa_route {
-	u32 route_dis;
-	u32 route_def_pipe;
-	u32 route_def_hdr_table;
-	u32 route_def_hdr_ofst;
-	u8  route_frag_def_pipe;
-};
-
-/**
- * enum ipa_pipe_mem_type - IPA pipe memory type
- * @IPA_SPS_PIPE_MEM: Default, SPS dedicated pipe memory
- * @IPA_PRIVATE_MEM: IPA's private memory
- * @IPA_SYSTEM_MEM: System RAM, requires allocation
- */
-enum ipa_pipe_mem_type {
-	IPA_SPS_PIPE_MEM = 0,
-	IPA_PRIVATE_MEM  = 1,
-	IPA_SYSTEM_MEM   = 2,
-};
-
-struct ipa_plat_drv_res {
-	bool use_ipa_teth_bridge;
-	u32 ipa_mem_base;
-	u32 ipa_mem_size;
-	u32 bam_mem_base;
-	u32 bam_mem_size;
-	u32 ipa_irq;
-	u32 bam_irq;
-	u32 ipa_pipe_mem_start_ofst;
-	u32 ipa_pipe_mem_size;
-	enum ipa_hw_type ipa_hw_type;
-	enum ipa_hw_mode ipa_hw_mode;
-	u32 ee;
-	bool ipa_bam_remote_mode;
-	bool modem_cfg_emb_pipe_flt;
-	u32 wan_rx_ring_size;
-	bool skip_uc_pipe_reset;
-	bool tethered_flow_control;
-};
-
-struct ipa_mem_partition {
-	u16 ofst_start;
-	u16 nat_ofst;
-	u16 nat_size;
-	u16 v4_flt_ofst;
-	u16 v4_flt_size;
-	u16 v4_flt_size_ddr;
-	u16 v6_flt_ofst;
-	u16 v6_flt_size;
-	u16 v6_flt_size_ddr;
-	u16 v4_rt_ofst;
-	u16 v4_num_index;
-	u16 v4_modem_rt_index_lo;
-	u16 v4_modem_rt_index_hi;
-	u16 v4_apps_rt_index_lo;
-	u16 v4_apps_rt_index_hi;
-	u16 v4_rt_size;
-	u16 v4_rt_size_ddr;
-	u16 v6_rt_ofst;
-	u16 v6_num_index;
-	u16 v6_modem_rt_index_lo;
-	u16 v6_modem_rt_index_hi;
-	u16 v6_apps_rt_index_lo;
-	u16 v6_apps_rt_index_hi;
-	u16 v6_rt_size;
-	u16 v6_rt_size_ddr;
-	u16 modem_hdr_ofst;
-	u16 modem_hdr_size;
-	u16 apps_hdr_ofst;
-	u16 apps_hdr_size;
-	u16 apps_hdr_size_ddr;
-	u16 modem_hdr_proc_ctx_ofst;
-	u16 modem_hdr_proc_ctx_size;
-	u16 apps_hdr_proc_ctx_ofst;
-	u16 apps_hdr_proc_ctx_size;
-	u16 apps_hdr_proc_ctx_size_ddr;
-	u16 modem_comp_decomp_ofst;
-	u16 modem_comp_decomp_size;
-	u16 modem_ofst;
-	u16 modem_size;
-	u16 apps_v4_flt_ofst;
-	u16 apps_v4_flt_size;
-	u16 apps_v6_flt_ofst;
-	u16 apps_v6_flt_size;
-	u16 uc_info_ofst;
-	u16 uc_info_size;
-	u16 end_ofst;
-	u16 apps_v4_rt_ofst;
-	u16 apps_v4_rt_size;
-	u16 apps_v6_rt_ofst;
-	u16 apps_v6_rt_size;
-};
-
-struct ipa_controller {
-	struct ipa_mem_partition mem_partition;
-	u32 ipa_clk_rate_turbo;
-	u32 ipa_clk_rate_nominal;
-	u32 ipa_clk_rate_svs;
-	u32 clock_scaling_bw_threshold_turbo;
-	u32 clock_scaling_bw_threshold_nominal;
-	u32 ipa_reg_base_ofst;
-	u32 max_holb_tmr_val;
-	void (*ipa_sram_read_settings)(void);
-	int (*ipa_init_sram)(void);
-	int (*ipa_init_hdr)(void);
-	int (*ipa_init_rt4)(void);
-	int (*ipa_init_rt6)(void);
-	int (*ipa_init_flt4)(void);
-	int (*ipa_init_flt6)(void);
-	void (*ipa_cfg_ep_hdr)(u32 pipe_number,
-			const struct ipa_ep_cfg_hdr *ipa_ep_hdr_cfg);
-	int (*ipa_cfg_ep_hdr_ext)(u32 pipe_number,
-		const struct ipa_ep_cfg_hdr_ext *ipa_ep_hdr_ext_cfg);
-	void (*ipa_cfg_ep_aggr)(u32 pipe_number,
-			const struct ipa_ep_cfg_aggr *ipa_ep_agrr_cfg);
-	int (*ipa_cfg_ep_deaggr)(u32 pipe_index,
-			const struct ipa_ep_cfg_deaggr *ep_deaggr);
-	void (*ipa_cfg_ep_nat)(u32 pipe_number,
-			const struct ipa_ep_cfg_nat *ipa_ep_nat_cfg);
-	void (*ipa_cfg_ep_mode)(u32 pipe_number, u32 dst_pipe_number,
-			const struct ipa_ep_cfg_mode *ep_mode);
-	void (*ipa_cfg_ep_route)(u32 pipe_index, u32 rt_tbl_index);
-	void (*ipa_cfg_ep_holb)(u32 pipe_index,
-			const struct ipa_ep_cfg_holb *ep_holb);
-	void (*ipa_cfg_route)(struct ipa_route *route);
-	int (*ipa_read_gen_reg)(char *buff, int max_len);
-	int (*ipa_read_ep_reg)(char *buff, int max_len, int pipe);
-	void (*ipa_write_dbg_cnt)(int option);
-	int (*ipa_read_dbg_cnt)(char *buf, int max_len);
-	void (*ipa_cfg_ep_status)(u32 clnt_hdl,
-			const struct ipa_ep_cfg_status *ep_status);
-	int (*ipa_commit_flt)(enum ipa_ip_type ip);
-	int (*ipa_commit_rt)(enum ipa_ip_type ip);
-	int (*ipa_generate_rt_hw_rule)(enum ipa_ip_type ip,
-		struct ipa_rt_entry *entry, u8 *buf);
-	int (*ipa_commit_hdr)(void);
-	void (*ipa_cfg_ep_cfg)(u32 clnt_hdl,
-			const struct ipa_ep_cfg_cfg *cfg);
-	void (*ipa_cfg_ep_metadata_mask)(u32 clnt_hdl,
-			const struct ipa_ep_cfg_metadata_mask *metadata_mask);
-	void (*ipa_enable_clks)(void);
-	void (*ipa_disable_clks)(void);
-	struct msm_bus_scale_pdata *msm_bus_data_ptr;
-
-	void (*ipa_cfg_ep_metadata)(u32 pipe_number,
-			const struct ipa_ep_cfg_metadata *);
-};
-
-extern struct ipa_context *ipa_ctx;
-
-/*
- * Tethering client info
- */
-void ipa_set_client(int index, enum ipacm_client_enum client, bool uplink);
-
-enum ipacm_client_enum ipa_get_client(int pipe_idx);
-
-bool ipa_get_client_uplink(int pipe_idx);
-
-int ipa_send_one(struct ipa_sys_context *sys, struct ipa_desc *desc,
-		bool in_atomic);
-int ipa_send(struct ipa_sys_context *sys, u32 num_desc, struct ipa_desc *desc,
-		bool in_atomic);
-int ipa_get_ep_mapping(enum ipa_client_type client);
-
-int ipa_generate_hw_rule(enum ipa_ip_type ip,
-			 const struct ipa_rule_attrib *attrib,
-			 u8 **buf,
-			 u16 *en_rule);
-u8 *ipa_write_32(u32 w, u8 *dest);
-u8 *ipa_write_16(u16 hw, u8 *dest);
-u8 *ipa_write_8(u8 b, u8 *dest);
-u8 *ipa_pad_to_32(u8 *dest);
-int ipa_init_hw(void);
-struct ipa_rt_tbl *__ipa_find_rt_tbl(enum ipa_ip_type ip, const char *name);
-int ipa_set_single_ndp_per_mbim(bool);
-int ipa_set_hw_timer_fix_for_mbim_aggr(bool);
-void ipa_debugfs_init(void);
-void ipa_debugfs_remove(void);
-
-void ipa_dump_buff_internal(void *base, dma_addr_t phy_base, u32 size);
-#ifdef IPA_DEBUG
-#define IPA_DUMP_BUFF(base, phy_base, size) \
-	ipa_dump_buff_internal(base, phy_base, size)
-#else
-#define IPA_DUMP_BUFF(base, phy_base, size)
-#endif
-int ipa_controller_static_bind(struct ipa_controller *controller,
-		enum ipa_hw_type ipa_hw_type);
-int ipa_cfg_route(struct ipa_route *route);
-int ipa_send_cmd(u16 num_desc, struct ipa_desc *descr);
-int ipa_cfg_filter(u32 disable);
-int ipa_pipe_mem_init(u32 start_ofst, u32 size);
-int ipa_pipe_mem_alloc(u32 *ofst, u32 size);
-int ipa_pipe_mem_free(u32 ofst, u32 size);
-int ipa_straddle_boundary(u32 start, u32 end, u32 boundary);
-struct ipa_context *ipa_get_ctx(void);
-void ipa_enable_clks(void);
-void ipa_disable_clks(void);
-void ipa_inc_client_enable_clks(void);
-int ipa_inc_client_enable_clks_no_block(void);
-void ipa_dec_client_disable_clks(void);
-int ipa_interrupts_init(u32 ipa_irq, u32 ee, struct device *ipa_dev);
-int ipa_del_hdr_by_user(struct ipa_ioc_del_hdr *hdls, bool by_user);
-int ipa_del_hdr_proc_ctx_by_user(struct ipa_ioc_del_hdr_proc_ctx *hdls,
-	bool by_user);
-int __ipa_del_rt_rule(u32 rule_hdl);
-int __ipa_del_hdr(u32 hdr_hdl, bool by_user);
-int __ipa_release_hdr(u32 hdr_hdl);
-int __ipa_release_hdr_proc_ctx(u32 proc_ctx_hdl);
-int _ipa_read_gen_reg_v1_1(char *buff, int max_len);
-int _ipa_read_gen_reg_v2_0(char *buff, int max_len);
-int _ipa_read_ep_reg_v1_1(char *buf, int max_len, int pipe);
-int _ipa_read_ep_reg_v2_0(char *buf, int max_len, int pipe);
-void _ipa_write_dbg_cnt_v1_1(int option);
-void _ipa_write_dbg_cnt_v2_0(int option);
-int _ipa_read_dbg_cnt_v1_1(char *buf, int max_len);
-int _ipa_read_dbg_cnt_v2_0(char *buf, int max_len);
-void _ipa_enable_clks_v1_1(void);
-void _ipa_enable_clks_v2_0(void);
-void _ipa_disable_clks_v1_1(void);
-void _ipa_disable_clks_v2_0(void);
-
-static inline u32 ipa_read_reg(void *base, u32 offset)
-{
-	return ioread32(base + offset);
-}
-
-static inline u32 ipa_read_reg_field(void *base, u32 offset,
-		u32 mask, u32 shift)
-{
-	return (ipa_read_reg(base, offset) & mask) >> shift;
-}
-
-static inline void ipa_write_reg(void *base, u32 offset, u32 val)
-{
-	iowrite32(val, base + offset);
-}
-
-int ipa_bridge_init(void);
-void ipa_bridge_cleanup(void);
-
-ssize_t ipa_read(struct file *filp, char __user *buf, size_t count,
-		 loff_t *f_pos);
-int ipa_pull_msg(struct ipa_msg_meta *meta, char *buff, size_t count);
-int ipa_query_intf(struct ipa_ioc_query_intf *lookup);
-int ipa_query_intf_tx_props(struct ipa_ioc_query_intf_tx_props *tx);
-int ipa_query_intf_rx_props(struct ipa_ioc_query_intf_rx_props *rx);
-int ipa_query_intf_ext_props(struct ipa_ioc_query_intf_ext_props *ext);
-
-void wwan_cleanup(void);
-
-int teth_bridge_driver_init(void);
-void ipa_lan_rx_cb(void *priv, enum ipa_dp_evt_type evt, unsigned long data);
-
-int _ipa_init_sram_v2(void);
-int _ipa_init_sram_v2_5(void);
-int _ipa_init_sram_v2_6L(void);
-int _ipa_init_hdr_v2(void);
-int _ipa_init_hdr_v2_5(void);
-int _ipa_init_hdr_v2_6L(void);
-int _ipa_init_rt4_v2(void);
-int _ipa_init_rt6_v2(void);
-int _ipa_init_flt4_v2(void);
-int _ipa_init_flt6_v2(void);
-
-int __ipa_commit_flt_v1_1(enum ipa_ip_type ip);
-int __ipa_commit_flt_v2(enum ipa_ip_type ip);
-int __ipa_commit_rt_v1_1(enum ipa_ip_type ip);
-int __ipa_commit_rt_v2(enum ipa_ip_type ip);
-int __ipa_generate_rt_hw_rule_v2(enum ipa_ip_type ip,
-	struct ipa_rt_entry *entry, u8 *buf);
-int __ipa_generate_rt_hw_rule_v2_5(enum ipa_ip_type ip,
-	struct ipa_rt_entry *entry, u8 *buf);
-int __ipa_generate_rt_hw_rule_v2_6L(enum ipa_ip_type ip,
-	struct ipa_rt_entry *entry, u8 *buf);
-
-int __ipa_commit_hdr_v1_1(void);
-int __ipa_commit_hdr_v2(void);
-int __ipa_commit_hdr_v2_5(void);
-int __ipa_commit_hdr_v2_6L(void);
-int ipa_generate_flt_eq(enum ipa_ip_type ip,
-		const struct ipa_rule_attrib *attrib,
-		struct ipa_ipfltri_rule_eq *eq_attrib);
-void ipa_skb_recycle(struct sk_buff *skb);
-void ipa_install_dflt_flt_rules(u32 ipa_ep_idx);
-void ipa_delete_dflt_flt_rules(u32 ipa_ep_idx);
-
-int ipa_enable_data_path(u32 clnt_hdl);
-int ipa_disable_data_path(u32 clnt_hdl);
-int ipa_id_alloc(void *ptr);
-void *ipa_id_find(u32 id);
-void ipa_id_remove(u32 id);
-
-int ipa_set_required_perf_profile(enum ipa_voltage_level floor_voltage,
-				  u32 bandwidth_mbps);
-
-int ipa_cfg_ep_status(u32 clnt_hdl, const struct ipa_ep_cfg_status *ipa_ep_cfg);
-int ipa_cfg_aggr_cntr_granularity(u8 aggr_granularity);
-int ipa_cfg_eot_coal_cntr_granularity(u8 eot_coal_granularity);
-
-int ipa_suspend_resource_no_block(enum ipa_rm_resource_name name);
-int ipa_suspend_resource_sync(enum ipa_rm_resource_name name);
-int ipa_resume_resource(enum ipa_rm_resource_name name);
-bool ipa_should_pipe_be_suspended(enum ipa_client_type client);
-int ipa_tag_aggr_force_close(int pipe_num);
-
-void ipa_active_clients_lock(void);
-int ipa_active_clients_trylock(unsigned long *flags);
-void ipa_active_clients_unlock(void);
-void ipa_active_clients_trylock_unlock(unsigned long *flags);
-int ipa_wdi_init(void);
-int ipa_write_qmapid_wdi_pipe(u32 clnt_hdl, u8 qmap_id);
-int ipa_tag_process(struct ipa_desc *desc, int num_descs,
-		    unsigned long timeout);
-
-int ipa_q6_pre_shutdown_cleanup(void);
-int ipa_q6_post_shutdown_cleanup(void);
-int ipa_init_q6_smem(void);
-int ipa_q6_monitor_holb_mitigation(bool enable);
-
-int ipa_sps_connect_safe(struct sps_pipe *h, struct sps_connect *connect,
-			 enum ipa_client_type ipa_client);
-
-int ipa_mhi_handle_ipa_config_req(struct ipa_config_req_msg_v01 *config_req);
-
-int ipa_uc_interface_init(void);
-int ipa_uc_reset_pipe(enum ipa_client_type ipa_client);
-int ipa_uc_monitor_holb(enum ipa_client_type ipa_client, bool enable);
-int ipa_uc_state_check(void);
-int ipa_uc_loaded_check(void);
-int ipa_uc_send_cmd(u32 cmd, u32 opcode, u32 expected_status,
-		    bool polling_mode, unsigned long timeout_jiffies);
-void ipa_register_panic_uc_hdlr(void);
-void ipa_uc_register_handlers(enum ipa_hw_features feature,
-			      struct ipa_uc_hdlrs *hdlrs);
-int create_nat_device(void);
-int ipa_uc_notify_clk_state(bool enabled);
-void ipa_dma_async_memcpy_notify_cb(void *priv,
-		enum ipa_dp_evt_type evt, unsigned long data);
-
-int ipa_uc_update_hw_flags(u32 flags);
-
-int ipa_uc_mhi_init(void (*ready_cb)(void), void (*wakeup_request_cb)(void));
-int ipa_uc_mhi_send_dl_ul_sync_info(union IpaHwMhiDlUlSyncCmdData_t cmd);
-int ipa_uc_mhi_init_engine(struct ipa_mhi_msi_info *msi, u32 mmio_addr,
-	u32 host_ctrl_addr, u32 host_data_addr, u32 first_ch_idx,
-	u32 first_evt_idx);
-int ipa_uc_mhi_init_channel(int ipa_ep_idx, int channelHandle,
-	int contexArrayIndex, int channelDirection);
-int ipa_uc_mhi_reset_channel(int channelHandle);
-int ipa_uc_mhi_suspend_channel(int channelHandle);
-int ipa_uc_mhi_resume_channel(int channelHandle, bool LPTransitionRejected);
-int ipa_uc_mhi_stop_event_update_channel(int channelHandle);
-int ipa_uc_mhi_print_stats(char *dbg_buff, int size);
-int ipa_uc_memcpy(phys_addr_t dest, phys_addr_t src, int len);
-u32 ipa_get_num_pipes(void);
-u32 ipa_get_sys_yellow_wm(void);
-int ipa_smmu_map_peer_bam(unsigned long dev);
-int ipa_smmu_unmap_peer_bam(unsigned long dev);
-struct ipa_smmu_cb_ctx *ipa_get_wlan_smmu_ctx(void);
-struct ipa_smmu_cb_ctx *ipa_get_uc_smmu_ctx(void);
-struct iommu_domain *ipa_get_uc_smmu_domain(void);
-void ipa_flow_control(enum ipa_client_type ipa_client, bool enable,
-			uint32_t qmap_id);
-void ipa_suspend_apps_pipes(bool suspend);
-void ipa_register_panic_gen_notifier(void);
-void ipa_update_repl_threshold(enum ipa_client_type ipa_client);
-void ipa_inc_acquire_wakelock(void);
-void ipa_dec_release_wakelock(void);
-void ipa_sps_irq_control_all(bool enable);
-#endif /* _IPA_I_H_ */
diff --git a/drivers/platform/msm/ipa/ipa_interrupts.c b/drivers/platform/msm/ipa/ipa_interrupts.c
deleted file mode 100644
index a1f75f2..00000000
--- a/drivers/platform/msm/ipa/ipa_interrupts.c
+++ /dev/null
@@ -1,313 +0,0 @@
-/* Copyright (c) 2014-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-#include <linux/interrupt.h>
-#include "ipa_i.h"
-
-#define INTERRUPT_WORKQUEUE_NAME "ipa_interrupt_wq"
-
-struct ipa_interrupt_info {
-	ipa_irq_handler_t handler;
-	void *private_data;
-	bool deferred_flag;
-};
-
-struct ipa_interrupt_work_wrap {
-	struct work_struct interrupt_work;
-	ipa_irq_handler_t handler;
-	enum ipa_irq_type interrupt;
-	void *private_data;
-	void *interrupt_data;
-};
-
-static struct ipa_interrupt_info ipa_interrupt_to_cb[IPA_IRQ_MAX];
-static struct workqueue_struct *ipa_interrupt_wq;
-static u32 ipa_ee;
-
-static void ipa_interrupt_defer(struct work_struct *work);
-static DECLARE_WORK(ipa_interrupt_defer_work, ipa_interrupt_defer);
-
-static void deferred_interrupt_work(struct work_struct *work)
-{
-	struct ipa_interrupt_work_wrap *work_data =
-			container_of(work,
-			struct ipa_interrupt_work_wrap,
-			interrupt_work);
-	IPADBG("call handler from workq...\n");
-	work_data->handler(work_data->interrupt, work_data->private_data,
-			work_data->interrupt_data);
-	kfree(work_data->interrupt_data);
-	kfree(work_data);
-}
-
-static bool is_valid_ep(u32 ep_suspend_data)
-{
-	u32 bmsk = 1;
-	u32 i = 0;
-
-	for (i = 0; i < ipa_ctx->ipa_num_pipes; i++) {
-		if ((ep_suspend_data & bmsk) && (ipa_ctx->ep[i].valid))
-			return true;
-		bmsk = bmsk << 1;
-	}
-	return false;
-}
-
-static int handle_interrupt(enum ipa_irq_type interrupt, bool isr_context)
-{
-	struct ipa_interrupt_info interrupt_info;
-	struct ipa_interrupt_work_wrap *work_data;
-	u32 suspend_data;
-	void *interrupt_data = NULL;
-	struct ipa_tx_suspend_irq_data *suspend_interrupt_data = NULL;
-	int res;
-
-	interrupt_info = ipa_interrupt_to_cb[interrupt];
-	if (interrupt_info.handler == NULL) {
-		IPAERR("A callback function wasn't set for interrupt type %d\n",
-				interrupt);
-		return -EINVAL;
-	}
-
-	switch (interrupt) {
-	case IPA_TX_SUSPEND_IRQ:
-		suspend_data = ipa_read_reg(ipa_ctx->mmio,
-					IPA_IRQ_SUSPEND_INFO_EE_n_ADDR(ipa_ee));
-		if (!is_valid_ep(suspend_data))
-			return 0;
-
-		suspend_interrupt_data =
-			kzalloc(sizeof(*suspend_interrupt_data), GFP_ATOMIC);
-		if (!suspend_interrupt_data) {
-			IPAERR("failed allocating suspend_interrupt_data\n");
-			return -ENOMEM;
-		}
-		suspend_interrupt_data->endpoints = suspend_data;
-		interrupt_data = suspend_interrupt_data;
-		break;
-	default:
-		break;
-	}
-
-	/* Force defer processing if in ISR context. */
-	if (interrupt_info.deferred_flag || isr_context) {
-		work_data = kzalloc(sizeof(struct ipa_interrupt_work_wrap),
-				GFP_ATOMIC);
-		if (!work_data) {
-			IPAERR("failed allocating ipa_interrupt_work_wrap\n");
-			res = -ENOMEM;
-			goto fail_alloc_work;
-		}
-		INIT_WORK(&work_data->interrupt_work, deferred_interrupt_work);
-		work_data->handler = interrupt_info.handler;
-		work_data->interrupt = interrupt;
-		work_data->private_data = interrupt_info.private_data;
-		work_data->interrupt_data = interrupt_data;
-		queue_work(ipa_interrupt_wq, &work_data->interrupt_work);
-
-	} else {
-		interrupt_info.handler(interrupt, interrupt_info.private_data,
-				interrupt_data);
-		kfree(interrupt_data);
-	}
-
-	return 0;
-
-fail_alloc_work:
-	kfree(interrupt_data);
-	return res;
-}
-
-static void ipa_process_interrupts(bool isr_context)
-{
-	u32 reg;
-	u32 bmsk;
-	u32 i = 0;
-	u32 en;
-
-	en = ipa_read_reg(ipa_ctx->mmio, IPA_IRQ_EN_EE_n_ADDR(ipa_ee));
-	reg = ipa_read_reg(ipa_ctx->mmio, IPA_IRQ_STTS_EE_n_ADDR(ipa_ee));
-	while (en & reg) {
-		/*
-		* Clear interrupt before processing to avoid
-		* clearing unhandled interrupts
-		*/
-		ipa_write_reg(ipa_ctx->mmio,
-				IPA_IRQ_CLR_EE_n_ADDR(ipa_ee), reg);
-
-		/*
-		* Process the interrupts
-		*/
-		bmsk = 1;
-		for (i = 0; i < IPA_IRQ_MAX; i++) {
-			if (en & reg & bmsk)
-				handle_interrupt(i, isr_context);
-			bmsk = bmsk << 1;
-		}
-
-		/*
-		* Check pending interrupts that may have
-		* been raised since last read
-		*/
-		reg = ipa_read_reg(ipa_ctx->mmio,
-				IPA_IRQ_STTS_EE_n_ADDR(ipa_ee));
-	}
-}
-
-static void ipa_interrupt_defer(struct work_struct *work)
-{
-	IPADBG("processing interrupts in wq\n");
-	ipa_inc_client_enable_clks();
-	ipa_process_interrupts(false);
-	ipa_dec_client_disable_clks();
-	IPADBG("Done\n");
-}
-
-static irqreturn_t ipa_isr(int irq, void *ctxt)
-{
-	unsigned long flags;
-
-	/* defer interrupt handling in case IPA is not clocked on */
-	if (ipa_active_clients_trylock(&flags) == 0) {
-		IPADBG("defer interrupt processing\n");
-		queue_work(ipa_ctx->power_mgmt_wq, &ipa_interrupt_defer_work);
-		return IRQ_HANDLED;
-	}
-
-	if (ipa_ctx->ipa_active_clients.cnt == 0) {
-		IPADBG("defer interrupt processing\n");
-		queue_work(ipa_ctx->power_mgmt_wq, &ipa_interrupt_defer_work);
-		goto bail;
-	}
-
-	ipa_process_interrupts(true);
-
-bail:
-	ipa_active_clients_trylock_unlock(&flags);
-	return IRQ_HANDLED;
-}
-/**
-* ipa_add_interrupt_handler() - Adds handler to an interrupt type
-* @interrupt:		Interrupt type
-* @handler:		The handler to be added
-* @deferred_flag:	whether the handler processing should be deferred in
-*			a workqueue
-* @private_data:	the client's private data
-*
-* Adds handler to an interrupt type and enable the specific bit
-* in IRQ_EN register, associated interrupt in IRQ_STTS register will be enabled
-*/
-int ipa_add_interrupt_handler(enum ipa_irq_type interrupt,
-		ipa_irq_handler_t handler,
-		bool deferred_flag,
-		void *private_data)
-{
-	u32 val;
-	u32 bmsk;
-
-	IPADBG("in ipa_add_interrupt_handler\n");
-	if (interrupt < IPA_BAD_SNOC_ACCESS_IRQ || interrupt >= IPA_IRQ_MAX) {
-		IPAERR("invalid interrupt number %d\n", interrupt);
-		return -EINVAL;
-	}
-	ipa_interrupt_to_cb[interrupt].deferred_flag = deferred_flag;
-	ipa_interrupt_to_cb[interrupt].handler = handler;
-	ipa_interrupt_to_cb[interrupt].private_data = private_data;
-
-	val = ipa_read_reg(ipa_ctx->mmio, IPA_IRQ_EN_EE_n_ADDR(ipa_ee));
-	IPADBG("read IPA_IRQ_EN_EE_n_ADDR register. reg = %d\n", val);
-	bmsk = 1<<interrupt;
-	val |= bmsk;
-	ipa_write_reg(ipa_ctx->mmio, IPA_IRQ_EN_EE_n_ADDR(ipa_ee), val);
-	IPADBG("wrote IPA_IRQ_EN_EE_n_ADDR register. reg = %d\n", val);
-	return 0;
-}
-EXPORT_SYMBOL(ipa_add_interrupt_handler);
-
-/**
-* ipa_remove_interrupt_handler() - Removes handler to an interrupt type
-* @interrupt:		Interrupt type
-*
-* Removes the handler and disable the specific bit in IRQ_EN register
-*/
-int ipa_remove_interrupt_handler(enum ipa_irq_type interrupt)
-{
-	u32 val;
-	u32 bmsk;
-
-	if (interrupt < IPA_BAD_SNOC_ACCESS_IRQ || interrupt >= IPA_IRQ_MAX) {
-		IPAERR("invalid interrupt number %d\n", interrupt);
-		return -EINVAL;
-	}
-	kfree(ipa_interrupt_to_cb[interrupt].private_data);
-	ipa_interrupt_to_cb[interrupt].deferred_flag = false;
-	ipa_interrupt_to_cb[interrupt].handler = NULL;
-	ipa_interrupt_to_cb[interrupt].private_data = NULL;
-	val = ipa_read_reg(ipa_ctx->mmio, IPA_IRQ_EN_EE_n_ADDR(ipa_ee));
-	bmsk = 1<<interrupt;
-	val &= ~bmsk;
-	ipa_write_reg(ipa_ctx->mmio, IPA_IRQ_EN_EE_n_ADDR(ipa_ee), val);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_remove_interrupt_handler);
-
-/**
-* ipa_interrupts_init() - Initialize the IPA interrupts framework
-* @ipa_irq:	The interrupt number to allocate
-* @ee:		Execution environment
-* @ipa_dev:	The basic device structure representing the IPA driver
-*
-* - Initialize the ipa_interrupt_to_cb array
-* - Clear interrupts status
-* - Register the ipa interrupt handler - ipa_isr
-* - Enable apps processor wakeup by IPA interrupts
-*/
-int ipa_interrupts_init(u32 ipa_irq, u32 ee, struct device *ipa_dev)
-{
-	int idx;
-	u32 reg = 0xFFFFFFFF;
-	int res = 0;
-
-	ipa_ee = ee;
-	for (idx = 0; idx < IPA_IRQ_MAX; idx++) {
-		ipa_interrupt_to_cb[idx].deferred_flag = false;
-		ipa_interrupt_to_cb[idx].handler = NULL;
-		ipa_interrupt_to_cb[idx].private_data = NULL;
-	}
-
-	ipa_interrupt_wq = create_singlethread_workqueue(
-			INTERRUPT_WORKQUEUE_NAME);
-	if (!ipa_interrupt_wq) {
-		IPAERR("workqueue creation failed\n");
-		return -ENOMEM;
-	}
-
-	/*Clearing interrupts status*/
-	ipa_write_reg(ipa_ctx->mmio, IPA_IRQ_CLR_EE_n_ADDR(ipa_ee), reg);
-
-	res = request_irq(ipa_irq, (irq_handler_t) ipa_isr,
-				IRQF_TRIGGER_RISING, "ipa", ipa_dev);
-	if (res) {
-		IPAERR("fail to register IPA IRQ handler irq=%d\n", ipa_irq);
-		return -ENODEV;
-	}
-	IPADBG("IPA IRQ handler irq=%d registered\n", ipa_irq);
-
-	res = enable_irq_wake(ipa_irq);
-	if (res)
-		IPAERR("fail to enable IPA IRQ wakeup irq=%d res=%d\n",
-				ipa_irq, res);
-	else
-		IPADBG("IPA IRQ wakeup enabled irq=%d\n", ipa_irq);
-
-	return 0;
-}
diff --git a/drivers/platform/msm/ipa/ipa_intf.c b/drivers/platform/msm/ipa/ipa_intf.c
deleted file mode 100644
index 8eb9676..00000000
--- a/drivers/platform/msm/ipa/ipa_intf.c
+++ /dev/null
@@ -1,642 +0,0 @@
-/* Copyright (c) 2013-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/fs.h>
-#include <linux/sched.h>
-#include "ipa_i.h"
-
-struct ipa_intf {
-	char name[IPA_RESOURCE_NAME_MAX];
-	struct list_head link;
-	u32 num_tx_props;
-	u32 num_rx_props;
-	u32 num_ext_props;
-	struct ipa_ioc_tx_intf_prop *tx;
-	struct ipa_ioc_rx_intf_prop *rx;
-	struct ipa_ioc_ext_intf_prop *ext;
-	enum ipa_client_type excp_pipe;
-};
-
-struct ipa_push_msg {
-	struct ipa_msg_meta meta;
-	ipa_msg_free_fn callback;
-	void *buff;
-	struct list_head link;
-};
-
-struct ipa_pull_msg {
-	struct ipa_msg_meta meta;
-	ipa_msg_pull_fn callback;
-	struct list_head link;
-};
-
-/**
- * ipa_register_intf() - register "logical" interface
- * @name: [in] interface name
- * @tx:	[in] TX properties of the interface
- * @rx:	[in] RX properties of the interface
- *
- * Register an interface and its tx and rx properties, this allows
- * configuration of rules from user-space
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_register_intf(const char *name, const struct ipa_tx_intf *tx,
-		       const struct ipa_rx_intf *rx)
-{
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	return ipa_register_intf_ext(name, tx, rx, NULL);
-}
-EXPORT_SYMBOL(ipa_register_intf);
-
-/**
- * ipa_register_intf_ext() - register "logical" interface which has only
- * extended properties
- * @name: [in] interface name
- * @tx:	[in] TX properties of the interface
- * @rx:	[in] RX properties of the interface
- * @ext: [in] EXT properties of the interface
- *
- * Register an interface and its tx, rx and ext properties, this allows
- * configuration of rules from user-space
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_register_intf_ext(const char *name, const struct ipa_tx_intf *tx,
-		       const struct ipa_rx_intf *rx,
-		       const struct ipa_ext_intf *ext)
-{
-	struct ipa_intf *intf;
-	u32 len;
-
-	if (name == NULL || (tx == NULL && rx == NULL && ext == NULL)) {
-		IPAERR("invalid params name=%p tx=%p rx=%p ext=%p\n", name,
-				tx, rx, ext);
-		return -EINVAL;
-	}
-
-	if (tx && tx->num_props > IPA_NUM_PROPS_MAX) {
-		IPAERR("invalid tx num_props=%d max=%d\n", tx->num_props,
-				IPA_NUM_PROPS_MAX);
-		return -EINVAL;
-	}
-
-	if (rx && rx->num_props > IPA_NUM_PROPS_MAX) {
-		IPAERR("invalid rx num_props=%d max=%d\n", rx->num_props,
-				IPA_NUM_PROPS_MAX);
-		return -EINVAL;
-	}
-
-	if (ext && ext->num_props > IPA_NUM_PROPS_MAX) {
-		IPAERR("invalid ext num_props=%d max=%d\n", ext->num_props,
-				IPA_NUM_PROPS_MAX);
-		return -EINVAL;
-	}
-
-	len = sizeof(struct ipa_intf);
-	intf = kzalloc(len, GFP_KERNEL);
-	if (intf == NULL) {
-		IPAERR("fail to alloc 0x%x bytes\n", len);
-		return -ENOMEM;
-	}
-
-	strlcpy(intf->name, name, IPA_RESOURCE_NAME_MAX);
-
-	if (tx) {
-		intf->num_tx_props = tx->num_props;
-		len = tx->num_props * sizeof(struct ipa_ioc_tx_intf_prop);
-		intf->tx = kzalloc(len, GFP_KERNEL);
-		if (intf->tx == NULL) {
-			IPAERR("fail to alloc 0x%x bytes\n", len);
-			kfree(intf);
-			return -ENOMEM;
-		}
-		memcpy(intf->tx, tx->prop, len);
-	}
-
-	if (rx) {
-		intf->num_rx_props = rx->num_props;
-		len = rx->num_props * sizeof(struct ipa_ioc_rx_intf_prop);
-		intf->rx = kzalloc(len, GFP_KERNEL);
-		if (intf->rx == NULL) {
-			IPAERR("fail to alloc 0x%x bytes\n", len);
-			kfree(intf->tx);
-			kfree(intf);
-			return -ENOMEM;
-		}
-		memcpy(intf->rx, rx->prop, len);
-	}
-
-	if (ext) {
-		intf->num_ext_props = ext->num_props;
-		len = ext->num_props * sizeof(struct ipa_ioc_ext_intf_prop);
-		intf->ext = kzalloc(len, GFP_KERNEL);
-		if (intf->ext == NULL) {
-			IPAERR("fail to alloc 0x%x bytes\n", len);
-			kfree(intf->rx);
-			kfree(intf->tx);
-			kfree(intf);
-			return -ENOMEM;
-		}
-		memcpy(intf->ext, ext->prop, len);
-	}
-
-	if (ext && ext->excp_pipe_valid)
-		intf->excp_pipe = ext->excp_pipe;
-	else
-		intf->excp_pipe = IPA_CLIENT_APPS_LAN_CONS;
-
-	mutex_lock(&ipa_ctx->lock);
-	list_add_tail(&intf->link, &ipa_ctx->intf_list);
-	mutex_unlock(&ipa_ctx->lock);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_register_intf_ext);
-
-/**
- * ipa_deregister_intf() - de-register previously registered logical interface
- * @name: [in] interface name
- *
- * De-register a previously registered interface
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_deregister_intf(const char *name)
-{
-	struct ipa_intf *entry;
-	struct ipa_intf *next;
-	int result = -EINVAL;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (name == NULL) {
-		IPAERR("invalid param name=%p\n", name);
-		return result;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	list_for_each_entry_safe(entry, next, &ipa_ctx->intf_list, link) {
-		if (!strncmp(entry->name, name, IPA_RESOURCE_NAME_MAX)) {
-			list_del(&entry->link);
-			kfree(entry->ext);
-			kfree(entry->rx);
-			kfree(entry->tx);
-			kfree(entry);
-			result = 0;
-			break;
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-EXPORT_SYMBOL(ipa_deregister_intf);
-
-/**
- * ipa_query_intf() - query logical interface properties
- * @lookup:	[inout] interface name and number of properties
- *
- * Obtain the handle and number of tx and rx properties for the named
- * interface, used as part of querying the tx and rx properties for
- * configuration of various rules from user-space
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_query_intf(struct ipa_ioc_query_intf *lookup)
-{
-	struct ipa_intf *entry;
-	int result = -EINVAL;
-
-	if (lookup == NULL) {
-		IPAERR("invalid param lookup=%p\n", lookup);
-		return result;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	list_for_each_entry(entry, &ipa_ctx->intf_list, link) {
-		if (!strncmp(entry->name, lookup->name,
-					IPA_RESOURCE_NAME_MAX)) {
-			lookup->num_tx_props = entry->num_tx_props;
-			lookup->num_rx_props = entry->num_rx_props;
-			lookup->num_ext_props = entry->num_ext_props;
-			lookup->excp_pipe = entry->excp_pipe;
-			result = 0;
-			break;
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-
-/**
- * ipa_query_intf_tx_props() - qeury TX props of an interface
- * @tx:  [inout] interface tx attributes
- *
- * Obtain the tx properties for the specifed interface
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_query_intf_tx_props(struct ipa_ioc_query_intf_tx_props *tx)
-{
-	struct ipa_intf *entry;
-	int result = -EINVAL;
-
-	if (tx == NULL) {
-		IPAERR("invalid param tx=%p\n", tx);
-		return result;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	list_for_each_entry(entry, &ipa_ctx->intf_list, link) {
-		if (!strncmp(entry->name, tx->name, IPA_RESOURCE_NAME_MAX)) {
-			/* add the entry check */
-			if (entry->num_tx_props != tx->num_tx_props) {
-				IPAERR("invalid entry number(%u %u)\n",
-					entry->num_tx_props,
-						tx->num_tx_props);
-				mutex_unlock(&ipa_ctx->lock);
-				return result;
-			}
-			memcpy(tx->tx, entry->tx, entry->num_tx_props *
-			       sizeof(struct ipa_ioc_tx_intf_prop));
-			result = 0;
-			break;
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-
-/**
- * ipa_query_intf_rx_props() - qeury RX props of an interface
- * @rx:  [inout] interface rx attributes
- *
- * Obtain the rx properties for the specifed interface
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_query_intf_rx_props(struct ipa_ioc_query_intf_rx_props *rx)
-{
-	struct ipa_intf *entry;
-	int result = -EINVAL;
-
-	if (rx == NULL) {
-		IPAERR("invalid param rx=%p\n", rx);
-		return result;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	list_for_each_entry(entry, &ipa_ctx->intf_list, link) {
-		if (!strncmp(entry->name, rx->name, IPA_RESOURCE_NAME_MAX)) {
-			/* add the entry check */
-			if (entry->num_rx_props != rx->num_rx_props) {
-				IPAERR("invalid entry number(%u %u)\n",
-					entry->num_rx_props,
-						rx->num_rx_props);
-				mutex_unlock(&ipa_ctx->lock);
-				return result;
-			}
-			memcpy(rx->rx, entry->rx, entry->num_rx_props *
-					sizeof(struct ipa_ioc_rx_intf_prop));
-			result = 0;
-			break;
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-
-/**
- * ipa_query_intf_ext_props() - qeury EXT props of an interface
- * @ext:  [inout] interface ext attributes
- *
- * Obtain the ext properties for the specifed interface
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_query_intf_ext_props(struct ipa_ioc_query_intf_ext_props *ext)
-{
-	struct ipa_intf *entry;
-	int result = -EINVAL;
-
-	if (ext == NULL) {
-		IPAERR("invalid param ext=%p\n", ext);
-		return result;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	list_for_each_entry(entry, &ipa_ctx->intf_list, link) {
-		if (!strcmp(entry->name, ext->name)) {
-			/* add the entry check */
-			if (entry->num_ext_props != ext->num_ext_props) {
-				IPAERR("invalid entry number(%u %u)\n",
-					entry->num_ext_props,
-						ext->num_ext_props);
-				mutex_unlock(&ipa_ctx->lock);
-				return result;
-			}
-			memcpy(ext->ext, entry->ext, entry->num_ext_props *
-					sizeof(struct ipa_ioc_ext_intf_prop));
-			result = 0;
-			break;
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-	return result;
-}
-
-/**
- * ipa_send_msg() - Send "message" from kernel client to IPA driver
- * @meta: [in] message meta-data
- * @buff: [in] the payload for message
- * @callback: [in] free callback
- *
- * Client supplies the message meta-data and payload which IPA driver buffers
- * till read by user-space. After read from user space IPA driver invokes the
- * callback supplied to free the message payload. Client must not touch/free
- * the message payload after calling this API.
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_send_msg(struct ipa_msg_meta *meta, void *buff,
-		  ipa_msg_free_fn callback)
-{
-	struct ipa_push_msg *msg;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (meta == NULL || (buff == NULL && callback != NULL) ||
-	    (buff != NULL && callback == NULL)) {
-		IPAERR("invalid param meta=%p buff=%p, callback=%p\n",
-		       meta, buff, callback);
-		return -EINVAL;
-	}
-
-	if (meta->msg_type >= IPA_EVENT_MAX_NUM) {
-		IPAERR("unsupported message type %d\n", meta->msg_type);
-		return -EINVAL;
-	}
-
-	msg = kzalloc(sizeof(struct ipa_push_msg), GFP_KERNEL);
-	if (msg == NULL) {
-		IPAERR("fail to alloc ipa_msg container\n");
-		return -ENOMEM;
-	}
-
-	msg->meta = *meta;
-	msg->buff = buff;
-	msg->callback = callback;
-
-	mutex_lock(&ipa_ctx->msg_lock);
-	list_add_tail(&msg->link, &ipa_ctx->msg_list);
-	mutex_unlock(&ipa_ctx->msg_lock);
-	IPA_STATS_INC_CNT(ipa_ctx->stats.msg_w[meta->msg_type]);
-
-	wake_up(&ipa_ctx->msg_waitq);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_send_msg);
-
-/**
- * ipa_register_pull_msg() - register pull message type
- * @meta: [in] message meta-data
- * @callback: [in] pull callback
- *
- * Register message callback by kernel client with IPA driver for IPA driver to
- * pull message on-demand.
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_register_pull_msg(struct ipa_msg_meta *meta, ipa_msg_pull_fn callback)
-{
-	struct ipa_pull_msg *msg;
-
-	if (meta == NULL || callback == NULL) {
-		IPAERR("invalid param meta=%p callback=%p\n", meta, callback);
-		return -EINVAL;
-	}
-
-	msg = kzalloc(sizeof(struct ipa_pull_msg), GFP_KERNEL);
-	if (msg == NULL) {
-		IPAERR("fail to alloc ipa_msg container\n");
-		return -ENOMEM;
-	}
-
-	msg->meta = *meta;
-	msg->callback = callback;
-
-	mutex_lock(&ipa_ctx->msg_lock);
-	list_add_tail(&msg->link, &ipa_ctx->pull_msg_list);
-	mutex_unlock(&ipa_ctx->msg_lock);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_register_pull_msg);
-
-/**
- * ipa_deregister_pull_msg() - De-register pull message type
- * @meta: [in] message meta-data
- *
- * De-register "message" by kernel client from IPA driver
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_deregister_pull_msg(struct ipa_msg_meta *meta)
-{
-	struct ipa_pull_msg *entry;
-	struct ipa_pull_msg *next;
-	int result = -EINVAL;
-
-	if (meta == NULL) {
-		IPAERR("invalid param name=%p\n", meta);
-		return result;
-	}
-
-	mutex_lock(&ipa_ctx->msg_lock);
-	list_for_each_entry_safe(entry, next, &ipa_ctx->pull_msg_list, link) {
-		if (entry->meta.msg_len == meta->msg_len &&
-		    entry->meta.msg_type == meta->msg_type) {
-			list_del(&entry->link);
-			kfree(entry);
-			result = 0;
-			break;
-		}
-	}
-	mutex_unlock(&ipa_ctx->msg_lock);
-	return result;
-}
-EXPORT_SYMBOL(ipa_deregister_pull_msg);
-
-/**
- * ipa_read() - read message from IPA device
- * @filp:	[in] file pointer
- * @buf:	[out] buffer to read into
- * @count:	[in] size of above buffer
- * @f_pos:	[inout] file position
- *
- * Uer-space should continually read from /dev/ipa, read wll block when there
- * are no messages to read. Upon return, user-space should read the ipa_msg_meta
- * from the start of the buffer to know what type of message was read and its
- * length in the remainder of the buffer. Buffer supplied must be big enough to
- * hold the message meta-data and the largest defined message type
- *
- * Returns:	how many bytes copied to buffer
- *
- * Note:	Should not be called from atomic context
- */
-ssize_t ipa_read(struct file *filp, char __user *buf, size_t count,
-		  loff_t *f_pos)
-{
-	char __user *start;
-	struct ipa_push_msg *msg = NULL;
-	int ret;
-	DEFINE_WAIT(wait);
-	int locked;
-
-	start = buf;
-
-	while (1) {
-		prepare_to_wait(&ipa_ctx->msg_waitq, &wait, TASK_INTERRUPTIBLE);
-
-		mutex_lock(&ipa_ctx->msg_lock);
-		locked = 1;
-		if (!list_empty(&ipa_ctx->msg_list)) {
-			msg = list_first_entry(&ipa_ctx->msg_list,
-					struct ipa_push_msg, link);
-			list_del(&msg->link);
-		}
-
-		IPADBG("msg=%p\n", msg);
-
-		if (msg) {
-			locked = 0;
-			mutex_unlock(&ipa_ctx->msg_lock);
-			if (copy_to_user(buf, &msg->meta,
-					  sizeof(struct ipa_msg_meta))) {
-				kfree(msg);
-				msg = NULL;
-				ret = -EFAULT;
-				break;
-			}
-			buf += sizeof(struct ipa_msg_meta);
-			count -= sizeof(struct ipa_msg_meta);
-			if (msg->buff) {
-				if (copy_to_user(buf, msg->buff,
-						  msg->meta.msg_len)) {
-					kfree(msg);
-					msg = NULL;
-					ret = -EFAULT;
-					break;
-				}
-				buf += msg->meta.msg_len;
-				count -= msg->meta.msg_len;
-				msg->callback(msg->buff, msg->meta.msg_len,
-					       msg->meta.msg_type);
-			}
-			IPA_STATS_INC_CNT(
-				ipa_ctx->stats.msg_r[msg->meta.msg_type]);
-			kfree(msg);
-		}
-
-		ret = -EAGAIN;
-		if (filp->f_flags & O_NONBLOCK)
-			break;
-
-		ret = -EINTR;
-		if (signal_pending(current))
-			break;
-
-		if (start != buf)
-			break;
-
-		locked = 0;
-		mutex_unlock(&ipa_ctx->msg_lock);
-		schedule();
-	}
-
-	finish_wait(&ipa_ctx->msg_waitq, &wait);
-	if (start != buf && ret != -EFAULT)
-		ret = buf - start;
-
-	if (locked)
-		mutex_unlock(&ipa_ctx->msg_lock);
-
-	return ret;
-}
-
-/**
- * ipa_pull_msg() - pull the specified message from client
- * @meta: [in] message meta-data
- * @buf:  [out] buffer to read into
- * @count: [in] size of above buffer
- *
- * Populate the supplied buffer with the pull message which is fetched
- * from client, the message must have previously been registered with
- * the IPA driver
- *
- * Returns:	how many bytes copied to buffer
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_pull_msg(struct ipa_msg_meta *meta, char *buff, size_t count)
-{
-	struct ipa_pull_msg *entry;
-	int result = -EINVAL;
-
-	if (meta == NULL || buff == NULL || !count) {
-		IPAERR("invalid param name=%p buff=%p count=%zu\n",
-				meta, buff, count);
-		return result;
-	}
-
-	mutex_lock(&ipa_ctx->msg_lock);
-	list_for_each_entry(entry, &ipa_ctx->pull_msg_list, link) {
-		if (entry->meta.msg_len == meta->msg_len &&
-		    entry->meta.msg_type == meta->msg_type) {
-			result = entry->callback(buff, count, meta->msg_type);
-			break;
-		}
-	}
-	mutex_unlock(&ipa_ctx->msg_lock);
-	return result;
-}
diff --git a/drivers/platform/msm/ipa/ipa_mhi.c b/drivers/platform/msm/ipa/ipa_mhi.c
deleted file mode 100644
index 700af33..00000000
--- a/drivers/platform/msm/ipa/ipa_mhi.c
+++ /dev/null
@@ -1,1865 +0,0 @@
-/* Copyright (c) 2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/debugfs.h>
-#include <linux/export.h>
-#include <linux/delay.h>
-#include <linux/module.h>
-#include <linux/mutex.h>
-#include <linux/ipa.h>
-#include "ipa_i.h"
-#include "ipa_qmi_service.h"
-
-#define IPA_MHI_DRV_NAME
-#define IPA_MHI_DBG(fmt, args...) \
-	pr_debug(IPA_MHI_DRV_NAME " %s:%d " fmt, \
-		 __func__, __LINE__, ## args)
-#define IPA_MHI_ERR(fmt, args...) \
-	pr_err(IPA_MHI_DRV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-#define IPA_MHI_FUNC_ENTRY() \
-	IPA_MHI_DBG("ENTRY\n")
-#define IPA_MHI_FUNC_EXIT() \
-	IPA_MHI_DBG("EXIT\n")
-
-#define IPA_MHI_RM_TIMEOUT_MSEC 10000
-
-#define IPA_MHI_BAM_EMPTY_TIMEOUT_MSEC 5
-
-#define IPA_MHI_MAX_UL_CHANNELS 1
-#define IPA_MHI_MAX_DL_CHANNELS 1
-
-#define IPA_MHI_SUSPEND_SLEEP_MIN 900
-#define IPA_MHI_SUSPEND_SLEEP_MAX 1100
-
-enum ipa_mhi_state {
-	IPA_MHI_STATE_INITIALIZED,
-	IPA_MHI_STATE_READY,
-	IPA_MHI_STATE_STARTED,
-	IPA_MHI_STATE_SUSPEND_IN_PROGRESS,
-	IPA_MHI_STATE_SUSPENDED,
-	IPA_MHI_STATE_RESUME_IN_PROGRESS,
-	IPA_MHI_STATE_MAX
-};
-
-static char *ipa_mhi_state_str[] = {
-	__stringify(IPA_MHI_STATE_INITIALIZED),
-	__stringify(IPA_MHI_STATE_READY),
-	__stringify(IPA_MHI_STATE_STARTED),
-	__stringify(IPA_MHI_STATE_SUSPEND_IN_PROGRESS),
-	__stringify(IPA_MHI_STATE_SUSPENDED),
-	__stringify(IPA_MHI_STATE_RESUME_IN_PROGRESS),
-};
-
-#define MHI_STATE_STR(state) \
-	(((state) >= 0 && (state) < IPA_MHI_STATE_MAX) ? \
-		ipa_mhi_state_str[(state)] : \
-		"INVALID")
-
-/**
- * struct ipa_mhi_channel_ctx - MHI Channel context
- * @valid: entry is valid
- * @id: MHI channel ID
- * @hdl: channel handle for uC
- * @client: IPA Client
- * @state: Channel state
- */
-struct ipa_mhi_channel_ctx {
-	bool valid;
-	u8 id;
-	u8 hdl;
-	enum ipa_client_type client;
-	enum ipa_hw_mhi_channel_states state;
-};
-
-enum ipa_mhi_rm_state {
-	IPA_MHI_RM_STATE_RELEASED,
-	IPA_MHI_RM_STATE_REQUESTED,
-	IPA_MHI_RM_STATE_GRANTED,
-	IPA_MHI_RM_STATE_MAX
-};
-
-/**
- * struct ipa_mhi_ctx - IPA MHI context
- * @state: IPA MHI state
- * @state_lock: lock for state read/write operations
- * @msi: Message Signaled Interrupts parameters
- * @mmio_addr: MHI MMIO physical address
- * @first_ch_idx: First channel ID for hardware accelerated channels.
- * @first_er_idx: First event ring ID for hardware accelerated channels.
- * @host_ctrl_addr: Base address of MHI control data structures
- * @host_data_addr: Base address of MHI data buffers
- * @cb_notify: client callback
- * @cb_priv: client private data to be provided in client callback
- * @ul_channels: IPA MHI uplink channel contexts
- * @dl_channels: IPA MHI downlink channel contexts
- * @total_channels: Total number of channels ever connected to IPA MHI
- * @rm_prod_granted_comp: Completion object for MHI producer resource in IPA RM
- * @rm_cons_state: MHI consumer resource state in IPA RM
- * @rm_cons_comp: Completion object for MHI consumer resource in IPA RM
- * @trigger_wakeup: trigger wakeup callback ?
- * @wakeup_notified: MHI Client wakeup function was called
- * @wq: workqueue for wakeup event
- * @qmi_req_id: QMI request unique id
- */
-struct ipa_mhi_ctx {
-	enum ipa_mhi_state state;
-	spinlock_t state_lock;
-	struct ipa_mhi_msi_info msi;
-	u32 mmio_addr;
-	u32 first_ch_idx;
-	u32 first_er_idx;
-	u32 host_ctrl_addr;
-	u32 host_data_addr;
-	mhi_client_cb cb_notify;
-	void *cb_priv;
-	struct ipa_mhi_channel_ctx ul_channels[IPA_MHI_MAX_UL_CHANNELS];
-	struct ipa_mhi_channel_ctx dl_channels[IPA_MHI_MAX_DL_CHANNELS];
-	u32 total_channels;
-	struct completion rm_prod_granted_comp;
-	enum ipa_mhi_rm_state rm_cons_state;
-	struct completion rm_cons_comp;
-	bool trigger_wakeup;
-	bool wakeup_notified;
-	struct workqueue_struct *wq;
-	u32 qmi_req_id;
-};
-
-static struct ipa_mhi_ctx *ipa_mhi_ctx;
-
-static void ipa_mhi_wq_notify_wakeup(struct work_struct *work);
-static DECLARE_WORK(ipa_mhi_notify_wakeup_work, ipa_mhi_wq_notify_wakeup);
-
-static void ipa_mhi_wq_notify_ready(struct work_struct *work);
-static DECLARE_WORK(ipa_mhi_notify_ready_work, ipa_mhi_wq_notify_ready);
-
-static union IpaHwMhiDlUlSyncCmdData_t cached_dl_ul_sync_info;
-
-#ifdef CONFIG_DEBUG_FS
-#define IPA_MHI_MAX_MSG_LEN 512
-static char dbg_buff[IPA_MHI_MAX_MSG_LEN];
-static struct dentry *dent;
-
-static char *ipa_mhi_channel_state_str[] = {
-	__stringify(IPA_HW_MHI_CHANNEL_STATE_DISABLE),
-	__stringify(IPA_HW_MHI_CHANNEL_STATE_ENABLE),
-	__stringify(IPA_HW_MHI_CHANNEL_STATE_RUN),
-	__stringify(IPA_HW_MHI_CHANNEL_STATE_SUSPEND),
-	__stringify(IPA_HW_MHI_CHANNEL_STATE_STOP),
-	__stringify(IPA_HW_MHI_CHANNEL_STATE_ERROR),
-};
-
-#define MHI_CH_STATE_STR(state) \
-	(((state) >= 0 && (state) <= IPA_HW_MHI_CHANNEL_STATE_ERROR) ? \
-	ipa_mhi_channel_state_str[(state)] : \
-	"INVALID")
-
-static ssize_t ipa_mhi_debugfs_stats(struct file *file,
-	char __user *ubuf,
-	size_t count,
-	loff_t *ppos)
-{
-	int nbytes = 0;
-	int i;
-	struct ipa_mhi_channel_ctx *channel;
-
-	nbytes += scnprintf(&dbg_buff[nbytes],
-		IPA_MHI_MAX_MSG_LEN - nbytes,
-		"IPA MHI state: %s\n", MHI_STATE_STR(ipa_mhi_ctx->state));
-
-	for (i = 0; i < IPA_MHI_MAX_UL_CHANNELS; i++) {
-		channel = &ipa_mhi_ctx->ul_channels[i];
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPA_MHI_MAX_MSG_LEN - nbytes,
-			"channel %d: ", i);
-		if (channel->valid) {
-			nbytes += scnprintf(&dbg_buff[nbytes],
-				IPA_MHI_MAX_MSG_LEN - nbytes,
-				"ch_id=%d client=%d state=%s",
-				channel->id, channel->client,
-				MHI_CH_STATE_STR(channel->state));
-		} else {
-			nbytes += scnprintf(&dbg_buff[nbytes],
-				IPA_MHI_MAX_MSG_LEN - nbytes,
-				"never connected");
-		}
-
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPA_MHI_MAX_MSG_LEN - nbytes, "\n");
-	}
-
-	for (i = 0; i < IPA_MHI_MAX_DL_CHANNELS; i++) {
-		channel = &ipa_mhi_ctx->dl_channels[i];
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPA_MHI_MAX_MSG_LEN - nbytes,
-			"channel %d: ", i);
-		if (channel->valid) {
-			nbytes += scnprintf(&dbg_buff[nbytes],
-				IPA_MHI_MAX_MSG_LEN - nbytes,
-				"ch_id=%d client=%d state=%s",
-				channel->id, channel->client,
-				MHI_CH_STATE_STR(channel->state));
-		} else {
-			nbytes += scnprintf(&dbg_buff[nbytes],
-				IPA_MHI_MAX_MSG_LEN - nbytes,
-				"never connected");
-		}
-
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			IPA_MHI_MAX_MSG_LEN - nbytes, "\n");
-	}
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, nbytes);
-}
-
-static ssize_t ipa_mhi_debugfs_uc_stats(struct file *file,
-	char __user *ubuf,
-	size_t count,
-	loff_t *ppos)
-{
-	int nbytes = 0;
-	nbytes += ipa_uc_mhi_print_stats(dbg_buff, IPA_MHI_MAX_MSG_LEN);
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, nbytes);
-}
-
-const struct file_operations ipa_mhi_stats_ops = {
-	.read = ipa_mhi_debugfs_stats,
-};
-
-const struct file_operations ipa_mhi_uc_stats_ops = {
-	.read = ipa_mhi_debugfs_uc_stats,
-};
-
-static void ipa_mhi_debugfs_init(void)
-{
-	const mode_t read_only_mode = S_IRUSR | S_IRGRP | S_IROTH;
-	struct dentry *file;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	dent = debugfs_create_dir("ipa_mhi", 0);
-	if (IS_ERR(dent)) {
-		IPA_MHI_ERR("fail to create folder ipa_mhi\n");
-		return;
-	}
-
-	file = debugfs_create_file("stats", read_only_mode, dent,
-		0, &ipa_mhi_stats_ops);
-	if (!file || IS_ERR(file)) {
-		IPA_MHI_ERR("fail to create file stats\n");
-		goto fail;
-	}
-
-	file = debugfs_create_file("uc_stats", read_only_mode, dent,
-		0, &ipa_mhi_uc_stats_ops);
-	if (!file || IS_ERR(file)) {
-		IPA_MHI_ERR("fail to create file stats\n");
-		goto fail;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return;
-fail:
-	debugfs_remove_recursive(dent);
-}
-
-static void ipa_mhi_debugfs_destroy(void)
-{
-	debugfs_remove_recursive(dent);
-}
-
-#else
-static void ipa_mhi_debugfs_init(void) {}
-static void ipa_mhi_debugfs_destroy(void) {}
-#endif /* CONFIG_DEBUG_FS */
-
-
-static void ipa_mhi_cache_dl_ul_sync_info(
-	struct ipa_config_req_msg_v01 *config_req)
-{
-	cached_dl_ul_sync_info.params.isDlUlSyncEnabled = true;
-	cached_dl_ul_sync_info.params.UlAccmVal =
-		(config_req->ul_accumulation_time_limit_valid) ?
-		config_req->ul_accumulation_time_limit : 0;
-	cached_dl_ul_sync_info.params.ulMsiEventThreshold =
-		(config_req->ul_msi_event_threshold_valid) ?
-		config_req->ul_msi_event_threshold : 0;
-	cached_dl_ul_sync_info.params.dlMsiEventThreshold =
-		(config_req->dl_msi_event_threshold_valid) ?
-		config_req->dl_msi_event_threshold : 0;
-}
-
-/**
- * ipa_mhi_wq_notify_wakeup() - Notify MHI client on data available
- *
- * This function is called from IPA MHI workqueue to notify
- * MHI client driver on data available event.
- */
-static void ipa_mhi_wq_notify_wakeup(struct work_struct *work)
-{
-	IPA_MHI_FUNC_ENTRY();
-	ipa_mhi_ctx->cb_notify(ipa_mhi_ctx->cb_priv,
-		IPA_MHI_EVENT_DATA_AVAILABLE, 0);
-	IPA_MHI_FUNC_EXIT();
-}
-
-/**
- * ipa_mhi_notify_wakeup() - Schedule work to notify data available
- *
- * This function will schedule a work to notify data available event.
- * In case this function is called more than once, only one notification will
- * be sent to MHI client driver. No further notifications will be sent until
- * IPA MHI state will become STARTED.
- */
-static void ipa_mhi_notify_wakeup(void)
-{
-	IPA_MHI_FUNC_ENTRY();
-	if (ipa_mhi_ctx->wakeup_notified) {
-		IPADBG("wakeup already called\n");
-		return;
-	}
-	queue_work(ipa_mhi_ctx->wq, &ipa_mhi_notify_wakeup_work);
-	ipa_mhi_ctx->wakeup_notified = true;
-	IPA_MHI_FUNC_EXIT();
-}
-
-/**
- * ipa_mhi_wq_notify_ready() - Notify MHI client on ready
- *
- * This function is called from IPA MHI workqueue to notify
- * MHI client driver on ready event when IPA uC is loaded
- */
-static void ipa_mhi_wq_notify_ready(struct work_struct *work)
-{
-	IPA_MHI_FUNC_ENTRY();
-	ipa_mhi_ctx->cb_notify(ipa_mhi_ctx->cb_priv,
-		IPA_MHI_EVENT_READY, 0);
-	IPA_MHI_FUNC_EXIT();
-}
-
-/**
- * ipa_mhi_notify_ready() - Schedule work to notify ready
- *
- * This function will schedule a work to notify ready event.
- */
-static void ipa_mhi_notify_ready(void)
-{
-	IPA_MHI_FUNC_ENTRY();
-	queue_work(ipa_mhi_ctx->wq, &ipa_mhi_notify_ready_work);
-	IPA_MHI_FUNC_EXIT();
-}
-
-/**
- * ipa_mhi_set_state() - Set new state to IPA MHI
- * @state: new state
- *
- * Sets a new state to IPA MHI if possible according to IPA MHI state machine.
- * In some state transitions a wakeup request will be triggered.
- *
- * Returns: 0 on success, -1 otherwise
- */
-static int ipa_mhi_set_state(enum ipa_mhi_state new_state)
-{
-	unsigned long flags;
-	int res = -EPERM;
-
-	spin_lock_irqsave(&ipa_mhi_ctx->state_lock, flags);
-	IPA_MHI_DBG("Current state: %s\n", MHI_STATE_STR(ipa_mhi_ctx->state));
-
-	switch (ipa_mhi_ctx->state) {
-	case IPA_MHI_STATE_INITIALIZED:
-		if (new_state == IPA_MHI_STATE_READY) {
-			ipa_mhi_notify_ready();
-			res = 0;
-		}
-		break;
-
-	case IPA_MHI_STATE_READY:
-		if (new_state == IPA_MHI_STATE_READY)
-			res = 0;
-		if (new_state == IPA_MHI_STATE_STARTED)
-			res = 0;
-		break;
-
-	case IPA_MHI_STATE_STARTED:
-		if (new_state == IPA_MHI_STATE_INITIALIZED)
-			res = 0;
-		else if (new_state == IPA_MHI_STATE_SUSPEND_IN_PROGRESS)
-			res = 0;
-		break;
-
-	case IPA_MHI_STATE_SUSPEND_IN_PROGRESS:
-		if (new_state == IPA_MHI_STATE_SUSPENDED) {
-			if (ipa_mhi_ctx->trigger_wakeup) {
-				ipa_mhi_ctx->trigger_wakeup = false;
-				ipa_mhi_notify_wakeup();
-			}
-			res = 0;
-		} else if (new_state == IPA_MHI_STATE_STARTED) {
-			ipa_mhi_ctx->wakeup_notified = false;
-			if (ipa_mhi_ctx->rm_cons_state ==
-				IPA_MHI_RM_STATE_REQUESTED) {
-				ipa_rm_notify_completion(
-					IPA_RM_RESOURCE_GRANTED,
-					IPA_RM_RESOURCE_MHI_CONS);
-				ipa_mhi_ctx->rm_cons_state =
-					IPA_MHI_RM_STATE_GRANTED;
-			}
-			res = 0;
-		}
-		break;
-
-	case IPA_MHI_STATE_SUSPENDED:
-		if (new_state == IPA_MHI_STATE_RESUME_IN_PROGRESS)
-			res = 0;
-		break;
-
-	case IPA_MHI_STATE_RESUME_IN_PROGRESS:
-		if (new_state == IPA_MHI_STATE_SUSPENDED) {
-			if (ipa_mhi_ctx->trigger_wakeup) {
-				ipa_mhi_ctx->trigger_wakeup = false;
-				ipa_mhi_notify_wakeup();
-			}
-			res = 0;
-		} else if (new_state == IPA_MHI_STATE_STARTED) {
-			ipa_mhi_ctx->wakeup_notified = false;
-			if (ipa_mhi_ctx->rm_cons_state ==
-				IPA_MHI_RM_STATE_REQUESTED) {
-				ipa_rm_notify_completion(
-					IPA_RM_RESOURCE_GRANTED,
-					IPA_RM_RESOURCE_MHI_CONS);
-				ipa_mhi_ctx->rm_cons_state =
-					IPA_MHI_RM_STATE_GRANTED;
-			}
-			res = 0;
-		}
-		break;
-
-	default:
-		IPA_MHI_ERR("invalied state %d\n", ipa_mhi_ctx->state);
-		WARN_ON(1);
-	}
-
-	if (res)
-		IPA_MHI_ERR("Invalid state change to %s\n",
-						MHI_STATE_STR(new_state));
-	else {
-		IPA_MHI_DBG("New state change to %s\n",
-						MHI_STATE_STR(new_state));
-		ipa_mhi_ctx->state = new_state;
-	}
-	spin_unlock_irqrestore(&ipa_mhi_ctx->state_lock, flags);
-	return res;
-}
-
-static void ipa_mhi_rm_prod_notify(void *user_data, enum ipa_rm_event event,
-	unsigned long data)
-{
-	IPA_MHI_FUNC_ENTRY();
-
-	switch (event) {
-	case IPA_RM_RESOURCE_GRANTED:
-		IPA_MHI_DBG("IPA_RM_RESOURCE_GRANTED\n");
-		complete_all(&ipa_mhi_ctx->rm_prod_granted_comp);
-		break;
-
-	case IPA_RM_RESOURCE_RELEASED:
-		IPA_MHI_DBG("IPA_RM_RESOURCE_RELEASED\n");
-		break;
-
-	default:
-		IPA_MHI_ERR("unexpected event %d\n", event);
-		WARN_ON(1);
-		break;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-}
-
-static void ipa_mhi_uc_ready_cb(void)
-{
-	IPA_MHI_FUNC_ENTRY();
-	ipa_mhi_set_state(IPA_MHI_STATE_READY);
-	IPA_MHI_FUNC_EXIT();
-}
-
-static void ipa_mhi_uc_wakeup_request_cb(void)
-{
-	unsigned long flags;
-
-	IPA_MHI_FUNC_ENTRY();
-	IPA_MHI_DBG("MHI state: %s\n", MHI_STATE_STR(ipa_mhi_ctx->state));
-	spin_lock_irqsave(&ipa_mhi_ctx->state_lock, flags);
-	if (ipa_mhi_ctx->state == IPA_MHI_STATE_SUSPENDED) {
-		ipa_mhi_notify_wakeup();
-	} else if (ipa_mhi_ctx->state == IPA_MHI_STATE_SUSPEND_IN_PROGRESS) {
-		/* wakeup event will be triggered after suspend finishes */
-		ipa_mhi_ctx->trigger_wakeup = true;
-	}
-	spin_unlock_irqrestore(&ipa_mhi_ctx->state_lock, flags);
-	IPA_MHI_FUNC_EXIT();
-}
-
-/**
- * ipa_mhi_rm_cons_request() - callback function for IPA RM request resource
- *
- * In case IPA MHI is not suspended, MHI CONS will be granted immediately.
- * In case IPA MHI is suspended, MHI CONS will be granted after resume.
- */
-static int ipa_mhi_rm_cons_request(void)
-{
-	unsigned long flags;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	IPA_MHI_DBG("%s\n", MHI_STATE_STR(ipa_mhi_ctx->state));
-	spin_lock_irqsave(&ipa_mhi_ctx->state_lock, flags);
-	ipa_mhi_ctx->rm_cons_state = IPA_MHI_RM_STATE_REQUESTED;
-	if (ipa_mhi_ctx->state == IPA_MHI_STATE_STARTED) {
-		ipa_mhi_ctx->rm_cons_state = IPA_MHI_RM_STATE_GRANTED;
-		res = 0;
-	} else if (ipa_mhi_ctx->state == IPA_MHI_STATE_SUSPENDED) {
-		ipa_mhi_notify_wakeup();
-		res = -EINPROGRESS;
-	} else if (ipa_mhi_ctx->state == IPA_MHI_STATE_SUSPEND_IN_PROGRESS) {
-		/* wakeup event will be trigger after suspend finishes */
-		ipa_mhi_ctx->trigger_wakeup = true;
-		res = -EINPROGRESS;
-	} else {
-		res = -EINPROGRESS;
-	}
-
-	spin_unlock_irqrestore(&ipa_mhi_ctx->state_lock, flags);
-	IPA_MHI_DBG("EXIT with %d\n", res);
-	return res;
-}
-
-static int ipa_mhi_rm_cons_release(void)
-{
-	unsigned long flags;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	spin_lock_irqsave(&ipa_mhi_ctx->state_lock, flags);
-	ipa_mhi_ctx->rm_cons_state = IPA_MHI_RM_STATE_RELEASED;
-	complete_all(&ipa_mhi_ctx->rm_cons_comp);
-	spin_unlock_irqrestore(&ipa_mhi_ctx->state_lock, flags);
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-static int ipa_mhi_wait_for_cons_release(void)
-{
-	unsigned long flags;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	INIT_COMPLETION(ipa_mhi_ctx->rm_cons_comp);
-	spin_lock_irqsave(&ipa_mhi_ctx->state_lock, flags);
-	if (ipa_mhi_ctx->rm_cons_state != IPA_MHI_RM_STATE_GRANTED) {
-		spin_unlock_irqrestore(&ipa_mhi_ctx->state_lock, flags);
-		return 0;
-	}
-	spin_unlock_irqrestore(&ipa_mhi_ctx->state_lock, flags);
-
-	res = wait_for_completion_timeout(
-		&ipa_mhi_ctx->rm_cons_comp,
-		msecs_to_jiffies(IPA_MHI_RM_TIMEOUT_MSEC));
-	if (res == 0) {
-		IPA_MHI_ERR("timeout release mhi cons\n");
-		return -ETIME;
-	}
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-static int ipa_mhi_request_prod(void)
-{
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	INIT_COMPLETION(ipa_mhi_ctx->rm_prod_granted_comp);
-	IPA_MHI_DBG("requesting mhi prod\n");
-	res = ipa_rm_request_resource(IPA_RM_RESOURCE_MHI_PROD);
-	if (res) {
-		if (res != -EINPROGRESS) {
-			IPA_MHI_ERR("failed to request mhi prod %d\n", res);
-			return res;
-		}
-		res = wait_for_completion_timeout(
-			&ipa_mhi_ctx->rm_prod_granted_comp,
-			msecs_to_jiffies(IPA_MHI_RM_TIMEOUT_MSEC));
-		if (res == 0) {
-			IPA_MHI_ERR("timeout request mhi prod\n");
-			return -ETIME;
-		}
-	}
-
-	IPA_MHI_DBG("mhi prod granted\n");
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-
-}
-
-static int ipa_mhi_release_prod(void)
-{
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	res = ipa_rm_release_resource(IPA_RM_RESOURCE_MHI_PROD);
-
-	IPA_MHI_FUNC_EXIT();
-	return res;
-
-}
-
-/**
- * ipa_mhi_get_channel_context() - Get corresponding channel context
- * @client: IPA client
- * @channel_id: Channel ID
- *
- * This function will return the corresponding channel context or allocate new
- * one in case channel context for channel does not exist.
- */
-static struct ipa_mhi_channel_ctx *ipa_mhi_get_channel_context(
-	enum ipa_client_type client, u8 channel_id)
-{
-	int ch_idx;
-	struct ipa_mhi_channel_ctx *channels;
-	int max_channels;
-
-	if (IPA_CLIENT_IS_PROD(client)) {
-		channels = ipa_mhi_ctx->ul_channels;
-		max_channels = IPA_MHI_MAX_UL_CHANNELS;
-	} else {
-		channels = ipa_mhi_ctx->dl_channels;
-		max_channels = IPA_MHI_MAX_DL_CHANNELS;
-	}
-
-	/* find the channel context according to channel id */
-	for (ch_idx = 0; ch_idx < max_channels; ch_idx++) {
-		if (channels[ch_idx].valid &&
-		    channels[ch_idx].id == channel_id)
-			return &channels[ch_idx];
-	}
-
-	/* channel context does not exists, allocate a new one */
-	for (ch_idx = 0; ch_idx < max_channels; ch_idx++) {
-		if (!channels[ch_idx].valid)
-			break;
-	}
-
-	if (ch_idx == max_channels) {
-		IPA_MHI_ERR("no more channels available\n");
-		return NULL;
-	}
-
-	channels[ch_idx].valid = true;
-	channels[ch_idx].id = channel_id;
-	channels[ch_idx].hdl = ipa_mhi_ctx->total_channels++;
-	channels[ch_idx].client = client;
-	channels[ch_idx].state = IPA_HW_MHI_CHANNEL_STATE_INVALID;
-
-	return &channels[ch_idx];
-}
-
-/**
- * ipa_mhi_get_channel_context_by_clnt_hdl() - Get corresponding channel context
- * @clnt_hdl: client handle as provided in ipa_mhi_connect_pipe()
- *
- * This function will return the corresponding channel context or NULL in case
- * that channel does not exist.
- */
-static struct ipa_mhi_channel_ctx *ipa_mhi_get_channel_context_by_clnt_hdl(
-	u32 clnt_hdl)
-{
-	int ch_idx;
-
-	for (ch_idx = 0; ch_idx < IPA_MHI_MAX_UL_CHANNELS; ch_idx++) {
-		if (ipa_mhi_ctx->ul_channels[ch_idx].valid &&
-		    ipa_get_ep_mapping(
-		    ipa_mhi_ctx->ul_channels[ch_idx].client) == clnt_hdl)
-			return &ipa_mhi_ctx->ul_channels[ch_idx];
-	}
-
-	for (ch_idx = 0; ch_idx < IPA_MHI_MAX_DL_CHANNELS; ch_idx++) {
-		if (ipa_mhi_ctx->dl_channels[ch_idx].valid &&
-		    ipa_get_ep_mapping(
-		    ipa_mhi_ctx->dl_channels[ch_idx].client) == clnt_hdl)
-			return &ipa_mhi_ctx->dl_channels[ch_idx];
-	}
-
-	return NULL;
-}
-
-static int ipa_mhi_enable_force_clear(u32 request_id, bool throttle_source)
-{
-	struct ipa_enable_force_clear_datapath_req_msg_v01 req;
-	int i;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	memset(&req, 0, sizeof(req));
-	req.request_id = request_id;
-	req.source_pipe_bitmask = 0;
-	for (i = 0; i < IPA_MHI_MAX_UL_CHANNELS; i++) {
-		if (!ipa_mhi_ctx->ul_channels[i].valid)
-			continue;
-		req.source_pipe_bitmask |= 1 << ipa_get_ep_mapping(
-					ipa_mhi_ctx->ul_channels[i].client);
-	}
-	if (throttle_source) {
-		req.throttle_source_valid = 1;
-		req.throttle_source = 1;
-	}
-	IPA_MHI_DBG("req_id=0x%x src_pipe_btmk=0x%x throt_src=%d\n",
-		req.request_id, req.source_pipe_bitmask,
-		req.throttle_source);
-	res = qmi_enable_force_clear_datapath_send(&req);
-	if (res) {
-		IPA_MHI_ERR("qmi_enable_force_clear_datapath_send failed %d\n",
-			res);
-		return res;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-static int ipa_mhi_disable_force_clear(u32 request_id)
-{
-	struct ipa_disable_force_clear_datapath_req_msg_v01 req;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	memset(&req, 0, sizeof(req));
-	req.request_id = request_id;
-	IPA_MHI_DBG("req_id=0x%x\n", req.request_id);
-	res = qmi_disable_force_clear_datapath_send(&req);
-	if (res) {
-		IPA_MHI_ERR("qmi_disable_force_clear_datapath_send failed %d\n",
-			res);
-		return res;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-/**
- * ipa_mhi_wait_for_bam_empty_timeout() - wait for pending packets in uplink
- * @msecs: timeout to wait
- *
- * This function will poll until there are no packets pending in uplink channels
- * or timeout occured.
- *
- * Return code: true - no pending packets in uplink channels
- *		false - timeout occurred
- */
-static bool ipa_mhi_wait_for_bam_empty_timeout(unsigned int msecs)
-{
-	unsigned long jiffies_timeout = msecs_to_jiffies(msecs);
-	unsigned long jiffies_start = jiffies;
-	bool empty = false;
-	bool pending;
-	int i;
-	u32 pipe_idx;
-
-	IPA_MHI_FUNC_ENTRY();
-	while (!empty) {
-		empty = true;
-		for (i = 0; i < IPA_MHI_MAX_UL_CHANNELS; i++) {
-			if (!ipa_mhi_ctx->ul_channels[i].valid)
-				continue;
-			pipe_idx = ipa_get_ep_mapping(
-				ipa_mhi_ctx->ul_channels[i].client);
-			if (sps_pipe_pending_desc(ipa_ctx->bam_handle,
-						pipe_idx, &pending)) {
-				IPA_MHI_ERR("sps_pipe_pending_desc failed\n");
-				WARN_ON(1);
-				return false;
-			}
-			empty &= !pending;
-		}
-
-		if (time_after(jiffies, jiffies_start + jiffies_timeout)) {
-			IPA_MHI_DBG("timeout waiting for BAM empty\n");
-			break;
-		}
-	}
-	IPA_MHI_DBG("Bam is %s\n", (empty) ? "empty" : "not empty");
-	IPA_MHI_FUNC_EXIT();
-	return empty;
-}
-
-static int ipa_mhi_reset_ul_channel(struct ipa_mhi_channel_ctx *channel)
-{
-	int res;
-	int i;
-	int ep_idx;
-	struct ipa_ep_cfg_holb ep_holb;
-	struct ipa_ep_cfg_holb old_ep_holb[IPA_MHI_MAX_DL_CHANNELS];
-	bool empty;
-
-	IPA_MHI_FUNC_ENTRY();
-	res = ipa_uc_mhi_reset_channel(channel->hdl);
-	if (res) {
-		IPA_MHI_ERR("ipa_uc_mhi_reset_channel failed %d\n", res);
-		return res;
-	}
-	empty = ipa_mhi_wait_for_bam_empty_timeout(
-		IPA_MHI_BAM_EMPTY_TIMEOUT_MSEC);
-	if (!empty) {
-		IPA_MHI_DBG("BAM not empty\n");
-		res = ipa_mhi_enable_force_clear(ipa_mhi_ctx->qmi_req_id,
-			true);
-		if (res) {
-			IPA_MHI_ERR("ipa_mhi_enable_force_clear failed %d\n",
-				res);
-			BUG();
-			return res;
-		}
-
-		/* enable packet drop on all DL channels */
-		for (i = 0; i < IPA_MHI_MAX_DL_CHANNELS; i++) {
-			if (!ipa_mhi_ctx->dl_channels[i].valid)
-				continue;
-			if (ipa_mhi_ctx->dl_channels[i].state ==
-			    IPA_HW_MHI_CHANNEL_STATE_INVALID)
-				continue;
-			ep_idx = ipa_get_ep_mapping(
-				ipa_mhi_ctx->dl_channels[i].client);
-			if (-1 == ep_idx) {
-				IPA_MHI_ERR("Client %u is not mapped\n",
-					ipa_mhi_ctx->dl_channels[i].client);
-				BUG();
-				return -EFAULT;
-			}
-			memset(&ep_holb, 0, sizeof(ep_holb));
-			ep_holb.en = 1;
-			ep_holb.tmr_val = 0;
-			old_ep_holb[i] = ipa_ctx->ep[ep_idx].holb;
-			res = ipa_cfg_ep_holb(ep_idx, &ep_holb);
-			if (res) {
-				IPA_MHI_ERR("ipa_cfg_ep_holb failed %d\n", res);
-				BUG();
-				return res;
-			}
-		}
-
-		res = ipa_tag_process(NULL, 0, HZ);
-		if (res)
-			IPAERR("TAG process failed\n");
-
-		/* disable packet drop on all DL channels */
-		for (i = 0; i < IPA_MHI_MAX_DL_CHANNELS; i++) {
-			if (!ipa_mhi_ctx->dl_channels[i].valid)
-				continue;
-			if (ipa_mhi_ctx->dl_channels[i].state ==
-				IPA_HW_MHI_CHANNEL_STATE_INVALID)
-				continue;
-			ep_idx = ipa_get_ep_mapping(
-				ipa_mhi_ctx->dl_channels[i].client);
-			res = ipa_cfg_ep_holb(ep_idx, &old_ep_holb[i]);
-			if (res) {
-				IPA_MHI_ERR("ipa_cfg_ep_holb failed %d\n", res);
-				BUG();
-				return res;
-			}
-		}
-
-		res = sps_pipe_disable(ipa_ctx->bam_handle,
-			ipa_get_ep_mapping(channel->client));
-		if (res) {
-			IPA_MHI_ERR("sps_pipe_disable failed %d\n", res);
-			BUG();
-			return res;
-		}
-
-		res = ipa_mhi_disable_force_clear(ipa_mhi_ctx->qmi_req_id);
-		if (res) {
-			IPA_MHI_ERR("ipa_mhi_disable_force_clear failed %d\n",
-				res);
-			BUG();
-			return res;
-		}
-		ipa_mhi_ctx->qmi_req_id++;
-	}
-
-	res = ipa_disable_data_path(ipa_get_ep_mapping(channel->client));
-	if (res) {
-		IPA_MHI_ERR("ipa_disable_data_path failed %d\n", res);
-		return res;
-	}
-	IPA_MHI_FUNC_EXIT();
-
-	return 0;
-}
-
-static int ipa_mhi_reset_dl_channel(struct ipa_mhi_channel_ctx *channel)
-{
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	res = ipa_disable_data_path(ipa_get_ep_mapping(channel->client));
-	if (res) {
-		IPA_MHI_ERR("ipa_disable_data_path failed %d\n", res);
-		return res;
-	}
-
-	res = ipa_uc_mhi_reset_channel(channel->hdl);
-	if (res) {
-		IPA_MHI_ERR("ipa_uc_mhi_reset_channel failed %d\n", res);
-		goto fail_reset_channel;
-	}
-	IPA_MHI_FUNC_EXIT();
-
-	return 0;
-
-fail_reset_channel:
-	ipa_enable_data_path(ipa_get_ep_mapping(channel->client));
-	return res;
-}
-
-static int ipa_mhi_reset_channel(struct ipa_mhi_channel_ctx *channel)
-{
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	if (IPA_CLIENT_IS_PROD(channel->client))
-		res = ipa_mhi_reset_ul_channel(channel);
-	else
-		res = ipa_mhi_reset_dl_channel(channel);
-	if (res) {
-		IPA_MHI_ERR("failed to reset channel error %d\n", res);
-		return res;
-	}
-
-	channel->state = IPA_HW_MHI_CHANNEL_STATE_DISABLE;
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-/**
- * ipa_mhi_init() - Initialize IPA MHI driver
- * @params: initialization params
- *
- * This function is called by MHI client driver on boot to initialize IPA MHI
- * Driver. When this function returns device can move to READY state.
- * This function is doing the following:
- *	- Initialize MHI IPA internal data structures
- *	- Create IPA RM resources
- *	- Initialize debugfs
- *
- * Return codes: 0	  : success
- *		 negative : error
- */
-int ipa_mhi_init(struct ipa_mhi_init_params *params)
-{
-	int res;
-	struct ipa_rm_create_params mhi_prod_params;
-	struct ipa_rm_create_params mhi_cons_params;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	if (!params) {
-		IPA_MHI_ERR("null args\n");
-		return -EINVAL;
-	}
-
-	if (!params->notify) {
-		IPA_MHI_ERR("null notify function\n");
-		return -EINVAL;
-	}
-
-	if (ipa_mhi_ctx) {
-		IPA_MHI_ERR("already initialized\n");
-		return -EPERM;
-	}
-
-	IPA_MHI_DBG("msi: addr_lo = 0x%x addr_hi = 0x%x\n",
-		params->msi.addr_low, params->msi.addr_hi);
-	IPA_MHI_DBG("msi: data = 0x%x mask = 0x%x\n",
-		params->msi.data, params->msi.mask);
-	IPA_MHI_DBG("mmio_addr = 0x%x\n", params->mmio_addr);
-	IPA_MHI_DBG("first_ch_idx = 0x%x\n", params->first_ch_idx);
-	IPA_MHI_DBG("first_er_idx = 0x%x\n", params->first_er_idx);
-	IPA_MHI_DBG("notify = %pF priv = %p\n", params->notify, params->priv);
-
-	/* Initialize context */
-	ipa_mhi_ctx = kzalloc(sizeof(*ipa_mhi_ctx), GFP_KERNEL);
-	if (!ipa_mhi_ctx) {
-		IPA_MHI_ERR("no memory\n");
-		res = -EFAULT;
-		goto fail_alloc_ctx;
-	}
-
-	ipa_mhi_ctx->state = IPA_MHI_STATE_INITIALIZED;
-	ipa_mhi_ctx->msi = params->msi;
-	ipa_mhi_ctx->mmio_addr = params->mmio_addr;
-	ipa_mhi_ctx->first_ch_idx = params->first_ch_idx;
-	ipa_mhi_ctx->first_er_idx = params->first_er_idx;
-	ipa_mhi_ctx->cb_notify = params->notify;
-	ipa_mhi_ctx->cb_priv = params->priv;
-	ipa_mhi_ctx->rm_cons_state = IPA_MHI_RM_STATE_RELEASED;
-	ipa_mhi_ctx->qmi_req_id = 0;
-	init_completion(&ipa_mhi_ctx->rm_prod_granted_comp);
-	spin_lock_init(&ipa_mhi_ctx->state_lock);
-	init_completion(&ipa_mhi_ctx->rm_cons_comp);
-
-	ipa_mhi_ctx->wq = create_singlethread_workqueue("ipa_mhi_wq");
-	if (!ipa_mhi_ctx->wq) {
-		IPA_MHI_ERR("failed to create workqueue\n");
-		res = -EFAULT;
-		goto fail_create_wq;
-	}
-
-	/* Initialize debugfs */
-	ipa_mhi_debugfs_init();
-
-	/* Create PROD in IPA RM */
-	memset(&mhi_prod_params, 0, sizeof(mhi_prod_params));
-	mhi_prod_params.name = IPA_RM_RESOURCE_MHI_PROD;
-	mhi_prod_params.floor_voltage = IPA_VOLTAGE_SVS;
-	mhi_prod_params.reg_params.notify_cb = ipa_mhi_rm_prod_notify;
-	res = ipa_rm_create_resource(&mhi_prod_params);
-	if (res) {
-		IPA_MHI_ERR("fail to create IPA_RM_RESOURCE_MHI_PROD\n");
-		goto fail_create_rm_prod;
-	}
-
-	/* Create CONS in IPA RM */
-	memset(&mhi_cons_params, 0, sizeof(mhi_cons_params));
-	mhi_cons_params.name = IPA_RM_RESOURCE_MHI_CONS;
-	mhi_cons_params.floor_voltage = IPA_VOLTAGE_SVS;
-	mhi_cons_params.request_resource = ipa_mhi_rm_cons_request;
-	mhi_cons_params.release_resource = ipa_mhi_rm_cons_release;
-	res = ipa_rm_create_resource(&mhi_cons_params);
-	if (res) {
-		IPA_MHI_ERR("fail to create IPA_RM_RESOURCE_MHI_CONS\n");
-		goto fail_create_rm_cons;
-	}
-
-	/* Initialize uC interface */
-	ipa_uc_mhi_init(ipa_mhi_uc_ready_cb, ipa_mhi_uc_wakeup_request_cb);
-	if (ipa_uc_state_check() == 0)
-		ipa_mhi_set_state(IPA_MHI_STATE_READY);
-
-	IPA_MHI_FUNC_EXIT();
-
-	return 0;
-
-fail_create_rm_cons:
-	ipa_rm_delete_resource(IPA_RM_RESOURCE_MHI_PROD);
-fail_create_rm_prod:
-	destroy_workqueue(ipa_mhi_ctx->wq);
-fail_create_wq:
-	kfree(ipa_mhi_ctx);
-	ipa_mhi_ctx = NULL;
-fail_alloc_ctx:
-	return res;
-}
-EXPORT_SYMBOL(ipa_mhi_init);
-
-/**
- * ipa_mhi_start() - Start IPA MHI engine
- * @params: pcie addresses for MHI
- *
- * This function is called by MHI client driver on MHI engine start for
- * handling MHI accelerated channels. This function is called after
- * ipa_mhi_init() was called and can be called after MHI reset to restart MHI
- * engine. When this function returns device can move to M0 state.
- * This function is doing the following:
- *	- Send command to uC for initialization of MHI engine
- *	- Add dependencies to IPA RM
- *	- Request MHI_PROD in IPA RM
- *
- * Return codes: 0	  : success
- *		 negative : error
- */
-int ipa_mhi_start(struct ipa_mhi_start_params *params)
-{
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	if (!params) {
-		IPA_MHI_ERR("null args\n");
-		return -EINVAL;
-	}
-
-	if (unlikely(!ipa_mhi_ctx)) {
-		IPA_MHI_ERR("IPA MHI was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (ipa_uc_state_check()) {
-		IPA_MHI_ERR("IPA uc is not loaded\n");
-		return -EAGAIN;
-	}
-
-	res = ipa_mhi_set_state(IPA_MHI_STATE_STARTED);
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_set_state %d\n", res);
-		return res;
-	}
-
-	ipa_mhi_ctx->host_ctrl_addr = params->host_ctrl_addr;
-	ipa_mhi_ctx->host_data_addr = params->host_data_addr;
-
-	/* Add MHI <-> Q6 dependencies to IPA RM */
-	res = ipa_rm_add_dependency(IPA_RM_RESOURCE_MHI_PROD,
-		IPA_RM_RESOURCE_Q6_CONS);
-	if (res && res != -EINPROGRESS) {
-		IPA_MHI_ERR("failed to add dependency %d\n", res);
-		goto fail_add_mhi_q6_dep;
-	}
-
-	res = ipa_rm_add_dependency(IPA_RM_RESOURCE_Q6_PROD,
-		IPA_RM_RESOURCE_MHI_CONS);
-	if (res && res != -EINPROGRESS) {
-		IPA_MHI_ERR("failed to add dependency %d\n", res);
-		goto fail_add_q6_mhi_dep;
-	}
-
-	res = ipa_mhi_request_prod();
-	if (res) {
-		IPA_MHI_ERR("failed request prod %d\n", res);
-		goto fail_request_prod;
-	}
-
-	/* Initialize IPA MHI engine */
-	res = ipa_uc_mhi_init_engine(&ipa_mhi_ctx->msi, ipa_mhi_ctx->mmio_addr,
-		ipa_mhi_ctx->host_ctrl_addr, ipa_mhi_ctx->host_data_addr,
-		ipa_mhi_ctx->first_ch_idx, ipa_mhi_ctx->first_er_idx);
-	if (res) {
-		IPA_MHI_ERR("failed to start MHI engine %d\n", res);
-		goto fail_init_engine;
-	}
-
-	/* Update UL/DL sync if valid */
-	res = ipa_uc_mhi_send_dl_ul_sync_info(cached_dl_ul_sync_info);
-	if (res) {
-		IPA_MHI_ERR("failed to update ul/dl sync %d\n", res);
-		goto fail_init_engine;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-
-fail_init_engine:
-	ipa_mhi_release_prod();
-fail_request_prod:
-	ipa_rm_delete_dependency(IPA_RM_RESOURCE_Q6_PROD,
-		IPA_RM_RESOURCE_MHI_CONS);
-fail_add_q6_mhi_dep:
-	ipa_rm_delete_dependency(IPA_RM_RESOURCE_MHI_PROD,
-		IPA_RM_RESOURCE_Q6_CONS);
-fail_add_mhi_q6_dep:
-	ipa_mhi_set_state(IPA_MHI_STATE_INITIALIZED);
-	return res;
-}
-EXPORT_SYMBOL(ipa_mhi_start);
-
-/**
- * ipa_mhi_connect_pipe() - Connect pipe to IPA and start corresponding
- * MHI channel
- * @in: connect parameters
- * @clnt_hdl: [out] client handle for this pipe
- *
- * This function is called by MHI client driver on MHI channel start.
- * This function is called after MHI engine was started.
- * This function is doing the following:
- *	- Send command to uC to start corresponding MHI channel
- *	- Configure IPA EP control
- *
- * Return codes: 0	  : success
- *		 negative : error
- */
-int ipa_mhi_connect_pipe(struct ipa_mhi_connect_params *in, u32 *clnt_hdl)
-{
-	struct ipa_ep_context *ep;
-	int ipa_ep_idx;
-	int res;
-	struct ipa_mhi_channel_ctx *channel = NULL;
-	unsigned long flags;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	if (!in || !clnt_hdl) {
-		IPA_MHI_ERR("NULL args\n");
-		return -EINVAL;
-	}
-
-	if (in->sys.client >= IPA_CLIENT_MAX) {
-		IPA_MHI_ERR("bad parm client:%d\n", in->sys.client);
-		return -EINVAL;
-	}
-
-	if (unlikely(!ipa_mhi_ctx)) {
-		IPA_MHI_ERR("IPA MHI was not initialized\n");
-		return -EINVAL;
-	}
-
-	spin_lock_irqsave(&ipa_mhi_ctx->state_lock, flags);
-	if (!ipa_mhi_ctx || ipa_mhi_ctx->state != IPA_MHI_STATE_STARTED) {
-		IPA_MHI_ERR("IPA MHI was not started\n");
-		spin_unlock_irqrestore(&ipa_mhi_ctx->state_lock, flags);
-		return -EINVAL;
-	}
-	spin_unlock_irqrestore(&ipa_mhi_ctx->state_lock, flags);
-
-	ipa_ep_idx = ipa_get_ep_mapping(in->sys.client);
-	if (ipa_ep_idx == -1) {
-		IPA_MHI_ERR("Invalid client.\n");
-		return -EINVAL;
-	}
-
-	ep = &ipa_ctx->ep[ipa_ep_idx];
-
-	channel = ipa_mhi_get_channel_context(in->sys.client,
-		in->channel_id);
-	if (!channel) {
-		IPA_MHI_ERR("ipa_mhi_get_channel_context failed\n");
-		return -EINVAL;
-	}
-
-	IPA_MHI_DBG("client %d channelHandle %d channelIndex %d\n",
-		channel->client, channel->hdl, channel->id);
-
-	ipa_inc_client_enable_clks();
-
-	if (ep->valid == 1) {
-		IPA_MHI_ERR("EP already allocated.\n");
-		goto fail_ep_exists;
-	}
-
-	memset(ep, 0, offsetof(struct ipa_ep_context, sys));
-	ep->valid = 1;
-	ep->skip_ep_cfg = in->sys.skip_ep_cfg;
-	ep->client = in->sys.client;
-	ep->client_notify = in->sys.notify;
-	ep->priv = in->sys.priv;
-	ep->keep_ipa_awake = in->sys.keep_ipa_awake;
-
-	/* start channel in uC */
-	if (channel->state == IPA_HW_MHI_CHANNEL_STATE_INVALID) {
-		IPA_MHI_DBG("Initializing channel\n");
-		res = ipa_uc_mhi_init_channel(ipa_ep_idx, channel->hdl,
-			channel->id, (IPA_CLIENT_IS_PROD(ep->client) ? 1 : 2));
-		if (res) {
-			IPA_MHI_ERR("init_channel failed %d\n", res);
-			goto fail_init_channel;
-		}
-	} else if (channel->state == IPA_HW_MHI_CHANNEL_STATE_DISABLE) {
-		if (channel->client != ep->client) {
-			IPA_MHI_ERR("previous channel client was %d\n",
-				ep->client);
-			goto fail_init_channel;
-		}
-		IPA_MHI_DBG("Starting channel\n");
-		res = ipa_uc_mhi_resume_channel(channel->hdl, false);
-		if (res) {
-			IPA_MHI_ERR("init_channel failed %d\n", res);
-			goto fail_init_channel;
-		}
-	} else {
-		IPA_MHI_ERR("Invalid channel state %d\n", channel->state);
-		goto fail_init_channel;
-	}
-
-	channel->state = IPA_HW_MHI_CHANNEL_STATE_RUN;
-
-	res = ipa_enable_data_path(ipa_ep_idx);
-	if (res) {
-		IPA_MHI_ERR("enable data path failed res=%d clnt=%d.\n", res,
-			ipa_ep_idx);
-		goto fail_enable_dp;
-	}
-
-	if (!ep->skip_ep_cfg) {
-		if (ipa_cfg_ep(ipa_ep_idx, &in->sys.ipa_ep_cfg)) {
-			IPAERR("fail to configure EP.\n");
-			goto fail_ep_cfg;
-		}
-		if (ipa_cfg_ep_status(ipa_ep_idx, &ep->status)) {
-			IPAERR("fail to configure status of EP.\n");
-			goto fail_ep_cfg;
-		}
-		IPA_MHI_DBG("ep configuration successful\n");
-	} else {
-		IPA_MHI_DBG("skipping ep configuration\n");
-	}
-
-	*clnt_hdl = ipa_ep_idx;
-
-	if (!ep->skip_ep_cfg && IPA_CLIENT_IS_PROD(in->sys.client))
-		ipa_install_dflt_flt_rules(ipa_ep_idx);
-
-	if (!ep->keep_ipa_awake)
-		ipa_dec_client_disable_clks();
-
-	ipa_ctx->skip_ep_cfg_shadow[ipa_ep_idx] = ep->skip_ep_cfg;
-	IPA_MHI_DBG("client %d (ep: %d) connected\n", in->sys.client,
-		ipa_ep_idx);
-
-	IPA_MHI_FUNC_EXIT();
-
-	return 0;
-
-fail_ep_cfg:
-	ipa_disable_data_path(ipa_ep_idx);
-fail_enable_dp:
-	ipa_uc_mhi_reset_channel(channel->hdl);
-	channel->state = IPA_HW_MHI_CHANNEL_STATE_DISABLE;
-fail_init_channel:
-	memset(ep, 0, offsetof(struct ipa_ep_context, sys));
-fail_ep_exists:
-	ipa_dec_client_disable_clks();
-	return -EPERM;
-}
-EXPORT_SYMBOL(ipa_mhi_connect_pipe);
-
-/**
- * ipa_mhi_disconnect_pipe() - Disconnect pipe from IPA and reset corresponding
- * MHI channel
- * @in: connect parameters
- * @clnt_hdl: [out] client handle for this pipe
- *
- * This function is called by MHI client driver on MHI channel reset.
- * This function is called after MHI channel was started.
- * This function is doing the following:
- *	- Send command to uC to reset corresponding MHI channel
- *	- Configure IPA EP control
- *
- * Return codes: 0	  : success
- *		 negative : error
- */
-int ipa_mhi_disconnect_pipe(u32 clnt_hdl)
-{
-	struct ipa_ep_context *ep;
-	static struct ipa_mhi_channel_ctx *channel;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes) {
-		IPAERR("invalid handle %d\n", clnt_hdl);
-		return -EINVAL;
-	}
-
-	if (ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("pipe was not connected %d\n", clnt_hdl);
-		return -EINVAL;
-	}
-
-	if (unlikely(!ipa_mhi_ctx)) {
-		IPA_MHI_ERR("IPA MHI was not initialized\n");
-		return -EINVAL;
-	}
-
-	channel = ipa_mhi_get_channel_context_by_clnt_hdl(clnt_hdl);
-	if (!channel) {
-		IPAERR("invalid clnt hdl\n");
-		return -EINVAL;
-	}
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (!ep->keep_ipa_awake)
-		ipa_inc_client_enable_clks();
-
-	res = ipa_mhi_reset_channel(channel);
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_reset_channel failed %d\n", res);
-		goto fail_reset_channel;
-	}
-
-	ep->valid = 0;
-	ipa_delete_dflt_flt_rules(clnt_hdl);
-
-	ipa_dec_client_disable_clks();
-
-	IPA_MHI_DBG("client (ep: %d) disconnected\n", clnt_hdl);
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-
-fail_reset_channel:
-	if (!ep->keep_ipa_awake)
-		ipa_dec_client_disable_clks();
-	return res;
-}
-EXPORT_SYMBOL(ipa_mhi_disconnect_pipe);
-
-static int ipa_mhi_suspend_ul_channels(void)
-{
-	int i;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	for (i = 0; i < IPA_MHI_MAX_UL_CHANNELS; i++) {
-		if (!ipa_mhi_ctx->ul_channels[i].valid)
-			continue;
-		if (ipa_mhi_ctx->ul_channels[i].state !=
-		    IPA_HW_MHI_CHANNEL_STATE_RUN)
-			continue;
-		IPA_MHI_DBG("suspending channel %d\n",
-			ipa_mhi_ctx->ul_channels[i].hdl);
-		res = ipa_uc_mhi_suspend_channel(
-			ipa_mhi_ctx->ul_channels[i].hdl);
-		if (res) {
-			IPA_MHI_ERR("failed to suspend channel %d error %d\n",
-				i, res);
-			return res;
-		}
-		ipa_mhi_ctx->ul_channels[i].state =
-			IPA_HW_MHI_CHANNEL_STATE_SUSPEND;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-static int ipa_mhi_resume_ul_channels(bool LPTransitionRejected)
-{
-	int i;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	for (i = 0; i < IPA_MHI_MAX_UL_CHANNELS; i++) {
-		if (!ipa_mhi_ctx->ul_channels[i].valid)
-			continue;
-		if (ipa_mhi_ctx->ul_channels[i].state !=
-		    IPA_HW_MHI_CHANNEL_STATE_SUSPEND)
-			continue;
-		IPA_MHI_DBG("suspending channel %d\n",
-			ipa_mhi_ctx->ul_channels[i].hdl);
-		res = ipa_uc_mhi_resume_channel(ipa_mhi_ctx->ul_channels[i].hdl,
-			LPTransitionRejected);
-		if (res) {
-			IPA_MHI_ERR("failed to suspend channel %d error %d\n",
-				i, res);
-			return res;
-		}
-		ipa_mhi_ctx->ul_channels[i].state =
-			IPA_HW_MHI_CHANNEL_STATE_RUN;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-static int ipa_mhi_stop_event_update_ul_channels(void)
-{
-	int i;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	for (i = 0; i < IPA_MHI_MAX_UL_CHANNELS; i++) {
-		if (!ipa_mhi_ctx->ul_channels[i].valid)
-			continue;
-		if (ipa_mhi_ctx->ul_channels[i].state !=
-		    IPA_HW_MHI_CHANNEL_STATE_SUSPEND)
-			continue;
-		IPA_MHI_DBG("stop update event channel %d\n",
-			ipa_mhi_ctx->ul_channels[i].hdl);
-		res = ipa_uc_mhi_stop_event_update_channel(
-			ipa_mhi_ctx->ul_channels[i].hdl);
-		if (res) {
-			IPA_MHI_ERR("failed stop event channel %d error %d\n",
-				i, res);
-			return res;
-		}
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-static int ipa_mhi_suspend_dl_channels(void)
-{
-	int i;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	for (i = 0; i < IPA_MHI_MAX_DL_CHANNELS; i++) {
-		if (!ipa_mhi_ctx->dl_channels[i].valid)
-			continue;
-		if (ipa_mhi_ctx->dl_channels[i].state !=
-		    IPA_HW_MHI_CHANNEL_STATE_RUN)
-			continue;
-		IPA_MHI_DBG("suspending channel %d\n",
-			ipa_mhi_ctx->dl_channels[i].hdl);
-		res = ipa_uc_mhi_suspend_channel(
-			ipa_mhi_ctx->dl_channels[i].hdl);
-		if (res) {
-			IPA_MHI_ERR("failed to suspend channel %d error %d\n",
-				i, res);
-			return res;
-		}
-		ipa_mhi_ctx->dl_channels[i].state =
-			IPA_HW_MHI_CHANNEL_STATE_SUSPEND;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-static int ipa_mhi_resume_dl_channels(bool LPTransitionRejected)
-{
-	int i;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	for (i = 0; i < IPA_MHI_MAX_DL_CHANNELS; i++) {
-		if (!ipa_mhi_ctx->dl_channels[i].valid)
-			continue;
-		if (ipa_mhi_ctx->dl_channels[i].state !=
-		    IPA_HW_MHI_CHANNEL_STATE_SUSPEND)
-			continue;
-		IPA_MHI_DBG("suspending channel %d\n",
-			ipa_mhi_ctx->dl_channels[i].hdl);
-		res = ipa_uc_mhi_resume_channel(ipa_mhi_ctx->dl_channels[i].hdl,
-			LPTransitionRejected);
-		if (res) {
-			IPA_MHI_ERR("failed to suspend channel %d error %d\n",
-				i, res);
-			return res;
-		}
-		ipa_mhi_ctx->dl_channels[i].state =
-			IPA_HW_MHI_CHANNEL_STATE_RUN;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-static int ipa_mhi_stop_event_update_dl_channels(void)
-{
-	int i;
-	int res;
-
-	IPA_MHI_FUNC_ENTRY();
-	for (i = 0; i < IPA_MHI_MAX_DL_CHANNELS; i++) {
-		if (!ipa_mhi_ctx->dl_channels[i].valid)
-			continue;
-		if (ipa_mhi_ctx->dl_channels[i].state !=
-		    IPA_HW_MHI_CHANNEL_STATE_SUSPEND)
-			continue;
-		IPA_MHI_DBG("stop update event channel %d\n",
-			ipa_mhi_ctx->dl_channels[i].hdl);
-		res = ipa_uc_mhi_stop_event_update_channel(
-			ipa_mhi_ctx->dl_channels[i].hdl);
-		if (res) {
-			IPA_MHI_ERR("failed stop event channel %d error %d\n",
-				i, res);
-			return res;
-		}
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-/**
- * ipa_mhi_suspend() - Suspend MHI accelerated channels
- * @force:
- *	false: in case of data pending in IPA, MHI channels will not be
- *		suspended and function will fail.
- *	true:  in case of data pending in IPA, make sure no further access from
- *		IPA to PCIe is possible. In this case suspend cannot fail.
- *
- * This function is called by MHI client driver on MHI suspend.
- * This function is called after MHI channel was started.
- * When this function returns device can move to M1/M2/M3/D3cold state.
- * This function is doing the following:
- *	- Send command to uC to suspend corresponding MHI channel
- *	- Make sure no further access is possible from IPA to PCIe
- *	- Release MHI_PROD in IPA RM
- *
- * Return codes: 0	  : success
- *		 negative : error
- */
-int ipa_mhi_suspend(bool force)
-{
-	int res;
-	bool bam_empty;
-	bool force_clear = false;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	if (unlikely(!ipa_mhi_ctx)) {
-		IPA_MHI_ERR("IPA MHI was not initialized\n");
-		return -EINVAL;
-	}
-
-	res = ipa_mhi_set_state(IPA_MHI_STATE_SUSPEND_IN_PROGRESS);
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_set_state failed %d\n", res);
-		return res;
-	}
-
-	res = ipa_mhi_suspend_ul_channels();
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_suspend_ul_channels failed %d\n", res);
-		goto fail_suspend_ul_channel;
-	}
-
-	bam_empty = ipa_mhi_wait_for_bam_empty_timeout(
-		IPA_MHI_BAM_EMPTY_TIMEOUT_MSEC);
-	if (!bam_empty) {
-		if (force) {
-			res = ipa_mhi_enable_force_clear(
-				ipa_mhi_ctx->qmi_req_id, false);
-			if (res) {
-				IPA_MHI_ERR("failed to enable force clear\n");
-				BUG();
-				return res;
-			}
-			force_clear = true;
-			IPA_MHI_DBG("force clear datapath enabled\n");
-
-			bam_empty = ipa_mhi_wait_for_bam_empty_timeout(
-				IPA_MHI_BAM_EMPTY_TIMEOUT_MSEC);
-			IPADBG("bam_empty=%d\n", bam_empty);
-
-		} else {
-			IPA_MHI_DBG("BAM not empty\n");
-			res = -EAGAIN;
-			goto fail_suspend_ul_channel;
-		}
-	}
-
-	res = ipa_mhi_stop_event_update_ul_channels();
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_stop_event_update_ul_channels failed %d\n",
-			res);
-		goto fail_suspend_ul_channel;
-	}
-
-	/*
-	 * in case BAM not empty, hold IPA clocks and release them after all
-	 * IPA RM resource are released to make sure tag process will not start
-	 */
-	if (!bam_empty)
-		ipa_inc_client_enable_clks();
-
-	IPA_MHI_DBG("release prod\n");
-	res = ipa_mhi_release_prod();
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_release_prod failed %d\n", res);
-		goto fail_release_prod;
-	}
-
-	IPA_MHI_DBG("wait for cons release\n");
-	res = ipa_mhi_wait_for_cons_release();
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_wait_for_cons_release failed %d\n", res);
-		goto fail_release_cons;
-	}
-
-	usleep_range(IPA_MHI_SUSPEND_SLEEP_MIN, IPA_MHI_SUSPEND_SLEEP_MAX);
-
-	res = ipa_mhi_suspend_dl_channels();
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_suspend_dl_channels failed %d\n", res);
-		goto fail_suspend_dl_channel;
-	}
-
-	res = ipa_mhi_stop_event_update_dl_channels();
-	if (res) {
-		IPA_MHI_ERR("failed to stop event update on DL %d\n", res);
-		goto fail_stop_event_update_dl_channel;
-	}
-
-	if (force_clear) {
-		res = ipa_mhi_disable_force_clear(ipa_mhi_ctx->qmi_req_id);
-		if (res) {
-			IPA_MHI_ERR("failed to disable force clear\n");
-			BUG();
-			return res;
-		}
-		IPA_MHI_DBG("force clear datapath disabled\n");
-		ipa_mhi_ctx->qmi_req_id++;
-	}
-
-	if (!bam_empty) {
-		ipa_ctx->tag_process_before_gating = false;
-		ipa_dec_client_disable_clks();
-	}
-
-	res = ipa_mhi_set_state(IPA_MHI_STATE_SUSPENDED);
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_set_state failed %d\n", res);
-		goto fail_release_cons;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-
-fail_stop_event_update_dl_channel:
-	ipa_mhi_resume_dl_channels(true);
-fail_suspend_dl_channel:
-fail_release_cons:
-	ipa_mhi_request_prod();
-fail_release_prod:
-fail_suspend_ul_channel:
-	ipa_mhi_resume_ul_channels(true);
-	ipa_mhi_set_state(IPA_MHI_STATE_STARTED);
-	return res;
-}
-EXPORT_SYMBOL(ipa_mhi_suspend);
-
-/**
- * ipa_mhi_resume() - Resume MHI accelerated channels
- *
- * This function is called by MHI client driver on MHI resume.
- * This function is called after MHI channel was suspended.
- * When this function returns device can move to M0 state.
- * This function is doing the following:
- *	- Send command to uC to resume corresponding MHI channel
- *	- Request MHI_PROD in IPA RM
- *	- Resume data to IPA
- *
- * Return codes: 0	  : success
- *		 negative : error
- */
-int ipa_mhi_resume(void)
-{
-	int res;
-	bool dl_channel_resumed = false;
-
-	IPA_MHI_FUNC_ENTRY();
-
-	if (unlikely(!ipa_mhi_ctx)) {
-		IPA_MHI_ERR("IPA MHI was not initialized\n");
-		return -EINVAL;
-	}
-
-	res = ipa_mhi_set_state(IPA_MHI_STATE_RESUME_IN_PROGRESS);
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_set_state failed %d\n", res);
-		return res;
-	}
-
-	if (ipa_mhi_ctx->rm_cons_state == IPA_MHI_RM_STATE_REQUESTED) {
-		/* resume all DL channels */
-		res = ipa_mhi_resume_dl_channels(false);
-		if (res) {
-			IPA_MHI_ERR("ipa_mhi_resume_dl_channels failed %d\n",
-				res);
-			goto fail_resume_dl_channels;
-		}
-		dl_channel_resumed = true;
-
-		ipa_rm_notify_completion(IPA_RM_RESOURCE_GRANTED,
-			IPA_RM_RESOURCE_MHI_CONS);
-		ipa_mhi_ctx->rm_cons_state = IPA_MHI_RM_STATE_GRANTED;
-	}
-
-	res = ipa_mhi_request_prod();
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_request_prod failed %d\n", res);
-		goto fail_request_prod;
-	}
-
-	/* resume all UL channels */
-	res = ipa_mhi_resume_ul_channels(false);
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_resume_ul_channels failed %d\n", res);
-		goto fail_resume_ul_channels;
-	}
-
-	if (!dl_channel_resumed) {
-		res = ipa_mhi_resume_dl_channels(true);
-		if (res) {
-			IPA_MHI_ERR("ipa_mhi_resume_dl_channels failed %d\n",
-				res);
-			goto fail_resume_dl_channels2;
-		}
-	}
-
-	res = ipa_mhi_set_state(IPA_MHI_STATE_STARTED);
-	if (res) {
-		IPA_MHI_ERR("ipa_mhi_set_state failed %d\n", res);
-		goto fail_set_state;
-	}
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-
-fail_set_state:
-	ipa_mhi_suspend_dl_channels();
-fail_resume_dl_channels2:
-	ipa_mhi_suspend_ul_channels();
-fail_resume_ul_channels:
-	ipa_mhi_release_prod();
-fail_request_prod:
-	ipa_mhi_suspend_dl_channels();
-fail_resume_dl_channels:
-	ipa_mhi_set_state(IPA_MHI_STATE_SUSPENDED);
-	return res;
-}
-EXPORT_SYMBOL(ipa_mhi_resume);
-
-/**
- * ipa_mhi_destroy() - Destroy MHI IPA
- *
- * This function is called by MHI client driver on MHI reset to destroy all IPA
- * MHI resources.
- *
- * Return codes: 0	  : success
- *		 negative : error
- */
-int ipa_mhi_destroy(void)
-{
-	IPA_MHI_FUNC_ENTRY();
-
-	if (unlikely(!ipa_mhi_ctx)) {
-		IPA_MHI_ERR("IPA MHI was not initialized\n");
-		return -EINVAL;
-	}
-
-	IPAERR("Not implemented Yet!\n");
-	ipa_mhi_debugfs_destroy();
-
-	IPA_MHI_FUNC_EXIT();
-	return -EPERM;
-}
-EXPORT_SYMBOL(ipa_mhi_destroy);
-
-/**
- * ipa_mhi_handle_ipa_config_req() - hanle IPA CONFIG QMI message
- *
- * This function is called by by IPA QMI service to indicate that IPA CONFIG
- * message was sent from modem. IPA MHI will update this information to IPA uC
- * or will cache it until IPA MHI will be initialized.
- *
- * Return codes: 0	  : success
- *		 negative : error
- */
-int ipa_mhi_handle_ipa_config_req(struct ipa_config_req_msg_v01 *config_req)
-{
-	IPA_MHI_FUNC_ENTRY();
-	ipa_mhi_cache_dl_ul_sync_info(config_req);
-
-	if (ipa_mhi_ctx && ipa_mhi_ctx->state != IPA_MHI_STATE_INITIALIZED)
-		ipa_uc_mhi_send_dl_ul_sync_info(cached_dl_ul_sync_info);
-
-	IPA_MHI_FUNC_EXIT();
-	return 0;
-}
-
-MODULE_LICENSE("GPL v2");
-MODULE_DESCRIPTION("IPA MHI driver");
diff --git a/drivers/platform/msm/ipa/ipa_nat.c b/drivers/platform/msm/ipa/ipa_nat.c
deleted file mode 100644
index 12691fa..00000000
--- a/drivers/platform/msm/ipa/ipa_nat.c
+++ /dev/null
@@ -1,839 +0,0 @@
-/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/device.h>
-#include <linux/fs.h>
-#include <linux/init.h>
-#include <linux/kernel.h>
-#include <linux/mm.h>
-#include <linux/uaccess.h>
-#include "ipa_i.h"
-
-#define IPA_NAT_PHYS_MEM_OFFSET  0
-#define IPA_NAT_PHYS_MEM_SIZE  IPA_RAM_NAT_SIZE
-
-#define IPA_NAT_SYSTEM_MEMORY  0
-#define IPA_NAT_SHARED_MEMORY  1
-#define IPA_NAT_TEMP_MEM_SIZE 128
-
-enum nat_table_type {
-	IPA_NAT_BASE_TBL = 0,
-	IPA_NAT_EXPN_TBL = 1,
-	IPA_NAT_INDX_TBL = 2,
-	IPA_NAT_INDEX_EXPN_TBL = 3,
-};
-
-#define NAT_TABLE_ENTRY_SIZE_BYTE 32
-#define NAT_INTEX_TABLE_ENTRY_SIZE_BYTE 4
-
-static int ipa_nat_vma_fault_remap(
-	 struct vm_area_struct *vma, struct vm_fault *vmf)
-{
-	IPADBG("\n");
-	vmf->page = NULL;
-
-	return VM_FAULT_SIGBUS;
-}
-
-/* VMA related file operations functions */
-static struct vm_operations_struct ipa_nat_remap_vm_ops = {
-	.fault = ipa_nat_vma_fault_remap,
-};
-
-static int ipa_nat_open(struct inode *inode, struct file *filp)
-{
-	struct ipa_nat_mem *nat_ctx;
-	IPADBG("\n");
-	nat_ctx = container_of(inode->i_cdev, struct ipa_nat_mem, cdev);
-	filp->private_data = nat_ctx;
-	IPADBG("return\n");
-	return 0;
-}
-
-static int ipa_nat_mmap(struct file *filp, struct vm_area_struct *vma)
-{
-	unsigned long vsize = vma->vm_end - vma->vm_start;
-	struct ipa_nat_mem *nat_ctx = (struct ipa_nat_mem *)filp->private_data;
-	unsigned long phys_addr;
-	int result;
-
-	mutex_lock(&nat_ctx->lock);
-	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
-	if (nat_ctx->is_sys_mem) {
-		IPADBG("Mapping system memory\n");
-		if (nat_ctx->is_mapped) {
-			IPAERR("mapping already exists, only 1 supported\n");
-			result = -EINVAL;
-			goto bail;
-		}
-		IPADBG("map sz=0x%zx\n", nat_ctx->size);
-		result =
-			dma_mmap_coherent(
-				 ipa_ctx->pdev, vma,
-				 nat_ctx->vaddr, nat_ctx->dma_handle,
-				 nat_ctx->size);
-
-		if (result) {
-			IPAERR("unable to map memory. Err:%d\n", result);
-			goto bail;
-		}
-		ipa_ctx->nat_mem.nat_base_address = nat_ctx->vaddr;
-	} else {
-		IPADBG("Mapping shared(local) memory\n");
-		IPADBG("map sz=0x%lx\n", vsize);
-
-		if ((IPA_NAT_PHYS_MEM_SIZE == 0) ||
-				(vsize > IPA_NAT_PHYS_MEM_SIZE)) {
-			result = -EINVAL;
-			goto bail;
-		}
-		phys_addr = ipa_ctx->ipa_wrapper_base +
-			ipa_ctx->ctrl->ipa_reg_base_ofst +
-			IPA_SRAM_DIRECT_ACCESS_N_OFST(IPA_NAT_PHYS_MEM_OFFSET);
-
-		if (remap_pfn_range(
-			 vma, vma->vm_start,
-			 phys_addr >> PAGE_SHIFT, vsize, vma->vm_page_prot)) {
-			IPAERR("remap failed\n");
-			result = -EAGAIN;
-			goto bail;
-		}
-		ipa_ctx->nat_mem.nat_base_address = (void *)vma->vm_start;
-	}
-	nat_ctx->is_mapped = true;
-	vma->vm_ops = &ipa_nat_remap_vm_ops;
-	IPADBG("return\n");
-	result = 0;
-bail:
-	mutex_unlock(&nat_ctx->lock);
-	return result;
-}
-
-static const struct file_operations ipa_nat_fops = {
-	.owner = THIS_MODULE,
-	.open = ipa_nat_open,
-	.mmap = ipa_nat_mmap
-};
-
-/**
- * allocate_temp_nat_memory() - Allocates temp nat memory
- *
- * Called during nat table delete
- */
-void allocate_temp_nat_memory(void)
-{
-	struct ipa_nat_mem *nat_ctx = &(ipa_ctx->nat_mem);
-	int gfp_flags = GFP_KERNEL | __GFP_ZERO;
-
-	nat_ctx->tmp_vaddr =
-		dma_alloc_coherent(ipa_ctx->pdev, IPA_NAT_TEMP_MEM_SIZE,
-				&nat_ctx->tmp_dma_handle, gfp_flags);
-
-	if (nat_ctx->tmp_vaddr == NULL) {
-		IPAERR("Temp Memory alloc failed\n");
-		nat_ctx->is_tmp_mem = false;
-		return;
-	}
-
-	nat_ctx->is_tmp_mem = true;
-	IPADBG("IPA NAT allocated temp memory successfully\n");
-	return;
-}
-
-/**
- * create_nat_device() - Create the NAT device
- *
- * Called during ipa init to create nat device
- *
- * Returns:	0 on success, negative on failure
- */
-int create_nat_device(void)
-{
-	struct ipa_nat_mem *nat_ctx = &(ipa_ctx->nat_mem);
-	int result;
-	IPADBG("\n");
-
-	mutex_lock(&nat_ctx->lock);
-	nat_ctx->class = class_create(THIS_MODULE, NAT_DEV_NAME);
-	if (IS_ERR(nat_ctx->class)) {
-		IPAERR("unable to create the class\n");
-		result = -ENODEV;
-		goto vaddr_alloc_fail;
-	}
-	result = alloc_chrdev_region(&nat_ctx->dev_num,
-					0,
-					1,
-					NAT_DEV_NAME);
-	if (result) {
-		IPAERR("alloc_chrdev_region err.\n");
-		result = -ENODEV;
-		goto alloc_chrdev_region_fail;
-	}
-
-	nat_ctx->dev =
-	   device_create(nat_ctx->class, NULL, nat_ctx->dev_num, nat_ctx,
-			"%s", NAT_DEV_NAME);
-
-	if (IS_ERR(nat_ctx->dev)) {
-		IPAERR("device_create err:%ld\n", PTR_ERR(nat_ctx->dev));
-		result = -ENODEV;
-		goto device_create_fail;
-	}
-
-	cdev_init(&nat_ctx->cdev, &ipa_nat_fops);
-	nat_ctx->cdev.owner = THIS_MODULE;
-	nat_ctx->cdev.ops = &ipa_nat_fops;
-
-	result = cdev_add(&nat_ctx->cdev, nat_ctx->dev_num, 1);
-	if (result) {
-		IPAERR("cdev_add err=%d\n", -result);
-		goto cdev_add_fail;
-	}
-	IPADBG("ipa nat dev added successful. major:%d minor:%d\n",
-			MAJOR(nat_ctx->dev_num),
-			MINOR(nat_ctx->dev_num));
-
-	nat_ctx->is_dev = true;
-	allocate_temp_nat_memory();
-	IPADBG("IPA NAT device created successfully\n");
-	result = 0;
-	goto bail;
-
-cdev_add_fail:
-	device_destroy(nat_ctx->class, nat_ctx->dev_num);
-device_create_fail:
-	unregister_chrdev_region(nat_ctx->dev_num, 1);
-alloc_chrdev_region_fail:
-	class_destroy(nat_ctx->class);
-vaddr_alloc_fail:
-	if (nat_ctx->vaddr) {
-		IPADBG("Releasing system memory\n");
-		dma_free_coherent(
-			 ipa_ctx->pdev, nat_ctx->size,
-			 nat_ctx->vaddr, nat_ctx->dma_handle);
-		nat_ctx->vaddr = NULL;
-		nat_ctx->dma_handle = 0;
-		nat_ctx->size = 0;
-	}
-
-bail:
-	mutex_unlock(&nat_ctx->lock);
-
-	return result;
-}
-
-/**
- * allocate_nat_device() - Allocates memory for the NAT device
- * @mem:	[in/out] memory parameters
- *
- * Called by NAT client driver to allocate memory for the NAT entries. Based on
- * the request size either shared or system memory will be used.
- *
- * Returns:	0 on success, negative on failure
- */
-int allocate_nat_device(struct ipa_ioc_nat_alloc_mem *mem)
-{
-	struct ipa_nat_mem *nat_ctx = &(ipa_ctx->nat_mem);
-	int gfp_flags = GFP_KERNEL | __GFP_ZERO;
-	int result;
-
-	IPADBG("passed memory size %zu\n", mem->size);
-
-	mutex_lock(&nat_ctx->lock);
-	if (strcmp(mem->dev_name, NAT_DEV_NAME)) {
-		IPAERR("Nat device name mismatch\n");
-		IPAERR("Expect: %s Recv: %s\n", NAT_DEV_NAME, mem->dev_name);
-		result = -EPERM;
-		goto bail;
-	}
-
-	if (nat_ctx->is_dev != true) {
-		IPAERR("Nat device not created successfully during boot up\n");
-		result = -EPERM;
-		goto bail;
-	}
-
-	if (nat_ctx->is_dev_init == true) {
-		IPAERR("Device already init\n");
-		result = 0;
-		goto bail;
-	}
-
-	if (mem->size <= 0 ||
-			nat_ctx->is_dev_init == true) {
-		IPAERR("Invalid Parameters or device is already init\n");
-		result = -EPERM;
-		goto bail;
-	}
-
-	if (mem->size > IPA_NAT_PHYS_MEM_SIZE) {
-		IPADBG("Allocating system memory\n");
-		nat_ctx->is_sys_mem = true;
-		nat_ctx->vaddr =
-		   dma_alloc_coherent(ipa_ctx->pdev, mem->size,
-				   &nat_ctx->dma_handle, gfp_flags);
-		if (nat_ctx->vaddr == NULL) {
-			IPAERR("memory alloc failed\n");
-			result = -ENOMEM;
-			goto bail;
-		}
-		nat_ctx->size = mem->size;
-	} else {
-		IPADBG("using shared(local) memory\n");
-		nat_ctx->is_sys_mem = false;
-	}
-
-	nat_ctx->is_dev_init = true;
-	IPADBG("IPA NAT dev init successfully\n");
-	result = 0;
-
-bail:
-	mutex_unlock(&nat_ctx->lock);
-
-	return result;
-}
-
-/* IOCTL function handlers */
-/**
- * ipa_nat_init_cmd() - Post IP_V4_NAT_INIT command to IPA HW
- * @init:	[in] initialization command attributes
- *
- * Called by NAT client driver to post IP_V4_NAT_INIT command to IPA HW
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_nat_init_cmd(struct ipa_ioc_v4_nat_init *init)
-{
-#define TBL_ENTRY_SIZE 32
-#define INDX_TBL_ENTRY_SIZE 4
-
-	struct ipa_register_write *reg_write_nop;
-	struct ipa_desc desc[2];
-	struct ipa_ip_v4_nat_init *cmd;
-	u16 size = sizeof(struct ipa_ip_v4_nat_init);
-	int result;
-	u32 offset = 0;
-	size_t tmp;
-
-	IPADBG("\n");
-	if (init->table_entries == 0) {
-		IPADBG("Table entries is zero\n");
-		return -EPERM;
-	}
-
-	/* check for integer overflow */
-	if (init->ipv4_rules_offset >
-		UINT_MAX - (TBL_ENTRY_SIZE * (init->table_entries + 1))) {
-			IPAERR("Detected overflow\n");
-			return -EPERM;
-	}
-	/* Check Table Entry offset is not
-	   beyond allocated size */
-	tmp = init->ipv4_rules_offset +
-		(TBL_ENTRY_SIZE * (init->table_entries + 1));
-	if (tmp > ipa_ctx->nat_mem.size) {
-		IPAERR("Table rules offset not valid\n");
-		IPAERR("offset:%d entrys:%d size:%zu mem_size:%zu\n",
-			init->ipv4_rules_offset, (init->table_entries + 1),
-			tmp, ipa_ctx->nat_mem.size);
-		return -EPERM;
-	}
-
-	/* check for integer overflow */
-	if (init->expn_rules_offset >
-		UINT_MAX - (TBL_ENTRY_SIZE * init->expn_table_entries)) {
-			IPAERR("Detected overflow\n");
-			return -EPERM;
-	}
-	/* Check Expn Table Entry offset is not
-	   beyond allocated size */
-	tmp = init->expn_rules_offset +
-		(TBL_ENTRY_SIZE * init->expn_table_entries);
-	if (tmp > ipa_ctx->nat_mem.size) {
-		IPAERR("Expn Table rules offset not valid\n");
-		IPAERR("offset:%d entrys:%d size:%zu mem_size:%zu\n",
-			init->expn_rules_offset, init->expn_table_entries,
-			tmp, ipa_ctx->nat_mem.size);
-		return -EPERM;
-	}
-
-  /* check for integer overflow */
-	if (init->index_offset >
-		UINT_MAX - (INDX_TBL_ENTRY_SIZE * (init->table_entries + 1))) {
-			IPAERR("Detected overflow\n");
-			return -EPERM;
-	}
-	/* Check Indx Table Entry offset is not
-	   beyond allocated size */
-	tmp = init->index_offset +
-		(INDX_TBL_ENTRY_SIZE * (init->table_entries + 1));
-	if (tmp > ipa_ctx->nat_mem.size) {
-		IPAERR("Indx Table rules offset not valid\n");
-		IPAERR("offset:%d entrys:%d size:%zu mem_size:%zu\n",
-			init->index_offset, (init->table_entries + 1),
-			tmp, ipa_ctx->nat_mem.size);
-		return -EPERM;
-	}
-
-  /* check for integer overflow */
-	if (init->index_expn_offset >
-		UINT_MAX - (INDX_TBL_ENTRY_SIZE * init->expn_table_entries)) {
-			IPAERR("Detected overflow\n");
-			return -EPERM;
-	}
-	/* Check Expn Table entry offset is not
-	   beyond allocated size */
-	tmp = init->index_expn_offset +
-		(INDX_TBL_ENTRY_SIZE * init->expn_table_entries);
-	if (tmp > ipa_ctx->nat_mem.size) {
-		IPAERR("Indx Expn Table rules offset not valid\n");
-		IPAERR("offset:%d entrys:%d size:%zu mem_size:%zu\n",
-			init->index_expn_offset, init->expn_table_entries,
-			tmp, ipa_ctx->nat_mem.size);
-		return -EPERM;
-	}
-
-	memset(&desc, 0, sizeof(desc));
-	/* NO-OP IC for ensuring that IPA pipeline is empty */
-	reg_write_nop = kzalloc(sizeof(*reg_write_nop), GFP_KERNEL);
-	if (!reg_write_nop) {
-		IPAERR("no mem\n");
-		result = -ENOMEM;
-		goto bail;
-	}
-
-	reg_write_nop->skip_pipeline_clear = 0;
-	reg_write_nop->value_mask = 0x0;
-
-	desc[0].opcode = IPA_REGISTER_WRITE;
-	desc[0].type = IPA_IMM_CMD_DESC;
-	desc[0].callback = NULL;
-	desc[0].user1 = NULL;
-	desc[0].user2 = 0;
-	desc[0].pyld = (void *)reg_write_nop;
-	desc[0].len = sizeof(*reg_write_nop);
-
-	cmd = kmalloc(size, GFP_KERNEL);
-	if (!cmd) {
-		IPAERR("Failed to alloc immediate command object\n");
-		result = -ENOMEM;
-		goto free_nop;
-	}
-	if (ipa_ctx->nat_mem.vaddr) {
-		IPADBG("using system memory for nat table\n");
-		cmd->ipv4_rules_addr_type = IPA_NAT_SYSTEM_MEMORY;
-		cmd->ipv4_expansion_rules_addr_type = IPA_NAT_SYSTEM_MEMORY;
-		cmd->index_table_addr_type = IPA_NAT_SYSTEM_MEMORY;
-		cmd->index_table_expansion_addr_type = IPA_NAT_SYSTEM_MEMORY;
-
-		offset = UINT_MAX - ipa_ctx->nat_mem.dma_handle;
-
-		if ((init->ipv4_rules_offset > offset) ||
-				(init->expn_rules_offset > offset) ||
-				(init->index_offset > offset) ||
-				(init->index_expn_offset > offset)) {
-			IPAERR("Failed due to integer overflow\n");
-			IPAERR("nat.mem.dma_handle: 0x%pa\n",
-				&ipa_ctx->nat_mem.dma_handle);
-			IPAERR("ipv4_rules_offset: 0x%x\n",
-				init->ipv4_rules_offset);
-			IPAERR("expn_rules_offset: 0x%x\n",
-				init->expn_rules_offset);
-			IPAERR("index_offset: 0x%x\n",
-				init->index_offset);
-			IPAERR("index_expn_offset: 0x%x\n",
-				init->index_expn_offset);
-			result = -EPERM;
-			goto free_mem;
-		}
-		cmd->ipv4_rules_addr =
-			ipa_ctx->nat_mem.dma_handle + init->ipv4_rules_offset;
-		IPADBG("ipv4_rules_offset:0x%x\n", init->ipv4_rules_offset);
-
-		cmd->ipv4_expansion_rules_addr =
-		   ipa_ctx->nat_mem.dma_handle + init->expn_rules_offset;
-		IPADBG("expn_rules_offset:0x%x\n", init->expn_rules_offset);
-
-		cmd->index_table_addr =
-			ipa_ctx->nat_mem.dma_handle + init->index_offset;
-		IPADBG("index_offset:0x%x\n", init->index_offset);
-
-		cmd->index_table_expansion_addr =
-		   ipa_ctx->nat_mem.dma_handle + init->index_expn_offset;
-		IPADBG("index_expn_offset:0x%x\n", init->index_expn_offset);
-	} else {
-		IPADBG("using shared(local) memory for nat table\n");
-		cmd->ipv4_rules_addr_type = IPA_NAT_SHARED_MEMORY;
-		cmd->ipv4_expansion_rules_addr_type = IPA_NAT_SHARED_MEMORY;
-		cmd->index_table_addr_type = IPA_NAT_SHARED_MEMORY;
-		cmd->index_table_expansion_addr_type = IPA_NAT_SHARED_MEMORY;
-
-		cmd->ipv4_rules_addr = init->ipv4_rules_offset +
-				IPA_RAM_NAT_OFST;
-
-		cmd->ipv4_expansion_rules_addr = init->expn_rules_offset +
-				IPA_RAM_NAT_OFST;
-
-		cmd->index_table_addr = init->index_offset  +
-				IPA_RAM_NAT_OFST;
-
-		cmd->index_table_expansion_addr = init->index_expn_offset +
-				IPA_RAM_NAT_OFST;
-	}
-	cmd->table_index = init->tbl_index;
-	IPADBG("Table index:0x%x\n", cmd->table_index);
-	cmd->size_base_tables = init->table_entries;
-	IPADBG("Base Table size:0x%x\n", cmd->size_base_tables);
-	cmd->size_expansion_tables = init->expn_table_entries;
-	IPADBG("Expansion Table size:0x%x\n", cmd->size_expansion_tables);
-	cmd->public_ip_addr = init->ip_addr;
-	IPADBG("Public ip address:0x%x\n", cmd->public_ip_addr);
-	desc[1].opcode = IPA_IP_V4_NAT_INIT;
-	desc[1].type = IPA_IMM_CMD_DESC;
-	desc[1].callback = NULL;
-	desc[1].user1 = NULL;
-	desc[1].user2 = 0;
-	desc[1].pyld = (void *)cmd;
-	desc[1].len = size;
-	IPADBG("posting v4 init command\n");
-	if (ipa_send_cmd(2, desc)) {
-		IPAERR("Fail to send immediate command\n");
-		result = -EPERM;
-		goto free_mem;
-	}
-
-	ipa_ctx->nat_mem.public_ip_addr = init->ip_addr;
-	IPADBG("Table ip address:0x%x", ipa_ctx->nat_mem.public_ip_addr);
-
-	ipa_ctx->nat_mem.ipv4_rules_addr =
-	 (char *)ipa_ctx->nat_mem.nat_base_address + init->ipv4_rules_offset;
-	IPADBG("ipv4_rules_addr: 0x%p\n",
-				 ipa_ctx->nat_mem.ipv4_rules_addr);
-
-	ipa_ctx->nat_mem.ipv4_expansion_rules_addr =
-	 (char *)ipa_ctx->nat_mem.nat_base_address + init->expn_rules_offset;
-	IPADBG("ipv4_expansion_rules_addr: 0x%p\n",
-				 ipa_ctx->nat_mem.ipv4_expansion_rules_addr);
-
-	ipa_ctx->nat_mem.index_table_addr =
-		 (char *)ipa_ctx->nat_mem.nat_base_address + init->index_offset;
-	IPADBG("index_table_addr: 0x%p\n",
-				 ipa_ctx->nat_mem.index_table_addr);
-
-	ipa_ctx->nat_mem.index_table_expansion_addr =
-	 (char *)ipa_ctx->nat_mem.nat_base_address + init->index_expn_offset;
-	IPADBG("index_table_expansion_addr: 0x%p\n",
-				 ipa_ctx->nat_mem.index_table_expansion_addr);
-
-	IPADBG("size_base_tables: %d\n", init->table_entries);
-	ipa_ctx->nat_mem.size_base_tables  = init->table_entries;
-
-	IPADBG("size_expansion_tables: %d\n", init->expn_table_entries);
-	ipa_ctx->nat_mem.size_expansion_tables = init->expn_table_entries;
-
-	IPADBG("return\n");
-	result = 0;
-free_mem:
-	kfree(cmd);
-free_nop:
-	kfree(reg_write_nop);
-bail:
-	return result;
-}
-
-/**
- * ipa_nat_dma_cmd() - Post NAT_DMA command to IPA HW
- * @dma:	[in] initialization command attributes
- *
- * Called by NAT client driver to post NAT_DMA command to IPA HW
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_nat_dma_cmd(struct ipa_ioc_nat_dma_cmd *dma)
-{
-#define NUM_OF_DESC 2
-
-	struct ipa_register_write *reg_write_nop = NULL;
-	struct ipa_nat_dma *cmd = NULL;
-	struct ipa_desc *desc = NULL;
-	u16 size = 0, cnt = 0;
-	int ret = 0;
-
-	IPADBG("\n");
-	if (dma->entries <= 0) {
-		IPAERR("Invalid number of commands %d\n",
-			dma->entries);
-		ret = -EPERM;
-		goto bail;
-	}
-
-	for (cnt = 0; cnt < dma->entries; cnt++) {
-		if (dma->dma[cnt].table_index >= 1) {
-			IPAERR("Invalid table index %d\n",
-				dma->dma[cnt].table_index);
-			ret = -EPERM;
-			goto bail;
-		}
-
-		switch (dma->dma[cnt].base_addr) {
-		case IPA_NAT_BASE_TBL:
-			if (dma->dma[cnt].offset >=
-				(ipa_ctx->nat_mem.size_base_tables + 1) *
-				NAT_TABLE_ENTRY_SIZE_BYTE) {
-				IPAERR("Invalid offset %d\n",
-					dma->dma[cnt].offset);
-				ret = -EPERM;
-				goto bail;
-			}
-
-			break;
-
-		case IPA_NAT_EXPN_TBL:
-			if (dma->dma[cnt].offset >=
-				ipa_ctx->nat_mem.size_expansion_tables *
-				NAT_TABLE_ENTRY_SIZE_BYTE) {
-				IPAERR("Invalid offset %d\n",
-					dma->dma[cnt].offset);
-				ret = -EPERM;
-				goto bail;
-			}
-
-			break;
-
-		case IPA_NAT_INDX_TBL:
-			if (dma->dma[cnt].offset >=
-				(ipa_ctx->nat_mem.size_base_tables + 1) *
-				NAT_INTEX_TABLE_ENTRY_SIZE_BYTE) {
-				IPAERR("Invalid offset %d\n",
-					dma->dma[cnt].offset);
-				ret = -EPERM;
-				goto bail;
-			}
-
-			break;
-
-		case IPA_NAT_INDEX_EXPN_TBL:
-			if (dma->dma[cnt].offset >=
-				ipa_ctx->nat_mem.size_expansion_tables *
-				NAT_INTEX_TABLE_ENTRY_SIZE_BYTE) {
-				IPAERR("Invalid offset %d\n",
-					dma->dma[cnt].offset);
-				ret = -EPERM;
-				goto bail;
-			}
-
-			break;
-
-		default:
-			IPAERR("Invalid base_addr %d\n",
-				dma->dma[cnt].base_addr);
-			ret = -EPERM;
-			goto bail;
-		}
-	}
-
-	size = sizeof(struct ipa_desc) * NUM_OF_DESC;
-	desc = kzalloc(size, GFP_KERNEL);
-	if (desc == NULL) {
-		IPAERR("Failed to alloc memory\n");
-		ret = -ENOMEM;
-		goto bail;
-	}
-
-	size = sizeof(struct ipa_nat_dma);
-	cmd = kzalloc(size, GFP_KERNEL);
-	if (cmd == NULL) {
-		IPAERR("Failed to alloc memory\n");
-		ret = -ENOMEM;
-		goto bail;
-	}
-
-	/* NO-OP IC for ensuring that IPA pipeline is empty */
-	reg_write_nop = kzalloc(sizeof(*reg_write_nop), GFP_KERNEL);
-	if (!reg_write_nop) {
-		IPAERR("Failed to alloc memory\n");
-		ret = -ENOMEM;
-		goto bail;
-	}
-
-	reg_write_nop->skip_pipeline_clear = 0;
-	reg_write_nop->value_mask = 0x0;
-
-	desc[0].type = IPA_IMM_CMD_DESC;
-	desc[0].opcode = IPA_REGISTER_WRITE;
-	desc[0].callback = NULL;
-	desc[0].user1 = NULL;
-	desc[0].user2 = 0;
-	desc[0].len = sizeof(*reg_write_nop);
-	desc[0].pyld = (void *)reg_write_nop;
-
-	for (cnt = 0; cnt < dma->entries; cnt++) {
-		cmd->table_index = dma->dma[cnt].table_index;
-		cmd->base_addr = dma->dma[cnt].base_addr;
-		cmd->offset = dma->dma[cnt].offset;
-		cmd->data = dma->dma[cnt].data;
-
-		desc[1].type = IPA_IMM_CMD_DESC;
-		desc[1].opcode = IPA_NAT_DMA;
-		desc[1].callback = NULL;
-		desc[1].user1 = NULL;
-		desc[1].user2 = 0;
-		desc[1].len = sizeof(struct ipa_nat_dma);
-		desc[1].pyld = (void *)cmd;
-
-		ret = ipa_send_cmd(NUM_OF_DESC, desc);
-		if (ret == -EPERM)
-			IPAERR("Fail to send immediate command %d\n", cnt);
-	}
-
-bail:
-	if (cmd != NULL)
-		kfree(cmd);
-
-	if (desc != NULL)
-		kfree(desc);
-
-	if (reg_write_nop != NULL)
-		kfree(reg_write_nop);
-
-	return ret;
-}
-
-/**
- * ipa_nat_free_mem_and_device() - free the NAT memory and remove the device
- * @nat_ctx:	[in] the IPA NAT memory to free
- *
- * Called by NAT client driver to free the NAT memory and remove the device
- */
-void ipa_nat_free_mem_and_device(struct ipa_nat_mem *nat_ctx)
-{
-	IPADBG("\n");
-	mutex_lock(&nat_ctx->lock);
-
-	if (nat_ctx->is_sys_mem) {
-		IPADBG("freeing the dma memory\n");
-		dma_free_coherent(
-			 ipa_ctx->pdev, nat_ctx->size,
-			 nat_ctx->vaddr, nat_ctx->dma_handle);
-		nat_ctx->size = 0;
-		nat_ctx->vaddr = NULL;
-	}
-	nat_ctx->is_mapped = false;
-	nat_ctx->is_sys_mem = false;
-	nat_ctx->is_dev_init = false;
-
-	mutex_unlock(&nat_ctx->lock);
-	IPADBG("return\n");
-	return;
-}
-
-/**
- * ipa_nat_del_cmd() - Delete a NAT table
- * @del:	[in] delete table table table parameters
- *
- * Called by NAT client driver to delete the nat table
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_nat_del_cmd(struct ipa_ioc_v4_nat_del *del)
-{
-	struct ipa_register_write *reg_write_nop;
-	struct ipa_desc desc[2];
-	struct ipa_ip_v4_nat_init *cmd;
-	u16 size = sizeof(struct ipa_ip_v4_nat_init);
-	u8 mem_type = IPA_NAT_SHARED_MEMORY;
-	u32 base_addr = IPA_NAT_PHYS_MEM_OFFSET;
-	int result;
-
-	IPADBG("\n");
-	if (ipa_ctx->nat_mem.is_tmp_mem) {
-		IPAERR("using temp memory during nat del\n");
-		mem_type = IPA_NAT_SYSTEM_MEMORY;
-		base_addr = ipa_ctx->nat_mem.tmp_dma_handle;
-	}
-
-	if (del->public_ip_addr == 0) {
-		IPADBG("Bad Parameter\n");
-		result = -EPERM;
-		goto bail;
-	}
-
-	memset(&desc, 0, sizeof(desc));
-	/* NO-OP IC for ensuring that IPA pipeline is empty */
-	reg_write_nop = kzalloc(sizeof(*reg_write_nop), GFP_KERNEL);
-	if (!reg_write_nop) {
-		IPAERR("no mem\n");
-		result = -ENOMEM;
-		goto bail;
-	}
-
-	reg_write_nop->skip_pipeline_clear = 0;
-	reg_write_nop->value_mask = 0x0;
-
-	desc[0].opcode = IPA_REGISTER_WRITE;
-	desc[0].type = IPA_IMM_CMD_DESC;
-	desc[0].callback = NULL;
-	desc[0].user1 = NULL;
-	desc[0].user2 = 0;
-	desc[0].pyld = (void *)reg_write_nop;
-	desc[0].len = sizeof(*reg_write_nop);
-
-	cmd = kmalloc(size, GFP_KERNEL);
-	if (cmd == NULL) {
-		IPAERR("Failed to alloc immediate command object\n");
-		result = -ENOMEM;
-		goto free_nop;
-	}
-	cmd->table_index = del->table_index;
-	cmd->ipv4_rules_addr = base_addr;
-	cmd->ipv4_rules_addr_type = mem_type;
-	cmd->ipv4_expansion_rules_addr = base_addr;
-	cmd->ipv4_expansion_rules_addr_type = mem_type;
-	cmd->index_table_addr = base_addr;
-	cmd->index_table_addr_type = mem_type;
-	cmd->index_table_expansion_addr = base_addr;
-	cmd->index_table_expansion_addr_type = mem_type;
-	cmd->size_base_tables = 0;
-	cmd->size_expansion_tables = 0;
-	cmd->public_ip_addr = 0;
-
-	desc[1].opcode = IPA_IP_V4_NAT_INIT;
-	desc[1].type = IPA_IMM_CMD_DESC;
-	desc[1].callback = NULL;
-	desc[1].user1 = NULL;
-	desc[1].user2 = 0;
-	desc[1].pyld = (void *)cmd;
-	desc[1].len = size;
-	if (ipa_send_cmd(2, desc)) {
-		IPAERR("Fail to send immediate command\n");
-		result = -EPERM;
-		goto free_mem;
-	}
-
-	ipa_ctx->nat_mem.size_base_tables = 0;
-	ipa_ctx->nat_mem.size_expansion_tables = 0;
-	ipa_ctx->nat_mem.public_ip_addr = 0;
-	ipa_ctx->nat_mem.ipv4_rules_addr = 0;
-	ipa_ctx->nat_mem.ipv4_expansion_rules_addr = 0;
-	ipa_ctx->nat_mem.index_table_addr = 0;
-	ipa_ctx->nat_mem.index_table_expansion_addr = 0;
-
-	ipa_nat_free_mem_and_device(&ipa_ctx->nat_mem);
-	IPADBG("return\n");
-	result = 0;
-free_mem:
-	kfree(cmd);
-free_nop:
-	kfree(reg_write_nop);
-bail:
-	return result;
-}
diff --git a/drivers/platform/msm/ipa/ipa_qmi_service.c b/drivers/platform/msm/ipa/ipa_qmi_service.c
deleted file mode 100644
index d95b442..00000000
--- a/drivers/platform/msm/ipa/ipa_qmi_service.c
+++ /dev/null
@@ -1,1210 +0,0 @@
-/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-
-#include <linux/module.h>
-#include <linux/slab.h>
-#include <linux/errno.h>
-#include <linux/delay.h>
-#include <linux/debugfs.h>
-#include <linux/qmi_encdec.h>
-#include <linux/delay.h>
-#include <linux/uaccess.h>
-#include <soc/qcom/subsystem_restart.h>
-#include <linux/ipa.h>
-#include <linux/vmalloc.h>
-
-#include "ipa_qmi_service.h"
-#include "ipa_ram_mmap.h"
-
-#define IPA_Q6_SVC_VERS 1
-#define IPA_A5_SVC_VERS 1
-#define Q6_QMI_COMPLETION_TIMEOUT (60*HZ)
-
-#define IPA_A5_SERVICE_SVC_ID 0x31
-#define IPA_A5_SERVICE_INS_ID 1
-#define IPA_Q6_SERVICE_SVC_ID 0x31
-#define IPA_Q6_SERVICE_INS_ID 2
-
-#define QMI_SEND_STATS_REQ_TIMEOUT_MS 5000
-#define QMI_SEND_REQ_TIMEOUT_MS 60000
-
-static struct qmi_handle *ipa_svc_handle;
-static void ipa_a5_svc_recv_msg(struct work_struct *work);
-static DECLARE_DELAYED_WORK(work_recv_msg, ipa_a5_svc_recv_msg);
-static struct workqueue_struct *ipa_svc_workqueue;
-static struct workqueue_struct *ipa_clnt_req_workqueue;
-static struct workqueue_struct *ipa_clnt_resp_workqueue;
-static void *curr_conn;
-static bool qmi_modem_init_fin, qmi_indication_fin;
-static struct work_struct ipa_qmi_service_init_work;
-static uint32_t ipa_wan_platform;
-struct ipa_qmi_context *ipa_qmi_ctx;
-static atomic_t workqueues_stopped;
-static atomic_t ipa_qmi_initialized;
-struct mutex ipa_qmi_lock;
-static bool first_time_handshake;
-
-/* QMI A5 service */
-
-static struct msg_desc ipa_indication_reg_req_desc = {
-	.max_msg_len = QMI_IPA_INDICATION_REGISTER_REQ_MAX_MSG_LEN_V01,
-	.msg_id = QMI_IPA_INDICATION_REGISTER_REQ_V01,
-	.ei_array = ipa_indication_reg_req_msg_data_v01_ei,
-};
-static struct msg_desc ipa_indication_reg_resp_desc = {
-	.max_msg_len = QMI_IPA_INDICATION_REGISTER_RESP_MAX_MSG_LEN_V01,
-	.msg_id = QMI_IPA_INDICATION_REGISTER_RESP_V01,
-	.ei_array = ipa_indication_reg_resp_msg_data_v01_ei,
-};
-static struct msg_desc ipa_master_driver_complete_indication_desc = {
-	.max_msg_len = QMI_IPA_MASTER_DRIVER_INIT_COMPLETE_IND_MAX_MSG_LEN_V01,
-	.msg_id = QMI_IPA_MASTER_DRIVER_INIT_COMPLETE_IND_V01,
-	.ei_array = ipa_master_driver_init_complt_ind_msg_data_v01_ei,
-};
-static struct msg_desc ipa_install_fltr_rule_req_desc = {
-	.max_msg_len = QMI_IPA_INSTALL_FILTER_RULE_REQ_MAX_MSG_LEN_V01,
-	.msg_id = QMI_IPA_INSTALL_FILTER_RULE_REQ_V01,
-	.ei_array = ipa_install_fltr_rule_req_msg_data_v01_ei,
-};
-static struct msg_desc ipa_install_fltr_rule_resp_desc = {
-	.max_msg_len = QMI_IPA_INSTALL_FILTER_RULE_RESP_MAX_MSG_LEN_V01,
-	.msg_id = QMI_IPA_INSTALL_FILTER_RULE_RESP_V01,
-	.ei_array = ipa_install_fltr_rule_resp_msg_data_v01_ei,
-};
-static struct msg_desc ipa_filter_installed_notif_req_desc = {
-	.max_msg_len = QMI_IPA_FILTER_INSTALLED_NOTIF_REQ_MAX_MSG_LEN_V01,
-	.msg_id = QMI_IPA_FILTER_INSTALLED_NOTIF_REQ_V01,
-	.ei_array = ipa_fltr_installed_notif_req_msg_data_v01_ei,
-};
-static struct msg_desc ipa_filter_installed_notif_resp_desc = {
-	.max_msg_len = QMI_IPA_FILTER_INSTALLED_NOTIF_RESP_MAX_MSG_LEN_V01,
-	.msg_id = QMI_IPA_FILTER_INSTALLED_NOTIF_RESP_V01,
-	.ei_array = ipa_fltr_installed_notif_resp_msg_data_v01_ei,
-};
-static struct msg_desc ipa_config_req_desc = {
-	.max_msg_len = QMI_IPA_CONFIG_REQ_MAX_MSG_LEN_V01,
-	.msg_id = QMI_IPA_CONFIG_REQ_V01,
-	.ei_array = ipa_config_req_msg_data_v01_ei,
-};
-static struct msg_desc ipa_config_resp_desc = {
-	.max_msg_len = QMI_IPA_CONFIG_RESP_MAX_MSG_LEN_V01,
-	.msg_id = QMI_IPA_CONFIG_RESP_V01,
-	.ei_array = ipa_config_resp_msg_data_v01_ei,
-};
-
-static int handle_indication_req(void *req_h, void *req)
-{
-	struct ipa_indication_reg_req_msg_v01 *indication_req;
-	struct ipa_indication_reg_resp_msg_v01 resp;
-	struct ipa_master_driver_init_complt_ind_msg_v01 ind;
-	int rc;
-
-	indication_req = (struct ipa_indication_reg_req_msg_v01 *)req;
-	IPAWANDBG("Received INDICATION Request\n");
-
-	memset(&resp, 0, sizeof(struct ipa_indication_reg_resp_msg_v01));
-	resp.resp.result = IPA_QMI_RESULT_SUCCESS_V01;
-	rc = qmi_send_resp_from_cb(ipa_svc_handle, curr_conn, req_h,
-			&ipa_indication_reg_resp_desc, &resp, sizeof(resp));
-	qmi_indication_fin = true;
-	/* check if need sending indication to modem */
-	if (qmi_modem_init_fin)	{
-		IPAWANDBG("send indication to modem (%d)\n",
-		qmi_modem_init_fin);
-		memset(&ind, 0, sizeof(struct
-				ipa_master_driver_init_complt_ind_msg_v01));
-		ind.master_driver_init_status.result =
-			IPA_QMI_RESULT_SUCCESS_V01;
-		rc = qmi_send_ind_from_cb(ipa_svc_handle, curr_conn,
-			&ipa_master_driver_complete_indication_desc,
-			&ind,
-			sizeof(ind));
-	} else {
-		IPAWANERR("not send indication\n");
-	}
-	return rc;
-}
-
-
-static int handle_install_filter_rule_req(void *req_h, void *req)
-{
-	struct ipa_install_fltr_rule_req_msg_v01 *rule_req;
-	struct ipa_install_fltr_rule_resp_msg_v01 resp;
-	uint32_t rule_hdl[MAX_NUM_Q6_RULE];
-	int rc = 0, i;
-
-	rule_req = (struct ipa_install_fltr_rule_req_msg_v01 *)req;
-	memset(rule_hdl, 0, sizeof(rule_hdl));
-	memset(&resp, 0, sizeof(struct ipa_install_fltr_rule_resp_msg_v01));
-	IPAWANDBG("Received install filter Request\n");
-
-	rc = copy_ul_filter_rule_to_ipa((struct
-		ipa_install_fltr_rule_req_msg_v01*)req, rule_hdl);
-	if (rc)
-		IPAWANERR("copy UL rules from modem is failed\n");
-
-	resp.resp.result = IPA_QMI_RESULT_SUCCESS_V01;
-	if (rule_req->filter_spec_list_valid == true) {
-		resp.filter_handle_list_valid = true;
-		if (rule_req->filter_spec_list_len > MAX_NUM_Q6_RULE) {
-			resp.filter_handle_list_len = MAX_NUM_Q6_RULE;
-			IPAWANERR("installed (%d) max Q6-UL rules ",
-			MAX_NUM_Q6_RULE);
-			IPAWANERR("but modem gives total (%d)\n",
-			rule_req->filter_spec_list_len);
-		} else {
-			resp.filter_handle_list_len =
-				rule_req->filter_spec_list_len;
-		}
-	} else {
-		resp.filter_handle_list_valid = false;
-	}
-
-	/* construct UL filter rules response to Modem*/
-	for (i = 0; i < resp.filter_handle_list_len; i++) {
-		resp.filter_handle_list[i].filter_spec_identifier =
-			rule_req->filter_spec_list[i].filter_spec_identifier;
-		resp.filter_handle_list[i].filter_handle = rule_hdl[i];
-	}
-
-	rc = qmi_send_resp_from_cb(ipa_svc_handle, curr_conn, req_h,
-			&ipa_install_fltr_rule_resp_desc, &resp, sizeof(resp));
-
-	IPAWANDBG("Replied to install filter request\n");
-	return rc;
-}
-
-static int handle_filter_installed_notify_req(void *req_h, void *req)
-{
-	struct ipa_fltr_installed_notif_resp_msg_v01 resp;
-	int rc = 0;
-
-	memset(&resp, 0, sizeof(struct ipa_fltr_installed_notif_resp_msg_v01));
-	IPAWANDBG("Received filter_install_notify Request\n");
-	resp.resp.result = IPA_QMI_RESULT_SUCCESS_V01;
-
-	rc = qmi_send_resp_from_cb(ipa_svc_handle, curr_conn, req_h,
-			&ipa_filter_installed_notif_resp_desc,
-			&resp, sizeof(resp));
-
-	IPAWANDBG("Responsed filter_install_notify Request\n");
-	return rc;
-}
-
-static int handle_ipa_config_req(void *req_h, void *req)
-{
-	struct ipa_config_resp_msg_v01 resp;
-	int rc;
-
-	memset(&resp, 0, sizeof(struct ipa_config_resp_msg_v01));
-	resp.resp.result = IPA_QMI_RESULT_SUCCESS_V01;
-	IPAWANDBG("Received IPA CONFIG Request\n");
-	rc = ipa_mhi_handle_ipa_config_req(
-		(struct ipa_config_req_msg_v01 *)req);
-	if (rc) {
-		IPAERR("ipa_mhi_handle_ipa_config_req failed %d\n", rc);
-		resp.resp.result = IPA_QMI_RESULT_FAILURE_V01;
-	}
-	rc = qmi_send_resp_from_cb(ipa_svc_handle, curr_conn, req_h,
-		&ipa_config_resp_desc,
-		&resp, sizeof(resp));
-	IPAWANDBG("Responsed IPA CONFIG Request\n");
-	return rc;
-}
-
-static int ipa_a5_svc_connect_cb(struct qmi_handle *handle,
-			       void *conn_h)
-{
-	if (ipa_svc_handle != handle || !conn_h)
-		return -EINVAL;
-
-	if (curr_conn) {
-		IPAWANERR("Service is busy\n");
-		return -ECONNREFUSED;
-	}
-	curr_conn = conn_h;
-	return 0;
-}
-
-static int ipa_a5_svc_disconnect_cb(struct qmi_handle *handle,
-				  void *conn_h)
-{
-	if (ipa_svc_handle != handle || curr_conn != conn_h)
-		return -EINVAL;
-
-	curr_conn = NULL;
-	return 0;
-}
-
-static int ipa_a5_svc_req_desc_cb(unsigned int msg_id,
-				struct msg_desc **req_desc)
-{
-	int rc;
-	switch (msg_id) {
-	case QMI_IPA_INDICATION_REGISTER_REQ_V01:
-		*req_desc = &ipa_indication_reg_req_desc;
-		rc = sizeof(struct ipa_indication_reg_req_msg_v01);
-		break;
-
-	case QMI_IPA_INSTALL_FILTER_RULE_REQ_V01:
-		*req_desc = &ipa_install_fltr_rule_req_desc;
-		rc = sizeof(struct ipa_install_fltr_rule_req_msg_v01);
-		break;
-	case QMI_IPA_FILTER_INSTALLED_NOTIF_REQ_V01:
-		*req_desc = &ipa_filter_installed_notif_req_desc;
-		rc = sizeof(struct ipa_fltr_installed_notif_req_msg_v01);
-		break;
-	case QMI_IPA_CONFIG_REQ_V01:
-		*req_desc = &ipa_config_req_desc;
-		rc = sizeof(struct ipa_config_req_msg_v01);
-		break;
-	default:
-		rc = -ENOTSUPP;
-		break;
-	}
-	return rc;
-}
-
-static int ipa_a5_svc_req_cb(struct qmi_handle *handle, void *conn_h,
-			void *req_h, unsigned int msg_id, void *req)
-{
-	int rc;
-	if (ipa_svc_handle != handle || curr_conn != conn_h)
-		return -EINVAL;
-
-	switch (msg_id) {
-	case QMI_IPA_INDICATION_REGISTER_REQ_V01:
-		rc = handle_indication_req(req_h, req);
-		break;
-	case QMI_IPA_INSTALL_FILTER_RULE_REQ_V01:
-		rc = handle_install_filter_rule_req(req_h, req);
-		rc = wwan_update_mux_channel_prop();
-		break;
-	case QMI_IPA_FILTER_INSTALLED_NOTIF_REQ_V01:
-		rc = handle_filter_installed_notify_req(req_h, req);
-		break;
-	case QMI_IPA_CONFIG_REQ_V01:
-		rc = handle_ipa_config_req(req_h, req);
-		break;
-	default:
-		rc = -ENOTSUPP;
-		break;
-	}
-	return rc;
-}
-
-static void ipa_a5_svc_recv_msg(struct work_struct *work)
-{
-	int rc;
-
-	do {
-		IPAWANDBG("Notified about a Receive Event");
-		rc = qmi_recv_msg(ipa_svc_handle);
-	} while (rc == 0);
-	if (rc != -ENOMSG)
-		IPAWANERR("Error receiving message\n");
-}
-
-static void qmi_ipa_a5_svc_ntfy(struct qmi_handle *handle,
-		enum qmi_event_type event, void *priv)
-{
-	switch (event) {
-	case QMI_RECV_MSG:
-		if (!atomic_read(&workqueues_stopped))
-			queue_delayed_work(ipa_svc_workqueue,
-					   &work_recv_msg, 0);
-		break;
-	default:
-		break;
-	}
-}
-
-static struct qmi_svc_ops_options ipa_a5_svc_ops_options = {
-	.version = 1,
-	.service_id = IPA_A5_SERVICE_SVC_ID,
-	.service_vers = IPA_A5_SVC_VERS,
-	.service_ins = IPA_A5_SERVICE_INS_ID,
-	.connect_cb = ipa_a5_svc_connect_cb,
-	.disconnect_cb = ipa_a5_svc_disconnect_cb,
-	.req_desc_cb = ipa_a5_svc_req_desc_cb,
-	.req_cb = ipa_a5_svc_req_cb,
-};
-
-
-/****************************************************/
-/*                 QMI A5 client ->Q6               */
-/****************************************************/
-static void ipa_q6_clnt_recv_msg(struct work_struct *work);
-static DECLARE_DELAYED_WORK(work_recv_msg_client, ipa_q6_clnt_recv_msg);
-static void ipa_q6_clnt_svc_arrive(struct work_struct *work);
-static DECLARE_DELAYED_WORK(work_svc_arrive, ipa_q6_clnt_svc_arrive);
-static void ipa_q6_clnt_svc_exit(struct work_struct *work);
-static DECLARE_DELAYED_WORK(work_svc_exit, ipa_q6_clnt_svc_exit);
-/* Test client port for IPC Router */
-static struct qmi_handle *ipa_q6_clnt;
-static int ipa_q6_clnt_reset;
-
-static int ipa_check_qmi_response(int rc,
-				  int req_id,
-				  enum ipa_qmi_result_type_v01 result,
-				  enum ipa_qmi_error_type_v01 error,
-				  char *resp_type)
-{
-	if (rc < 0) {
-		if (rc == -ETIMEDOUT && ipa_rmnet_ctx.ipa_rmnet_ssr) {
-			IPAWANERR(
-			"Timeout for qmi request id %d\n", req_id);
-			return rc;
-		}
-		if (rc == -ENETRESET) {
-			IPAWANERR(
-			"SSR while waiting for qmi request id %d\n", req_id);
-			return rc;
-		}
-		IPAWANERR("Error sending qmi request id %d, rc = %d\n",
-			req_id, rc);
-		return rc;
-	}
-	if (result != IPA_QMI_RESULT_SUCCESS_V01 &&
-	    ipa_rmnet_ctx.ipa_rmnet_ssr) {
-		IPAWANERR(
-		"Got bad response %d from request id %d (error %d)\n",
-		req_id, result, error);
-		return result;
-	}
-	IPAWANDBG("Received %s successfully\n", resp_type);
-	return 0;
-}
-
-static int qmi_init_modem_send_sync_msg(void)
-{
-	struct ipa_init_modem_driver_req_msg_v01 req;
-	struct ipa_init_modem_driver_resp_msg_v01 resp;
-	struct msg_desc req_desc, resp_desc;
-	int rc;
-	u16 smem_restr_bytes = ipa_get_smem_restr_bytes();
-
-	memset(&req, 0, sizeof(struct ipa_init_modem_driver_req_msg_v01));
-	memset(&resp, 0, sizeof(struct ipa_init_modem_driver_resp_msg_v01));
-
-	req.platform_type_valid = true;
-	req.platform_type = ipa_wan_platform;
-
-	req.hdr_tbl_info_valid = (IPA_MEM_PART(modem_hdr_size) != 0);
-	req.hdr_tbl_info.modem_offset_start =
-		IPA_MEM_PART(modem_hdr_ofst) + smem_restr_bytes;
-	req.hdr_tbl_info.modem_offset_end = IPA_MEM_PART(modem_hdr_ofst) +
-		smem_restr_bytes + IPA_MEM_PART(modem_hdr_size) - 1;
-
-	req.v4_route_tbl_info_valid = true;
-	req.v4_route_tbl_info.route_tbl_start_addr = IPA_MEM_PART(v4_rt_ofst) +
-		smem_restr_bytes;
-	req.v4_route_tbl_info.num_indices = IPA_MEM_PART(v4_modem_rt_index_hi);
-	req.v6_route_tbl_info_valid = true;
-
-	req.v6_route_tbl_info.route_tbl_start_addr = IPA_MEM_PART(v6_rt_ofst) +
-		smem_restr_bytes;
-	req.v6_route_tbl_info.num_indices = IPA_MEM_PART(v6_modem_rt_index_hi);
-
-	req.v4_filter_tbl_start_addr_valid = true;
-	req.v4_filter_tbl_start_addr =
-		IPA_MEM_PART(v4_flt_ofst) + smem_restr_bytes;
-
-	req.v6_filter_tbl_start_addr_valid = true;
-	req.v6_filter_tbl_start_addr =
-		IPA_MEM_PART(v6_flt_ofst) + smem_restr_bytes;
-
-	req.modem_mem_info_valid = (IPA_MEM_PART(modem_size) != 0);
-	req.modem_mem_info.block_start_addr =
-		IPA_MEM_PART(modem_ofst) + smem_restr_bytes;
-	req.modem_mem_info.size = IPA_MEM_PART(modem_size);
-
-	req.ctrl_comm_dest_end_pt_valid = true;
-	req.ctrl_comm_dest_end_pt =
-		ipa_get_ep_mapping(IPA_CLIENT_APPS_WAN_CONS);
-
-	req.hdr_proc_ctx_tbl_info_valid =
-		(IPA_MEM_PART(modem_hdr_proc_ctx_size) != 0);
-	req.hdr_proc_ctx_tbl_info.modem_offset_start =
-		IPA_MEM_PART(modem_hdr_proc_ctx_ofst) + smem_restr_bytes;
-	req.hdr_proc_ctx_tbl_info.modem_offset_end =
-		IPA_MEM_PART(modem_hdr_proc_ctx_ofst) +
-		IPA_MEM_PART(modem_hdr_proc_ctx_size) + smem_restr_bytes - 1;
-
-	req.zip_tbl_info_valid = (IPA_MEM_PART(modem_comp_decomp_size) != 0);
-	req.zip_tbl_info.modem_offset_start =
-		IPA_MEM_PART(modem_comp_decomp_size) + smem_restr_bytes;
-	req.zip_tbl_info.modem_offset_end =
-		IPA_MEM_PART(modem_comp_decomp_ofst) +
-		IPA_MEM_PART(modem_comp_decomp_size) + smem_restr_bytes - 1;
-
-	if (!ipa_uc_loaded_check()) {  /* First time boot */
-		req.is_ssr_bootup_valid = false;
-		req.is_ssr_bootup = 0;
-	} else {  /* After SSR boot */
-		req.is_ssr_bootup_valid = true;
-		req.is_ssr_bootup = 1;
-	}
-
-	IPAWANDBG("platform_type %d\n", req.platform_type);
-	IPAWANDBG("hdr_tbl_info.modem_offset_start %d\n",
-			req.hdr_tbl_info.modem_offset_start);
-	IPAWANDBG("hdr_tbl_info.modem_offset_end %d\n",
-			req.hdr_tbl_info.modem_offset_end);
-	IPAWANDBG("v4_route_tbl_info.route_tbl_start_addr %d\n",
-			req.v4_route_tbl_info.route_tbl_start_addr);
-	IPAWANDBG("v4_route_tbl_info.num_indices %d\n",
-			req.v4_route_tbl_info.num_indices);
-	IPAWANDBG("v6_route_tbl_info.route_tbl_start_addr %d\n",
-			req.v6_route_tbl_info.route_tbl_start_addr);
-	IPAWANDBG("v6_route_tbl_info.num_indices %d\n",
-			req.v6_route_tbl_info.num_indices);
-	IPAWANDBG("v4_filter_tbl_start_addr %d\n",
-			req.v4_filter_tbl_start_addr);
-	IPAWANDBG("v6_filter_tbl_start_addr %d\n",
-			req.v6_filter_tbl_start_addr);
-	IPAWANDBG("modem_mem_info.block_start_addr %d\n",
-			req.modem_mem_info.block_start_addr);
-	IPAWANDBG("modem_mem_info.size %d\n",
-			req.modem_mem_info.size);
-	IPAWANDBG("ctrl_comm_dest_end_pt %d\n",
-			req.ctrl_comm_dest_end_pt);
-	IPAWANDBG("is_ssr_bootup %d\n",
-			req.is_ssr_bootup);
-
-	req_desc.max_msg_len = QMI_IPA_INIT_MODEM_DRIVER_REQ_MAX_MSG_LEN_V01;
-	req_desc.msg_id = QMI_IPA_INIT_MODEM_DRIVER_REQ_V01;
-	req_desc.ei_array = ipa_init_modem_driver_req_msg_data_v01_ei;
-
-	resp_desc.max_msg_len = QMI_IPA_INIT_MODEM_DRIVER_RESP_MAX_MSG_LEN_V01;
-	resp_desc.msg_id = QMI_IPA_INIT_MODEM_DRIVER_RESP_V01;
-	resp_desc.ei_array = ipa_init_modem_driver_resp_msg_data_v01_ei;
-
-	pr_info("Sending QMI_IPA_INIT_MODEM_DRIVER_REQ_V01\n");
-	rc = qmi_send_req_wait(ipa_q6_clnt, &req_desc, &req, sizeof(req),
-			&resp_desc, &resp, sizeof(resp),
-			QMI_SEND_REQ_TIMEOUT_MS);
-	pr_info("QMI_IPA_INIT_MODEM_DRIVER_REQ_V01 response received\n");
-	return ipa_check_qmi_response(rc,
-		QMI_IPA_INIT_MODEM_DRIVER_REQ_V01, resp.resp.result,
-		resp.resp.error, "ipa_init_modem_driver_resp_msg_v01");
-}
-
-/* sending filter-install-request to modem*/
-int qmi_filter_request_send(struct ipa_install_fltr_rule_req_msg_v01 *req)
-{
-	struct ipa_install_fltr_rule_resp_msg_v01 resp;
-	struct msg_desc req_desc, resp_desc;
-	int rc;
-
-	/* check if the filter rules from IPACM is valid */
-	if (req->filter_spec_list_len == 0) {
-		IPAWANDBG("IPACM pass zero rules to Q6\n");
-	} else {
-		IPAWANDBG("IPACM pass %u rules to Q6\n",
-		req->filter_spec_list_len);
-	}
-
-	mutex_lock(&ipa_qmi_lock);
-	if (ipa_qmi_ctx != NULL) {
-		/* cache the qmi_filter_request */
-		memcpy(&(ipa_qmi_ctx->ipa_install_fltr_rule_req_msg_cache[
-			ipa_qmi_ctx->num_ipa_install_fltr_rule_req_msg]),
-			req,
-			sizeof(struct ipa_install_fltr_rule_req_msg_v01));
-		ipa_qmi_ctx->num_ipa_install_fltr_rule_req_msg++;
-		ipa_qmi_ctx->num_ipa_install_fltr_rule_req_msg %= 10;
-	}
-	mutex_unlock(&ipa_qmi_lock);
-
-	req_desc.max_msg_len = QMI_IPA_INSTALL_FILTER_RULE_REQ_MAX_MSG_LEN_V01;
-	req_desc.msg_id = QMI_IPA_INSTALL_FILTER_RULE_REQ_V01;
-	req_desc.ei_array = ipa_install_fltr_rule_req_msg_data_v01_ei;
-
-	memset(&resp, 0, sizeof(struct ipa_install_fltr_rule_resp_msg_v01));
-	resp_desc.max_msg_len =
-		QMI_IPA_INSTALL_FILTER_RULE_RESP_MAX_MSG_LEN_V01;
-	resp_desc.msg_id = QMI_IPA_INSTALL_FILTER_RULE_RESP_V01;
-	resp_desc.ei_array = ipa_install_fltr_rule_resp_msg_data_v01_ei;
-
-	rc = qmi_send_req_wait(ipa_q6_clnt, &req_desc,
-			req,
-			sizeof(struct ipa_install_fltr_rule_req_msg_v01),
-			&resp_desc, &resp, sizeof(resp),
-			QMI_SEND_REQ_TIMEOUT_MS);
-	return ipa_check_qmi_response(rc,
-		QMI_IPA_INSTALL_FILTER_RULE_REQ_V01, resp.resp.result,
-		resp.resp.error, "ipa_install_filter");
-}
-
-
-int qmi_enable_force_clear_datapath_send(
-	struct ipa_enable_force_clear_datapath_req_msg_v01 *req)
-{
-	struct ipa_enable_force_clear_datapath_resp_msg_v01 resp;
-	struct msg_desc req_desc, resp_desc;
-	int rc = 0;
-
-
-	if (!req || !req->source_pipe_bitmask) {
-		IPAWANERR("invalid params\n");
-		return -EINVAL;
-	}
-
-	req_desc.max_msg_len =
-	QMI_IPA_ENABLE_FORCE_CLEAR_DATAPATH_REQ_MAX_MSG_LEN_V01;
-	req_desc.msg_id = QMI_IPA_ENABLE_FORCE_CLEAR_DATAPATH_REQ_V01;
-	req_desc.ei_array = ipa_enable_force_clear_datapath_req_msg_data_v01_ei;
-
-	memset(&resp, 0, sizeof(struct ipa_fltr_installed_notif_resp_msg_v01));
-	resp_desc.max_msg_len =
-		QMI_IPA_ENABLE_FORCE_CLEAR_DATAPATH_RESP_MAX_MSG_LEN_V01;
-	resp_desc.msg_id = QMI_IPA_ENABLE_FORCE_CLEAR_DATAPATH_RESP_V01;
-	resp_desc.ei_array =
-		ipa_enable_force_clear_datapath_resp_msg_data_v01_ei;
-
-	rc = qmi_send_req_wait(ipa_q6_clnt,
-			&req_desc,
-			req,
-			sizeof(*req),
-			&resp_desc, &resp, sizeof(resp), 0);
-	if (rc < 0) {
-		IPAWANERR("send req failed %d\n", rc);
-		return rc;
-	}
-	if (resp.resp.result != IPA_QMI_RESULT_SUCCESS_V01) {
-		IPAWANERR("filter_notify failed %d\n",
-			resp.resp.result);
-		return resp.resp.result;
-	}
-	IPAWANDBG("SUCCESS\n");
-	return rc;
-}
-
-int qmi_disable_force_clear_datapath_send(
-	struct ipa_disable_force_clear_datapath_req_msg_v01 *req)
-{
-	struct ipa_disable_force_clear_datapath_resp_msg_v01 resp;
-	struct msg_desc req_desc, resp_desc;
-	int rc = 0;
-
-
-	if (!req) {
-		IPAWANERR("invalid params\n");
-		return -EINVAL;
-	}
-
-	req_desc.max_msg_len =
-		QMI_IPA_DISABLE_FORCE_CLEAR_DATAPATH_REQ_MAX_MSG_LEN_V01;
-	req_desc.msg_id = QMI_IPA_DISABLE_FORCE_CLEAR_DATAPATH_REQ_V01;
-	req_desc.ei_array =
-		ipa_disable_force_clear_datapath_req_msg_data_v01_ei;
-
-	memset(&resp, 0, sizeof(struct ipa_fltr_installed_notif_resp_msg_v01));
-	resp_desc.max_msg_len =
-		QMI_IPA_DISABLE_FORCE_CLEAR_DATAPATH_RESP_MAX_MSG_LEN_V01;
-	resp_desc.msg_id = QMI_IPA_DISABLE_FORCE_CLEAR_DATAPATH_RESP_V01;
-	resp_desc.ei_array =
-		ipa_disable_force_clear_datapath_resp_msg_data_v01_ei;
-
-	rc = qmi_send_req_wait(ipa_q6_clnt,
-			&req_desc,
-			req,
-			sizeof(*req),
-			&resp_desc, &resp, sizeof(resp), 0);
-	if (rc < 0) {
-		IPAWANERR("send req failed %d\n", rc);
-		return rc;
-	}
-	if (resp.resp.result != IPA_QMI_RESULT_SUCCESS_V01) {
-		IPAWANERR("filter_notify failed %d\n",
-			resp.resp.result);
-		return resp.resp.result;
-	}
-	IPAWANDBG("SUCCESS\n");
-	return rc;
-}
-
-/* sending filter-installed-notify-request to modem*/
-int qmi_filter_notify_send(struct ipa_fltr_installed_notif_req_msg_v01 *req)
-{
-	struct ipa_fltr_installed_notif_resp_msg_v01 resp;
-	struct msg_desc req_desc, resp_desc;
-	int rc = 0, i = 0;
-
-	/* check if the filter rules from IPACM is valid */
-	if (req->filter_index_list_len == 0) {
-		IPAWANERR(" delete UL filter rule for pipe %d\n",
-		req->source_pipe_index);
-		return -EINVAL;
-	} else if (req->filter_index_list_len > QMI_IPA_MAX_FILTERS_V01) {
-		IPAWANERR(" UL filter rule for pipe %d exceed max (%u)\n",
-		req->source_pipe_index,
-		req->filter_index_list_len);
-		return -EINVAL;
-	} else if (req->filter_index_list[0].filter_index == 0 &&
-		req->source_pipe_index !=
-		ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_WAN_PROD)) {
-		IPAWANERR(" get index wrong for pipe %d\n",
-			req->source_pipe_index);
-		for (i = 0; i < req->filter_index_list_len; i++)
-			IPAWANERR(" %d-st handle %d index %d\n",
-				i,
-				req->filter_index_list[i].filter_handle,
-				req->filter_index_list[i].filter_index);
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_qmi_lock);
-	if (ipa_qmi_ctx != NULL) {
-		/* cache the qmi_filter_request */
-		memcpy(&(ipa_qmi_ctx->ipa_fltr_installed_notif_req_msg_cache[
-			ipa_qmi_ctx->num_ipa_fltr_installed_notif_req_msg]),
-			req,
-			sizeof(struct ipa_fltr_installed_notif_req_msg_v01));
-		ipa_qmi_ctx->num_ipa_fltr_installed_notif_req_msg++;
-		ipa_qmi_ctx->num_ipa_fltr_installed_notif_req_msg %= 10;
-	}
-	mutex_unlock(&ipa_qmi_lock);
-	req_desc.max_msg_len =
-	QMI_IPA_FILTER_INSTALLED_NOTIF_REQ_MAX_MSG_LEN_V01;
-	req_desc.msg_id = QMI_IPA_FILTER_INSTALLED_NOTIF_REQ_V01;
-	req_desc.ei_array = ipa_fltr_installed_notif_req_msg_data_v01_ei;
-
-	memset(&resp, 0, sizeof(struct ipa_fltr_installed_notif_resp_msg_v01));
-	resp_desc.max_msg_len =
-		QMI_IPA_FILTER_INSTALLED_NOTIF_RESP_MAX_MSG_LEN_V01;
-	resp_desc.msg_id = QMI_IPA_FILTER_INSTALLED_NOTIF_RESP_V01;
-	resp_desc.ei_array = ipa_fltr_installed_notif_resp_msg_data_v01_ei;
-
-	rc = qmi_send_req_wait(ipa_q6_clnt,
-			&req_desc,
-			req,
-			sizeof(struct ipa_fltr_installed_notif_req_msg_v01),
-			&resp_desc, &resp, sizeof(resp),
-			QMI_SEND_REQ_TIMEOUT_MS);
-	return ipa_check_qmi_response(rc,
-		QMI_IPA_FILTER_INSTALLED_NOTIF_REQ_V01, resp.resp.result,
-		resp.resp.error, "ipa_fltr_installed_notif_resp");
-}
-
-static void ipa_q6_clnt_recv_msg(struct work_struct *work)
-{
-	int rc;
-
-	do {
-		IPAWANDBG("Notified about a Receive Event");
-		rc = qmi_recv_msg(ipa_q6_clnt);
-	} while (rc == 0);
-	if (rc != -ENOMSG)
-		IPAWANERR("Error receiving message\n");
-}
-
-static void ipa_q6_clnt_notify(struct qmi_handle *handle,
-			     enum qmi_event_type event, void *notify_priv)
-{
-	switch (event) {
-	case QMI_RECV_MSG:
-		IPAWANDBG("client qmi recv message called");
-		if (!atomic_read(&workqueues_stopped))
-			queue_delayed_work(ipa_clnt_resp_workqueue,
-					   &work_recv_msg_client, 0);
-		break;
-	default:
-		break;
-	}
-}
-
-static void ipa_q6_clnt_ind_cb(struct qmi_handle *handle, unsigned int msg_id,
-			       void *msg, unsigned int msg_len,
-			       void *ind_cb_priv)
-{
-	struct ipa_data_usage_quota_reached_ind_msg_v01 qmi_ind;
-	struct msg_desc qmi_ind_desc;
-	int rc = 0;
-
-	if (handle != ipa_q6_clnt) {
-		IPAWANERR("Wrong client\n");
-		return;
-	}
-
-	if (QMI_IPA_DATA_USAGE_QUOTA_REACHED_IND_V01 == msg_id) {
-		memset(&qmi_ind, 0, sizeof(
-			struct ipa_data_usage_quota_reached_ind_msg_v01));
-		qmi_ind_desc.max_msg_len =
-			QMI_IPA_DATA_USAGE_QUOTA_REACHED_IND_MAX_MSG_LEN_V01;
-		qmi_ind_desc.msg_id = QMI_IPA_DATA_USAGE_QUOTA_REACHED_IND_V01;
-		qmi_ind_desc.ei_array =
-			ipa_data_usage_quota_reached_ind_msg_data_v01_ei;
-
-		rc = qmi_kernel_decode(&qmi_ind_desc, &qmi_ind, msg, msg_len);
-		if (rc < 0) {
-			IPAWANERR("Error decoding msg_id %d\n", msg_id);
-			return;
-		}
-		IPAWANDBG("Quota reached indication on qmux(%d) Mbytes(%lu)\n",
-			  qmi_ind.apn.mux_id,
-			  (long unsigned int) qmi_ind.apn.num_Mbytes);
-		ipa_broadcast_quota_reach_ind(qmi_ind.apn.mux_id);
-	}
-}
-
-static void ipa_q6_clnt_svc_arrive(struct work_struct *work)
-{
-	int rc;
-	struct ipa_master_driver_init_complt_ind_msg_v01 ind;
-
-	/* Create a Local client port for QMI communication */
-	ipa_q6_clnt = qmi_handle_create(ipa_q6_clnt_notify, NULL);
-	if (!ipa_q6_clnt) {
-		IPAWANERR("QMI client handle alloc failed\n");
-		return;
-	}
-
-	IPAWANDBG("Lookup server name, get client-hdl(%p)\n",
-		ipa_q6_clnt);
-	rc = qmi_connect_to_service(ipa_q6_clnt,
-			IPA_Q6_SERVICE_SVC_ID,
-			IPA_Q6_SVC_VERS,
-			IPA_Q6_SERVICE_INS_ID);
-	if (rc < 0) {
-		IPAWANERR("Server not found\n");
-		ipa_q6_clnt_svc_exit(0);
-		return;
-	}
-
-	rc = qmi_register_ind_cb(ipa_q6_clnt, ipa_q6_clnt_ind_cb, NULL);
-	if (rc < 0)
-		IPAWANERR("Unable to register for indications\n");
-
-	ipa_q6_clnt_reset = 0;
-	IPAWANDBG("Q6 QMI service available now\n");
-	/* Initialize modem IPA-driver */
-	IPAWANDBG("send qmi_init_modem_send_sync_msg to modem\n");
-	rc = qmi_init_modem_send_sync_msg();
-	if (rc == -ENETRESET) {
-		IPAWANERR("qmi_init_modem_send_sync_msg failed due to SSR!\n");
-		/* Cleanup will take place when ipa_wwan_remove is called */
-		return;
-	}
-	if (rc != 0) {
-		IPAWANERR("qmi_init_modem_send_sync_msg failed\n");
-		/*
-		 * This is a very unexpected scenario, which requires a kernel
-		 * panic in order to force dumps for QMI/Q6 side analysis.
-		 */
-		BUG();
-		return;
-	}
-	qmi_modem_init_fin = true;
-	/* In cold-bootup, first_time_handshake = false */
-	ipa_q6_handshake_complete(first_time_handshake);
-	first_time_handshake = true;
-	IPAWANDBG("complete, qmi_modem_init_fin : %d\n",
-		qmi_modem_init_fin);
-
-	if (qmi_indication_fin)	{
-		IPAWANDBG("send indication to modem (%d)\n",
-		qmi_indication_fin);
-		memset(&ind, 0, sizeof(struct
-				ipa_master_driver_init_complt_ind_msg_v01));
-		ind.master_driver_init_status.result =
-			IPA_QMI_RESULT_SUCCESS_V01;
-		rc = qmi_send_ind(ipa_svc_handle, curr_conn,
-			&ipa_master_driver_complete_indication_desc,
-			&ind,
-			sizeof(ind));
-		IPAWANDBG("ipa_qmi_service_client good\n");
-	} else {
-		IPAWANERR("not send indication (%d)\n",
-		qmi_indication_fin);
-	}
-
-}
-
-
-static void ipa_q6_clnt_svc_exit(struct work_struct *work)
-{
-	mutex_lock(&ipa_qmi_lock);
-
-	if (ipa_q6_clnt)
-		qmi_handle_destroy(ipa_q6_clnt);
-	ipa_q6_clnt_reset = 1;
-	ipa_q6_clnt = NULL;
-
-	mutex_unlock(&ipa_qmi_lock);
-}
-
-
-static int ipa_q6_clnt_svc_event_notify(struct notifier_block *this,
-				      unsigned long code,
-				      void *_cmd)
-{
-	IPAWANDBG("event %ld\n", code);
-	switch (code) {
-	case QMI_SERVER_ARRIVE:
-		if (!atomic_read(&workqueues_stopped))
-			queue_delayed_work(ipa_clnt_req_workqueue,
-					   &work_svc_arrive, 0);
-		break;
-	case QMI_SERVER_EXIT:
-		if (!atomic_read(&workqueues_stopped))
-			queue_delayed_work(ipa_clnt_req_workqueue,
-					   &work_svc_exit, 0);
-		break;
-	default:
-		break;
-	}
-	return 0;
-}
-
-
-static struct notifier_block ipa_q6_clnt_nb = {
-	.notifier_call = ipa_q6_clnt_svc_event_notify,
-};
-
-static void ipa_qmi_service_init_worker(struct work_struct *work)
-{
-	int rc;
-
-	/* Initialize QMI-service*/
-	IPAWANDBG("IPA A7 QMI init OK :>>>>\n");
-
-	/* start the QMI msg cache */
-	ipa_qmi_ctx = vzalloc(sizeof(*ipa_qmi_ctx));
-	if (!ipa_qmi_ctx) {
-		IPAWANERR(":kzalloc err.\n");
-		return;
-	}
-	ipa_qmi_ctx->modem_cfg_emb_pipe_flt =
-		ipa_get_modem_cfg_emb_pipe_flt();
-
-	ipa_svc_workqueue = create_singlethread_workqueue("ipa_A7_svc");
-	if (!ipa_svc_workqueue) {
-		IPAWANERR("Creating ipa_A7_svc workqueue failed\n");
-		vfree(ipa_qmi_ctx);
-		ipa_qmi_ctx = NULL;
-		return;
-	}
-
-	ipa_svc_handle = qmi_handle_create(qmi_ipa_a5_svc_ntfy, NULL);
-	if (!ipa_svc_handle) {
-		IPAWANERR("Creating ipa_A7_svc qmi handle failed\n");
-		goto destroy_ipa_A7_svc_wq;
-	}
-
-	/*
-	 * Setting the current connection to NULL, as due to a race between
-	 * server and client clean-up in SSR, the disconnect_cb might not
-	 * have necessarily been called
-	 */
-	curr_conn = NULL;
-
-	rc = qmi_svc_register(ipa_svc_handle, &ipa_a5_svc_ops_options);
-	if (rc < 0) {
-		IPAWANERR("Registering ipa_a5 svc failed %d\n",
-				rc);
-		goto destroy_qmi_handle;
-	}
-
-	/* Initialize QMI-client */
-
-	ipa_clnt_req_workqueue = create_singlethread_workqueue("clnt_req");
-	if (!ipa_clnt_req_workqueue) {
-		IPAWANERR("Creating clnt_req workqueue failed\n");
-		goto deregister_qmi_srv;
-	}
-
-	ipa_clnt_resp_workqueue = create_singlethread_workqueue("clnt_resp");
-	if (!ipa_clnt_resp_workqueue) {
-		IPAWANERR("Creating clnt_resp workqueue failed\n");
-		goto destroy_clnt_req_wq;
-	}
-
-	rc = qmi_svc_event_notifier_register(IPA_Q6_SERVICE_SVC_ID,
-				IPA_Q6_SVC_VERS,
-				IPA_Q6_SERVICE_INS_ID, &ipa_q6_clnt_nb);
-	if (rc < 0) {
-		IPAWANERR("notifier register failed\n");
-		goto destroy_clnt_resp_wq;
-	}
-
-	atomic_set(&ipa_qmi_initialized, 1);
-	/* get Q6 service and start send modem-initial to Q6 */
-	IPAWANDBG("wait service available\n");
-	return;
-
-destroy_clnt_resp_wq:
-	destroy_workqueue(ipa_clnt_resp_workqueue);
-	ipa_clnt_resp_workqueue = NULL;
-destroy_clnt_req_wq:
-	destroy_workqueue(ipa_clnt_req_workqueue);
-	ipa_clnt_req_workqueue = NULL;
-deregister_qmi_srv:
-	qmi_svc_unregister(ipa_svc_handle);
-destroy_qmi_handle:
-	qmi_handle_destroy(ipa_svc_handle);
-	ipa_svc_handle = 0;
-destroy_ipa_A7_svc_wq:
-	destroy_workqueue(ipa_svc_workqueue);
-	ipa_svc_workqueue = NULL;
-	vfree(ipa_qmi_ctx);
-	ipa_qmi_ctx = NULL;
-	return;
-}
-
-int ipa_qmi_service_init(uint32_t wan_platform_type)
-{
-	ipa_wan_platform = wan_platform_type;
-	qmi_modem_init_fin = false;
-	qmi_indication_fin = false;
-	atomic_set(&workqueues_stopped, 0);
-
-	if (0 == atomic_read(&ipa_qmi_initialized)) {
-		INIT_WORK(&ipa_qmi_service_init_work,
-			ipa_qmi_service_init_worker);
-		schedule_work(&ipa_qmi_service_init_work);
-	}
-	return 0;
-}
-
-void ipa_qmi_service_exit(void)
-{
-	int ret = 0;
-
-	atomic_set(&workqueues_stopped, 1);
-
-	/* qmi-service */
-	if (ipa_svc_handle) {
-		ret = qmi_svc_unregister(ipa_svc_handle);
-		if (ret < 0)
-			IPAWANERR("unregister qmi handle %p failed, ret=%d\n",
-			ipa_svc_handle, ret);
-	}
-	if (ipa_svc_workqueue) {
-		flush_workqueue(ipa_svc_workqueue);
-		destroy_workqueue(ipa_svc_workqueue);
-		ipa_svc_workqueue = NULL;
-	}
-
-	if (ipa_svc_handle) {
-		ret = qmi_handle_destroy(ipa_svc_handle);
-		if (ret < 0)
-			IPAWANERR("Error destroying qmi handle %p, ret=%d\n",
-			ipa_svc_handle, ret);
-	}
-	ipa_svc_handle = 0;
-
-	/* qmi-client */
-
-	/* Unregister from events */
-	ret = qmi_svc_event_notifier_unregister(IPA_Q6_SERVICE_SVC_ID,
-				IPA_Q6_SVC_VERS,
-				IPA_Q6_SERVICE_INS_ID, &ipa_q6_clnt_nb);
-	if (ret < 0)
-		IPAWANERR(
-		"Error qmi_svc_event_notifier_unregister service %d, ret=%d\n",
-		IPA_Q6_SERVICE_SVC_ID, ret);
-	/* Release client handle */
-	ipa_q6_clnt_svc_exit(0);
-	if (ipa_clnt_req_workqueue) {
-		destroy_workqueue(ipa_clnt_req_workqueue);
-		ipa_clnt_req_workqueue = NULL;
-	}
-	if (ipa_clnt_resp_workqueue) {
-		destroy_workqueue(ipa_clnt_resp_workqueue);
-		ipa_clnt_resp_workqueue = NULL;
-	}
-
-	mutex_lock(&ipa_qmi_lock);
-	/* clean the QMI msg cache */
-	if (ipa_qmi_ctx != NULL) {
-		vfree(ipa_qmi_ctx);
-		ipa_qmi_ctx = NULL;
-	}
-	mutex_unlock(&ipa_qmi_lock);
-	qmi_modem_init_fin = false;
-	qmi_indication_fin = false;
-	atomic_set(&ipa_qmi_initialized, 0);
-}
-
-void ipa_qmi_stop_workqueues(void)
-{
-	IPAWANDBG("Stopping all QMI workqueues\n");
-
-	/* Stopping all workqueues so new work won't be scheduled */
-	atomic_set(&workqueues_stopped, 1);
-
-	/* Making sure that the current scheduled work won't be executed */
-	cancel_delayed_work(&work_recv_msg);
-	cancel_delayed_work(&work_recv_msg_client);
-	cancel_delayed_work(&work_svc_arrive);
-	cancel_delayed_work(&work_svc_exit);
-}
-
-/* voting for bus BW to ipa_rm*/
-int vote_for_bus_bw(uint32_t *bw_mbps)
-{
-	struct ipa_rm_perf_profile profile;
-	int ret;
-
-	if (bw_mbps == NULL) {
-		IPAWANERR("Bus BW is invalid\n");
-		return -EINVAL;
-	}
-
-	memset(&profile, 0, sizeof(profile));
-	profile.max_supported_bandwidth_mbps = *bw_mbps;
-	ret = ipa_rm_set_perf_profile(IPA_RM_RESOURCE_Q6_PROD,
-			&profile);
-	if (ret)
-		IPAWANERR("Failed to set perf profile to BW %u\n",
-			profile.max_supported_bandwidth_mbps);
-	else
-		IPAWANDBG("Succeeded to set perf profile to BW %u\n",
-			profile.max_supported_bandwidth_mbps);
-
-	return ret;
-}
-
-int ipa_qmi_get_data_stats(struct ipa_get_data_stats_req_msg_v01 *req,
-			   struct ipa_get_data_stats_resp_msg_v01 *resp)
-{
-	struct msg_desc req_desc, resp_desc;
-	int rc;
-
-	req_desc.max_msg_len = QMI_IPA_GET_DATA_STATS_REQ_MAX_MSG_LEN_V01;
-	req_desc.msg_id = QMI_IPA_GET_DATA_STATS_REQ_V01;
-	req_desc.ei_array = ipa_get_data_stats_req_msg_data_v01_ei;
-
-	resp_desc.max_msg_len = QMI_IPA_GET_DATA_STATS_RESP_MAX_MSG_LEN_V01;
-	resp_desc.msg_id = QMI_IPA_GET_DATA_STATS_RESP_V01;
-	resp_desc.ei_array = ipa_get_data_stats_resp_msg_data_v01_ei;
-
-	IPAWANDBG("Sending QMI_IPA_GET_DATA_STATS_REQ_V01\n");
-
-	rc = qmi_send_req_wait(ipa_q6_clnt, &req_desc, req,
-			sizeof(struct ipa_get_data_stats_req_msg_v01),
-			&resp_desc, resp,
-			sizeof(struct ipa_get_data_stats_resp_msg_v01),
-			QMI_SEND_STATS_REQ_TIMEOUT_MS);
-
-	IPAWANDBG("QMI_IPA_GET_DATA_STATS_RESP_V01 received\n");
-
-	return ipa_check_qmi_response(rc,
-		QMI_IPA_GET_DATA_STATS_REQ_V01, resp->resp.result,
-		resp->resp.error, "ipa_get_data_stats_resp_msg_v01");
-}
-
-int ipa_qmi_get_network_stats(struct ipa_get_apn_data_stats_req_msg_v01 *req,
-			      struct ipa_get_apn_data_stats_resp_msg_v01 *resp)
-{
-	struct msg_desc req_desc, resp_desc;
-	int rc;
-
-	req_desc.max_msg_len = QMI_IPA_GET_APN_DATA_STATS_REQ_MAX_MSG_LEN_V01;
-	req_desc.msg_id = QMI_IPA_GET_APN_DATA_STATS_REQ_V01;
-	req_desc.ei_array = ipa_get_apn_data_stats_req_msg_data_v01_ei;
-
-	resp_desc.max_msg_len = QMI_IPA_GET_APN_DATA_STATS_RESP_MAX_MSG_LEN_V01;
-	resp_desc.msg_id = QMI_IPA_GET_APN_DATA_STATS_RESP_V01;
-	resp_desc.ei_array = ipa_get_apn_data_stats_resp_msg_data_v01_ei;
-
-	IPAWANDBG("Sending QMI_IPA_GET_APN_DATA_STATS_REQ_V01\n");
-
-	rc = qmi_send_req_wait(ipa_q6_clnt, &req_desc, req,
-			sizeof(struct ipa_get_apn_data_stats_req_msg_v01),
-			&resp_desc, resp,
-			sizeof(struct ipa_get_apn_data_stats_resp_msg_v01),
-			QMI_SEND_STATS_REQ_TIMEOUT_MS);
-
-	IPAWANDBG("QMI_IPA_GET_APN_DATA_STATS_RESP_V01 received\n");
-
-	return ipa_check_qmi_response(rc,
-		QMI_IPA_GET_APN_DATA_STATS_REQ_V01, resp->resp.result,
-		resp->resp.error, "ipa_get_apn_data_stats_req_msg_v01");
-}
-
-int ipa_qmi_set_data_quota(struct ipa_set_data_usage_quota_req_msg_v01 *req)
-{
-	struct ipa_set_data_usage_quota_resp_msg_v01 resp;
-	struct msg_desc req_desc, resp_desc;
-	int rc;
-
-	memset(&resp, 0, sizeof(struct ipa_set_data_usage_quota_resp_msg_v01));
-
-	req_desc.max_msg_len = QMI_IPA_SET_DATA_USAGE_QUOTA_REQ_MAX_MSG_LEN_V01;
-	req_desc.msg_id = QMI_IPA_SET_DATA_USAGE_QUOTA_REQ_V01;
-	req_desc.ei_array = ipa_set_data_usage_quota_req_msg_data_v01_ei;
-
-	resp_desc.max_msg_len =
-		QMI_IPA_SET_DATA_USAGE_QUOTA_RESP_MAX_MSG_LEN_V01;
-	resp_desc.msg_id = QMI_IPA_SET_DATA_USAGE_QUOTA_RESP_V01;
-	resp_desc.ei_array = ipa_set_data_usage_quota_resp_msg_data_v01_ei;
-
-	IPAWANDBG("Sending QMI_IPA_SET_DATA_USAGE_QUOTA_REQ_V01\n");
-
-	rc = qmi_send_req_wait(ipa_q6_clnt, &req_desc, req,
-			sizeof(struct ipa_set_data_usage_quota_req_msg_v01),
-			&resp_desc, &resp, sizeof(resp),
-			QMI_SEND_STATS_REQ_TIMEOUT_MS);
-
-	IPAWANDBG("QMI_IPA_SET_DATA_USAGE_QUOTA_RESP_V01 received\n");
-
-	return ipa_check_qmi_response(rc,
-		QMI_IPA_SET_DATA_USAGE_QUOTA_REQ_V01, resp.resp.result,
-		resp.resp.error, "ipa_set_data_usage_quota_req_msg_v01");
-}
-
-int ipa_qmi_stop_data_qouta(void)
-{
-	struct ipa_stop_data_usage_quota_req_msg_v01 req;
-	struct ipa_stop_data_usage_quota_resp_msg_v01 resp;
-	struct msg_desc req_desc, resp_desc;
-	int rc;
-
-	memset(&req, 0, sizeof(struct ipa_stop_data_usage_quota_req_msg_v01));
-	memset(&resp, 0, sizeof(struct ipa_stop_data_usage_quota_resp_msg_v01));
-
-	req_desc.max_msg_len =
-		QMI_IPA_STOP_DATA_USAGE_QUOTA_REQ_MAX_MSG_LEN_V01;
-	req_desc.msg_id = QMI_IPA_STOP_DATA_USAGE_QUOTA_REQ_V01;
-	req_desc.ei_array = ipa_stop_data_usage_quota_req_msg_data_v01_ei;
-
-	resp_desc.max_msg_len =
-		QMI_IPA_STOP_DATA_USAGE_QUOTA_RESP_MAX_MSG_LEN_V01;
-	resp_desc.msg_id = QMI_IPA_STOP_DATA_USAGE_QUOTA_RESP_V01;
-	resp_desc.ei_array = ipa_stop_data_usage_quota_resp_msg_data_v01_ei;
-
-	IPAWANDBG("Sending QMI_IPA_STOP_DATA_USAGE_QUOTA_REQ_V01\n");
-
-	rc = qmi_send_req_wait(ipa_q6_clnt, &req_desc, &req, sizeof(req),
-		&resp_desc, &resp, sizeof(resp),
-		QMI_SEND_STATS_REQ_TIMEOUT_MS);
-
-	IPAWANDBG("QMI_IPA_STOP_DATA_USAGE_QUOTA_RESP_V01 received\n");
-
-	return ipa_check_qmi_response(rc,
-		QMI_IPA_STOP_DATA_USAGE_QUOTA_REQ_V01, resp.resp.result,
-		resp.resp.error, "ipa_stop_data_usage_quota_req_msg_v01");
-}
-
-void ipa_qmi_init(void)
-{
-	mutex_init(&ipa_qmi_lock);
-}
-
-void ipa_qmi_cleanup(void)
-{
-	mutex_destroy(&ipa_qmi_lock);
-}
diff --git a/drivers/platform/msm/ipa/ipa_qmi_service.h b/drivers/platform/msm/ipa/ipa_qmi_service.h
deleted file mode 100644
index fc98940..00000000
--- a/drivers/platform/msm/ipa/ipa_qmi_service.h
+++ /dev/null
@@ -1,286 +0,0 @@
-/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef IPA_QMI_SERVICE_H
-#define IPA_QMI_SERVICE_H
-
-#include <linux/ipa.h>
-#include <linux/ipa_qmi_service_v01.h>
-#include <uapi/linux/msm_rmnet.h>
-#include <soc/qcom/msm_qmi_interface.h>
-#include "ipa_i.h"
-#include <linux/rmnet_ipa_fd_ioctl.h>
-
-/**
- * name of the DL wwan default routing tables for v4 and v6
- */
-#define IPA_A7_QMAP_HDR_NAME "ipa_qmap_hdr"
-#define IPA_DFLT_WAN_RT_TBL_NAME "ipa_dflt_wan_rt"
-#define MAX_NUM_Q6_RULE 35
-#define MAX_NUM_QMI_RULE_CACHE 10
-#define DEV_NAME "ipa-wan"
-#define SUBSYS_MODEM "modem"
-
-#define IPAWANDBG(fmt, args...) \
-	pr_debug(DEV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-#define IPAWANERR(fmt, args...) \
-	pr_err(DEV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-
-extern struct ipa_qmi_context *ipa_qmi_ctx;
-extern struct mutex ipa_qmi_lock;
-
-struct ipa_qmi_context {
-struct ipa_ioc_ext_intf_prop q6_ul_filter_rule[MAX_NUM_Q6_RULE];
-u32 q6_ul_filter_rule_hdl[MAX_NUM_Q6_RULE];
-int num_ipa_install_fltr_rule_req_msg;
-struct ipa_install_fltr_rule_req_msg_v01
-		ipa_install_fltr_rule_req_msg_cache[MAX_NUM_QMI_RULE_CACHE];
-int num_ipa_fltr_installed_notif_req_msg;
-struct ipa_fltr_installed_notif_req_msg_v01
-		ipa_fltr_installed_notif_req_msg_cache[MAX_NUM_QMI_RULE_CACHE];
-bool modem_cfg_emb_pipe_flt;
-};
-
-struct rmnet_mux_val {
-	uint32_t  mux_id;
-	int8_t    vchannel_name[IFNAMSIZ];
-	bool mux_channel_set;
-	bool ul_flt_reg;
-	bool mux_hdr_set;
-	uint32_t  hdr_hdl;
-};
-
-int rmnet_ipa_poll_tethering_stats(struct wan_ioctl_poll_tethering_stats *data);
-int rmnet_ipa_set_data_quota(struct wan_ioctl_set_data_quota *data);
-void ipa_broadcast_quota_reach_ind(uint32_t mux_id);
-int rmnet_ipa_set_tether_client_pipe(struct wan_ioctl_set_tether_client_pipe
-	*data);
-int rmnet_ipa_query_tethering_stats(struct wan_ioctl_query_tether_stats *data,
-	bool reset);
-
-int ipa_qmi_get_data_stats(struct ipa_get_data_stats_req_msg_v01 *req,
-	struct ipa_get_data_stats_resp_msg_v01 *resp);
-int ipa_qmi_get_network_stats(struct ipa_get_apn_data_stats_req_msg_v01 *req,
-	struct ipa_get_apn_data_stats_resp_msg_v01 *resp);
-int ipa_qmi_set_data_quota(struct ipa_set_data_usage_quota_req_msg_v01 *req);
-int ipa_qmi_stop_data_qouta(void);
-void ipa_q6_handshake_complete(bool);
-void ipa_qmi_init(void);
-void ipa_qmi_cleanup(void);
-
-extern struct elem_info ipa_init_modem_driver_req_msg_data_v01_ei[];
-extern struct elem_info ipa_init_modem_driver_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_indication_reg_req_msg_data_v01_ei[];
-extern struct elem_info ipa_indication_reg_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_master_driver_init_complt_ind_msg_data_v01_ei[];
-extern struct elem_info ipa_install_fltr_rule_req_msg_data_v01_ei[];
-extern struct elem_info ipa_install_fltr_rule_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_fltr_installed_notif_req_msg_data_v01_ei[];
-extern struct elem_info ipa_fltr_installed_notif_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_enable_force_clear_datapath_req_msg_data_v01_ei[];
-extern struct elem_info ipa_enable_force_clear_datapath_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_disable_force_clear_datapath_req_msg_data_v01_ei[];
-extern struct elem_info ipa_disable_force_clear_datapath_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_config_req_msg_data_v01_ei[];
-extern struct elem_info ipa_config_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_get_data_stats_req_msg_data_v01_ei[];
-extern struct elem_info ipa_get_data_stats_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_get_apn_data_stats_req_msg_data_v01_ei[];
-extern struct elem_info ipa_get_apn_data_stats_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_set_data_usage_quota_req_msg_data_v01_ei[];
-extern struct elem_info ipa_set_data_usage_quota_resp_msg_data_v01_ei[];
-extern struct elem_info ipa_data_usage_quota_reached_ind_msg_data_v01_ei[];
-extern struct elem_info ipa_stop_data_usage_quota_req_msg_data_v01_ei[];
-extern struct elem_info ipa_stop_data_usage_quota_resp_msg_data_v01_ei[];
-
-/**
- * struct ipa_rmnet_context - IPA rmnet context
- * @ipa_rmnet_ssr: support modem SSR
- * @polling_interval: Requested interval for polling tethered statistics
- * @metered_mux_id: The mux ID on which quota has been set
- */
-struct ipa_rmnet_context {
-	bool ipa_rmnet_ssr;
-	u64 polling_interval;
-	u32 metered_mux_id;
-};
-
-extern struct ipa_rmnet_context ipa_rmnet_ctx;
-
-#ifdef CONFIG_RMNET_IPA
-
-int ipa_qmi_service_init(uint32_t wan_platform_type);
-
-void ipa_qmi_service_exit(void);
-
-/* sending filter-install-request to modem*/
-int qmi_filter_request_send(struct ipa_install_fltr_rule_req_msg_v01 *req);
-
-/* sending filter-installed-notify-request to modem*/
-int qmi_filter_notify_send(struct ipa_fltr_installed_notif_req_msg_v01 *req);
-
-/* voting for bus BW to ipa_rm*/
-int vote_for_bus_bw(uint32_t *bw_mbps);
-
-int qmi_enable_force_clear_datapath_send(
-	struct ipa_enable_force_clear_datapath_req_msg_v01 *req);
-
-int qmi_disable_force_clear_datapath_send(
-	struct ipa_disable_force_clear_datapath_req_msg_v01 *req);
-
-int copy_ul_filter_rule_to_ipa(struct ipa_install_fltr_rule_req_msg_v01
-	*rule_req, uint32_t *rule_hdl);
-
-int wwan_update_mux_channel_prop(void);
-
-int wan_ioctl_init(void);
-
-void wan_ioctl_stop_qmi_messages(void);
-
-void wan_ioctl_enable_qmi_messages(void);
-
-void wan_ioctl_deinit(void);
-
-void ipa_qmi_stop_workqueues(void);
-
-#else /* CONFIG_RMNET_IPA */
-
-static inline int ipa_qmi_service_init(uint32_t wan_platform_type)
-{
-	return -EPERM;
-}
-
-static inline void ipa_qmi_service_exit(void)
-{
-	return;
-}
-
-/* sending filter-install-request to modem*/
-static inline int qmi_filter_request_send(
-	struct ipa_install_fltr_rule_req_msg_v01 *req)
-{
-	return -EPERM;
-}
-
-/* sending filter-installed-notify-request to modem*/
-static inline int qmi_filter_notify_send(
-	struct ipa_fltr_installed_notif_req_msg_v01 *req)
-{
-	return -EPERM;
-}
-
-static inline int qmi_enable_force_clear_datapath_send(
-	struct ipa_enable_force_clear_datapath_req_msg_v01 *req)
-{
-	return -EPERM;
-}
-
-static inline int qmi_disable_force_clear_datapath_send(
-	struct ipa_disable_force_clear_datapath_req_msg_v01 *req)
-{
-	return -EPERM;
-}
-
-static inline int copy_ul_filter_rule_to_ipa(
-	struct ipa_install_fltr_rule_req_msg_v01 *rule_req, uint32_t *rule_hdl)
-{
-	return -EPERM;
-}
-
-static inline int wwan_update_mux_channel_prop(void)
-{
-	return -EPERM;
-}
-
-static inline int wan_ioctl_init(void)
-{
-	return -EPERM;
-}
-
-static inline void wan_ioctl_stop_qmi_messages(void)
-{
-	return;
-}
-
-static inline void wan_ioctl_enable_qmi_messages(void)
-{
-	return;
-}
-
-static inline void wan_ioctl_deinit(void)
-{
-	return;
-}
-
-static inline void ipa_qmi_stop_workqueues(void)
-{
-	return;
-}
-
-static inline int vote_for_bus_bw(uint32_t *bw_mbps)
-{
-	return -EPERM;
-}
-
-int rmnet_ipa_poll_tethering_stats(struct wan_ioctl_poll_tethering_stats *data)
-{
-	return -EPERM;
-}
-
-int rmnet_ipa_set_data_quota(struct wan_ioctl_set_data_quota *data)
-{
-	return -EPERM;
-}
-
-void ipa_broadcast_quota_reach_ind(uint8_t mux_id)
-{
-	return;
-}
-
-int ipa_qmi_get_data_stats(struct ipa_get_data_stats_req_msg_v01 *req,
-	struct ipa_get_data_stats_resp_msg_v01 *resp);
-{
-	return -EPERM;
-}
-
-int ipa_qmi_get_network_stats(struct ipa_get_apn_data_stats_req_msg_v01 *req,
-	struct ipa_get_apn_data_stats_resp_msg_v01 *resp);
-{
-	return -EPERM;
-}
-
-int ipa_qmi_set_data_quota(struct ipa_set_network_quota_req_msg_v01 *req)
-{
-	return -EPERM;
-}
-
-int ipa_qmi_stop_data_qouta(void)
-{
-	return -EPERM;
-}
-
-void ipa_q6_handshake_complete(bool)
-{
-	return;
-}
-
-void ipa_qmi_init(void)
-{
-}
-
-void ipa_qmi_cleanup(void)
-{
-}
-
-#endif /* CONFIG_RMNET_IPA */
-
-#endif /* IPA_QMI_SERVICE_H */
diff --git a/drivers/platform/msm/ipa/ipa_qmi_service_v01.c b/drivers/platform/msm/ipa/ipa_qmi_service_v01.c
deleted file mode 100644
index dd591407..00000000
--- a/drivers/platform/msm/ipa/ipa_qmi_service_v01.c
+++ /dev/null
@@ -1,2366 +0,0 @@
-/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-
-#include <linux/qmi_encdec.h>
-#include <linux/ipa_qmi_service_v01.h>
-
-#include <soc/qcom/msm_qmi_interface.h>
-
-/* Type Definitions  */
-static struct elem_info ipa_hdr_tbl_info_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_hdr_tbl_info_type_v01,
-					modem_offset_start),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_hdr_tbl_info_type_v01,
-					modem_offset_end),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_route_tbl_info_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_route_tbl_info_type_v01,
-					route_tbl_start_addr),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_route_tbl_info_type_v01,
-					num_indices),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_modem_mem_info_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_modem_mem_info_type_v01,
-					block_start_addr),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_modem_mem_info_type_v01,
-					size),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_hdr_proc_ctx_tbl_info_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_hdr_proc_ctx_tbl_info_type_v01,
-			modem_offset_start),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_hdr_proc_ctx_tbl_info_type_v01,
-			modem_offset_end),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_zip_tbl_info_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_zip_tbl_info_type_v01,
-					modem_offset_start),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_zip_tbl_info_type_v01,
-					modem_offset_end),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_ipfltr_range_eq_16_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_ipfltr_range_eq_16_type_v01,
-			offset),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_2_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint16_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_ipfltr_range_eq_16_type_v01,
-			range_low),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_2_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint16_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_ipfltr_range_eq_16_type_v01,
-			range_high),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_ipfltr_mask_eq_32_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-				struct ipa_ipfltr_mask_eq_32_type_v01,
-				offset),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-				struct ipa_ipfltr_mask_eq_32_type_v01,
-				mask),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_ipfltr_mask_eq_32_type_v01,
-			value),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_ipfltr_eq_16_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_ipfltr_eq_16_type_v01,
-			offset),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_2_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint16_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_ipfltr_eq_16_type_v01,
-					value),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_ipfltr_eq_32_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_ipfltr_eq_32_type_v01,
-					offset),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_ipfltr_eq_32_type_v01,
-					value),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_ipfltr_mask_eq_128_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_ipfltr_mask_eq_128_type_v01,
-			offset),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 16,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= STATIC_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_ipfltr_mask_eq_128_type_v01,
-			mask),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 16,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= STATIC_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_ipfltr_mask_eq_128_type_v01,
-			value),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_filter_rule_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_2_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint16_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_filter_rule_type_v01,
-			rule_eq_bitmap),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_filter_rule_type_v01,
-			tos_eq_present),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					tos_eq),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					protocol_eq_present),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					protocol_eq),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					num_ihl_offset_range_16),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_IPFLTR_NUM_IHL_RANGE_16_EQNS_V01,
-		.elem_size	= sizeof(
-			struct ipa_ipfltr_range_eq_16_type_v01),
-		.is_array	= STATIC_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					ihl_offset_range_16),
-		.ei_array	= ipa_ipfltr_range_eq_16_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					num_offset_meq_32),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_IPFLTR_NUM_MEQ_32_EQNS_V01,
-		.elem_size	= sizeof(struct ipa_ipfltr_mask_eq_32_type_v01),
-		.is_array	= STATIC_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					offset_meq_32),
-		.ei_array	= ipa_ipfltr_mask_eq_32_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					tc_eq_present),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					tc_eq),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					flow_eq_present),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					flow_eq),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					ihl_offset_eq_16_present),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct ipa_ipfltr_eq_16_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					ihl_offset_eq_16),
-		.ei_array	= ipa_ipfltr_eq_16_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					ihl_offset_eq_32_present),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct ipa_ipfltr_eq_32_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					ihl_offset_eq_32),
-		.ei_array	= ipa_ipfltr_eq_32_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					num_ihl_offset_meq_32),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_IPFLTR_NUM_IHL_MEQ_32_EQNS_V01,
-		.elem_size	= sizeof(struct ipa_ipfltr_mask_eq_32_type_v01),
-		.is_array	= STATIC_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					ihl_offset_meq_32),
-		.ei_array	= ipa_ipfltr_mask_eq_32_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					num_offset_meq_128),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	=
-			QMI_IPA_IPFLTR_NUM_MEQ_128_EQNS_V01,
-		.elem_size	= sizeof(
-			struct ipa_ipfltr_mask_eq_128_type_v01),
-		.is_array	= STATIC_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_filter_rule_type_v01,
-			offset_meq_128),
-		.ei_array	= ipa_ipfltr_mask_eq_128_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					metadata_meq32_present),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct ipa_ipfltr_mask_eq_32_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					metadata_meq32),
-		.ei_array	= ipa_ipfltr_mask_eq_32_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_rule_type_v01,
-					ipv4_frag_eq_present),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_filter_spec_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_spec_type_v01,
-					filter_spec_identifier),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_spec_type_v01,
-					ip_type),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct ipa_filter_rule_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_spec_type_v01,
-					filter_rule),
-		.ei_array	= ipa_filter_rule_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_spec_type_v01,
-					filter_action),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_spec_type_v01,
-					is_routing_table_index_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_spec_type_v01,
-					route_table_index),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_spec_type_v01,
-					is_mux_id_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_filter_spec_type_v01,
-					mux_id),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info
-	ipa_filter_rule_identifier_to_handle_map_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_filter_rule_identifier_to_handle_map_v01,
-			filter_spec_identifier),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_filter_rule_identifier_to_handle_map_v01,
-			filter_handle),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_filter_handle_to_index_map_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_filter_handle_to_index_map_v01,
-			filter_handle),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(
-			struct ipa_filter_handle_to_index_map_v01,
-			filter_index),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_init_modem_driver_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			platform_type_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			platform_type),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			hdr_tbl_info_valid),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct ipa_hdr_tbl_info_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			hdr_tbl_info),
-		.ei_array	= ipa_hdr_tbl_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			v4_route_tbl_info_valid),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct ipa_route_tbl_info_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			v4_route_tbl_info),
-		.ei_array	= ipa_route_tbl_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			v6_route_tbl_info_valid),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct ipa_route_tbl_info_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			v6_route_tbl_info),
-		.ei_array	= ipa_route_tbl_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x14,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			v4_filter_tbl_start_addr_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x14,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			v4_filter_tbl_start_addr),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x15,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			v6_filter_tbl_start_addr_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x15,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			v6_filter_tbl_start_addr),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x16,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			modem_mem_info_valid),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct ipa_modem_mem_info_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x16,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			modem_mem_info),
-		.ei_array	= ipa_modem_mem_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x17,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			ctrl_comm_dest_end_pt_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x17,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			ctrl_comm_dest_end_pt),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x18,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			is_ssr_bootup_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x18,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			is_ssr_bootup),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x19,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			hdr_proc_ctx_tbl_info_valid),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(
-			struct ipa_hdr_proc_ctx_tbl_info_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x19,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			hdr_proc_ctx_tbl_info),
-		.ei_array	= ipa_hdr_proc_ctx_tbl_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x1A,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			zip_tbl_info_valid),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct ipa_zip_tbl_info_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x1A,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_req_msg_v01,
-			zip_tbl_info),
-		.ei_array	= ipa_zip_tbl_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_init_modem_driver_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_resp_msg_v01,
-			ctrl_comm_dest_end_pt_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_resp_msg_v01,
-			ctrl_comm_dest_end_pt),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_resp_msg_v01,
-			default_end_pt_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_init_modem_driver_resp_msg_v01,
-			default_end_pt),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_indication_reg_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_indication_reg_req_msg_v01,
-			master_driver_init_complete_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_indication_reg_req_msg_v01,
-			master_driver_init_complete),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_indication_reg_req_msg_v01,
-			data_usage_quota_reached_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_indication_reg_req_msg_v01,
-			data_usage_quota_reached),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_indication_reg_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_indication_reg_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_master_driver_init_complt_ind_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(struct
-			ipa_master_driver_init_complt_ind_msg_v01,
-			master_driver_init_status),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_install_fltr_rule_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			filter_spec_list_valid),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			filter_spec_list_len),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_MAX_FILTERS_V01,
-		.elem_size	= sizeof(struct ipa_filter_spec_type_v01),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			filter_spec_list),
-		.ei_array	= ipa_filter_spec_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			source_pipe_index_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			source_pipe_index),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			num_ipv4_filters_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			num_ipv4_filters),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			num_ipv6_filters_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			num_ipv6_filters),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x14,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			xlat_filter_indices_list_valid),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x14,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			xlat_filter_indices_list_len),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= QMI_IPA_MAX_FILTERS_V01,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x14,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_req_msg_v01,
-			xlat_filter_indices_list),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_install_fltr_rule_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_resp_msg_v01,
-			resp),
-		.ei_array       = get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_resp_msg_v01,
-			filter_handle_list_valid),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_resp_msg_v01,
-			filter_handle_list_len),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_MAX_FILTERS_V01,
-		.elem_size	= sizeof(
-			struct ipa_filter_rule_identifier_to_handle_map_v01),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_install_fltr_rule_resp_msg_v01,
-			filter_handle_list),
-		.ei_array	=
-			ipa_filter_rule_identifier_to_handle_map_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_fltr_installed_notif_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x01,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			source_pipe_index),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_2_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint16_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			install_status),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x03,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			filter_index_list_len),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_MAX_FILTERS_V01,
-		.elem_size	= sizeof(
-			struct ipa_filter_handle_to_index_map_v01),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x03,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			filter_index_list),
-		.ei_array	= ipa_filter_handle_to_index_map_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			embedded_pipe_index_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			embedded_pipe_index),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			retain_header_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			retain_header),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			embedded_call_mux_id_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			embedded_call_mux_id),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			num_ipv4_filters_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			num_ipv4_filters),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x14,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			num_ipv6_filters_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x14,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			num_ipv6_filters),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x15,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			start_ipv4_filter_idx_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x15,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			start_ipv4_filter_idx),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x16,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			start_ipv6_filter_idx_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x16,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_req_msg_v01,
-			start_ipv6_filter_idx),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_fltr_installed_notif_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_fltr_installed_notif_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_enable_force_clear_datapath_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x01,
-		.offset		= offsetof(
-			struct ipa_enable_force_clear_datapath_req_msg_v01,
-			source_pipe_bitmask),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_enable_force_clear_datapath_req_msg_v01,
-			request_id),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_enable_force_clear_datapath_req_msg_v01,
-			throttle_source_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_enable_force_clear_datapath_req_msg_v01,
-			throttle_source),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_enable_force_clear_datapath_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_enable_force_clear_datapath_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_disable_force_clear_datapath_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x01,
-		.offset		= offsetof(
-			struct ipa_disable_force_clear_datapath_req_msg_v01,
-			request_id),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_disable_force_clear_datapath_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_disable_force_clear_datapath_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_config_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			peripheral_type_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			peripheral_type),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			hw_deaggr_supported_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			hw_deaggr_supported),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			max_aggr_frame_size_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-					max_aggr_frame_size),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			ipa_ingress_pipe_mode_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			ipa_ingress_pipe_mode),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x14,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			peripheral_speed_info_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x14,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			peripheral_speed_info),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x15,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			dl_accumulation_time_limit_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x15,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			dl_accumulation_time_limit),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x16,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			dl_accumulation_pkt_limit_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x16,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			dl_accumulation_pkt_limit),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x17,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			dl_accumulation_byte_limit_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x17,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			dl_accumulation_byte_limit),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x18,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			ul_accumulation_time_limit_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x18,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			ul_accumulation_time_limit),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x19,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			hw_control_flags_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x19,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			hw_control_flags),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x1A,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			ul_msi_event_threshold_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x1A,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			ul_msi_event_threshold),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x1B,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			dl_msi_event_threshold_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x1B,
-		.offset		= offsetof(
-			struct ipa_config_req_msg_v01,
-			dl_msi_event_threshold),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_config_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_config_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_get_data_stats_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x01,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_req_msg_v01,
-			ipa_stats_type),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_req_msg_v01,
-			reset_stats_valid),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_1_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_req_msg_v01,
-			reset_stats),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_pipe_stats_info_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_pipe_stats_info_type_v01,
-					pipe_index),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_pipe_stats_info_type_v01,
-					num_ipv4_packets),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_pipe_stats_info_type_v01,
-					num_ipv4_bytes),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_pipe_stats_info_type_v01,
-					num_ipv6_packets),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct ipa_pipe_stats_info_type_v01,
-					num_ipv6_bytes),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_stats_type_filter_rule_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct
-					ipa_stats_type_filter_rule_v01,
-					filter_rule_index),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct
-					ipa_stats_type_filter_rule_v01,
-					num_packets),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_get_data_stats_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			ipa_stats_type_valid),
-	},
-	{
-		.data_type	= QMI_SIGNED_4_BYTE_ENUM,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			ipa_stats_type),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			ul_src_pipe_stats_list_valid),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			ul_src_pipe_stats_list_len),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_MAX_PIPES_V01,
-		.elem_size	= sizeof(struct ipa_pipe_stats_info_type_v01),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x11,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			ul_src_pipe_stats_list),
-		.ei_array	= ipa_pipe_stats_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			dl_dst_pipe_stats_list_valid),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			dl_dst_pipe_stats_list_len),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_MAX_PIPES_V01,
-		.elem_size	= sizeof(struct ipa_pipe_stats_info_type_v01),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x12,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			dl_dst_pipe_stats_list),
-		.ei_array	= ipa_pipe_stats_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			dl_filter_rule_stats_list_valid),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			dl_filter_rule_stats_list_len),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_MAX_FILTERS_V01,
-		.elem_size	= sizeof(struct ipa_pipe_stats_info_type_v01),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x13,
-		.offset		= offsetof(
-			struct ipa_get_data_stats_resp_msg_v01,
-			dl_filter_rule_stats_list),
-		.ei_array	= ipa_stats_type_filter_rule_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_apn_data_stats_info_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct
-					ipa_apn_data_stats_info_type_v01,
-					mux_id),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct
-					ipa_apn_data_stats_info_type_v01,
-					num_ul_packets),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct
-					ipa_apn_data_stats_info_type_v01,
-					num_ul_bytes),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct
-					ipa_apn_data_stats_info_type_v01,
-					num_dl_packets),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct
-					ipa_apn_data_stats_info_type_v01,
-					num_dl_bytes),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_get_apn_data_stats_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_apn_data_stats_req_msg_v01,
-			mux_id_list_valid),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_apn_data_stats_req_msg_v01,
-			mux_id_list_len),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= QMI_IPA_MAX_APN_V01,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_apn_data_stats_req_msg_v01,
-			mux_id_list),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_get_apn_data_stats_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_get_apn_data_stats_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_apn_data_stats_resp_msg_v01,
-			apn_data_stats_list_valid),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_apn_data_stats_resp_msg_v01,
-			apn_data_stats_list_len),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_MAX_APN_V01,
-		.elem_size	= sizeof(struct
-					ipa_apn_data_stats_info_type_v01),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_get_apn_data_stats_resp_msg_v01,
-			apn_data_stats_list),
-		.ei_array	= ipa_apn_data_stats_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-static struct elem_info ipa_data_usage_quota_info_type_data_v01_ei[] = {
-	{
-		.data_type	= QMI_UNSIGNED_4_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint32_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct
-					ipa_data_usage_quota_info_type_v01,
-					mux_id),
-	},
-	{
-		.data_type	= QMI_UNSIGNED_8_BYTE,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint64_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-		.offset		= offsetof(struct
-					ipa_data_usage_quota_info_type_v01,
-					num_Mbytes),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_set_data_usage_quota_req_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_OPT_FLAG,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_set_data_usage_quota_req_msg_v01,
-			apn_quota_list_valid),
-	},
-	{
-		.data_type	= QMI_DATA_LEN,
-		.elem_len	= 1,
-		.elem_size	= sizeof(uint8_t),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_set_data_usage_quota_req_msg_v01,
-			apn_quota_list_len),
-	},
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= QMI_IPA_MAX_APN_V01,
-		.elem_size	= sizeof(struct
-					ipa_data_usage_quota_info_type_v01),
-		.is_array	= VAR_LEN_ARRAY,
-		.tlv_type	= 0x10,
-		.offset		= offsetof(
-			struct ipa_set_data_usage_quota_req_msg_v01,
-			apn_quota_list),
-		.ei_array	= ipa_data_usage_quota_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_set_data_usage_quota_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_set_data_usage_quota_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_data_usage_quota_reached_ind_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct
-					ipa_data_usage_quota_info_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x01,
-		.offset		= offsetof(
-			struct ipa_data_usage_quota_reached_ind_msg_v01,
-			apn),
-		.ei_array	= ipa_data_usage_quota_info_type_data_v01_ei,
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_stop_data_usage_quota_req_msg_data_v01_ei[] = {
-	/* ipa_stop_data_usage_quota_req_msg is empty */
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
-
-struct elem_info ipa_stop_data_usage_quota_resp_msg_data_v01_ei[] = {
-	{
-		.data_type	= QMI_STRUCT,
-		.elem_len	= 1,
-		.elem_size	= sizeof(struct qmi_response_type_v01),
-		.is_array	= NO_ARRAY,
-		.tlv_type	= 0x02,
-		.offset		= offsetof(
-			struct ipa_stop_data_usage_quota_resp_msg_v01,
-			resp),
-		.ei_array	= get_qmi_response_type_v01_ei(),
-	},
-	{
-		.data_type	= QMI_EOTI,
-		.is_array	= NO_ARRAY,
-		.tlv_type	= QMI_COMMON_TLV_TYPE,
-	},
-};
diff --git a/drivers/platform/msm/ipa/ipa_ram_mmap.h b/drivers/platform/msm/ipa/ipa_ram_mmap.h
deleted file mode 100644
index 56ada21b..00000000
--- a/drivers/platform/msm/ipa/ipa_ram_mmap.h
+++ /dev/null
@@ -1,560 +0,0 @@
-/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef _IPA_RAM_MMAP_H_
-#define _IPA_RAM_MMAP_H_
-
-/*
- * This header defines the memory map of the IPA RAM (not all SRAM is
- * available for SW use)
- * In case of restricted bytes the actual starting address will be
- * advanced by the number of needed bytes
- */
-
-#define IPA_RAM_NAT_OFST    0
-#define IPA_RAM_NAT_SIZE    0
-
-#define IPA_MEM_v1_RAM_HDR_OFST    (IPA_RAM_NAT_OFST + IPA_RAM_NAT_SIZE)
-#define IPA_MEM_v1_RAM_HDR_SIZE    1664
-#define IPA_MEM_v1_RAM_V4_FLT_OFST (IPA_MEM_v1_RAM_HDR_OFST +\
-	IPA_MEM_v1_RAM_HDR_SIZE)
-#define IPA_MEM_v1_RAM_V4_FLT_SIZE 2176
-#define IPA_MEM_v1_RAM_V4_RT_OFST  (IPA_MEM_v1_RAM_V4_FLT_OFST +\
-	IPA_MEM_v1_RAM_V4_FLT_SIZE)
-#define IPA_MEM_v1_RAM_V4_RT_SIZE  512
-#define IPA_MEM_v1_RAM_V6_FLT_OFST (IPA_MEM_v1_RAM_V4_RT_OFST +\
-	IPA_MEM_v1_RAM_V4_RT_SIZE)
-#define IPA_MEM_v1_RAM_V6_FLT_SIZE 1792
-#define IPA_MEM_v1_RAM_V6_RT_OFST  (IPA_MEM_v1_RAM_V6_FLT_OFST +\
-	IPA_MEM_v1_RAM_V6_FLT_SIZE)
-#define IPA_MEM_v1_RAM_V6_RT_SIZE  512
-#define IPA_MEM_v1_RAM_END_OFST    (IPA_MEM_v1_RAM_V6_RT_OFST +\
-	IPA_MEM_v1_RAM_V6_RT_SIZE)
-
-#define IPA_MEM_RAM_V6_RT_SIZE_DDR 16384
-#define IPA_MEM_RAM_V4_RT_SIZE_DDR 16384
-#define IPA_MEM_RAM_V6_FLT_SIZE_DDR 16384
-#define IPA_MEM_RAM_V4_FLT_SIZE_DDR 16384
-#define IPA_MEM_RAM_HDR_PROC_CTX_SIZE_DDR 0
-
-#define IPA_MEM_CANARY_SIZE 4
-#define IPA_MEM_CANARY_VAL 0xdeadbeef
-
-#define IPA_MEM_RAM_MODEM_NETWORK_STATS_SIZE 256
-/*
- * IPA v2.0 and v2.1 SRAM memory layout:
- * +-------------+
- * | V4 FLT HDR  |
- * +-------------+
- * |    CANARY   |
- * +-------------+
- * |    CANARY   |
- * +-------------+
- * | V6 FLT HDR  |
- * +-------------+
- * |    CANARY   |
- * +-------------+
- * |    CANARY   |
- * +-------------+
- * | V4 RT HDR   |
- * +-------------+
- * |    CANARY   |
- * +-------------+
- * | V6 RT HDR   |
- * +-------------+
- * |    CANARY   |
- * +-------------+
- * |  MODEM HDR  |
- * +-------------+
- * |  APPS  HDR  |
- * +-------------+
- * |    CANARY   |
- * +-------------+
- * |  MODEM MEM  |
- * +-------------+
- * |    CANARY   |
- * +-------------+
- * | APPS V4 FLT |
- * +-------------+
- * | APPS V6 FLT |
- * +-------------+
- * |    CANARY   |
- * +-------------+
- * |   UC INFO   |
- * +-------------+
- */
-#define IPA_MEM_v2_RAM_OFST_START 128
-#define IPA_MEM_v2_RAM_V4_FLT_OFST IPA_MEM_v2_RAM_OFST_START
-#define IPA_MEM_v2_RAM_V4_FLT_SIZE 88
-
-/* V4 filtering header table is 8B aligned */
-#if (IPA_MEM_v2_RAM_V4_FLT_OFST & 7)
-#error V4 filtering header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_V6_FLT_OFST (IPA_MEM_v2_RAM_V4_FLT_OFST + \
-		IPA_MEM_v2_RAM_V4_FLT_SIZE + 2*IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_RAM_V6_FLT_SIZE 88
-
-/* V6 filtering header table is 8B aligned */
-#if (IPA_MEM_v2_RAM_V6_FLT_OFST & 7)
-#error V6 filtering header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_V4_RT_OFST (IPA_MEM_v2_RAM_V6_FLT_OFST + \
-		IPA_MEM_v2_RAM_V6_FLT_SIZE + 2*IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_RAM_V4_NUM_INDEX 11
-#define IPA_MEM_v2_V4_MODEM_RT_INDEX_LO 0
-#define IPA_MEM_v2_V4_MODEM_RT_INDEX_HI 3
-#define IPA_MEM_v2_V4_APPS_RT_INDEX_LO 4
-#define IPA_MEM_v2_V4_APPS_RT_INDEX_HI 10
-#define IPA_MEM_v2_RAM_V4_RT_SIZE (IPA_MEM_v2_RAM_V4_NUM_INDEX * 4)
-
-/* V4 routing header table is 8B aligned */
-#if (IPA_MEM_v2_RAM_V4_RT_OFST & 7)
-#error V4 routing header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_V6_RT_OFST (IPA_MEM_v2_RAM_V4_RT_OFST + \
-		IPA_MEM_v2_RAM_V4_RT_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_RAM_V6_NUM_INDEX 11
-#define IPA_MEM_v2_V6_MODEM_RT_INDEX_LO 0
-#define IPA_MEM_v2_V6_MODEM_RT_INDEX_HI 3
-#define IPA_MEM_v2_V6_APPS_RT_INDEX_LO 4
-#define IPA_MEM_v2_V6_APPS_RT_INDEX_HI 10
-#define IPA_MEM_v2_RAM_V6_RT_SIZE (IPA_MEM_v2_RAM_V6_NUM_INDEX * 4)
-
-/* V6 routing header table is 8B aligned */
-#if (IPA_MEM_v2_RAM_V6_RT_OFST & 7)
-#error V6 routing header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_MODEM_HDR_OFST (IPA_MEM_v2_RAM_V6_RT_OFST + \
-		IPA_MEM_v2_RAM_V6_RT_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_RAM_MODEM_HDR_SIZE 320
-
-/* header table is 8B aligned */
-#if (IPA_MEM_v2_RAM_MODEM_HDR_OFST & 7)
-#error header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_APPS_HDR_OFST (IPA_MEM_v2_RAM_MODEM_HDR_OFST + \
-		IPA_MEM_v2_RAM_MODEM_HDR_SIZE)
-#define IPA_MEM_v2_RAM_APPS_HDR_SIZE 72
-
-/* header table is 8B aligned */
-#if (IPA_MEM_v2_RAM_APPS_HDR_OFST & 7)
-#error header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_MODEM_OFST (IPA_MEM_v2_RAM_APPS_HDR_OFST + \
-		IPA_MEM_v2_RAM_APPS_HDR_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_RAM_MODEM_SIZE 3532
-
-/* modem memory is 4B aligned */
-#if (IPA_MEM_v2_RAM_MODEM_OFST & 3)
-#error modem memory is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_APPS_V4_FLT_OFST (IPA_MEM_v2_RAM_MODEM_OFST + \
-		IPA_MEM_v2_RAM_MODEM_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_RAM_APPS_V4_FLT_SIZE 1920
-
-/* filtering rule is 4B aligned */
-#if (IPA_MEM_v2_RAM_APPS_V4_FLT_OFST & 3)
-#error filtering rule is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_APPS_V6_FLT_OFST (IPA_MEM_v2_RAM_APPS_V4_FLT_OFST + \
-		IPA_MEM_v2_RAM_APPS_V4_FLT_SIZE)
-#define IPA_MEM_v2_RAM_APPS_V6_FLT_SIZE 1372
-
-/* filtering rule is 4B aligned */
-#if (IPA_MEM_v2_RAM_APPS_V6_FLT_OFST & 3)
-#error filtering rule is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_UC_INFO_OFST (IPA_MEM_v2_RAM_APPS_V6_FLT_OFST + \
-		IPA_MEM_v2_RAM_APPS_V6_FLT_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_RAM_UC_INFO_SIZE 292
-
-/* uC info 4B aligned */
-#if (IPA_MEM_v2_RAM_UC_INFO_OFST & 3)
-#error uC info is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_RAM_END_OFST (IPA_MEM_v2_RAM_UC_INFO_OFST + \
-		IPA_MEM_v2_RAM_UC_INFO_SIZE)
-#define IPA_MEM_v2_RAM_APPS_V4_RT_OFST IPA_MEM_v2_RAM_END_OFST
-#define IPA_MEM_v2_RAM_APPS_V4_RT_SIZE 0
-#define IPA_MEM_v2_RAM_APPS_V6_RT_OFST IPA_MEM_v2_RAM_END_OFST
-#define IPA_MEM_v2_RAM_APPS_V6_RT_SIZE 0
-#define IPA_MEM_v2_RAM_HDR_SIZE_DDR 4096
-
-/*
- * IPA v2.5/v2.6 SRAM memory layout:
- * +----------------+
- * |    UC INFO     |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | V4 FLT HDR     |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | V6 FLT HDR     |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | V4 RT HDR      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | V6 RT HDR      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |  MODEM HDR     |
- * +----------------+
- * |  APPS  HDR     |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | MODEM PROC CTX |
- * +----------------+
- * | APPS PROC CTX  |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |  MODEM MEM     |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- */
-
-#define IPA_MEM_v2_5_RAM_UC_MEM_SIZE 128
-#define IPA_MEM_v2_5_RAM_UC_INFO_OFST IPA_MEM_v2_5_RAM_UC_MEM_SIZE
-#define IPA_MEM_v2_5_RAM_UC_INFO_SIZE 512
-
-/* uC info 4B aligned */
-#if (IPA_MEM_v2_5_RAM_UC_INFO_OFST & 3)
-#error uC info is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_OFST_START (IPA_MEM_v2_5_RAM_UC_INFO_OFST + \
-	IPA_MEM_v2_5_RAM_UC_INFO_SIZE)
-
-#define IPA_MEM_v2_5_RAM_V4_FLT_OFST (IPA_MEM_v2_5_RAM_OFST_START + \
-	2 * IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_5_RAM_V4_FLT_SIZE 88
-
-/* V4 filtering header table is 8B aligned */
-#if (IPA_MEM_v2_5_RAM_V4_FLT_OFST & 7)
-#error V4 filtering header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_V6_FLT_OFST (IPA_MEM_v2_5_RAM_V4_FLT_OFST + \
-	IPA_MEM_v2_5_RAM_V4_FLT_SIZE + 2 * IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_5_RAM_V6_FLT_SIZE 88
-
-/* V6 filtering header table is 8B aligned */
-#if (IPA_MEM_v2_5_RAM_V6_FLT_OFST & 7)
-#error V6 filtering header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_V4_RT_OFST (IPA_MEM_v2_5_RAM_V6_FLT_OFST + \
-	IPA_MEM_v2_5_RAM_V6_FLT_SIZE + 2 * IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_5_RAM_V4_NUM_INDEX 15
-#define IPA_MEM_v2_5_V4_MODEM_RT_INDEX_LO 0
-#define IPA_MEM_v2_5_V4_MODEM_RT_INDEX_HI 6
-#define IPA_MEM_v2_5_V4_APPS_RT_INDEX_LO \
-					(IPA_MEM_v2_5_V4_MODEM_RT_INDEX_HI + 1)
-#define IPA_MEM_v2_5_V4_APPS_RT_INDEX_HI \
-					(IPA_MEM_v2_5_RAM_V4_NUM_INDEX - 1)
-#define IPA_MEM_v2_5_RAM_V4_RT_SIZE (IPA_MEM_v2_5_RAM_V4_NUM_INDEX * 4)
-
-/* V4 routing header table is 8B aligned */
-#if (IPA_MEM_v2_5_RAM_V4_RT_OFST & 7)
-#error V4 routing header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_V6_RT_OFST (IPA_MEM_v2_5_RAM_V4_RT_OFST + \
-	IPA_MEM_v2_5_RAM_V4_RT_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_5_RAM_V6_NUM_INDEX 15
-#define IPA_MEM_v2_5_V6_MODEM_RT_INDEX_LO 0
-#define IPA_MEM_v2_5_V6_MODEM_RT_INDEX_HI 6
-#define IPA_MEM_v2_5_V6_APPS_RT_INDEX_LO \
-					(IPA_MEM_v2_5_V6_MODEM_RT_INDEX_HI + 1)
-#define IPA_MEM_v2_5_V6_APPS_RT_INDEX_HI \
-					(IPA_MEM_v2_5_RAM_V6_NUM_INDEX - 1)
-#define IPA_MEM_v2_5_RAM_V6_RT_SIZE (IPA_MEM_v2_5_RAM_V6_NUM_INDEX * 4)
-
-/* V6 routing header table is 8B aligned */
-#if (IPA_MEM_v2_5_RAM_V6_RT_OFST & 7)
-#error V6 routing header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_MODEM_HDR_OFST (IPA_MEM_v2_5_RAM_V6_RT_OFST + \
-	IPA_MEM_v2_5_RAM_V6_RT_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_5_RAM_MODEM_HDR_SIZE 320
-
-/* header table is 8B aligned */
-#if (IPA_MEM_v2_5_RAM_MODEM_HDR_OFST & 7)
-#error header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_APPS_HDR_OFST (IPA_MEM_v2_5_RAM_MODEM_HDR_OFST + \
-	IPA_MEM_v2_5_RAM_MODEM_HDR_SIZE)
-#define IPA_MEM_v2_5_RAM_APPS_HDR_SIZE 0
-
-/* header table is 8B aligned */
-#if (IPA_MEM_v2_5_RAM_APPS_HDR_OFST & 7)
-#error header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_MODEM_HDR_PROC_CTX_OFST \
-	(IPA_MEM_v2_5_RAM_APPS_HDR_OFST + IPA_MEM_v2_5_RAM_APPS_HDR_SIZE + \
-	2 * IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_5_RAM_MODEM_HDR_PROC_CTX_SIZE 512
-
-/* header processing context table is 8B aligned */
-#if (IPA_MEM_v2_5_RAM_MODEM_HDR_PROC_CTX_OFST & 7)
-#error header processing context table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_APPS_HDR_PROC_CTX_OFST \
-	(IPA_MEM_v2_5_RAM_MODEM_HDR_PROC_CTX_OFST + \
-	IPA_MEM_v2_5_RAM_MODEM_HDR_PROC_CTX_SIZE)
-#define IPA_MEM_v2_5_RAM_APPS_HDR_PROC_CTX_SIZE 512
-
-/* header processing context table is 8B aligned */
-#if (IPA_MEM_v2_5_RAM_APPS_HDR_PROC_CTX_OFST & 7)
-#error header processing context table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_MODEM_OFST (IPA_MEM_v2_5_RAM_APPS_HDR_PROC_CTX_OFST + \
-	IPA_MEM_v2_5_RAM_APPS_HDR_PROC_CTX_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_5_RAM_MODEM_SIZE 5800
-
-/* modem memory is 4B aligned */
-#if (IPA_MEM_v2_5_RAM_MODEM_OFST & 3)
-#error modem memory is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_APPS_V4_FLT_OFST (IPA_MEM_v2_5_RAM_MODEM_OFST + \
-	IPA_MEM_v2_5_RAM_MODEM_SIZE)
-#define IPA_MEM_v2_5_RAM_APPS_V4_FLT_SIZE 0
-
-/* filtering rule is 4B aligned */
-#if (IPA_MEM_v2_5_RAM_APPS_V4_FLT_OFST & 3)
-#error filtering rule is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_APPS_V6_FLT_OFST (IPA_MEM_v2_5_RAM_APPS_V4_FLT_OFST + \
-	IPA_MEM_v2_5_RAM_APPS_V4_FLT_SIZE)
-#define IPA_MEM_v2_5_RAM_APPS_V6_FLT_SIZE 0
-
-/* filtering rule is 4B aligned */
-#if (IPA_MEM_v2_5_RAM_APPS_V6_FLT_OFST & 3)
-#error filtering rule is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_5_RAM_END_OFST (IPA_MEM_v2_5_RAM_APPS_V6_FLT_OFST + \
-	IPA_MEM_v2_5_RAM_APPS_V6_FLT_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_5_RAM_APPS_V4_RT_OFST IPA_MEM_v2_5_RAM_END_OFST
-#define IPA_MEM_v2_5_RAM_APPS_V4_RT_SIZE 0
-#define IPA_MEM_v2_5_RAM_APPS_V6_RT_OFST IPA_MEM_v2_5_RAM_END_OFST
-#define IPA_MEM_v2_5_RAM_APPS_V6_RT_SIZE 0
-#define IPA_MEM_v2_5_RAM_HDR_SIZE_DDR 2048
-
-/*
- * IPA v2.6Lite SRAM memory layout:
- * +----------------+
- * |   UC INFO      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | V4 FLT HDR     |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | V6 FLT HDR     |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | V4 RT HDR      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | V6 RT HDR      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |  MODEM HDR     |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * | COMP / DECOMP  |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- * |  MODEM MEM     |
- * +----------------+
- * |    CANARY      |
- * +----------------+
- */
-
-#define IPA_MEM_v2_6L_RAM_UC_MEM_SIZE 128
-#define IPA_MEM_v2_6L_RAM_UC_INFO_OFST IPA_MEM_v2_6L_RAM_UC_MEM_SIZE
-#define IPA_MEM_v2_6L_RAM_UC_INFO_SIZE 512
-
-/* uC info 4B aligned */
-#if (IPA_MEM_v2_6L_RAM_UC_INFO_OFST & 3)
-#error uC info is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_OFST_START (IPA_MEM_v2_6L_RAM_UC_INFO_OFST + \
-	IPA_MEM_v2_6L_RAM_UC_INFO_SIZE)
-
-#define IPA_MEM_v2_6L_RAM_V4_FLT_OFST (IPA_MEM_v2_6L_RAM_OFST_START + \
-	2 * IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_6L_RAM_V4_FLT_SIZE 88
-
-/* V4 filtering header table is 8B aligned */
-#if (IPA_MEM_v2_6L_RAM_V4_FLT_OFST & 7)
-#error V4 filtering header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_V6_FLT_OFST (IPA_MEM_v2_6L_RAM_V4_FLT_OFST + \
-	IPA_MEM_v2_6L_RAM_V4_FLT_SIZE + 2 * IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_6L_RAM_V6_FLT_SIZE 88
-
-/* V6 filtering header table is 8B aligned */
-#if (IPA_MEM_v2_6L_RAM_V6_FLT_OFST & 7)
-#error V6 filtering header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_V4_RT_OFST (IPA_MEM_v2_6L_RAM_V6_FLT_OFST + \
-	IPA_MEM_v2_6L_RAM_V6_FLT_SIZE + 2 * IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_6L_RAM_V4_NUM_INDEX 15
-#define IPA_MEM_v2_6L_V4_MODEM_RT_INDEX_LO 0
-#define IPA_MEM_v2_6L_V4_MODEM_RT_INDEX_HI 6
-#define IPA_MEM_v2_6L_V4_APPS_RT_INDEX_LO \
-	(IPA_MEM_v2_6L_V4_MODEM_RT_INDEX_HI + 1)
-#define IPA_MEM_v2_6L_V4_APPS_RT_INDEX_HI \
-	(IPA_MEM_v2_6L_RAM_V4_NUM_INDEX - 1)
-#define IPA_MEM_v2_6L_RAM_V4_RT_SIZE (IPA_MEM_v2_6L_RAM_V4_NUM_INDEX * 4)
-
-/* V4 routing header table is 8B aligned */
-#if (IPA_MEM_v2_6L_RAM_V4_RT_OFST & 7)
-#error V4 routing header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_V6_RT_OFST (IPA_MEM_v2_6L_RAM_V4_RT_OFST + \
-	IPA_MEM_v2_6L_RAM_V4_RT_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_6L_RAM_V6_NUM_INDEX 15
-#define IPA_MEM_v2_6L_V6_MODEM_RT_INDEX_LO 0
-#define IPA_MEM_v2_6L_V6_MODEM_RT_INDEX_HI 6
-#define IPA_MEM_v2_6L_V6_APPS_RT_INDEX_LO \
-	(IPA_MEM_v2_6L_V6_MODEM_RT_INDEX_HI + 1)
-#define IPA_MEM_v2_6L_V6_APPS_RT_INDEX_HI \
-	(IPA_MEM_v2_6L_RAM_V6_NUM_INDEX - 1)
-#define IPA_MEM_v2_6L_RAM_V6_RT_SIZE (IPA_MEM_v2_6L_RAM_V6_NUM_INDEX * 4)
-
-/* V6 routing header table is 8B aligned */
-#if (IPA_MEM_v2_6L_RAM_V6_RT_OFST & 7)
-#error V6 routing header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_MODEM_HDR_OFST (IPA_MEM_v2_6L_RAM_V6_RT_OFST + \
-	IPA_MEM_v2_6L_RAM_V6_RT_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_6L_RAM_MODEM_HDR_SIZE 320
-
-/* header table is 8B aligned */
-#if (IPA_MEM_v2_6L_RAM_MODEM_HDR_OFST & 7)
-#error header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_APPS_HDR_OFST (IPA_MEM_v2_6L_RAM_MODEM_HDR_OFST + \
-	IPA_MEM_v2_6L_RAM_MODEM_HDR_SIZE)
-#define IPA_MEM_v2_6L_RAM_APPS_HDR_SIZE 0
-
-/* header table is 8B aligned */
-#if (IPA_MEM_v2_6L_RAM_APPS_HDR_OFST & 7)
-#error header table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_MODEM_COMP_DECOMP_OFST \
-	(IPA_MEM_v2_6L_RAM_APPS_HDR_OFST + IPA_MEM_v2_6L_RAM_APPS_HDR_SIZE + \
-	2 * IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_6L_RAM_MODEM_COMP_DECOMP_SIZE 512
-
-/* comp/decomp memory region is 8B aligned */
-#if (IPA_MEM_v2_6L_RAM_MODEM_COMP_DECOMP_OFST & 7)
-#error header processing context table is not 8B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_MODEM_OFST \
-	(IPA_MEM_v2_6L_RAM_MODEM_COMP_DECOMP_OFST + \
-	IPA_MEM_v2_6L_RAM_MODEM_COMP_DECOMP_SIZE + IPA_MEM_CANARY_SIZE)
-#define IPA_MEM_v2_6L_RAM_MODEM_SIZE 6376
-
-/* modem memory is 4B aligned */
-#if (IPA_MEM_v2_6L_RAM_MODEM_OFST & 3)
-#error modem memory is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_APPS_V4_FLT_OFST (IPA_MEM_v2_6L_RAM_MODEM_OFST + \
-	IPA_MEM_v2_6L_RAM_MODEM_SIZE)
-#define IPA_MEM_v2_6L_RAM_APPS_V4_FLT_SIZE 0
-
-/* filtering rule is 4B aligned */
-#if (IPA_MEM_v2_6L_RAM_APPS_V4_FLT_OFST & 3)
-#error filtering rule is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_APPS_V6_FLT_OFST \
-	(IPA_MEM_v2_6L_RAM_APPS_V4_FLT_OFST + \
-	IPA_MEM_v2_6L_RAM_APPS_V4_FLT_SIZE)
-#define IPA_MEM_v2_6L_RAM_APPS_V6_FLT_SIZE 0
-
-/* filtering rule is 4B aligned */
-#if (IPA_MEM_v2_6L_RAM_APPS_V6_FLT_OFST & 3)
-#error filtering rule is not 4B aligned
-#endif
-
-#define IPA_MEM_v2_6L_RAM_END_OFST (IPA_MEM_v2_6L_RAM_APPS_V6_FLT_OFST + \
-	IPA_MEM_v2_6L_RAM_APPS_V6_FLT_SIZE + IPA_MEM_CANARY_SIZE)
-
-#define IPA_MEM_v2_6L_RAM_APPS_V4_RT_OFST IPA_MEM_v2_6L_RAM_END_OFST
-#define IPA_MEM_v2_6L_RAM_APPS_V4_RT_SIZE 0
-#define IPA_MEM_v2_6L_RAM_APPS_V6_RT_OFST IPA_MEM_v2_6L_RAM_END_OFST
-#define IPA_MEM_v2_6L_RAM_APPS_V6_RT_SIZE 0
-#define IPA_MEM_v2_6L_RAM_HDR_SIZE_DDR 2048
-
-#endif /* _IPA_RAM_MMAP_H_ */
diff --git a/drivers/platform/msm/ipa/ipa_reg.h b/drivers/platform/msm/ipa/ipa_reg.h
deleted file mode 100644
index 8880f71..00000000
--- a/drivers/platform/msm/ipa/ipa_reg.h
+++ /dev/null
@@ -1,317 +0,0 @@
-/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef __IPA_REG_H__
-#define __IPA_REG_H__
-
-/*
- * IPA's BAM specific registers
- * Used for IPA HW 1.0 only
- */
-
-#define IPA_BAM_REG_BASE_OFST 0x00004000
-#define IPA_BAM_CNFG_BITS_OFST 0x7c
-#define IPA_BAM_REMAP_SIZE (0x1000)
-
-#define IPA_FILTER_FILTER_EN_BMSK 0x1
-#define IPA_FILTER_FILTER_EN_SHFT 0x0
-#define IPA_AGGREGATION_SPARE_REG_2_OFST 0x00002094
-#define IPA_AGGREGATION_QCNCM_SIG0_SHFT 16
-#define IPA_AGGREGATION_QCNCM_SIG1_SHFT 8
-
-#define IPA_AGGREGATION_SPARE_REG_1_OFST 0x00002090
-#define IPA_AGGREGATION_SPARE_REG_2_OFST 0x00002094
-
-#define IPA_AGGREGATION_SINGLE_NDP_MSK 0x1
-#define IPA_AGGREGATION_SINGLE_NDP_BMSK 0xfffffffe
-
-#define IPA_AGGREGATION_MODE_MSK 0x1
-#define IPA_AGGREGATION_MODE_SHFT 31
-#define IPA_AGGREGATION_MODE_BMSK 0x7fffffff
-
-#define IPA_AGGREGATION_QCNCM_SIG_BMSK 0xff000000
-
-#define IPA_FILTER_FILTER_EN_BMSK 0x1
-#define IPA_FILTER_FILTER_EN_SHFT 0x0
-
-#define IPA_AGGREGATION_HW_TIMER_FIX_MBIM_AGGR_SHFT 2
-#define IPA_AGGREGATION_HW_TIMER_FIX_MBIM_AGGR_BMSK 0x4
-
-#define IPA_HEAD_OF_LINE_BLOCK_EN_OFST 0x00000044
-
-/*
- * End of IPA 1.0 Registers
- */
-
-
-/*
- * IPA HW 2.0 Registers
- */
-#define IPA_REG_BASE 0x0
-
-#define IPA_IRQ_STTS_EE_n_ADDR(n) (IPA_REG_BASE + 0x00001008 + 0x1000 * (n))
-#define IPA_IRQ_STTS_EE_n_MAXn 3
-
-#define IPA_IRQ_EN_EE_n_ADDR(n) (IPA_REG_BASE + 0x0000100c + 0x1000 * (n))
-#define IPA_IRQ_EN_EE_n_MAXn 3
-
-
-#define IPA_IRQ_CLR_EE_n_ADDR(n) (IPA_REG_BASE + 0x00001010 + 0x1000 * (n))
-#define IPA_IRQ_CLR_EE_n_MAXn 3
-
-#define IPA_IRQ_SUSPEND_INFO_EE_n_ADDR(n) \
-				(IPA_REG_BASE + 0x00001098 + 0x1000 * (n))
-#define IPA_IRQ_SUSPEND_INFO_EE_n_MAXn 3
-/*
- * End of IPA 2.0 Registers
- */
-
-/*
- * IPA HW 2.5 Registers
- */
-#define IPA_BCR_OFST 0x000005B0
-#define IPA_COUNTER_CFG_OFST 0x000005E8
-#define IPA_COUNTER_CFG_EOT_COAL_GRAN_BMSK 0xF
-#define IPA_COUNTER_CFG_EOT_COAL_GRAN_SHFT 0x0
-#define IPA_COUNTER_CFG_AGGR_GRAN_BMSK 0x1F0
-#define IPA_COUNTER_CFG_AGGR_GRAN_SHFT 0x4
- /*
- * End of IPA 2.5 Registers
- */
-
-/*
- * IPA HW 2.6/2.6L Registers
- */
-#define IPA_ENABLED_PIPES_OFST 0x000005DC
-#define IPA_YELLOW_MARKER_SYS_CFG_OFST 0x00000728
-/*
- * End of IPA 2.6/2.6L Registers
- */
-
-/*
-Common Registers
-*/
-#define IPA_REG_BASE_OFST_v2_0 0x00020000
-#define IPA_REG_BASE_OFST_v2_5 0x00040000
-#define IPA_REG_BASE_OFST_v2_6L IPA_REG_BASE_OFST_v2_5
-#define IPA_COMP_SW_RESET_OFST 0x0000003c
-
-#define IPA_VERSION_OFST 0x00000034
-#define IPA_COMP_HW_VERSION_OFST 0x00000030
-
-#define IPA_SHARED_MEM_SIZE_OFST_v1_1 0x00000050
-#define IPA_SHARED_MEM_SIZE_OFST_v2_0 0x00000050
-#define IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_BMSK_v2_0 0xffff0000
-#define IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_SHFT_v2_0 0x10
-#define IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_BMSK_v2_0  0xffff
-#define IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_SHFT_v2_0  0x0
-
-#define IPA_ENDP_INIT_AGGR_N_OFST_v1_1(n) (0x000001c0 + 0x4 * (n))
-#define IPA_ENDP_INIT_AGGR_N_OFST_v2_0(n) (0x00000320 + 0x4 * (n))
-
-#define IPA_ENDP_INIT_ROUTE_N_OFST_v1_1(n) (0x00000220 + 0x4 * (n))
-#define IPA_ENDP_INIT_ROUTE_N_OFST_v2_0(n) (0x00000370 + 0x4 * (n))
-#define IPA_ENDP_INIT_ROUTE_N_ROUTE_TABLE_INDEX_BMSK 0x1f
-#define IPA_ENDP_INIT_ROUTE_N_ROUTE_TABLE_INDEX_SHFT 0x0
-
-#define IPA_ROUTE_OFST_v1_1 0x00000044
-
-#define IPA_ROUTE_ROUTE_DIS_SHFT 0x0
-#define IPA_ROUTE_ROUTE_DIS_BMSK 0x1
-#define IPA_ROUTE_ROUTE_DEF_PIPE_SHFT 0x1
-#define IPA_ROUTE_ROUTE_DEF_PIPE_BMSK 0x3e
-#define IPA_ROUTE_ROUTE_DEF_HDR_TABLE_SHFT 0x6
-#define IPA_ROUTE_ROUTE_DEF_HDR_OFST_SHFT 0x7
-#define IPA_ROUTE_ROUTE_DEF_HDR_OFST_BMSK 0x1ff80
-#define IPA_ROUTE_ROUTE_FRAG_DEF_PIPE_BMSK 0x3e0000
-#define IPA_ROUTE_ROUTE_FRAG_DEF_PIPE_SHFT 0x11
-
-#define IPA_FILTER_OFST_v1_1 0x00000048
-
-#define IPA_SRAM_DIRECT_ACCESS_N_OFST_v1_1(n) (0x00004000 + 0x4 * (n))
-#define IPA_SRAM_DIRECT_ACCESS_N_OFST_v2_0(n) (0x00005000 + 0x4 * (n))
-#define IPA_SRAM_DIRECT_ACCESS_N_OFST(n) (0x00004000 + 0x4 * (n))
-#define IPA_SRAM_SW_FIRST_v2_5 0x00005000
-#define IPA_ROUTE_ROUTE_DEF_HDR_TABLE_BMSK 0x40
-#define IPA_ENDP_INIT_NAT_N_NAT_EN_SHFT 0x0
-#define IPA_COMP_CFG_OFST 0x00000038
-
-#define IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_BMSK 0x1
-#define IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_SHFT 0x16
-#define IPA_ENDP_INIT_AGGR_n_AGGR_PKT_LIMIT_BMSK 0x1f8000
-#define IPA_ENDP_INIT_AGGR_n_AGGR_PKT_LIMIT_SHFT 0xf
-#define IPA_ENDP_INIT_AGGR_N_AGGR_TIME_LIMIT_BMSK 0x7c00
-#define IPA_ENDP_INIT_AGGR_N_AGGR_TIME_LIMIT_SHFT 0xa
-#define IPA_ENDP_INIT_AGGR_N_AGGR_BYTE_LIMIT_BMSK 0x3e0
-#define IPA_ENDP_INIT_AGGR_N_AGGR_BYTE_LIMIT_SHFT 0x5
-#define IPA_ENDP_INIT_AGGR_N_AGGR_TYPE_BMSK 0x1c
-#define IPA_ENDP_INIT_AGGR_N_AGGR_TYPE_SHFT 0x2
-#define IPA_ENDP_INIT_AGGR_N_AGGR_EN_BMSK 0x3
-#define IPA_ENDP_INIT_AGGR_N_AGGR_EN_SHFT 0x0
-
-#define IPA_ENDP_INIT_MODE_N_OFST_v1_1(n) (0x00000170 + 0x4 * (n))
-#define IPA_ENDP_INIT_MODE_N_OFST_v2_0(n) (0x000002c0 + 0x4 * (n))
-#define IPA_ENDP_INIT_MODE_N_RMSK 0x7f
-#define IPA_ENDP_INIT_MODE_N_MAX 19
-#define IPA_ENDP_INIT_MODE_N_DEST_PIPE_INDEX_BMSK_v1_1 0x7c
-#define IPA_ENDP_INIT_MODE_N_DEST_PIPE_INDEX_SHFT_v1_1 0x2
-#define IPA_ENDP_INIT_MODE_N_DEST_PIPE_INDEX_BMSK_v2_0 0x1f0
-#define IPA_ENDP_INIT_MODE_N_DEST_PIPE_INDEX_SHFT_v2_0 0x4
-#define IPA_ENDP_INIT_MODE_N_MODE_BMSK 0x7
-#define IPA_ENDP_INIT_MODE_N_MODE_SHFT 0x0
-
-#define IPA_ENDP_INIT_HDR_N_OFST_v1_1(n) (0x00000120 + 0x4 * (n))
-#define IPA_ENDP_INIT_HDR_N_OFST_v2_0(n) (0x00000170 + 0x4 * (n))
-#define IPA_ENDP_INIT_HDR_N_HDR_LEN_BMSK 0x3f
-#define IPA_ENDP_INIT_HDR_N_HDR_LEN_SHFT 0x0
-#define IPA_ENDP_INIT_HDR_N_HDR_ADDITIONAL_CONST_LEN_BMSK 0x7e000
-#define IPA_ENDP_INIT_HDR_N_HDR_ADDITIONAL_CONST_LEN_SHFT 0xd
-#define IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_BMSK 0x3f00000
-#define IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_SHFT 0x14
-#define IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_VALID_BMSK 0x80000
-#define IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_VALID_SHFT 0x13
-#define IPA_ENDP_INIT_HDR_N_HDR_METADATA_REG_VALID_BMSK_v2 0x10000000
-#define IPA_ENDP_INIT_HDR_N_HDR_METADATA_REG_VALID_SHFT_v2 0x1c
-#define IPA_ENDP_INIT_HDR_N_HDR_LEN_INC_DEAGG_HDR_BMSK_v2 0x8000000
-#define IPA_ENDP_INIT_HDR_N_HDR_LEN_INC_DEAGG_HDR_SHFT_v2 0x1b
-#define IPA_ENDP_INIT_HDR_N_HDR_A5_MUX_BMSK 0x4000000
-#define IPA_ENDP_INIT_HDR_N_HDR_A5_MUX_SHFT 0x1a
-#define IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_VALID_BMSK 0x40
-#define IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_VALID_SHFT 0x6
-#define IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_SHFT 0x7
-#define IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_BMSK 0x1f80
-
-#define IPA_ENDP_INIT_NAT_N_OFST_v1_1(n) (0x000000c0 + 0x4 * (n))
-#define IPA_ENDP_INIT_NAT_N_OFST_v2_0(n) (0x00000120 + 0x4 * (n))
-#define IPA_ENDP_INIT_NAT_N_NAT_EN_BMSK 0x3
-#define IPA_ENDP_INIT_NAT_N_NAT_EN_SHFT 0x0
-
-
-#define IPA_ENDP_INIT_HDR_EXT_n_OFST_v2_0(n) (0x000001c0 + 0x4 * (n))
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_ENDIANESS_BMSK 0x1
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_ENDIANESS_SHFT 0x0
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_VALID_BMSK 0x2
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_VALID_SHFT 0x1
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_BMSK 0x4
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_SHFT 0x2
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_PAYLOAD_LEN_INC_PADDING_BMSK 0x8
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_PAYLOAD_LEN_INC_PADDING_SHFT 0x3
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_OFFSET_BMSK 0x3f0
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_OFFSET_SHFT 0x4
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_PAD_TO_ALIGNMENT_BMSK_v2_0 0x1c00
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_PAD_TO_ALIGNMENT_SHFT 0xa
-#define IPA_ENDP_INIT_HDR_EXT_n_HDR_PAD_TO_ALIGNMENT_BMSK_v2_5 0x3c00
-
-
-
-/*
- IPA HW 1.1 specific Registers
-*/
-
-#define IPA_FILTER_FILTER_DIS_BMSK 0x1
-#define IPA_FILTER_FILTER_DIS_SHFT 0x0
-#define IPA_SINGLE_NDP_MODE_OFST 0x00000064
-#define IPA_QCNCM_OFST 0x00000060
-
-#define IPA_ENDP_INIT_CTRL_N_OFST(n) (0x00000070 + 0x4 * (n))
-#define IPA_ENDP_INIT_CTRL_N_RMSK 0x1
-#define IPA_ENDP_INIT_CTRL_N_MAX 19
-#define IPA_ENDP_INIT_CTRL_N_ENDP_SUSPEND_BMSK 0x1
-#define IPA_ENDP_INIT_CTRL_N_ENDP_SUSPEND_SHFT 0x0
-#define IPA_ENDP_INIT_CTRL_N_ENDP_DELAY_BMSK 0x2
-#define IPA_ENDP_INIT_CTRL_N_ENDP_DELAY_SHFT 0x1
-
-#define IPA_ENDP_INIT_HOL_BLOCK_EN_N_OFST_v1_1(n) (0x00000270 + 0x4 * (n))
-#define IPA_ENDP_INIT_HOL_BLOCK_EN_N_OFST_v2_0(n) (0x000003c0 + 0x4 * (n))
-#define IPA_ENDP_INIT_HOL_BLOCK_EN_N_RMSK 0x1
-#define IPA_ENDP_INIT_HOL_BLOCK_EN_N_MAX 19
-#define IPA_ENDP_INIT_HOL_BLOCK_EN_N_EN_BMSK 0x1
-#define IPA_ENDP_INIT_HOL_BLOCK_EN_N_EN_SHFT 0x0
-
-#define IPA_ENDP_INIT_DEAGGR_n_OFST_v2_0(n) (0x00000470 + 0x04 * (n))
-#define IPA_ENDP_INIT_DEAGGR_n_DEAGGR_HDR_LEN_BMSK 0x3F
-#define IPA_ENDP_INIT_DEAGGR_n_DEAGGR_HDR_LEN_SHFT 0x0
-#define IPA_ENDP_INIT_DEAGGR_n_PACKET_OFFSET_VALID_BMSK 0x40
-#define IPA_ENDP_INIT_DEAGGR_n_PACKET_OFFSET_VALID_SHFT 0x6
-#define IPA_ENDP_INIT_DEAGGR_n_PACKET_OFFSET_LOCATION_BMSK 0x3F00
-#define IPA_ENDP_INIT_DEAGGR_n_PACKET_OFFSET_LOCATION_SHFT 0x8
-#define IPA_ENDP_INIT_DEAGGR_n_MAX_PACKET_LEN_BMSK 0xFFFF0000
-#define IPA_ENDP_INIT_DEAGGR_n_MAX_PACKET_LEN_SHFT 0x10
-
-#define IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_OFST_v1_1(n) (0x000002c0 + 0x4 * (n))
-#define IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_OFST_v2_0(n) (0x00000420 + 0x4 * (n))
-#define IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_RMSK 0x1ff
-#define IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_MAX 19
-#define IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_TIMER_BMSK 0x1ff
-#define IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_TIMER_SHFT 0x0
-
-#define IPA_DEBUG_CNT_REG_N_OFST_v1_1(n) (0x00000340 + 0x4 * (n))
-#define IPA_DEBUG_CNT_REG_N_OFST_v2_0(n) (0x00000600 + 0x4 * (n))
-#define IPA_DEBUG_CNT_REG_N_RMSK 0xffffffff
-#define IPA_DEBUG_CNT_REG_N_MAX 15
-#define IPA_DEBUG_CNT_REG_N_DBG_CNT_REG_BMSK 0xffffffff
-#define IPA_DEBUG_CNT_REG_N_DBG_CNT_REG_SHFT 0x0
-
-#define IPA_DEBUG_CNT_CTRL_N_OFST_v1_1(n) (0x00000380 + 0x4 * (n))
-#define IPA_DEBUG_CNT_CTRL_N_OFST_v2_0(n) (0x00000640 + 0x4 * (n))
-#define IPA_DEBUG_CNT_CTRL_N_RMSK 0x1ff1f171
-#define IPA_DEBUG_CNT_CTRL_N_MAX 15
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_RULE_INDEX_BMSK 0x1ff00000
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_RULE_INDEX_SHFT 0x14
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_SOURCE_PIPE_BMSK 0x1f000
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_SOURCE_PIPE_SHFT 0xc
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_PRODUCT_BMSK 0x100
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_PRODUCT_SHFT 0x8
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_TYPE_BMSK 0x70
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_TYPE_SHFT 0x4
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_EN_BMSK 0x1
-#define IPA_DEBUG_CNT_CTRL_N_DBG_CNT_EN_SHFT 0x0
-
-#define IPA_ENDP_STATUS_n_OFST(n) (0x000004c0 + 0x4 * (n))
-#define IPA_ENDP_STATUS_n_STATUS_ENDP_BMSK 0x3e
-#define IPA_ENDP_STATUS_n_STATUS_ENDP_SHFT 0x1
-#define IPA_ENDP_STATUS_n_STATUS_EN_BMSK 0x1
-#define IPA_ENDP_STATUS_n_STATUS_EN_SHFT 0x0
-
-#define IPA_ENDP_INIT_CFG_n_OFST(n) (0x000000c0 + 0x4 * (n))
-#define IPA_ENDP_INIT_CFG_n_RMSK 0x7f
-#define IPA_ENDP_INIT_CFG_n_MAXn 19
-#define IPA_ENDP_INIT_CFG_n_CS_METADATA_HDR_OFFSET_BMSK 0x78
-#define IPA_ENDP_INIT_CFG_n_CS_METADATA_HDR_OFFSET_SHFT 0x3
-#define IPA_ENDP_INIT_CFG_n_CS_OFFLOAD_EN_BMSK 0x6
-#define IPA_ENDP_INIT_CFG_n_CS_OFFLOAD_EN_SHFT 0x1
-#define IPA_ENDP_INIT_CFG_n_FRAG_OFFLOAD_EN_BMSK 0x1
-#define IPA_ENDP_INIT_CFG_n_FRAG_OFFLOAD_EN_SHFT 0x0
-
-#define IPA_ENDP_INIT_HDR_METADATA_MASK_n_OFST(n) (0x00000220 + 0x4 * (n))
-#define IPA_ENDP_INIT_HDR_METADATA_MASK_n_RMSK 0xffffffff
-#define IPA_ENDP_INIT_HDR_METADATA_MASK_n_MAXn 19
-#define IPA_ENDP_INIT_HDR_METADATA_MASK_n_METADATA_MASK_BMSK 0xffffffff
-#define IPA_ENDP_INIT_HDR_METADATA_MASK_n_METADATA_MASK_SHFT 0x0
-
-#define IPA_ENDP_INIT_HDR_METADATA_n_OFST(n) (0x00000270 + 0x4 * (n))
-#define IPA_ENDP_INIT_HDR_METADATA_n_MUX_ID_BMASK 0xFF0000
-#define IPA_ENDP_INIT_HDR_METADATA_n_MUX_ID_SHFT 0x10
-
-#define IPA_IRQ_EE_UC_n_OFFS(n) (0x0000101c + 0x1000 * (n))
-#define IPA_IRQ_EE_UC_n_RMSK 0x1
-#define IPA_IRQ_EE_UC_n_MAXn 3
-#define IPA_IRQ_EE_UC_n_INT_BMSK 0x1
-#define IPA_IRQ_EE_UC_n_INT_SHFT 0x0
-
-#define IPA_UC_MAILBOX_m_n_OFFS(m, n) (0x0001a000 + 0x80 * (m) + 0x4 * (n))
-#define IPA_UC_MAILBOX_m_n_OFFS_v2_5(m, n) (0x00022000 + 0x80 * (m) + 0x4 * (n))
-
-#define IPA_SYS_PKT_PROC_CNTXT_BASE_OFST (0x000005d8)
-#define IPA_LOCAL_PKT_PROC_CNTXT_BASE_OFST (0x000005e0)
-
-#endif
diff --git a/drivers/platform/msm/ipa/ipa_rm.c b/drivers/platform/msm/ipa/ipa_rm.c
deleted file mode 100644
index b1eef17..00000000
--- a/drivers/platform/msm/ipa/ipa_rm.c
+++ /dev/null
@@ -1,1079 +0,0 @@
-/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/slab.h>
-#include <linux/workqueue.h>
-#include <linux/ipa.h>
-#include "ipa_i.h"
-#include "ipa_rm_dependency_graph.h"
-#include "ipa_rm_i.h"
-
-static const char *resource_name_to_str[IPA_RM_RESOURCE_MAX] = {
-	__stringify(IPA_RM_RESOURCE_Q6_PROD),
-	__stringify(IPA_RM_RESOURCE_USB_PROD),
-	__stringify(IPA_RM_RESOURCE_HSIC_PROD),
-	__stringify(IPA_RM_RESOURCE_STD_ECM_PROD),
-	__stringify(IPA_RM_RESOURCE_RNDIS_PROD),
-	__stringify(IPA_RM_RESOURCE_WWAN_0_PROD),
-	__stringify(IPA_RM_RESOURCE_WLAN_PROD),
-	__stringify(IPA_RM_RESOURCE_ODU_ADAPT_PROD),
-	__stringify(IPA_RM_RESOURCE_MHI_PROD),
-	__stringify(IPA_RM_RESOURCE_Q6_CONS),
-	__stringify(IPA_RM_RESOURCE_USB_CONS),
-	__stringify(IPA_RM_RESOURCE_HSIC_CONS),
-	__stringify(IPA_RM_RESOURCE_WLAN_CONS),
-	__stringify(IPA_RM_RESOURCE_APPS_CONS),
-	__stringify(IPA_RM_RESOURCE_ODU_ADAPT_CONS),
-	__stringify(IPA_RM_RESOURCE_MHI_CONS),
-};
-
-struct ipa_rm_profile_vote_type {
-	enum ipa_voltage_level volt[IPA_RM_RESOURCE_MAX];
-	enum ipa_voltage_level curr_volt;
-	u32 bw_prods[IPA_RM_RESOURCE_PROD_MAX];
-	u32 bw_cons[IPA_RM_RESOURCE_CONS_MAX];
-	u32 curr_bw;
-};
-
-struct ipa_rm_context_type {
-	struct ipa_rm_dep_graph *dep_graph;
-	struct workqueue_struct *ipa_rm_wq;
-	spinlock_t ipa_rm_lock;
-	struct ipa_rm_profile_vote_type prof_vote;
-};
-static struct ipa_rm_context_type *ipa_rm_ctx;
-
-struct ipa_rm_notify_ipa_work_type {
-	struct work_struct		work;
-	enum ipa_voltage_level		volt;
-	u32				bandwidth_mbps;
-};
-
-/**
- * ipa_rm_create_resource() - create resource
- * @create_params: [in] parameters needed
- *                  for resource initialization
- *
- * Returns: 0 on success, negative on failure
- *
- * This function is called by IPA RM client to initialize client's resources.
- * This API should be called before any other IPA RM API on a given resource
- * name.
- *
- */
-int ipa_rm_create_resource(struct ipa_rm_create_params *create_params)
-{
-	struct ipa_rm_resource *resource;
-	unsigned long flags;
-	int result;
-
-	if (unlikely(!ipa_rm_ctx)) {
-		IPA_RM_ERR("IPA RM was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (!create_params) {
-		IPA_RM_ERR("invalid args\n");
-		return -EINVAL;
-	}
-	IPA_RM_DBG("%s\n", ipa_rm_resource_str(create_params->name));
-
-	if (create_params->floor_voltage < 0 ||
-		create_params->floor_voltage >= IPA_VOLTAGE_MAX) {
-		IPA_RM_ERR("invalid voltage %d\n",
-			create_params->floor_voltage);
-		return -EINVAL;
-	}
-
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-					  create_params->name,
-					  &resource) == 0) {
-		IPA_RM_ERR("resource already exists\n");
-		result = -EEXIST;
-		goto bail;
-	}
-	result = ipa_rm_resource_create(create_params,
-			&resource);
-	if (result) {
-		IPA_RM_ERR("ipa_rm_resource_create() failed\n");
-		goto bail;
-	}
-	result = ipa_rm_dep_graph_add(ipa_rm_ctx->dep_graph, resource);
-	if (result) {
-		IPA_RM_ERR("ipa_rm_dep_graph_add() failed\n");
-		ipa_rm_resource_delete(resource);
-		goto bail;
-	}
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_create_resource);
-
-/**
- * ipa_rm_delete_resource() - delete resource
- * @resource_name: name of resource to be deleted
- *
- * Returns: 0 on success, negative on failure
- *
- * This function is called by IPA RM client to delete client's resources.
- *
- */
-int ipa_rm_delete_resource(enum ipa_rm_resource_name resource_name)
-{
-	struct ipa_rm_resource *resource;
-	unsigned long flags;
-	int result;
-
-	if (unlikely(!ipa_rm_ctx)) {
-		IPA_RM_ERR("IPA RM was not initialized\n");
-		return -EINVAL;
-	}
-
-	IPA_RM_DBG("%s\n", ipa_rm_resource_str(resource_name));
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-					resource_name,
-						&resource) != 0) {
-		IPA_RM_ERR("resource does not exist\n");
-		result = -EINVAL;
-		goto bail;
-	}
-	result = ipa_rm_resource_delete(resource);
-	if (result) {
-		IPA_RM_ERR("ipa_rm_resource_delete() failed\n");
-		goto bail;
-	}
-	result = ipa_rm_dep_graph_remove(ipa_rm_ctx->dep_graph,
-								resource_name);
-	if (result) {
-		IPA_RM_ERR("ipa_rm_dep_graph_remove() failed\n");
-		goto bail;
-	}
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_delete_resource);
-
-/**
- * ipa_rm_add_dependency() - create dependency
- *					between 2 resources
- * @resource_name: name of dependent resource
- * @depends_on_name: name of its dependency
- *
- * Returns: 0 on success, negative on failure
- *
- * Side effects: IPA_RM_RESORCE_GRANTED could be generated
- * in case client registered with IPA RM
- */
-int ipa_rm_add_dependency(enum ipa_rm_resource_name resource_name,
-			enum ipa_rm_resource_name depends_on_name)
-{
-	unsigned long flags;
-	int result;
-
-	if (unlikely(!ipa_rm_ctx)) {
-		IPA_RM_ERR("IPA RM was not initialized\n");
-		return -EINVAL;
-	}
-
-	IPA_RM_DBG("%s -> %s\n", ipa_rm_resource_str(resource_name),
-				 ipa_rm_resource_str(depends_on_name));
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	result = ipa_rm_dep_graph_add_dependency(
-						ipa_rm_ctx->dep_graph,
-						resource_name,
-						depends_on_name);
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_add_dependency);
-
-/**
- * ipa_rm_add_dependency_sync() - Create a dependency between 2 resources
- * in a synchronized fashion. In case a producer resource is in GRANTED state
- * and the newly added consumer resource is in RELEASED state, the consumer
- * entity will be requested and the function will block until the consumer
- * is granted.
- * @resource_name: name of dependent resource
- * @depends_on_name: name of its dependency
- *
- * Returns: 0 on success, negative on failure
- *
- * Side effects: May block. See documentation above.
- */
-int ipa_rm_add_dependency_sync(enum ipa_rm_resource_name resource_name,
-		enum ipa_rm_resource_name depends_on_name)
-{
-	int result;
-	struct ipa_rm_resource *consumer;
-	unsigned long time;
-	unsigned long flags;
-
-	if (unlikely(!ipa_rm_ctx)) {
-		IPA_RM_ERR("IPA RM was not initialized\n");
-		return -EINVAL;
-	}
-
-	IPA_RM_DBG("%s -> %s\n", ipa_rm_resource_str(resource_name),
-				 ipa_rm_resource_str(depends_on_name));
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	result = ipa_rm_dep_graph_add_dependency(
-						ipa_rm_ctx->dep_graph,
-						resource_name,
-						depends_on_name);
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (result == -EINPROGRESS) {
-		ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-				depends_on_name,
-				&consumer);
-		IPA_RM_DBG("%s waits for GRANT of %s.\n",
-				ipa_rm_resource_str(resource_name),
-				ipa_rm_resource_str(depends_on_name));
-		time = wait_for_completion_timeout(
-				&((struct ipa_rm_resource_cons *)consumer)->
-				request_consumer_in_progress,
-				HZ);
-		result = 0;
-		if (!time) {
-			IPA_RM_ERR("TIMEOUT waiting for %s GRANT event.",
-					ipa_rm_resource_str(depends_on_name));
-			result = -ETIME;
-		}
-		IPA_RM_DBG("%s waited for %s GRANT %lu time.\n",
-				ipa_rm_resource_str(resource_name),
-				ipa_rm_resource_str(depends_on_name),
-				time);
-	}
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_add_dependency_sync);
-
-/**
- * ipa_rm_delete_dependency() - create dependency
- *					between 2 resources
- * @resource_name: name of dependent resource
- * @depends_on_name: name of its dependency
- *
- * Returns: 0 on success, negative on failure
- *
- * Side effects: IPA_RM_RESORCE_GRANTED could be generated
- * in case client registered with IPA RM
- */
-int ipa_rm_delete_dependency(enum ipa_rm_resource_name resource_name,
-			enum ipa_rm_resource_name depends_on_name)
-{
-	unsigned long flags;
-	int result;
-
-	if (unlikely(!ipa_rm_ctx)) {
-		IPA_RM_ERR("IPA RM was not initialized\n");
-		return -EINVAL;
-	}
-
-	IPA_RM_DBG("%s -> %s\n", ipa_rm_resource_str(resource_name),
-				 ipa_rm_resource_str(depends_on_name));
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	result = ipa_rm_dep_graph_delete_dependency(
-			  ipa_rm_ctx->dep_graph,
-			  resource_name,
-			  depends_on_name);
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_delete_dependency);
-
-/**
- * ipa_rm_request_resource() - request resource
- * @resource_name: [in] name of the requested resource
- *
- * Returns: 0 on success, negative on failure
- *
- * All registered callbacks are called with IPA_RM_RESOURCE_GRANTED
- * on successful completion of this operation.
- */
-int ipa_rm_request_resource(enum ipa_rm_resource_name resource_name)
-{
-	struct ipa_rm_resource *resource;
-	unsigned long flags;
-	int result;
-
-	if (unlikely(!ipa_rm_ctx)) {
-		IPA_RM_ERR("IPA RM was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (!IPA_RM_RESORCE_IS_PROD(resource_name)) {
-		IPA_RM_ERR("can be called on PROD only\n");
-		return -EINVAL;
-	}
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-			resource_name,
-			&resource) != 0) {
-		IPA_RM_ERR("resource does not exists\n");
-		result = -EPERM;
-		goto bail;
-	}
-	result = ipa_rm_resource_producer_request(
-			(struct ipa_rm_resource_prod *)resource);
-
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_request_resource);
-
-void delayed_release_work_func(struct work_struct *work)
-{
-	unsigned long flags;
-	struct ipa_rm_resource *resource;
-	struct ipa_rm_delayed_release_work_type *rwork = container_of(
-			to_delayed_work(work),
-			struct ipa_rm_delayed_release_work_type,
-			work);
-
-	if (!IPA_RM_RESORCE_IS_CONS(rwork->resource_name)) {
-		IPA_RM_ERR("can be called on CONS only\n");
-		kfree(rwork);
-		return;
-	}
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-					rwork->resource_name,
-					&resource) != 0) {
-		IPA_RM_ERR("resource does not exists\n");
-		goto bail;
-	}
-
-	ipa_rm_resource_consumer_release(
-		(struct ipa_rm_resource_cons *)resource, rwork->needed_bw,
-		rwork->dec_usage_count);
-
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-	kfree(rwork);
-
-}
-
-/**
- * ipa_rm_request_resource_with_timer() - requests the specified consumer
- * resource and releases it after 1 second
- * @resource_name: name of the requested resource
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_request_resource_with_timer(enum ipa_rm_resource_name resource_name)
-{
-	unsigned long flags;
-	struct ipa_rm_resource *resource;
-	struct ipa_rm_delayed_release_work_type *release_work;
-	int result;
-
-	if (!IPA_RM_RESORCE_IS_CONS(resource_name)) {
-		IPA_RM_ERR("can be called on CONS only\n");
-		return -EINVAL;
-	}
-
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-			resource_name,
-			&resource) != 0) {
-		IPA_RM_ERR("resource does not exists\n");
-		result = -EPERM;
-		goto bail;
-	}
-	result = ipa_rm_resource_consumer_request(
-			(struct ipa_rm_resource_cons *)resource, 0, false);
-	if (result != 0 && result != -EINPROGRESS) {
-		IPA_RM_ERR("consumer request returned error %d\n", result);
-		result = -EPERM;
-		goto bail;
-	}
-
-	release_work = kzalloc(sizeof(*release_work), GFP_ATOMIC);
-	if (!release_work) {
-		result = -ENOMEM;
-		goto bail;
-	}
-	release_work->resource_name = resource->name;
-	release_work->needed_bw = 0;
-	release_work->dec_usage_count = false;
-	INIT_DELAYED_WORK(&release_work->work, delayed_release_work_func);
-	schedule_delayed_work(&release_work->work,
-			msecs_to_jiffies(IPA_RM_RELEASE_DELAY_IN_MSEC));
-	result = 0;
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-
-	return result;
-}
-/**
- * ipa_rm_release_resource() - release resource
- * @resource_name: [in] name of the requested resource
- *
- * Returns: 0 on success, negative on failure
- *
- * All registered callbacks are called with IPA_RM_RESOURCE_RELEASED
- * on successful completion of this operation.
- */
-int ipa_rm_release_resource(enum ipa_rm_resource_name resource_name)
-{
-	unsigned long flags;
-	struct ipa_rm_resource *resource;
-	int result;
-
-	if (unlikely(!ipa_rm_ctx)) {
-		IPA_RM_ERR("IPA RM was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (!IPA_RM_RESORCE_IS_PROD(resource_name)) {
-		IPA_RM_ERR("can be called on PROD only\n");
-		return -EINVAL;
-	}
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-					  resource_name,
-					  &resource) != 0) {
-		IPA_RM_ERR("resource does not exists\n");
-		result = -EPERM;
-		goto bail;
-	}
-	result = ipa_rm_resource_producer_release(
-		    (struct ipa_rm_resource_prod *)resource);
-
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_release_resource);
-
-/**
- * ipa_rm_register() - register for event
- * @resource_name: resource name
- * @reg_params: [in] registration parameters
- *
- * Returns: 0 on success, negative on failure
- *
- * Registration parameters provided here should be the same
- * as provided later in  ipa_rm_deregister() call.
- */
-int ipa_rm_register(enum ipa_rm_resource_name resource_name,
-			struct ipa_rm_register_params *reg_params)
-{
-	int result;
-	unsigned long flags;
-	struct ipa_rm_resource *resource;
-
-	IPA_RM_DBG("%s\n", ipa_rm_resource_str(resource_name));
-
-	if (!IPA_RM_RESORCE_IS_PROD(resource_name)) {
-		IPA_RM_ERR("can be called on PROD only\n");
-		return -EINVAL;
-	}
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-				resource_name,
-				&resource) != 0) {
-		IPA_RM_ERR("resource does not exists\n");
-		result = -EPERM;
-		goto bail;
-	}
-	result = ipa_rm_resource_producer_register(
-			(struct ipa_rm_resource_prod *)resource,
-			reg_params,
-			true);
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_register);
-
-/**
- * ipa_rm_deregister() - cancel the registration
- * @resource_name: resource name
- * @reg_params: [in] registration parameters
- *
- * Returns: 0 on success, negative on failure
- *
- * Registration parameters provided here should be the same
- * as provided in  ipa_rm_register() call.
- */
-int ipa_rm_deregister(enum ipa_rm_resource_name resource_name,
-			struct ipa_rm_register_params *reg_params)
-{
-	int result;
-	unsigned long flags;
-	struct ipa_rm_resource *resource;
-
-	IPA_RM_DBG("%s\n", ipa_rm_resource_str(resource_name));
-
-	if (!IPA_RM_RESORCE_IS_PROD(resource_name)) {
-		IPA_RM_ERR("can be called on PROD only\n");
-		return -EINVAL;
-	}
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-			resource_name,
-			&resource) != 0) {
-		IPA_RM_ERR("resource does not exists\n");
-		result = -EPERM;
-		goto bail;
-	}
-	result = ipa_rm_resource_producer_deregister(
-			(struct ipa_rm_resource_prod *)resource,
-			reg_params);
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_deregister);
-
-/**
- * ipa_rm_set_perf_profile() - set performance profile
- * @resource_name: resource name
- * @profile: [in] profile information.
- *
- * Returns: 0 on success, negative on failure
- *
- * Set resource performance profile.
- * Updates IPA driver if performance level changed.
- */
-int ipa_rm_set_perf_profile(enum ipa_rm_resource_name resource_name,
-			struct ipa_rm_perf_profile *profile)
-{
-	int result;
-	unsigned long flags;
-	struct ipa_rm_resource *resource;
-
-	if (unlikely(!ipa_rm_ctx)) {
-		IPA_RM_ERR("IPA RM was not initialized\n");
-		return -EINVAL;
-	}
-
-	IPA_RM_DBG("%s\n", ipa_rm_resource_str(resource_name));
-
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-				resource_name,
-				&resource) != 0) {
-		IPA_RM_ERR("resource does not exists\n");
-		result = -EPERM;
-		goto bail;
-	}
-	result = ipa_rm_resource_set_perf_profile(resource, profile);
-	if (result) {
-		IPA_RM_ERR("ipa_rm_resource_set_perf_profile failed %d\n",
-			result);
-		goto bail;
-	}
-
-	result = 0;
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_set_perf_profile);
-
-/**
- * ipa_rm_notify_completion() -
- *	consumer driver notification for
- *	request_resource / release_resource operations
- *	completion
- * @event: notified event
- * @resource_name: resource name
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_notify_completion(enum ipa_rm_event event,
-		enum ipa_rm_resource_name resource_name)
-{
-	int result;
-
-	if (unlikely(!ipa_rm_ctx)) {
-		IPA_RM_ERR("IPA RM was not initialized\n");
-		return -EINVAL;
-	}
-
-	IPA_RM_DBG("event %d on %s\n", event,
-				ipa_rm_resource_str(resource_name));
-	if (!IPA_RM_RESORCE_IS_CONS(resource_name)) {
-		IPA_RM_ERR("can be called on CONS only\n");
-		result = -EINVAL;
-		goto bail;
-	}
-	ipa_rm_wq_send_cmd(IPA_RM_WQ_RESOURCE_CB,
-			resource_name,
-			event,
-			false);
-	result = 0;
-bail:
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_rm_notify_completion);
-
-static void ipa_rm_wq_handler(struct work_struct *work)
-{
-	unsigned long flags;
-	struct ipa_rm_resource *resource;
-	struct ipa_rm_wq_work_type *ipa_rm_work =
-			container_of(work,
-					struct ipa_rm_wq_work_type,
-					work);
-	IPA_RM_DBG("%s cmd=%d event=%d notify_registered_only=%d\n",
-		ipa_rm_resource_str(ipa_rm_work->resource_name),
-		ipa_rm_work->wq_cmd,
-		ipa_rm_work->event,
-		ipa_rm_work->notify_registered_only);
-	switch (ipa_rm_work->wq_cmd) {
-	case IPA_RM_WQ_NOTIFY_PROD:
-		if (!IPA_RM_RESORCE_IS_PROD(ipa_rm_work->resource_name)) {
-			IPA_RM_ERR("resource is not PROD\n");
-			goto free_work;
-		}
-		spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-		if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-						ipa_rm_work->resource_name,
-						&resource) != 0){
-			IPA_RM_ERR("resource does not exists\n");
-			spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-			goto free_work;
-		}
-		ipa_rm_resource_producer_notify_clients(
-				(struct ipa_rm_resource_prod *)resource,
-				ipa_rm_work->event,
-				ipa_rm_work->notify_registered_only);
-		spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-		break;
-	case IPA_RM_WQ_NOTIFY_CONS:
-		break;
-	case IPA_RM_WQ_RESOURCE_CB:
-		spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-		if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-						ipa_rm_work->resource_name,
-						&resource) != 0){
-			IPA_RM_ERR("resource does not exists\n");
-			spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-			goto free_work;
-		}
-		ipa_rm_resource_consumer_handle_cb(
-				(struct ipa_rm_resource_cons *)resource,
-				ipa_rm_work->event);
-		spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-		break;
-	default:
-		break;
-	}
-
-free_work:
-	kfree((void *) work);
-}
-
-static void ipa_rm_wq_resume_handler(struct work_struct *work)
-{
-	unsigned long flags;
-	struct ipa_rm_resource *resource;
-	struct ipa_rm_wq_suspend_resume_work_type *ipa_rm_work =
-			container_of(work,
-			struct ipa_rm_wq_suspend_resume_work_type,
-			work);
-	IPA_RM_DBG("resume work handler: %s",
-		ipa_rm_resource_str(ipa_rm_work->resource_name));
-
-	if (!IPA_RM_RESORCE_IS_CONS(ipa_rm_work->resource_name)) {
-		IPA_RM_ERR("resource is not CONS\n");
-		return;
-	}
-	ipa_inc_client_enable_clks();
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-					ipa_rm_work->resource_name,
-					&resource) != 0){
-		IPA_RM_ERR("resource does not exists\n");
-		spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-		ipa_dec_client_disable_clks();
-		goto bail;
-	}
-	ipa_rm_resource_consumer_request_work(
-			(struct ipa_rm_resource_cons *)resource,
-			ipa_rm_work->prev_state, ipa_rm_work->needed_bw, true);
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-bail:
-	kfree(ipa_rm_work);
-}
-
-
-static void ipa_rm_wq_suspend_handler(struct work_struct *work)
-{
-	unsigned long flags;
-	struct ipa_rm_resource *resource;
-	struct ipa_rm_wq_suspend_resume_work_type *ipa_rm_work =
-			container_of(work,
-			struct ipa_rm_wq_suspend_resume_work_type,
-			work);
-	IPA_RM_DBG("suspend work handler: %s",
-		ipa_rm_resource_str(ipa_rm_work->resource_name));
-
-	if (!IPA_RM_RESORCE_IS_CONS(ipa_rm_work->resource_name)) {
-		IPA_RM_ERR("resource is not CONS\n");
-		return;
-	}
-	ipa_suspend_resource_sync(ipa_rm_work->resource_name);
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-					ipa_rm_work->resource_name,
-					&resource) != 0){
-		IPA_RM_ERR("resource does not exists\n");
-		spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-		return;
-	}
-	ipa_rm_resource_consumer_release_work(
-			(struct ipa_rm_resource_cons *)resource,
-			ipa_rm_work->prev_state,
-			true);
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-
-	kfree(ipa_rm_work);
-}
-
-/**
- * ipa_rm_wq_send_cmd() - send a command for deferred work
- * @wq_cmd: command that should be executed
- * @resource_name: resource on which command should be executed
- * @notify_registered_only: notify only clients registered by
- *	ipa_rm_register()
- *
- * Returns: 0 on success, negative otherwise
- */
-int ipa_rm_wq_send_cmd(enum ipa_rm_wq_cmd wq_cmd,
-		enum ipa_rm_resource_name resource_name,
-		enum ipa_rm_event event,
-		bool notify_registered_only)
-{
-	int result = -ENOMEM;
-	struct ipa_rm_wq_work_type *work = kzalloc(sizeof(*work), GFP_ATOMIC);
-	if (work) {
-		INIT_WORK((struct work_struct *)work, ipa_rm_wq_handler);
-		work->wq_cmd = wq_cmd;
-		work->resource_name = resource_name;
-		work->event = event;
-		work->notify_registered_only = notify_registered_only;
-		result = queue_work(ipa_rm_ctx->ipa_rm_wq,
-				(struct work_struct *)work);
-	} else {
-		IPA_RM_ERR("no mem\n");
-	}
-
-	return result;
-}
-
-int ipa_rm_wq_send_suspend_cmd(enum ipa_rm_resource_name resource_name,
-		enum ipa_rm_resource_state prev_state,
-		u32 needed_bw)
-{
-	int result = -ENOMEM;
-	struct ipa_rm_wq_suspend_resume_work_type *work = kzalloc(sizeof(*work),
-			GFP_ATOMIC);
-	if (work) {
-		INIT_WORK((struct work_struct *)work,
-				ipa_rm_wq_suspend_handler);
-		work->resource_name = resource_name;
-		work->prev_state = prev_state;
-		work->needed_bw = needed_bw;
-		result = queue_work(ipa_rm_ctx->ipa_rm_wq,
-				(struct work_struct *)work);
-	} else {
-		IPA_RM_ERR("no mem\n");
-	}
-
-	return result;
-}
-
-int ipa_rm_wq_send_resume_cmd(enum ipa_rm_resource_name resource_name,
-		enum ipa_rm_resource_state prev_state,
-		u32 needed_bw)
-{
-	int result = -ENOMEM;
-	struct ipa_rm_wq_suspend_resume_work_type *work = kzalloc(sizeof(*work),
-			GFP_ATOMIC);
-	if (work) {
-		INIT_WORK((struct work_struct *)work, ipa_rm_wq_resume_handler);
-		work->resource_name = resource_name;
-		work->prev_state = prev_state;
-		work->needed_bw = needed_bw;
-		result = queue_work(ipa_rm_ctx->ipa_rm_wq,
-				(struct work_struct *)work);
-	} else {
-		IPA_RM_ERR("no mem\n");
-	}
-
-	return result;
-}
-/**
- * ipa_rm_initialize() - initialize IPA RM component
- *
- * Returns: 0 on success, negative otherwise
- */
-int ipa_rm_initialize(void)
-{
-	int result;
-
-	ipa_rm_ctx = kzalloc(sizeof(*ipa_rm_ctx), GFP_KERNEL);
-	if (!ipa_rm_ctx) {
-		IPA_RM_ERR("no mem\n");
-		result = -ENOMEM;
-		goto bail;
-	}
-	ipa_rm_ctx->ipa_rm_wq = create_singlethread_workqueue("ipa_rm_wq");
-	if (!ipa_rm_ctx->ipa_rm_wq) {
-		IPA_RM_ERR("create workqueue failed\n");
-		result = -ENOMEM;
-		goto create_wq_fail;
-	}
-	result = ipa_rm_dep_graph_create(&(ipa_rm_ctx->dep_graph));
-	if (result) {
-		IPA_RM_ERR("create dependency graph failed\n");
-		goto graph_alloc_fail;
-	}
-	spin_lock_init(&ipa_rm_ctx->ipa_rm_lock);
-	IPA_RM_DBG("SUCCESS\n");
-
-	return 0;
-graph_alloc_fail:
-	destroy_workqueue(ipa_rm_ctx->ipa_rm_wq);
-create_wq_fail:
-	kfree(ipa_rm_ctx);
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_stat() - print RM stat
- * @buf: [in] The user buff used to print
- * @size: [in] The size of buf
- * Returns: number of bytes used on success, negative on failure
- *
- * This function is called by ipa_debugfs in order to receive
- * a full picture of the current state of the RM
- */
-
-int ipa_rm_stat(char *buf, int size)
-{
-	unsigned long flags;
-	int i, cnt = 0, result = EINVAL;
-	struct ipa_rm_resource *resource = NULL;
-
-	if (!buf || size < 0)
-		return result;
-
-	spin_lock_irqsave(&ipa_rm_ctx->ipa_rm_lock, flags);
-	for (i = 0; i < IPA_RM_RESOURCE_PROD_MAX; ++i) {
-		result = ipa_rm_dep_graph_get_resource(
-				ipa_rm_ctx->dep_graph,
-				i,
-				&resource);
-		if (!result) {
-			result = ipa_rm_resource_producer_print_stat(
-							resource, buf + cnt,
-							size-cnt);
-			if (result < 0)
-				goto bail;
-			cnt += result;
-		}
-	}
-	result = cnt;
-bail:
-	spin_unlock_irqrestore(&ipa_rm_ctx->ipa_rm_lock, flags);
-
-	return result;
-}
-
-/**
- * ipa_rm_resource_str() - returns string that represent the resource
- * @resource_name: [in] resource name
- */
-const char *ipa_rm_resource_str(enum ipa_rm_resource_name resource_name)
-{
-	if (resource_name < 0 || resource_name >= IPA_RM_RESOURCE_MAX)
-		return "INVALID RESOURCE";
-
-	return resource_name_to_str[resource_name];
-};
-
-static void ipa_rm_perf_profile_notify_to_ipa_work(struct work_struct *work)
-{
-	struct ipa_rm_notify_ipa_work_type *notify_work = container_of(work,
-				struct ipa_rm_notify_ipa_work_type,
-				work);
-	int res;
-
-	IPA_RM_DBG("calling to IPA driver. voltage %d bandwidth %d\n",
-		notify_work->volt, notify_work->bandwidth_mbps);
-
-	res = ipa_set_required_perf_profile(notify_work->volt,
-		notify_work->bandwidth_mbps);
-	if (res) {
-		IPA_RM_ERR("ipa_set_required_perf_profile failed %d\n", res);
-		goto bail;
-	}
-
-	IPA_RM_DBG("IPA driver notified\n");
-bail:
-	kfree(notify_work);
-}
-
-static void ipa_rm_perf_profile_notify_to_ipa(enum ipa_voltage_level volt,
-					      u32 bandwidth)
-{
-	struct ipa_rm_notify_ipa_work_type *work;
-
-	work = kzalloc(sizeof(*work), GFP_ATOMIC);
-	if (!work) {
-		IPA_RM_ERR("no mem\n");
-		return;
-	}
-
-	INIT_WORK(&work->work, ipa_rm_perf_profile_notify_to_ipa_work);
-	work->volt = volt;
-	work->bandwidth_mbps = bandwidth;
-	queue_work(ipa_rm_ctx->ipa_rm_wq, &work->work);
-}
-
-/**
- * ipa_rm_perf_profile_change() - change performance profile vote for resource
- * @resource_name: [in] resource name
- *
- * change bandwidth and voltage vote based on resource state.
- */
-void ipa_rm_perf_profile_change(enum ipa_rm_resource_name resource_name)
-{
-	enum ipa_voltage_level old_volt;
-	u32 *bw_ptr;
-	u32 old_bw;
-	struct ipa_rm_resource *resource;
-	int i;
-	u32 sum_bw_prod = 0;
-	u32 sum_bw_cons = 0;
-
-	IPA_RM_DBG("%s\n", ipa_rm_resource_str(resource_name));
-
-	if (ipa_rm_dep_graph_get_resource(ipa_rm_ctx->dep_graph,
-					  resource_name,
-					  &resource) != 0) {
-			IPA_RM_ERR("resource does not exists\n");
-			WARN_ON(1);
-			return;
-	}
-
-	old_volt = ipa_rm_ctx->prof_vote.curr_volt;
-	old_bw = ipa_rm_ctx->prof_vote.curr_bw;
-
-	if (IPA_RM_RESORCE_IS_PROD(resource_name))
-		bw_ptr = &ipa_rm_ctx->prof_vote.bw_prods[resource_name];
-	else
-		bw_ptr = &ipa_rm_ctx->prof_vote.bw_cons[
-				resource_name - IPA_RM_RESOURCE_PROD_MAX];
-
-	switch (resource->state) {
-	case IPA_RM_GRANTED:
-	case IPA_RM_REQUEST_IN_PROGRESS:
-		IPA_RM_DBG("max_bw = %d, needed_bw = %d\n",
-			resource->max_bw, resource->needed_bw);
-		*bw_ptr = min(resource->max_bw, resource->needed_bw);
-		ipa_rm_ctx->prof_vote.volt[resource_name] =
-						resource->floor_voltage;
-		break;
-
-	case IPA_RM_RELEASE_IN_PROGRESS:
-	case IPA_RM_RELEASED:
-		*bw_ptr = 0;
-		ipa_rm_ctx->prof_vote.volt[resource_name] = 0;
-		break;
-
-	default:
-		IPA_RM_ERR("unknown state %d\n", resource->state);
-		WARN_ON(1);
-		return;
-	}
-	IPA_RM_DBG("resource bandwidth: %d voltage: %d\n", *bw_ptr,
-					resource->floor_voltage);
-
-	ipa_rm_ctx->prof_vote.curr_volt = IPA_VOLTAGE_UNSPECIFIED;
-	for (i = 0; i < IPA_RM_RESOURCE_MAX; i++) {
-		if (ipa_rm_ctx->prof_vote.volt[i] >
-				ipa_rm_ctx->prof_vote.curr_volt) {
-			ipa_rm_ctx->prof_vote.curr_volt =
-				ipa_rm_ctx->prof_vote.volt[i];
-		}
-	}
-
-	for (i = 0; i < IPA_RM_RESOURCE_PROD_MAX; i++)
-		sum_bw_prod += ipa_rm_ctx->prof_vote.bw_prods[i];
-
-	for (i = 0; i < IPA_RM_RESOURCE_CONS_MAX; i++)
-		sum_bw_cons += ipa_rm_ctx->prof_vote.bw_cons[i];
-
-	IPA_RM_DBG("all prod bandwidth: %d all cons bandwidth: %d\n",
-		sum_bw_prod, sum_bw_cons);
-	ipa_rm_ctx->prof_vote.curr_bw = min(sum_bw_prod, sum_bw_cons);
-
-	if (ipa_rm_ctx->prof_vote.curr_volt == old_volt &&
-		ipa_rm_ctx->prof_vote.curr_bw == old_bw) {
-		IPA_RM_DBG("same voting\n");
-		return;
-	}
-
-	IPA_RM_DBG("new voting: voltage %d bandwidth %d\n",
-		ipa_rm_ctx->prof_vote.curr_volt,
-		ipa_rm_ctx->prof_vote.curr_bw);
-
-	ipa_rm_perf_profile_notify_to_ipa(ipa_rm_ctx->prof_vote.curr_volt,
-			ipa_rm_ctx->prof_vote.curr_bw);
-
-	return;
-};
-
-/**
- * ipa_rm_exit() - free all IPA RM resources
- */
-void ipa_rm_exit(void)
-{
-	IPA_RM_DBG("ENTER\n");
-	ipa_rm_dep_graph_delete(ipa_rm_ctx->dep_graph);
-	destroy_workqueue(ipa_rm_ctx->ipa_rm_wq);
-	kfree(ipa_rm_ctx);
-	ipa_rm_ctx = NULL;
-	IPA_RM_DBG("EXIT\n");
-}
diff --git a/drivers/platform/msm/ipa/ipa_rm_dependency_graph.c b/drivers/platform/msm/ipa/ipa_rm_dependency_graph.c
deleted file mode 100644
index f1d6776..00000000
--- a/drivers/platform/msm/ipa/ipa_rm_dependency_graph.c
+++ /dev/null
@@ -1,245 +0,0 @@
-/* Copyright (c) 2013-2014, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/slab.h>
-#include "ipa_rm_dependency_graph.h"
-#include "ipa_rm_i.h"
-
-static int ipa_rm_dep_get_index(enum ipa_rm_resource_name resource_name)
-{
-	int resource_index = IPA_RM_INDEX_INVALID;
-
-	if (IPA_RM_RESORCE_IS_PROD(resource_name))
-		resource_index = ipa_rm_prod_index(resource_name);
-	else if (IPA_RM_RESORCE_IS_CONS(resource_name))
-		resource_index = ipa_rm_cons_index(resource_name);
-
-	return resource_index;
-}
-
-/**
- * ipa_rm_dep_graph_create() - creates graph
- * @dep_graph: [out] created dependency graph
- *
- * Returns: dependency graph on success, NULL on failure
- */
-int  ipa_rm_dep_graph_create(struct ipa_rm_dep_graph **dep_graph)
-{
-	int result = 0;
-
-	*dep_graph = kzalloc(sizeof(**dep_graph), GFP_KERNEL);
-	if (!*dep_graph) {
-		IPA_RM_ERR("no mem\n");
-		result = -ENOMEM;
-		goto bail;
-	}
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_dep_graph_delete() - destroyes the graph
- * @graph: [in] dependency graph
- *
- * Frees all resources.
- */
-void ipa_rm_dep_graph_delete(struct ipa_rm_dep_graph *graph)
-{
-	int resource_index;
-
-	if (!graph) {
-		IPA_RM_ERR("invalid params\n");
-		return;
-	}
-	for (resource_index = 0;
-			resource_index < IPA_RM_RESOURCE_MAX;
-			resource_index++)
-		kfree(graph->resource_table[resource_index]);
-	memset(graph->resource_table, 0, sizeof(graph->resource_table));
-}
-
-/**
- * ipa_rm_dep_graph_get_resource() - provides a resource by name
- * @graph: [in] dependency graph
- * @name: [in] name of the resource
- * @resource: [out] resource in case of success
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_dep_graph_get_resource(
-				struct ipa_rm_dep_graph *graph,
-				enum ipa_rm_resource_name resource_name,
-				struct ipa_rm_resource **resource)
-{
-	int result;
-	int resource_index;
-
-	if (!graph) {
-		result = -EINVAL;
-		goto bail;
-	}
-	resource_index = ipa_rm_dep_get_index(resource_name);
-	if (resource_index == IPA_RM_INDEX_INVALID) {
-		result = -EINVAL;
-		goto bail;
-	}
-	*resource = graph->resource_table[resource_index];
-	if (!*resource) {
-		result = -EINVAL;
-		goto bail;
-	}
-	result = 0;
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_dep_graph_add() - adds resource to graph
- * @graph: [in] dependency graph
- * @resource: [in] resource to add
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_dep_graph_add(struct ipa_rm_dep_graph *graph,
-			 struct ipa_rm_resource *resource)
-{
-	int result = 0;
-	int resource_index;
-
-	if (!graph || !resource) {
-		result = -EINVAL;
-		goto bail;
-	}
-	resource_index = ipa_rm_dep_get_index(resource->name);
-	if (resource_index == IPA_RM_INDEX_INVALID) {
-		result = -EINVAL;
-		goto bail;
-	}
-	graph->resource_table[resource_index] = resource;
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_dep_graph_remove() - removes resource from graph
- * @graph: [in] dependency graph
- * @resource: [in] resource to add
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_dep_graph_remove(struct ipa_rm_dep_graph *graph,
-		enum ipa_rm_resource_name resource_name)
-{
-	if (!graph)
-		return -EINVAL;
-	graph->resource_table[resource_name] = NULL;
-
-	return 0;
-}
-
-/**
- * ipa_rm_dep_graph_add_dependency() - adds dependency between
- *				two nodes in graph
- * @graph: [in] dependency graph
- * @resource_name: [in] resource to add
- * @depends_on_name: [in] resource to add
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_dep_graph_add_dependency(struct ipa_rm_dep_graph *graph,
-				    enum ipa_rm_resource_name resource_name,
-				    enum ipa_rm_resource_name depends_on_name)
-{
-	struct ipa_rm_resource *dependant = NULL;
-	struct ipa_rm_resource *dependency = NULL;
-	int result;
-
-	if (!graph ||
-		!IPA_RM_RESORCE_IS_PROD(resource_name) ||
-		!IPA_RM_RESORCE_IS_CONS(depends_on_name)) {
-		IPA_RM_ERR("invalid params\n");
-		result = -EINVAL;
-		goto bail;
-	}
-	if (ipa_rm_dep_graph_get_resource(graph,
-					  resource_name,
-					  &dependant)) {
-		IPA_RM_ERR("%s does not exist\n",
-					ipa_rm_resource_str(resource_name));
-		result = -EINVAL;
-		goto bail;
-	}
-	if (ipa_rm_dep_graph_get_resource(graph,
-					depends_on_name,
-					  &dependency)) {
-		IPA_RM_ERR("%s does not exist\n",
-					ipa_rm_resource_str(depends_on_name));
-		result = -EINVAL;
-		goto bail;
-	}
-	result = ipa_rm_resource_add_dependency(dependant, dependency);
-bail:
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-
-/**
- * ipa_rm_dep_graph_delete_dependency() - deleted dependency between
- *				two nodes in graph
- * @graph: [in] dependency graph
- * @resource_name: [in] resource to delete
- * @depends_on_name: [in] resource to delete
- *
- * Returns: 0 on success, negative on failure
- *
- */
-int ipa_rm_dep_graph_delete_dependency(struct ipa_rm_dep_graph *graph,
-				enum ipa_rm_resource_name resource_name,
-				enum ipa_rm_resource_name depends_on_name)
-{
-	struct ipa_rm_resource *dependant = NULL;
-	struct ipa_rm_resource *dependency = NULL;
-	int result;
-
-	if (!graph ||
-		!IPA_RM_RESORCE_IS_PROD(resource_name) ||
-		!IPA_RM_RESORCE_IS_CONS(depends_on_name)) {
-		IPA_RM_ERR("invalid params\n");
-		result = -EINVAL;
-		goto bail;
-	}
-
-	if (ipa_rm_dep_graph_get_resource(graph,
-					  resource_name,
-					  &dependant)) {
-		IPA_RM_ERR("%s does not exist\n",
-					ipa_rm_resource_str(resource_name));
-		result = -EINVAL;
-		goto bail;
-	}
-
-	if (ipa_rm_dep_graph_get_resource(graph,
-					  depends_on_name,
-					  &dependency)) {
-		IPA_RM_ERR("%s does not exist\n",
-					ipa_rm_resource_str(depends_on_name));
-		result = -EINVAL;
-		goto bail;
-	}
-
-	result = ipa_rm_resource_delete_dependency(dependant, dependency);
-bail:
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
diff --git a/drivers/platform/msm/ipa/ipa_rm_dependency_graph.h b/drivers/platform/msm/ipa/ipa_rm_dependency_graph.h
deleted file mode 100644
index 4396c38..00000000
--- a/drivers/platform/msm/ipa/ipa_rm_dependency_graph.h
+++ /dev/null
@@ -1,47 +0,0 @@
-/* Copyright (c) 2013-2014, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef _IPA_RM_DEPENDENCY_GRAPH_H_
-#define _IPA_RM_DEPENDENCY_GRAPH_H_
-
-#include <linux/list.h>
-#include <linux/ipa.h>
-#include "ipa_rm_resource.h"
-
-struct ipa_rm_dep_graph {
-	struct ipa_rm_resource *resource_table[IPA_RM_RESOURCE_MAX];
-};
-
-int ipa_rm_dep_graph_get_resource(
-				struct ipa_rm_dep_graph *graph,
-				enum ipa_rm_resource_name name,
-				struct ipa_rm_resource **resource);
-
-int ipa_rm_dep_graph_create(struct ipa_rm_dep_graph **dep_graph);
-
-void ipa_rm_dep_graph_delete(struct ipa_rm_dep_graph *graph);
-
-int ipa_rm_dep_graph_add(struct ipa_rm_dep_graph *graph,
-			 struct ipa_rm_resource *resource);
-
-int ipa_rm_dep_graph_remove(struct ipa_rm_dep_graph *graph,
-				enum ipa_rm_resource_name resource_name);
-
-int ipa_rm_dep_graph_add_dependency(struct ipa_rm_dep_graph *graph,
-				enum ipa_rm_resource_name resource_name,
-				enum ipa_rm_resource_name depends_on_name);
-
-int ipa_rm_dep_graph_delete_dependency(struct ipa_rm_dep_graph *graph,
-				enum ipa_rm_resource_name resource_name,
-				enum ipa_rm_resource_name depends_on_name);
-
-#endif /* _IPA_RM_DEPENDENCY_GRAPH_H_ */
diff --git a/drivers/platform/msm/ipa/ipa_rm_i.h b/drivers/platform/msm/ipa/ipa_rm_i.h
deleted file mode 100644
index c1e4955..00000000
--- a/drivers/platform/msm/ipa/ipa_rm_i.h
+++ /dev/null
@@ -1,128 +0,0 @@
-/* Copyright (c) 2013-2014, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef _IPA_RM_I_H_
-#define _IPA_RM_I_H_
-
-#include <linux/workqueue.h>
-#include <linux/ipa.h>
-#include "ipa_rm_resource.h"
-
-#define IPA_RM_DRV_NAME "ipa_rm"
-
-#define IPA_RM_DBG(fmt, args...) \
-	pr_debug(IPA_RM_DRV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-#define IPA_RM_ERR(fmt, args...) \
-	pr_err(IPA_RM_DRV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-
-#define IPA_RM_RESOURCE_CONS_MAX \
-	(IPA_RM_RESOURCE_MAX - IPA_RM_RESOURCE_PROD_MAX)
-#define IPA_RM_RESORCE_IS_PROD(x) \
-	(x >= IPA_RM_RESOURCE_PROD && x < IPA_RM_RESOURCE_PROD_MAX)
-#define IPA_RM_RESORCE_IS_CONS(x) \
-	(x >= IPA_RM_RESOURCE_PROD_MAX && x < IPA_RM_RESOURCE_MAX)
-#define IPA_RM_INDEX_INVALID	(-1)
-#define IPA_RM_RELEASE_DELAY_IN_MSEC 1000
-
-int ipa_rm_prod_index(enum ipa_rm_resource_name resource_name);
-int ipa_rm_cons_index(enum ipa_rm_resource_name resource_name);
-
-/**
- * struct ipa_rm_delayed_release_work_type - IPA RM delayed resource release
- *				work type
- * @delayed_work: work struct
- * @ipa_rm_resource_name: name of the resource on which this work should be done
- * @needed_bw: bandwidth required for resource in Mbps
- * @dec_usage_count: decrease usage count on release ?
- */
-struct ipa_rm_delayed_release_work_type {
-	struct delayed_work		work;
-	enum ipa_rm_resource_name	resource_name;
-	u32				needed_bw;
-	bool				dec_usage_count;
-
-};
-
-/**
- * enum ipa_rm_wq_cmd - workqueue commands
- */
-enum ipa_rm_wq_cmd {
-	IPA_RM_WQ_NOTIFY_PROD,
-	IPA_RM_WQ_NOTIFY_CONS,
-	IPA_RM_WQ_RESOURCE_CB
-};
-
-/**
- * struct ipa_rm_wq_work_type - IPA RM worqueue specific
- *				work type
- * @work: work struct
- * @wq_cmd: command that should be processed in workqueue context
- * @resource_name: name of the resource on which this work
- *			should be done
- * @dep_graph: data structure to search for resource if exists
- * @event: event to notify
- * @notify_registered_only: notify only clients registered by
- *	ipa_rm_register()
- */
-struct ipa_rm_wq_work_type {
-	struct work_struct		work;
-	enum ipa_rm_wq_cmd		wq_cmd;
-	enum ipa_rm_resource_name	resource_name;
-	enum ipa_rm_event		event;
-	bool				notify_registered_only;
-};
-
-/**
- * struct ipa_rm_wq_suspend_resume_work_type - IPA RM worqueue resume or
- *				suspend work type
- * @work: work struct
- * @resource_name: name of the resource on which this work
- *			should be done
- * @prev_state:
- * @needed_bw:
- */
-struct ipa_rm_wq_suspend_resume_work_type {
-	struct work_struct		work;
-	enum ipa_rm_resource_name	resource_name;
-	enum ipa_rm_resource_state	prev_state;
-	u32				needed_bw;
-
-};
-
-int ipa_rm_wq_send_cmd(enum ipa_rm_wq_cmd wq_cmd,
-		enum ipa_rm_resource_name resource_name,
-		enum ipa_rm_event event,
-		bool notify_registered_only);
-
-int ipa_rm_wq_send_resume_cmd(enum ipa_rm_resource_name resource_name,
-		enum ipa_rm_resource_state prev_state,
-		u32 needed_bw);
-
-int ipa_rm_wq_send_suspend_cmd(enum ipa_rm_resource_name resource_name,
-		enum ipa_rm_resource_state prev_state,
-		u32 needed_bw);
-
-int ipa_rm_initialize(void);
-
-int ipa_rm_stat(char *buf, int size);
-
-const char *ipa_rm_resource_str(enum ipa_rm_resource_name resource_name);
-
-void ipa_rm_perf_profile_change(enum ipa_rm_resource_name resource_name);
-
-int ipa_rm_request_resource_with_timer(enum ipa_rm_resource_name resource_name);
-
-void delayed_release_work_func(struct work_struct *work);
-
-void ipa_rm_exit(void);
-
-#endif /* _IPA_RM_I_H_ */
diff --git a/drivers/platform/msm/ipa/ipa_rm_inactivity_timer.c b/drivers/platform/msm/ipa/ipa_rm_inactivity_timer.c
deleted file mode 100644
index bd5606a..00000000
--- a/drivers/platform/msm/ipa/ipa_rm_inactivity_timer.c
+++ /dev/null
@@ -1,268 +0,0 @@
-/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/jiffies.h>
-#include <linux/kernel.h>
-#include <linux/slab.h>
-#include <linux/spinlock.h>
-#include <linux/timer.h>
-#include <linux/unistd.h>
-#include <linux/workqueue.h>
-#include <linux/ipa.h>
-#include "ipa_i.h"
-
-/**
- * struct ipa_rm_it_private - IPA RM Inactivity Timer private
- *	data
- * @initied: indicates if instance was initialized
- * @lock - spinlock for mutual exclusion
- * @resource_name - resource name
- * @work: delayed work object for running delayed releas
- *	function
- * @resource_requested: boolean flag indicates if resource was requested
- * @reschedule_work: boolean flag indicates to not release and to
- *	reschedule the release work.
- * @work_in_progress: boolean flag indicates is release work was scheduled.
- * @jiffies: number of jiffies for timeout
- *
- * WWAN private - holds all relevant info about WWAN driver
- */
-struct ipa_rm_it_private {
-	bool initied;
-	enum ipa_rm_resource_name resource_name;
-	spinlock_t lock;
-	struct delayed_work work;
-	bool resource_requested;
-	bool reschedule_work;
-	bool work_in_progress;
-	unsigned long jiffies;
-};
-
-static struct ipa_rm_it_private ipa_rm_it_handles[IPA_RM_RESOURCE_MAX];
-
-/**
- * ipa_rm_inactivity_timer_func() - called when timer expired in
- * the context of the shared workqueue. Checks internally if
- * reschedule_work flag is set. In case it is not set this function calls to
- * ipa_rm_release_resource(). In case reschedule_work is set this function
- * reschedule the work. This flag is cleared cleared when
- * calling to ipa_rm_inactivity_timer_release_resource().
- *
- * @work: work object provided by the work queue
- *
- * Return codes:
- * None
- */
-static void ipa_rm_inactivity_timer_func(struct work_struct *work)
-{
-
-	struct ipa_rm_it_private *me = container_of(to_delayed_work(work),
-						    struct ipa_rm_it_private,
-						    work);
-	unsigned long flags;
-
-	IPADBG("%s: timer expired for resource %d!\n", __func__,
-	    me->resource_name);
-
-	spin_lock_irqsave(
-		&ipa_rm_it_handles[me->resource_name].lock, flags);
-	if (ipa_rm_it_handles[me->resource_name].reschedule_work) {
-		IPADBG("%s: setting delayed work\n", __func__);
-		ipa_rm_it_handles[me->resource_name].reschedule_work = false;
-		schedule_delayed_work(
-			&ipa_rm_it_handles[me->resource_name].work,
-			ipa_rm_it_handles[me->resource_name].jiffies);
-	} else if (ipa_rm_it_handles[me->resource_name].resource_requested) {
-		IPADBG("%s: not calling release\n", __func__);
-		ipa_rm_it_handles[me->resource_name].work_in_progress = false;
-	} else {
-		IPADBG("%s: calling release_resource on resource %d!\n",
-			__func__, me->resource_name);
-		ipa_rm_release_resource(me->resource_name);
-		ipa_rm_it_handles[me->resource_name].work_in_progress = false;
-	}
-	spin_unlock_irqrestore(
-		&ipa_rm_it_handles[me->resource_name].lock, flags);
-}
-
-/**
-* ipa_rm_inactivity_timer_init() - Init function for IPA RM
-* inactivity timer. This function shall be called prior calling
-* any other API of IPA RM inactivity timer.
-*
-* @resource_name: Resource name. @see ipa_rm.h
-* @msecs: time in miliseccond, that IPA RM inactivity timer
-* shall wait prior calling to ipa_rm_release_resource().
-*
-* Return codes:
-* 0: success
-* -EINVAL: invalid parameters
-*/
-int ipa_rm_inactivity_timer_init(enum ipa_rm_resource_name resource_name,
-				 unsigned long msecs)
-{
-	IPADBG("%s: resource %d\n", __func__, resource_name);
-
-	if (resource_name < 0 ||
-	    resource_name >= IPA_RM_RESOURCE_MAX) {
-		IPAERR("%s: Invalid parameter\n", __func__);
-		return -EINVAL;
-	}
-
-	if (ipa_rm_it_handles[resource_name].initied) {
-		IPAERR("%s: resource %d already inited\n",
-		    __func__, resource_name);
-		return -EINVAL;
-	}
-
-	spin_lock_init(&ipa_rm_it_handles[resource_name].lock);
-	ipa_rm_it_handles[resource_name].resource_name = resource_name;
-	ipa_rm_it_handles[resource_name].jiffies = msecs_to_jiffies(msecs);
-	ipa_rm_it_handles[resource_name].resource_requested = false;
-	ipa_rm_it_handles[resource_name].reschedule_work = false;
-	ipa_rm_it_handles[resource_name].work_in_progress = false;
-
-	INIT_DELAYED_WORK(&ipa_rm_it_handles[resource_name].work,
-			  ipa_rm_inactivity_timer_func);
-	ipa_rm_it_handles[resource_name].initied = 1;
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_rm_inactivity_timer_init);
-
-/**
-* ipa_rm_inactivity_timer_destroy() - De-Init function for IPA
-* RM inactivity timer.
-*
-* @resource_name: Resource name. @see ipa_rm.h
-*
-* Return codes:
-* 0: success
-* -EINVAL: invalid parameters
-*/
-int ipa_rm_inactivity_timer_destroy(enum ipa_rm_resource_name resource_name)
-{
-	IPADBG("%s: resource %d\n", __func__, resource_name);
-
-	if (resource_name < 0 ||
-	    resource_name >= IPA_RM_RESOURCE_MAX) {
-		IPAERR("%s: Invalid parameter\n", __func__);
-		return -EINVAL;
-	}
-
-	if (!ipa_rm_it_handles[resource_name].initied) {
-		IPAERR("%s: resource %d already inited\n",
-		    __func__, resource_name);
-		return -EINVAL;
-	}
-
-	cancel_delayed_work_sync(&ipa_rm_it_handles[resource_name].work);
-
-	memset(&ipa_rm_it_handles[resource_name], 0,
-	       sizeof(struct ipa_rm_it_private));
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_rm_inactivity_timer_destroy);
-
-/**
-* ipa_rm_inactivity_timer_request_resource() - Same as
-* ipa_rm_request_resource(), with a difference that calling to
-* this function will also cancel the inactivity timer, if
-* ipa_rm_inactivity_timer_release_resource() was called earlier.
-*
-* @resource_name: Resource name. @see ipa_rm.h
-*
-* Return codes:
-* 0: success
-* -EINVAL: invalid parameters
-*/
-int ipa_rm_inactivity_timer_request_resource(
-				enum ipa_rm_resource_name resource_name)
-{
-	int ret;
-	unsigned long flags;
-	IPADBG("%s: resource %d\n", __func__, resource_name);
-
-	if (resource_name < 0 ||
-	    resource_name >= IPA_RM_RESOURCE_MAX) {
-		IPAERR("%s: Invalid parameter\n", __func__);
-		return -EINVAL;
-	}
-
-	if (!ipa_rm_it_handles[resource_name].initied) {
-		IPAERR("%s: Not initialized\n", __func__);
-		return -EINVAL;
-	}
-
-	spin_lock_irqsave(&ipa_rm_it_handles[resource_name].lock, flags);
-	ipa_rm_it_handles[resource_name].resource_requested = true;
-	spin_unlock_irqrestore(&ipa_rm_it_handles[resource_name].lock, flags);
-	ret = ipa_rm_request_resource(resource_name);
-	IPADBG("%s: resource %d: returning %d\n", __func__, resource_name, ret);
-	return ret;
-}
-EXPORT_SYMBOL(ipa_rm_inactivity_timer_request_resource);
-
-/**
-* ipa_rm_inactivity_timer_release_resource() - Sets the
-* inactivity timer to the timeout set by
-* ipa_rm_inactivity_timer_init(). When the timeout expires, IPA
-* RM inactivity timer will call to ipa_rm_release_resource().
-* If a call to ipa_rm_inactivity_timer_request_resource() was
-* made BEFORE the timout has expired, rge timer will be
-* cancelled.
-*
-* @resource_name: Resource name. @see ipa_rm.h
-*
-* Return codes:
-* 0: success
-* -EINVAL: invalid parameters
-*/
-int ipa_rm_inactivity_timer_release_resource(
-				enum ipa_rm_resource_name resource_name)
-{
-	unsigned long flags;
-	IPADBG("%s: resource %d\n", __func__, resource_name);
-
-	if (resource_name < 0 ||
-	    resource_name >= IPA_RM_RESOURCE_MAX) {
-		IPAERR("%s: Invalid parameter\n", __func__);
-		return -EINVAL;
-	}
-
-	if (!ipa_rm_it_handles[resource_name].initied) {
-		IPAERR("%s: Not initialized\n", __func__);
-		return -EINVAL;
-	}
-
-	spin_lock_irqsave(&ipa_rm_it_handles[resource_name].lock, flags);
-	ipa_rm_it_handles[resource_name].resource_requested = false;
-	if (ipa_rm_it_handles[resource_name].work_in_progress) {
-		IPADBG("%s: Timer already set, not scheduling again %d\n",
-		    __func__, resource_name);
-		ipa_rm_it_handles[resource_name].reschedule_work = true;
-		spin_unlock_irqrestore(
-			&ipa_rm_it_handles[resource_name].lock, flags);
-		return 0;
-	}
-	ipa_rm_it_handles[resource_name].work_in_progress = true;
-	ipa_rm_it_handles[resource_name].reschedule_work = false;
-	IPADBG("%s: setting delayed work\n", __func__);
-	schedule_delayed_work(&ipa_rm_it_handles[resource_name].work,
-			      ipa_rm_it_handles[resource_name].jiffies);
-	spin_unlock_irqrestore(&ipa_rm_it_handles[resource_name].lock, flags);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_rm_inactivity_timer_release_resource);
-
diff --git a/drivers/platform/msm/ipa/ipa_rm_peers_list.c b/drivers/platform/msm/ipa/ipa_rm_peers_list.c
deleted file mode 100644
index 41ae204..00000000
--- a/drivers/platform/msm/ipa/ipa_rm_peers_list.c
+++ /dev/null
@@ -1,247 +0,0 @@
-/* Copyright (c) 2013-2014, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/slab.h>
-#include "ipa_i.h"
-#include "ipa_rm_i.h"
-
-/**
- * ipa_rm_peers_list_get_resource_index() - resource name to index
- *	of this resource in corresponding peers list
- * @resource_name: [in] resource name
- *
- * Returns: resource index mapping, IPA_RM_INDEX_INVALID
- * in case provided resource name isn't contained in enum
- * ipa_rm_resource_name.
- *
- */
-static int ipa_rm_peers_list_get_resource_index(
-		enum ipa_rm_resource_name resource_name)
-{
-	int resource_index = IPA_RM_INDEX_INVALID;
-
-	if (IPA_RM_RESORCE_IS_PROD(resource_name))
-		resource_index = ipa_rm_prod_index(resource_name);
-	else if (IPA_RM_RESORCE_IS_CONS(resource_name)) {
-		resource_index = ipa_rm_cons_index(resource_name);
-		if (resource_index != IPA_RM_INDEX_INVALID)
-			resource_index =
-				resource_index - IPA_RM_RESOURCE_PROD_MAX;
-	}
-
-	return resource_index;
-}
-
-static bool ipa_rm_peers_list_check_index(int index,
-		struct ipa_rm_peers_list *peers_list)
-{
-	return !(index > peers_list->max_peers || index < 0);
-}
-
-/**
- * ipa_rm_peers_list_create() - creates the peers list
- *
- * @max_peers: maximum number of peers in new list
- * @peers_list: [out] newly created peers list
- *
- * Returns: 0 in case of SUCCESS, negative otherwise
- */
-int ipa_rm_peers_list_create(int max_peers,
-		struct ipa_rm_peers_list **peers_list)
-{
-	int result;
-
-	*peers_list = kzalloc(sizeof(**peers_list), GFP_ATOMIC);
-	if (!*peers_list) {
-		IPA_RM_ERR("no mem\n");
-		result = -ENOMEM;
-		goto bail;
-	}
-
-	(*peers_list)->max_peers = max_peers;
-	(*peers_list)->peers = kzalloc((*peers_list)->max_peers *
-				sizeof(struct ipa_rm_resource *), GFP_ATOMIC);
-	if (!((*peers_list)->peers)) {
-		IPA_RM_ERR("no mem\n");
-		result = -ENOMEM;
-		goto list_alloc_fail;
-	}
-
-	return 0;
-
-list_alloc_fail:
-	kfree(*peers_list);
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_peers_list_delete() - deletes the peers list
- *
- * @peers_list: peers list
- *
- */
-void ipa_rm_peers_list_delete(struct ipa_rm_peers_list *peers_list)
-{
-	if (peers_list) {
-		kfree(peers_list->peers);
-		kfree(peers_list);
-	}
-}
-
-/**
- * ipa_rm_peers_list_remove_peer() - removes peer from the list
- *
- * @peers_list: peers list
- * @resource_name: name of the resource to remove
- *
- */
-void ipa_rm_peers_list_remove_peer(
-		struct ipa_rm_peers_list *peers_list,
-		enum ipa_rm_resource_name resource_name)
-{
-	if (!peers_list)
-		return;
-
-	peers_list->peers[ipa_rm_peers_list_get_resource_index(
-			resource_name)] = NULL;
-	peers_list->peers_count--;
-}
-
-/**
- * ipa_rm_peers_list_add_peer() - adds peer to the list
- *
- * @peers_list: peers list
- * @resource: resource to add
- *
- */
-void ipa_rm_peers_list_add_peer(
-		struct ipa_rm_peers_list *peers_list,
-		struct ipa_rm_resource *resource)
-{
-	if (!peers_list || !resource)
-		return;
-
-	peers_list->peers[ipa_rm_peers_list_get_resource_index(
-			resource->name)] =
-			resource;
-	peers_list->peers_count++;
-}
-
-/**
- * ipa_rm_peers_list_is_empty() - checks
- *	if resource peers list is empty
- *
- * @peers_list: peers list
- *
- * Returns: true if the list is empty, false otherwise
- */
-bool ipa_rm_peers_list_is_empty(struct ipa_rm_peers_list *peers_list)
-{
-	bool result = true;
-
-	if (!peers_list)
-		goto bail;
-
-	if (peers_list->peers_count > 0)
-		result = false;
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_peers_list_has_last_peer() - checks
- *	if resource peers list has exactly one peer
- *
- * @peers_list: peers list
- *
- * Returns: true if the list has exactly one peer, false otherwise
- */
-bool ipa_rm_peers_list_has_last_peer(
-		struct ipa_rm_peers_list *peers_list)
-{
-	bool result = false;
-
-	if (!peers_list)
-		goto bail;
-
-	if (peers_list->peers_count == 1)
-		result = true;
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_peers_list_check_dependency() - check dependency
- *	between 2 peer lists
- * @resource_peers: first peers list
- * @resource_name: first peers list resource name
- * @depends_on_peers: second peers list
- * @depends_on_name: second peers list resource name
- *
- * Returns: true if there is dependency, false otherwise
- *
- */
-bool ipa_rm_peers_list_check_dependency(
-		struct ipa_rm_peers_list *resource_peers,
-		enum ipa_rm_resource_name resource_name,
-		struct ipa_rm_peers_list *depends_on_peers,
-		enum ipa_rm_resource_name depends_on_name)
-{
-	bool result = false;
-
-	if (!resource_peers || !depends_on_peers)
-		return result;
-
-	if (resource_peers->peers[ipa_rm_peers_list_get_resource_index(
-			depends_on_name)] != NULL)
-		result = true;
-
-	if (depends_on_peers->peers[ipa_rm_peers_list_get_resource_index(
-						resource_name)] != NULL)
-		result = true;
-
-	return result;
-}
-
-/**
- * ipa_rm_peers_list_get_resource() - get resource by
- *	resource index
- * @resource_index: resource index
- * @resource_peers: peers list
- *
- * Returns: the resource if found, NULL otherwise
- */
-struct ipa_rm_resource *ipa_rm_peers_list_get_resource(int resource_index,
-		struct ipa_rm_peers_list *resource_peers)
-{
-	struct ipa_rm_resource *result = NULL;
-
-	if (!ipa_rm_peers_list_check_index(resource_index, resource_peers))
-		goto bail;
-
-	result = resource_peers->peers[resource_index];
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_peers_list_get_size() - get peers list sise
- *
- * @peers_list: peers list
- *
- * Returns: the size of the peers list
- */
-int ipa_rm_peers_list_get_size(struct ipa_rm_peers_list *peers_list)
-{
-	return peers_list->max_peers;
-}
diff --git a/drivers/platform/msm/ipa/ipa_rm_peers_list.h b/drivers/platform/msm/ipa/ipa_rm_peers_list.h
deleted file mode 100644
index 86856e2..00000000
--- a/drivers/platform/msm/ipa/ipa_rm_peers_list.h
+++ /dev/null
@@ -1,53 +0,0 @@
-/* Copyright (c) 2013-2014, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef _IPA_RM_PEERS_LIST_H_
-#define _IPA_RM_PEERS_LIST_H_
-
-#include "ipa_rm_resource.h"
-
-/**
- * struct ipa_rm_peers_list - IPA RM resource peers list
- * @peers: the list of references to resources dependent on this resource
- *          in case of producer or list of dependencies in case of consumer
- * @max_peers: maximum number of peers for this resource
- * @peers_count: actual number of peers for this resource
- */
-struct ipa_rm_peers_list {
-	struct ipa_rm_resource		**peers;
-	int				max_peers;
-	int				peers_count;
-};
-
-int ipa_rm_peers_list_create(int max_peers,
-		struct ipa_rm_peers_list **peers_list);
-void ipa_rm_peers_list_delete(struct ipa_rm_peers_list *peers_list);
-void ipa_rm_peers_list_remove_peer(
-		struct ipa_rm_peers_list *peers_list,
-		enum ipa_rm_resource_name resource_name);
-void ipa_rm_peers_list_add_peer(
-		struct ipa_rm_peers_list *peers_list,
-		struct ipa_rm_resource *resource);
-bool ipa_rm_peers_list_check_dependency(
-		struct ipa_rm_peers_list *resource_peers,
-		enum ipa_rm_resource_name resource_name,
-		struct ipa_rm_peers_list *depends_on_peers,
-		enum ipa_rm_resource_name depends_on_name);
-struct ipa_rm_resource *ipa_rm_peers_list_get_resource(int resource_index,
-		struct ipa_rm_peers_list *peers_list);
-int ipa_rm_peers_list_get_size(struct ipa_rm_peers_list *peers_list);
-bool ipa_rm_peers_list_is_empty(struct ipa_rm_peers_list *peers_list);
-bool ipa_rm_peers_list_has_last_peer(
-		struct ipa_rm_peers_list *peers_list);
-
-
-#endif /* _IPA_RM_PEERS_LIST_H_ */
diff --git a/drivers/platform/msm/ipa/ipa_rm_resource.c b/drivers/platform/msm/ipa/ipa_rm_resource.c
deleted file mode 100644
index 1ec97ce..00000000
--- a/drivers/platform/msm/ipa/ipa_rm_resource.c
+++ /dev/null
@@ -1,1164 +0,0 @@
-/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/slab.h>
-#include "ipa_i.h"
-#include "ipa_rm_resource.h"
-#include "ipa_rm_i.h"
-
-/**
- * ipa_rm_dep_prod_index() - producer name to producer index mapping
- * @resource_name: [in] resource name (should be of producer)
- *
- * Returns: resource index mapping, IPA_RM_INDEX_INVALID
- *	in case provided resource name isn't contained
- *	in enum ipa_rm_resource_name or is not of producers.
- *
- */
-int ipa_rm_prod_index(enum ipa_rm_resource_name resource_name)
-{
-	int result = resource_name;
-
-	switch (resource_name) {
-	case IPA_RM_RESOURCE_Q6_PROD:
-	case IPA_RM_RESOURCE_USB_PROD:
-	case IPA_RM_RESOURCE_HSIC_PROD:
-	case IPA_RM_RESOURCE_STD_ECM_PROD:
-	case IPA_RM_RESOURCE_RNDIS_PROD:
-	case IPA_RM_RESOURCE_WWAN_0_PROD:
-	case IPA_RM_RESOURCE_WLAN_PROD:
-	case IPA_RM_RESOURCE_ODU_ADAPT_PROD:
-	case IPA_RM_RESOURCE_MHI_PROD:
-		break;
-	default:
-		result = IPA_RM_INDEX_INVALID;
-		break;
-	}
-
-	return result;
-}
-
-/**
- * ipa_rm_cons_index() - consumer name to consumer index mapping
- * @resource_name: [in] resource name (should be of consumer)
- *
- * Returns: resource index mapping, IPA_RM_INDEX_INVALID
- *	in case provided resource name isn't contained
- *	in enum ipa_rm_resource_name or is not of consumers.
- *
- */
-int ipa_rm_cons_index(enum ipa_rm_resource_name resource_name)
-{
-	int result = resource_name;
-
-	switch (resource_name) {
-	case IPA_RM_RESOURCE_Q6_CONS:
-	case IPA_RM_RESOURCE_USB_CONS:
-	case IPA_RM_RESOURCE_HSIC_CONS:
-	case IPA_RM_RESOURCE_WLAN_CONS:
-	case IPA_RM_RESOURCE_APPS_CONS:
-	case IPA_RM_RESOURCE_ODU_ADAPT_CONS:
-	case IPA_RM_RESOURCE_MHI_CONS:
-		break;
-	default:
-		result = IPA_RM_INDEX_INVALID;
-		break;
-	}
-
-	return result;
-}
-
-int ipa_rm_resource_consumer_release_work(
-		struct ipa_rm_resource_cons *consumer,
-		enum ipa_rm_resource_state prev_state,
-		bool notify_completion)
-{
-	int driver_result;
-
-	IPA_RM_DBG("calling driver CB\n");
-	driver_result = consumer->release_resource();
-	IPA_RM_DBG("driver CB returned with %d\n", driver_result);
-	/*
-	 * Treat IPA_RM_RELEASE_IN_PROGRESS as IPA_RM_RELEASED
-	 * for CONS which remains in RELEASE_IN_PROGRESS.
-	 */
-	if (driver_result == -EINPROGRESS)
-		driver_result = 0;
-	if (driver_result != 0 && driver_result != -EINPROGRESS) {
-		IPA_RM_ERR("driver CB returned error %d\n", driver_result);
-		consumer->resource.state = prev_state;
-		goto bail;
-	}
-	if (driver_result == 0) {
-		if (notify_completion)
-			ipa_rm_resource_consumer_handle_cb(consumer,
-					IPA_RM_RESOURCE_RELEASED);
-		else
-			consumer->resource.state = IPA_RM_RELEASED;
-	}
-	complete_all(&consumer->request_consumer_in_progress);
-
-	ipa_rm_perf_profile_change(consumer->resource.name);
-bail:
-	return driver_result;
-}
-
-int ipa_rm_resource_consumer_request_work(struct ipa_rm_resource_cons *consumer,
-		enum ipa_rm_resource_state prev_state,
-		u32 prod_needed_bw,
-		bool notify_completion)
-{
-	int driver_result;
-
-	IPA_RM_DBG("calling driver CB\n");
-	driver_result = consumer->request_resource();
-	IPA_RM_DBG("driver CB returned with %d\n", driver_result);
-	if (driver_result == 0) {
-		if (notify_completion) {
-			ipa_rm_resource_consumer_handle_cb(consumer,
-					IPA_RM_RESOURCE_GRANTED);
-		} else {
-			consumer->resource.state = IPA_RM_GRANTED;
-			ipa_rm_perf_profile_change(consumer->resource.name);
-			ipa_resume_resource(consumer->resource.name);
-		}
-	} else if (driver_result != -EINPROGRESS) {
-		consumer->resource.state = prev_state;
-		consumer->resource.needed_bw -= prod_needed_bw;
-		consumer->usage_count--;
-	}
-
-	return driver_result;
-}
-
-int ipa_rm_resource_consumer_request(
-		struct ipa_rm_resource_cons *consumer,
-		u32 prod_needed_bw,
-		bool inc_usage_count)
-{
-	int result = 0;
-	enum ipa_rm_resource_state prev_state;
-
-	IPA_RM_DBG("%s state: %d\n",
-			ipa_rm_resource_str(consumer->resource.name),
-			consumer->resource.state);
-
-	prev_state = consumer->resource.state;
-	consumer->resource.needed_bw += prod_needed_bw;
-	switch (consumer->resource.state) {
-	case IPA_RM_RELEASED:
-	case IPA_RM_RELEASE_IN_PROGRESS:
-		INIT_COMPLETION(consumer->request_consumer_in_progress);
-		consumer->resource.state = IPA_RM_REQUEST_IN_PROGRESS;
-		if (prev_state == IPA_RM_RELEASE_IN_PROGRESS ||
-				ipa_inc_client_enable_clks_no_block() != 0) {
-			IPA_RM_DBG("async resume work for %s\n",
-				ipa_rm_resource_str(consumer->resource.name));
-			ipa_rm_wq_send_resume_cmd(consumer->resource.name,
-						prev_state,
-						prod_needed_bw);
-			result = -EINPROGRESS;
-			break;
-		}
-		result = ipa_rm_resource_consumer_request_work(consumer,
-						prev_state,
-						prod_needed_bw,
-						false);
-		break;
-	case IPA_RM_GRANTED:
-		ipa_rm_perf_profile_change(consumer->resource.name);
-		break;
-	case IPA_RM_REQUEST_IN_PROGRESS:
-		result = -EINPROGRESS;
-		break;
-	default:
-		consumer->resource.needed_bw -= prod_needed_bw;
-		result = -EPERM;
-		goto bail;
-	}
-	if (inc_usage_count)
-		consumer->usage_count++;
-bail:
-	IPA_RM_DBG("%s new state: %d\n",
-		ipa_rm_resource_str(consumer->resource.name),
-		consumer->resource.state);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-
-int ipa_rm_resource_consumer_release(
-		struct ipa_rm_resource_cons *consumer,
-		u32 prod_needed_bw,
-		bool dec_usage_count)
-{
-	int result = 0;
-	enum ipa_rm_resource_state save_state;
-
-	IPA_RM_DBG("%s state: %d\n",
-		ipa_rm_resource_str(consumer->resource.name),
-		consumer->resource.state);
-	save_state = consumer->resource.state;
-	consumer->resource.needed_bw -= prod_needed_bw;
-	switch (consumer->resource.state) {
-	case IPA_RM_RELEASED:
-		break;
-	case IPA_RM_GRANTED:
-	case IPA_RM_REQUEST_IN_PROGRESS:
-		if (dec_usage_count && consumer->usage_count > 0)
-			consumer->usage_count--;
-		if (consumer->usage_count == 0) {
-			consumer->resource.state = IPA_RM_RELEASE_IN_PROGRESS;
-			if (save_state == IPA_RM_REQUEST_IN_PROGRESS ||
-			    ipa_suspend_resource_no_block(
-						consumer->resource.name) != 0) {
-				ipa_rm_wq_send_suspend_cmd(
-						consumer->resource.name,
-						save_state,
-						prod_needed_bw);
-				result = -EINPROGRESS;
-				goto bail;
-			}
-			result = ipa_rm_resource_consumer_release_work(consumer,
-					save_state, false);
-			goto bail;
-		} else if (consumer->resource.state == IPA_RM_GRANTED) {
-			ipa_rm_perf_profile_change(consumer->resource.name);
-		}
-		break;
-	case IPA_RM_RELEASE_IN_PROGRESS:
-		if (dec_usage_count && consumer->usage_count > 0)
-			consumer->usage_count--;
-		result = -EINPROGRESS;
-		break;
-	default:
-		result = -EPERM;
-		goto bail;
-	}
-bail:
-	IPA_RM_DBG("%s new state: %d\n",
-		ipa_rm_resource_str(consumer->resource.name),
-		consumer->resource.state);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-
-/**
- * ipa_rm_resource_producer_notify_clients() - notify
- *	all registered clients of given producer
- * @producer: producer
- * @event: event to notify
- * @notify_registered_only: notify only clients registered by
- *	ipa_rm_register()
- */
-void ipa_rm_resource_producer_notify_clients(
-				struct ipa_rm_resource_prod *producer,
-				enum ipa_rm_event event,
-				bool notify_registered_only)
-{
-	struct ipa_rm_notification_info *reg_info;
-
-	IPA_RM_DBG("%s event: %d notify_registered_only: %d\n",
-		ipa_rm_resource_str(producer->resource.name),
-		event,
-		notify_registered_only);
-
-	list_for_each_entry(reg_info, &(producer->event_listeners), link) {
-		if (notify_registered_only && !reg_info->explicit)
-			continue;
-
-		IPA_RM_DBG("Notifying %s event: %d\n",
-			   ipa_rm_resource_str(producer->resource.name), event);
-		reg_info->reg_params.notify_cb(reg_info->reg_params.user_data,
-					       event,
-					       0);
-		IPA_RM_DBG("back from client CB\n");
-	}
-
-	return;
-}
-
-static int ipa_rm_resource_producer_create(struct ipa_rm_resource **resource,
-		struct ipa_rm_resource_prod **producer,
-		struct ipa_rm_create_params *create_params,
-		int *max_peers)
-{
-	int result = 0;
-
-	*producer = kzalloc(sizeof(**producer), GFP_ATOMIC);
-	if (*producer == NULL) {
-		IPA_RM_ERR("no mem\n");
-		result = -ENOMEM;
-		goto bail;
-	}
-
-	INIT_LIST_HEAD(&((*producer)->event_listeners));
-	result = ipa_rm_resource_producer_register(*producer,
-			&(create_params->reg_params),
-			false);
-	if (result) {
-		IPA_RM_ERR("ipa_rm_resource_producer_register() failed\n");
-		goto register_fail;
-	}
-
-	(*resource) = (struct ipa_rm_resource *) (*producer);
-	(*resource)->type = IPA_RM_PRODUCER;
-	*max_peers = IPA_RM_RESOURCE_CONS_MAX;
-	goto bail;
-register_fail:
-	kfree(*producer);
-bail:
-	return result;
-}
-
-static void ipa_rm_resource_producer_delete(
-				struct ipa_rm_resource_prod *producer)
-{
-	struct ipa_rm_notification_info *reg_info;
-	struct list_head *pos, *q;
-
-	ipa_rm_resource_producer_release(producer);
-	list_for_each_safe(pos, q, &(producer->event_listeners)) {
-		reg_info = list_entry(pos,
-				struct ipa_rm_notification_info,
-				link);
-		list_del(pos);
-		kfree(reg_info);
-	}
-}
-
-static int ipa_rm_resource_consumer_create(struct ipa_rm_resource **resource,
-		struct ipa_rm_resource_cons **consumer,
-		struct ipa_rm_create_params *create_params,
-		int *max_peers)
-{
-	int result = 0;
-
-	*consumer = kzalloc(sizeof(**consumer), GFP_ATOMIC);
-	if (*consumer == NULL) {
-		IPA_RM_ERR("no mem\n");
-		result = -ENOMEM;
-		goto bail;
-	}
-
-	(*consumer)->request_resource = create_params->request_resource;
-	(*consumer)->release_resource = create_params->release_resource;
-	(*resource) = (struct ipa_rm_resource *) (*consumer);
-	(*resource)->type = IPA_RM_CONSUMER;
-	init_completion(&((*consumer)->request_consumer_in_progress));
-	*max_peers = IPA_RM_RESOURCE_PROD_MAX;
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_resource_create() - creates resource
- * @create_params: [in] parameters needed
- *			for resource initialization with IPA RM
- * @resource: [out] created resource
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_resource_create(
-		struct ipa_rm_create_params *create_params,
-		struct ipa_rm_resource **resource)
-{
-	struct ipa_rm_resource_cons *consumer;
-	struct ipa_rm_resource_prod *producer;
-	int max_peers;
-	int result = 0;
-
-	if (!create_params) {
-		result = -EINVAL;
-		goto bail;
-	}
-
-	if (IPA_RM_RESORCE_IS_PROD(create_params->name)) {
-		result = ipa_rm_resource_producer_create(resource,
-				&producer,
-				create_params,
-				&max_peers);
-		if (result) {
-			IPA_RM_ERR("ipa_rm_resource_producer_create failed\n");
-			goto bail;
-		}
-	} else if (IPA_RM_RESORCE_IS_CONS(create_params->name)) {
-		result = ipa_rm_resource_consumer_create(resource,
-				&consumer,
-				create_params,
-				&max_peers);
-		if (result) {
-			IPA_RM_ERR("ipa_rm_resource_producer_create failed\n");
-			goto bail;
-		}
-	} else {
-		IPA_RM_ERR("invalied resource\n");
-		result = -EPERM;
-		goto bail;
-	}
-
-	result = ipa_rm_peers_list_create(max_peers,
-			&((*resource)->peers_list));
-	if (result) {
-		IPA_RM_ERR("ipa_rm_peers_list_create failed\n");
-		goto peers_alloc_fail;
-	}
-	(*resource)->name = create_params->name;
-	(*resource)->floor_voltage = create_params->floor_voltage;
-	(*resource)->state = IPA_RM_RELEASED;
-	goto bail;
-
-peers_alloc_fail:
-	ipa_rm_resource_delete(*resource);
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_resource_delete() - deletes resource
- * @resource: [in] resource
- *			for resource initialization with IPA RM
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_resource_delete(struct ipa_rm_resource *resource)
-{
-	struct ipa_rm_resource *consumer;
-	struct ipa_rm_resource *producer;
-	int peers_index;
-	int result = 0;
-	int list_size;
-
-	if (!resource) {
-		IPA_RM_ERR("invalid params\n");
-		return -EINVAL;
-	}
-
-	IPA_RM_DBG("ipa_rm_resource_delete ENTER with resource %d\n",
-					resource->name);
-	if (resource->type == IPA_RM_PRODUCER) {
-		if (resource->peers_list) {
-			list_size = ipa_rm_peers_list_get_size(
-				resource->peers_list);
-			for (peers_index = 0;
-				peers_index < list_size;
-				peers_index++) {
-				consumer = ipa_rm_peers_list_get_resource(
-						peers_index,
-						resource->peers_list);
-				if (consumer)
-					ipa_rm_resource_delete_dependency(
-						resource,
-						consumer);
-			}
-		}
-
-		ipa_rm_resource_producer_delete(
-				(struct ipa_rm_resource_prod *) resource);
-	} else if (resource->type == IPA_RM_CONSUMER) {
-		if (resource->peers_list) {
-			list_size = ipa_rm_peers_list_get_size(
-				resource->peers_list);
-			for (peers_index = 0;
-					peers_index < list_size;
-					peers_index++){
-				producer = ipa_rm_peers_list_get_resource(
-							peers_index,
-							resource->peers_list);
-				if (producer)
-					ipa_rm_resource_delete_dependency(
-							producer,
-							resource);
-			}
-		}
-	}
-	ipa_rm_peers_list_delete(resource->peers_list);
-	kfree(resource);
-	return result;
-}
-
-/**
- * ipa_rm_resource_register() - register resource
- * @resource: [in] resource
- * @reg_params: [in] registration parameters
- * @explicit: [in] registered explicitly by ipa_rm_register()
- *
- * Returns: 0 on success, negative on failure
- *
- * Producer resource is expected for this call.
- *
- */
-int ipa_rm_resource_producer_register(struct ipa_rm_resource_prod *producer,
-		struct ipa_rm_register_params *reg_params,
-		bool explicit)
-{
-	int result = 0;
-	struct ipa_rm_notification_info *reg_info;
-	struct list_head *pos;
-
-	if (!producer || !reg_params) {
-		IPA_RM_ERR("invalid params\n");
-		result = -EPERM;
-		goto bail;
-	}
-
-	list_for_each(pos, &(producer->event_listeners)) {
-		reg_info = list_entry(pos,
-					struct ipa_rm_notification_info,
-					link);
-		if (reg_info->reg_params.notify_cb ==
-						reg_params->notify_cb) {
-			IPA_RM_ERR("already registered\n");
-			result = -EPERM;
-			goto bail;
-		}
-
-	}
-
-	reg_info = kzalloc(sizeof(*reg_info), GFP_ATOMIC);
-	if (reg_info == NULL) {
-		IPA_RM_ERR("no mem\n");
-		result = -ENOMEM;
-		goto bail;
-	}
-
-	reg_info->reg_params.user_data = reg_params->user_data;
-	reg_info->reg_params.notify_cb = reg_params->notify_cb;
-	reg_info->explicit = explicit;
-	INIT_LIST_HEAD(&reg_info->link);
-	list_add(&reg_info->link, &producer->event_listeners);
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_resource_deregister() - register resource
- * @resource: [in] resource
- * @reg_params: [in] registration parameters
- *
- * Returns: 0 on success, negative on failure
- *
- * Producer resource is expected for this call.
- * This function deleted only single instance of
- * registration info.
- *
- */
-int ipa_rm_resource_producer_deregister(struct ipa_rm_resource_prod *producer,
-		struct ipa_rm_register_params *reg_params)
-{
-	int result = -EINVAL;
-	struct ipa_rm_notification_info *reg_info;
-	struct list_head *pos, *q;
-
-	if (!producer || !reg_params) {
-		IPA_RM_ERR("invalid params\n");
-		return -EINVAL;
-	}
-
-	list_for_each_safe(pos, q, &(producer->event_listeners)) {
-		reg_info = list_entry(pos,
-				struct ipa_rm_notification_info,
-				link);
-		if (reg_info->reg_params.notify_cb ==
-						reg_params->notify_cb) {
-			list_del(pos);
-			kfree(reg_info);
-			result = 0;
-			goto bail;
-		}
-	}
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_resource_add_dependency() - add dependency between two
- *				given resources
- * @resource: [in] resource resource
- * @depends_on: [in] depends_on resource
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_resource_add_dependency(struct ipa_rm_resource *resource,
-				   struct ipa_rm_resource *depends_on)
-{
-	int result = 0;
-	int consumer_result;
-
-	if (!resource || !depends_on) {
-		IPA_RM_ERR("invalid params\n");
-		return -EINVAL;
-	}
-
-	if (ipa_rm_peers_list_check_dependency(resource->peers_list,
-			resource->name,
-			depends_on->peers_list,
-			depends_on->name)) {
-		IPA_RM_ERR("dependency already exists\n");
-		return -EEXIST;
-	}
-
-	ipa_rm_peers_list_add_peer(resource->peers_list, depends_on);
-	ipa_rm_peers_list_add_peer(depends_on->peers_list, resource);
-	IPA_RM_DBG("%s state: %d\n", ipa_rm_resource_str(resource->name),
-				resource->state);
-
-	resource->needed_bw += depends_on->max_bw;
-	switch (resource->state) {
-	case IPA_RM_RELEASED:
-	case IPA_RM_RELEASE_IN_PROGRESS:
-		break;
-	case IPA_RM_GRANTED:
-	case IPA_RM_REQUEST_IN_PROGRESS:
-	{
-		enum ipa_rm_resource_state prev_state = resource->state;
-		resource->state = IPA_RM_REQUEST_IN_PROGRESS;
-		((struct ipa_rm_resource_prod *)
-					resource)->pending_request++;
-		consumer_result = ipa_rm_resource_consumer_request(
-				(struct ipa_rm_resource_cons *)depends_on,
-				resource->max_bw,
-				true);
-		if (consumer_result != -EINPROGRESS) {
-			resource->state = prev_state;
-			((struct ipa_rm_resource_prod *)
-					resource)->pending_request--;
-			ipa_rm_perf_profile_change(resource->name);
-		}
-		result = consumer_result;
-		break;
-	}
-	default:
-		IPA_RM_ERR("invalid state\n");
-		result = -EPERM;
-		goto bail;
-	}
-bail:
-	IPA_RM_DBG("%s new state: %d\n", ipa_rm_resource_str(resource->name),
-					resource->state);
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-
-/**
- * ipa_rm_resource_delete_dependency() - add dependency between two
- *				given resources
- * @resource: [in] resource resource
- * @depends_on: [in] depends_on resource
- *
- * Returns: 0 on success, negative on failure
- * In case the resource state was changed, a notification
- * will be sent to the RM client
- */
-int ipa_rm_resource_delete_dependency(struct ipa_rm_resource *resource,
-				   struct ipa_rm_resource *depends_on)
-{
-	int result = 0;
-	bool state_changed = false;
-	bool release_consumer = false;
-	enum ipa_rm_event evt;
-
-	if (!resource || !depends_on) {
-		IPA_RM_ERR("invalid params\n");
-		return -EINVAL;
-	}
-
-	if (!ipa_rm_peers_list_check_dependency(resource->peers_list,
-			resource->name,
-			depends_on->peers_list,
-			depends_on->name)) {
-		IPA_RM_ERR("dependency does not exist\n");
-		return -EINVAL;
-	}
-	IPA_RM_DBG("%s state: %d\n", ipa_rm_resource_str(resource->name),
-				resource->state);
-
-	resource->needed_bw -= depends_on->max_bw;
-	switch (resource->state) {
-	case IPA_RM_RELEASED:
-		break;
-	case IPA_RM_GRANTED:
-		ipa_rm_perf_profile_change(resource->name);
-		release_consumer = true;
-		break;
-	case IPA_RM_RELEASE_IN_PROGRESS:
-		if (((struct ipa_rm_resource_prod *)
-			resource)->pending_release > 0)
-				((struct ipa_rm_resource_prod *)
-					resource)->pending_release--;
-		if (depends_on->state == IPA_RM_RELEASE_IN_PROGRESS &&
-			((struct ipa_rm_resource_prod *)
-			resource)->pending_release == 0) {
-			resource->state = IPA_RM_RELEASED;
-			state_changed = true;
-			evt = IPA_RM_RESOURCE_RELEASED;
-			ipa_rm_perf_profile_change(resource->name);
-		}
-		break;
-	case IPA_RM_REQUEST_IN_PROGRESS:
-		release_consumer = true;
-		if (((struct ipa_rm_resource_prod *)
-			resource)->pending_request > 0)
-				((struct ipa_rm_resource_prod *)
-					resource)->pending_request--;
-		if (depends_on->state == IPA_RM_REQUEST_IN_PROGRESS &&
-			((struct ipa_rm_resource_prod *)
-				resource)->pending_request == 0) {
-			resource->state = IPA_RM_GRANTED;
-			state_changed = true;
-			evt = IPA_RM_RESOURCE_GRANTED;
-			ipa_rm_perf_profile_change(resource->name);
-		}
-		break;
-	default:
-		result = -EINVAL;
-		goto bail;
-	}
-	if (state_changed) {
-		(void) ipa_rm_wq_send_cmd(IPA_RM_WQ_NOTIFY_PROD,
-				resource->name,
-				evt,
-				false);
-	}
-	IPA_RM_DBG("%s new state: %d\n", ipa_rm_resource_str(resource->name),
-					resource->state);
-	ipa_rm_peers_list_remove_peer(resource->peers_list,
-			depends_on->name);
-	ipa_rm_peers_list_remove_peer(depends_on->peers_list,
-			resource->name);
-	if (release_consumer)
-		(void) ipa_rm_resource_consumer_release(
-				(struct ipa_rm_resource_cons *)depends_on,
-				resource->max_bw,
-				true);
-bail:
-	IPA_RM_DBG("EXIT with %d\n", result);
-
-	return result;
-}
-
-/**
- * ipa_rm_resource_producer_request() - producer resource request
- * @producer: [in] producer
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_rm_resource_producer_request(struct ipa_rm_resource_prod *producer)
-{
-	int peers_index;
-	int result = 0;
-	struct ipa_rm_resource *consumer;
-	int consumer_result;
-	enum ipa_rm_resource_state state;
-
-	state = producer->resource.state;
-	switch (producer->resource.state) {
-	case IPA_RM_RELEASED:
-	case IPA_RM_RELEASE_IN_PROGRESS:
-		producer->resource.state = IPA_RM_REQUEST_IN_PROGRESS;
-		break;
-	case IPA_RM_GRANTED:
-		goto unlock_and_bail;
-	case IPA_RM_REQUEST_IN_PROGRESS:
-		result = -EINPROGRESS;
-		goto unlock_and_bail;
-	default:
-		result = -EINVAL;
-		goto unlock_and_bail;
-	}
-
-	producer->pending_request = 0;
-	for (peers_index = 0;
-		peers_index < ipa_rm_peers_list_get_size(
-				producer->resource.peers_list);
-		peers_index++) {
-		consumer = ipa_rm_peers_list_get_resource(peers_index,
-				producer->resource.peers_list);
-		if (consumer) {
-			producer->pending_request++;
-			consumer_result = ipa_rm_resource_consumer_request(
-				(struct ipa_rm_resource_cons *)consumer,
-				producer->resource.max_bw,
-				true);
-			if (consumer_result == -EINPROGRESS) {
-				result = -EINPROGRESS;
-			} else {
-				producer->pending_request--;
-				if (consumer_result != 0) {
-					result = consumer_result;
-					goto bail;
-				}
-			}
-		}
-	}
-
-	if (producer->pending_request == 0) {
-		producer->resource.state = IPA_RM_GRANTED;
-		ipa_rm_perf_profile_change(producer->resource.name);
-		(void) ipa_rm_wq_send_cmd(IPA_RM_WQ_NOTIFY_PROD,
-			producer->resource.name,
-			IPA_RM_RESOURCE_GRANTED,
-			true);
-		result = 0;
-	}
-unlock_and_bail:
-	if (state != producer->resource.state)
-		IPA_RM_DBG("%s state changed %d->%d\n",
-			ipa_rm_resource_str(producer->resource.name),
-			state,
-			producer->resource.state);
-bail:
-	return result;
-}
-
-/**
- * ipa_rm_resource_producer_release() - producer resource release
- * producer: [in] producer resource
- *
- * Returns: 0 on success, negative on failure
- *
- */
-int ipa_rm_resource_producer_release(struct ipa_rm_resource_prod *producer)
-{
-	int peers_index;
-	int result = 0;
-	struct ipa_rm_resource *consumer;
-	int consumer_result;
-	enum ipa_rm_resource_state state;
-
-	state = producer->resource.state;
-	switch (producer->resource.state) {
-	case IPA_RM_RELEASED:
-		goto bail;
-	case IPA_RM_GRANTED:
-	case IPA_RM_REQUEST_IN_PROGRESS:
-		producer->resource.state = IPA_RM_RELEASE_IN_PROGRESS;
-		break;
-	case IPA_RM_RELEASE_IN_PROGRESS:
-		result = -EINPROGRESS;
-		goto bail;
-	default:
-		result = -EPERM;
-		goto bail;
-	}
-
-	producer->pending_release = 0;
-	for (peers_index = 0;
-		peers_index < ipa_rm_peers_list_get_size(
-				producer->resource.peers_list);
-		peers_index++) {
-		consumer = ipa_rm_peers_list_get_resource(peers_index,
-				producer->resource.peers_list);
-		if (consumer) {
-			producer->pending_release++;
-			consumer_result = ipa_rm_resource_consumer_release(
-				(struct ipa_rm_resource_cons *)consumer,
-				producer->resource.max_bw,
-				true);
-			producer->pending_release--;
-		}
-	}
-
-	if (producer->pending_release == 0) {
-		producer->resource.state = IPA_RM_RELEASED;
-		ipa_rm_perf_profile_change(producer->resource.name);
-		(void) ipa_rm_wq_send_cmd(IPA_RM_WQ_NOTIFY_PROD,
-			producer->resource.name,
-			IPA_RM_RESOURCE_RELEASED,
-			true);
-	}
-bail:
-	if (state != producer->resource.state)
-		IPA_RM_DBG("%s state changed %d->%d\n",
-		ipa_rm_resource_str(producer->resource.name),
-		state,
-		producer->resource.state);
-
-	return result;
-}
-
-static void ipa_rm_resource_producer_handle_cb(
-		struct ipa_rm_resource_prod *producer,
-		enum ipa_rm_event event)
-{
-	IPA_RM_DBG("%s state: %d event: %d pending_request: %d\n",
-		ipa_rm_resource_str(producer->resource.name),
-		producer->resource.state,
-		event,
-		producer->pending_request);
-
-	switch (producer->resource.state) {
-	case IPA_RM_REQUEST_IN_PROGRESS:
-		if (event != IPA_RM_RESOURCE_GRANTED)
-			goto unlock_and_bail;
-		if (producer->pending_request > 0) {
-			producer->pending_request--;
-			if (producer->pending_request == 0) {
-				producer->resource.state =
-						IPA_RM_GRANTED;
-				ipa_rm_perf_profile_change(
-					producer->resource.name);
-				ipa_rm_resource_producer_notify_clients(
-						producer,
-						IPA_RM_RESOURCE_GRANTED,
-						false);
-				goto bail;
-			}
-		}
-		break;
-	case IPA_RM_RELEASE_IN_PROGRESS:
-		if (event != IPA_RM_RESOURCE_RELEASED)
-			goto unlock_and_bail;
-		if (producer->pending_release > 0) {
-			producer->pending_release--;
-			if (producer->pending_release == 0) {
-				producer->resource.state =
-						IPA_RM_RELEASED;
-				ipa_rm_perf_profile_change(
-					producer->resource.name);
-				ipa_rm_resource_producer_notify_clients(
-						producer,
-						IPA_RM_RESOURCE_RELEASED,
-						false);
-				goto bail;
-			}
-		}
-		break;
-	case IPA_RM_GRANTED:
-	case IPA_RM_RELEASED:
-	default:
-		goto unlock_and_bail;
-	}
-unlock_and_bail:
-	IPA_RM_DBG("%s new state: %d\n",
-		ipa_rm_resource_str(producer->resource.name),
-		producer->resource.state);
-bail:
-	return;
-}
-
-/**
- * ipa_rm_resource_consumer_handle_cb() - propagates resource
- *	notification to all dependent producers
- * @consumer: [in] notifying resource
- *
- */
-void ipa_rm_resource_consumer_handle_cb(struct ipa_rm_resource_cons *consumer,
-				enum ipa_rm_event event)
-{
-	int peers_index;
-	struct ipa_rm_resource *producer;
-
-	if (!consumer) {
-		IPA_RM_ERR("invalid params\n");
-		return;
-	}
-	IPA_RM_DBG("%s state: %d event: %d\n",
-		ipa_rm_resource_str(consumer->resource.name),
-		consumer->resource.state,
-		event);
-
-	switch (consumer->resource.state) {
-	case IPA_RM_REQUEST_IN_PROGRESS:
-		if (event == IPA_RM_RESOURCE_RELEASED)
-			goto bail;
-		consumer->resource.state = IPA_RM_GRANTED;
-		ipa_rm_perf_profile_change(consumer->resource.name);
-		ipa_resume_resource(consumer->resource.name);
-		complete_all(&consumer->request_consumer_in_progress);
-		break;
-	case IPA_RM_RELEASE_IN_PROGRESS:
-		if (event == IPA_RM_RESOURCE_GRANTED)
-			goto bail;
-		consumer->resource.state = IPA_RM_RELEASED;
-		break;
-	case IPA_RM_GRANTED:
-	case IPA_RM_RELEASED:
-	default:
-		goto bail;
-	}
-
-	for (peers_index = 0;
-		peers_index < ipa_rm_peers_list_get_size(
-				consumer->resource.peers_list);
-		peers_index++) {
-		producer = ipa_rm_peers_list_get_resource(peers_index,
-				consumer->resource.peers_list);
-		if (producer)
-			ipa_rm_resource_producer_handle_cb(
-					(struct ipa_rm_resource_prod *)
-						producer,
-						event);
-	}
-
-	return;
-bail:
-	IPA_RM_DBG("%s new state: %d\n",
-		ipa_rm_resource_str(consumer->resource.name),
-		consumer->resource.state);
-
-	return;
-}
-
-/*
- * ipa_rm_resource_set_perf_profile() - sets the performance profile to
- *					resource.
- *
- * @resource: [in] resource
- * @profile: [in] profile to be set
- *
- * sets the profile to the given resource, In case the resource is
- * granted, update bandwidth vote of the resource
- */
-int ipa_rm_resource_set_perf_profile(struct ipa_rm_resource *resource,
-				     struct ipa_rm_perf_profile *profile)
-{
-	int peers_index;
-	struct ipa_rm_resource *peer;
-
-	if (!resource || !profile) {
-		IPA_RM_ERR("invalid params\n");
-		return -EINVAL;
-	}
-
-	if (profile->max_supported_bandwidth_mbps == resource->max_bw) {
-		IPA_RM_DBG("same profile\n");
-		return 0;
-	}
-
-	if ((resource->type == IPA_RM_PRODUCER &&
-	    (resource->state == IPA_RM_GRANTED ||
-	    resource->state == IPA_RM_REQUEST_IN_PROGRESS)) ||
-	    resource->type == IPA_RM_CONSUMER) {
-		for (peers_index = 0;
-		     peers_index < ipa_rm_peers_list_get_size(
-		     resource->peers_list);
-		     peers_index++) {
-			peer = ipa_rm_peers_list_get_resource(peers_index,
-				resource->peers_list);
-			if (!peer)
-				continue;
-			peer->needed_bw -= resource->max_bw;
-			peer->needed_bw +=
-				profile->max_supported_bandwidth_mbps;
-			if (peer->state == IPA_RM_GRANTED)
-				ipa_rm_perf_profile_change(peer->name);
-		}
-	}
-
-	resource->max_bw = profile->max_supported_bandwidth_mbps;
-	if (resource->state == IPA_RM_GRANTED)
-		ipa_rm_perf_profile_change(resource->name);
-
-	return 0;
-}
-
-
-/*
- * ipa_rm_resource_producer_print_stat() - print the
- * resource status and all his dependencies
- *
- * @resource: [in] Resource resource
- * @buff: [in] The buf used to print
- * @size: [in] Buf size
- *
- * Returns: number of bytes used on success, negative on failure
- */
-int ipa_rm_resource_producer_print_stat(
-				struct ipa_rm_resource *resource,
-				char *buf,
-				int size){
-
-	int i;
-	int nbytes;
-	int cnt = 0;
-	struct ipa_rm_resource *consumer;
-
-	if (!buf || size < 0)
-		return -EINVAL;
-
-	nbytes = scnprintf(buf + cnt, size - cnt,
-		ipa_rm_resource_str(resource->name));
-	cnt += nbytes;
-	nbytes = scnprintf(buf + cnt, size - cnt, "[");
-	cnt += nbytes;
-
-	switch (resource->state) {
-	case IPA_RM_RELEASED:
-		nbytes = scnprintf(buf + cnt, size - cnt,
-			"Released] -> ");
-		cnt += nbytes;
-		break;
-	case IPA_RM_REQUEST_IN_PROGRESS:
-		nbytes = scnprintf(buf + cnt, size - cnt,
-			"Request In Progress] -> ");
-		cnt += nbytes;
-		break;
-	case IPA_RM_GRANTED:
-		nbytes = scnprintf(buf + cnt, size - cnt,
-			"Granted] -> ");
-		cnt += nbytes;
-		break;
-	case IPA_RM_RELEASE_IN_PROGRESS:
-		nbytes = scnprintf(buf + cnt, size - cnt,
-			"Release In Progress] -> ");
-		cnt += nbytes;
-		break;
-	default:
-		return -EPERM;
-	}
-
-	for (i = 0; i < resource->peers_list->max_peers; ++i) {
-		consumer =
-			ipa_rm_peers_list_get_resource(
-			i,
-			resource->peers_list);
-		if (consumer) {
-			nbytes = scnprintf(buf + cnt, size - cnt,
-				ipa_rm_resource_str(consumer->name));
-			cnt += nbytes;
-			nbytes = scnprintf(buf + cnt, size - cnt, "[");
-			cnt += nbytes;
-
-			switch (consumer->state) {
-			case IPA_RM_RELEASED:
-				nbytes = scnprintf(buf + cnt, size - cnt,
-					"Released], ");
-				cnt += nbytes;
-				break;
-			case IPA_RM_REQUEST_IN_PROGRESS:
-				nbytes = scnprintf(buf + cnt, size - cnt,
-						"Request In Progress], ");
-				cnt += nbytes;
-					break;
-			case IPA_RM_GRANTED:
-				nbytes = scnprintf(buf + cnt, size - cnt,
-						"Granted], ");
-				cnt += nbytes;
-				break;
-			case IPA_RM_RELEASE_IN_PROGRESS:
-				nbytes = scnprintf(buf + cnt, size - cnt,
-						"Release In Progress], ");
-				cnt += nbytes;
-				break;
-			default:
-				return -EPERM;
-			}
-		}
-	}
-	nbytes = scnprintf(buf + cnt, size - cnt, "\n");
-	cnt += nbytes;
-
-	return cnt;
-}
diff --git a/drivers/platform/msm/ipa/ipa_rm_resource.h b/drivers/platform/msm/ipa/ipa_rm_resource.h
deleted file mode 100644
index d1c60a8..00000000
--- a/drivers/platform/msm/ipa/ipa_rm_resource.h
+++ /dev/null
@@ -1,162 +0,0 @@
-/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef _IPA_RM_RESOURCE_H_
-#define _IPA_RM_RESOURCE_H_
-
-#include <linux/list.h>
-#include <linux/ipa.h>
-#include "ipa_rm_peers_list.h"
-
-/**
- * enum ipa_rm_resource_state - resource state
- */
-enum ipa_rm_resource_state {
-	IPA_RM_RELEASED,
-	IPA_RM_REQUEST_IN_PROGRESS,
-	IPA_RM_GRANTED,
-	IPA_RM_RELEASE_IN_PROGRESS
-};
-
-/**
- * enum ipa_rm_resource_type - IPA resource manager resource type
- */
-enum ipa_rm_resource_type {
-	IPA_RM_PRODUCER,
-	IPA_RM_CONSUMER
-};
-
-/**
- * struct ipa_rm_notification_info - notification information
- *				of IPA RM client
- * @reg_params: registration parameters
- * @explicit: registered explicitly by ipa_rm_register()
- * @link: link to the list of all registered clients information
- */
-struct ipa_rm_notification_info {
-	struct ipa_rm_register_params	reg_params;
-	bool				explicit;
-	struct list_head		link;
-};
-
-/**
- * struct ipa_rm_resource - IPA RM resource
- * @name: name identifying resource
- * @type: type of resource (PRODUCER or CONSUMER)
- * @floor_voltage: minimum voltage level for operation
- * @max_bw: maximum bandwidth required for resource in Mbps
- * @state: state of the resource
- * @peers_list: list of the peers of the resource
- */
-struct ipa_rm_resource {
-	enum ipa_rm_resource_name	name;
-	enum ipa_rm_resource_type	type;
-	enum ipa_voltage_level		floor_voltage;
-	u32				max_bw;
-	u32				needed_bw;
-	enum ipa_rm_resource_state	state;
-	struct ipa_rm_peers_list	*peers_list;
-};
-
-/**
- * struct ipa_rm_resource_cons - IPA RM consumer
- * @resource: resource
- * @usage_count: number of producers in GRANTED / REQUESTED state
- *		using this consumer
- * @request_consumer_in_progress: when set, the consumer is during its request
- *		phase
- * @request_resource: function which should be called to request resource
- *			from resource manager
- * @release_resource: function which should be called to release resource
- *			from resource manager
- * Add new fields after @resource only.
- */
-struct ipa_rm_resource_cons {
-	struct ipa_rm_resource resource;
-	int usage_count;
-	struct completion request_consumer_in_progress;
-	int (*request_resource)(void);
-	int (*release_resource)(void);
-};
-
-/**
- * struct ipa_rm_resource_prod - IPA RM producer
- * @resource: resource
- * @event_listeners: clients registered with this producer
- *		for notifications in resource state
- * list Add new fields after @resource only.
- */
-struct ipa_rm_resource_prod {
-	struct ipa_rm_resource	resource;
-	struct list_head	event_listeners;
-	int			pending_request;
-	int			pending_release;
-};
-
-int ipa_rm_resource_create(
-		struct ipa_rm_create_params *create_params,
-		struct ipa_rm_resource **resource);
-
-int ipa_rm_resource_delete(struct ipa_rm_resource *resource);
-
-int ipa_rm_resource_producer_register(struct ipa_rm_resource_prod *producer,
-				struct ipa_rm_register_params *reg_params,
-				bool explicit);
-
-int ipa_rm_resource_producer_deregister(struct ipa_rm_resource_prod *producer,
-				struct ipa_rm_register_params *reg_params);
-
-int ipa_rm_resource_add_dependency(struct ipa_rm_resource *resource,
-				   struct ipa_rm_resource *depends_on);
-
-int ipa_rm_resource_delete_dependency(struct ipa_rm_resource *resource,
-				      struct ipa_rm_resource *depends_on);
-
-int ipa_rm_resource_producer_request(struct ipa_rm_resource_prod *producer);
-
-int ipa_rm_resource_producer_release(struct ipa_rm_resource_prod *producer);
-
-int ipa_rm_resource_consumer_request(struct ipa_rm_resource_cons *consumer,
-				u32 needed_bw,
-				bool inc_usage_count);
-
-int ipa_rm_resource_consumer_release(struct ipa_rm_resource_cons *consumer,
-				u32 needed_bw,
-				bool dec_usage_count);
-
-int ipa_rm_resource_set_perf_profile(struct ipa_rm_resource *resource,
-				     struct ipa_rm_perf_profile *profile);
-
-void ipa_rm_resource_consumer_handle_cb(struct ipa_rm_resource_cons *consumer,
-				enum ipa_rm_event event);
-
-void ipa_rm_resource_producer_notify_clients(
-				struct ipa_rm_resource_prod *producer,
-				enum ipa_rm_event event,
-				bool notify_registered_only);
-
-int ipa_rm_resource_producer_print_stat(
-		struct ipa_rm_resource *resource,
-		char *buf,
-		int size);
-
-int ipa_rm_resource_consumer_request_work(struct ipa_rm_resource_cons *consumer,
-		enum ipa_rm_resource_state prev_state,
-		u32 needed_bw,
-		bool notify_completion);
-
-int ipa_rm_resource_consumer_release_work(
-		struct ipa_rm_resource_cons *consumer,
-		enum ipa_rm_resource_state prev_state,
-		bool notify_completion);
-
-#endif /* _IPA_RM_RESOURCE_H_ */
diff --git a/drivers/platform/msm/ipa/ipa_rt.c b/drivers/platform/msm/ipa/ipa_rt.c
deleted file mode 100644
index e5786ff..00000000
--- a/drivers/platform/msm/ipa/ipa_rt.c
+++ /dev/null
@@ -1,1489 +0,0 @@
-/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/bitops.h>
-#include "ipa_i.h"
-
-#define IPA_RT_TABLE_INDEX_NOT_FOUND	(-1)
-#define IPA_RT_TABLE_WORD_SIZE		(4)
-#define IPA_RT_INDEX_BITMAP_SIZE	(32)
-#define IPA_RT_TABLE_MEMORY_ALLIGNMENT	(127)
-#define IPA_RT_ENTRY_MEMORY_ALLIGNMENT	(3)
-#define IPA_RT_BIT_MASK			(0x1)
-#define IPA_RT_STATUS_OF_ADD_FAILED	(-1)
-#define IPA_RT_STATUS_OF_DEL_FAILED	(-1)
-#define IPA_RT_STATUS_OF_MDFY_FAILED (-1)
-
-/**
- * __ipa_generate_rt_hw_rule_v2() - generates the routing hardware rule
- * @ip: the ip address family type
- * @entry: routing entry
- * @buf: output buffer, buf == NULL means
- *		caller wants to know the size of the rule as seen
- *		by HW so they did not pass a valid buffer, we will use a
- *		scratch buffer instead.
- *		With this scheme we are going to
- *		generate the rule twice, once to know size using scratch
- *		buffer and second to write the rule to the actual caller
- *		supplied buffer which is of required size
- *
- * Returns:	0 on success, negative on failure
- *
- * caller needs to hold any needed locks to ensure integrity
- *
- */
-int __ipa_generate_rt_hw_rule_v2(enum ipa_ip_type ip,
-		struct ipa_rt_entry *entry, u8 *buf)
-{
-	struct ipa_rt_rule_hw_hdr *rule_hdr;
-	const struct ipa_rt_rule *rule =
-		(const struct ipa_rt_rule *)&entry->rule;
-	u16 en_rule = 0;
-	u32 tmp[IPA_RT_FLT_HW_RULE_BUF_SIZE/4];
-	u8 *start;
-	int pipe_idx;
-
-	if (buf == NULL) {
-		memset(tmp, 0, IPA_RT_FLT_HW_RULE_BUF_SIZE);
-		buf = (u8 *)tmp;
-	}
-
-	start = buf;
-	rule_hdr = (struct ipa_rt_rule_hw_hdr *)buf;
-	pipe_idx = ipa_get_ep_mapping(entry->rule.dst);
-	if (pipe_idx == -1) {
-		IPAERR("Wrong destination pipe specified in RT rule\n");
-		WARN_ON(1);
-		return -EPERM;
-	}
-	if (!IPA_CLIENT_IS_CONS(entry->rule.dst)) {
-		IPAERR("No RT rule on IPA_client_producer pipe.\n");
-		IPAERR("pipe_idx: %d dst_pipe: %d\n",
-				pipe_idx, entry->rule.dst);
-		WARN_ON(1);
-		return -EPERM;
-	}
-	rule_hdr->u.hdr.pipe_dest_idx = pipe_idx;
-	rule_hdr->u.hdr.system = !ipa_ctx->hdr_tbl_lcl;
-	if (entry->hdr) {
-		rule_hdr->u.hdr.hdr_offset =
-			entry->hdr->offset_entry->offset >> 2;
-	} else {
-		rule_hdr->u.hdr.hdr_offset = 0;
-	}
-	buf += sizeof(struct ipa_rt_rule_hw_hdr);
-
-	if (ipa_generate_hw_rule(ip, &rule->attrib, &buf, &en_rule)) {
-		IPAERR("fail to generate hw rule\n");
-		return -EPERM;
-	}
-
-	IPADBG("en_rule 0x%x\n", en_rule);
-
-	rule_hdr->u.hdr.en_rule = en_rule;
-	ipa_write_32(rule_hdr->u.word, (u8 *)rule_hdr);
-
-	if (entry->hw_len == 0) {
-		entry->hw_len = buf - start;
-	} else if (entry->hw_len != (buf - start)) {
-			IPAERR(
-			"hw_len differs b/w passes passed=0x%x calc=0x%td\n",
-			entry->hw_len,
-			(buf - start));
-			return -EPERM;
-		}
-
-	return 0;
-}
-
-/**
- * __ipa_generate_rt_hw_rule_v2_5() - generates the routing hardware rule
- * @ip: the ip address family type
- * @entry: routing entry
- * @buf: output buffer, buf == NULL means
- *		caller wants to know the size of the rule as seen
- *		by HW so they did not pass a valid buffer, we will use a
- *		scratch buffer instead.
- *		With this scheme we are going to
- *		generate the rule twice, once to know size using scratch
- *		buffer and second to write the rule to the actual caller
- *		supplied buffer which is of required size
- *
- * Returns:	0 on success, negative on failure
- *
- * caller needs to hold any needed locks to ensure integrity
- *
- */
-int __ipa_generate_rt_hw_rule_v2_5(enum ipa_ip_type ip,
-		struct ipa_rt_entry *entry, u8 *buf)
-{
-	struct ipa_rt_rule_hw_hdr *rule_hdr;
-	const struct ipa_rt_rule *rule =
-		(const struct ipa_rt_rule *)&entry->rule;
-	u16 en_rule = 0;
-	u32 tmp[IPA_RT_FLT_HW_RULE_BUF_SIZE/4];
-	u8 *start;
-	int pipe_idx;
-
-	if (buf == NULL) {
-		memset(tmp, 0, IPA_RT_FLT_HW_RULE_BUF_SIZE);
-		buf = (u8 *)tmp;
-	}
-
-	start = buf;
-	rule_hdr = (struct ipa_rt_rule_hw_hdr *)buf;
-	pipe_idx = ipa_get_ep_mapping(entry->rule.dst);
-	if (pipe_idx == -1) {
-		IPAERR("Wrong destination pipe specified in RT rule\n");
-		WARN_ON(1);
-		return -EPERM;
-	}
-	if (!IPA_CLIENT_IS_CONS(entry->rule.dst)) {
-		IPAERR("No RT rule on IPA_client_producer pipe.\n");
-		IPAERR("pipe_idx: %d dst_pipe: %d\n",
-				pipe_idx, entry->rule.dst);
-		WARN_ON(1);
-		return -EPERM;
-	}
-	rule_hdr->u.hdr_v2_5.pipe_dest_idx = pipe_idx;
-	if (entry->proc_ctx || (entry->hdr && entry->hdr->is_hdr_proc_ctx)) {
-		struct ipa_hdr_proc_ctx_entry *proc_ctx;
-		proc_ctx = (entry->proc_ctx) ? : entry->hdr->proc_ctx;
-		rule_hdr->u.hdr_v2_5.system = !ipa_ctx->hdr_proc_ctx_tbl_lcl;
-		BUG_ON(proc_ctx->offset_entry->offset & 31);
-		rule_hdr->u.hdr_v2_5.proc_ctx = 1;
-		rule_hdr->u.hdr_v2_5.hdr_offset =
-			(proc_ctx->offset_entry->offset +
-			ipa_ctx->hdr_proc_ctx_tbl.start_offset) >> 5;
-	} else if (entry->hdr) {
-		rule_hdr->u.hdr_v2_5.system = !ipa_ctx->hdr_tbl_lcl;
-		BUG_ON(entry->hdr->offset_entry->offset & 3);
-		rule_hdr->u.hdr_v2_5.proc_ctx = 0;
-		rule_hdr->u.hdr_v2_5.hdr_offset =
-				entry->hdr->offset_entry->offset >> 2;
-	} else {
-		rule_hdr->u.hdr_v2_5.proc_ctx = 0;
-		rule_hdr->u.hdr_v2_5.hdr_offset = 0;
-	}
-	buf += sizeof(struct ipa_rt_rule_hw_hdr);
-
-	if (ipa_generate_hw_rule(ip, &rule->attrib, &buf, &en_rule)) {
-		IPAERR("fail to generate hw rule\n");
-		return -EPERM;
-	}
-
-	IPADBG("en_rule 0x%x\n", en_rule);
-
-	rule_hdr->u.hdr_v2_5.en_rule = en_rule;
-	ipa_write_32(rule_hdr->u.word, (u8 *)rule_hdr);
-
-	if (entry->hw_len == 0) {
-		entry->hw_len = buf - start;
-	} else if (entry->hw_len != (buf - start)) {
-		IPAERR("hw_len differs b/w passes passed=0x%x calc=0x%td\n",
-			entry->hw_len, (buf - start));
-		return -EPERM;
-	}
-
-	return 0;
-}
-
-/**
- * __ipa_generate_rt_hw_rule_v2_6L() - generates the routing hardware rule
- * @ip: the ip address family type
- * @entry: routing entry
- * @buf: output buffer, buf == NULL means that the caller wants to know the size
- *       of the rule as seen by HW so they did not pass a valid buffer, we will
- *       use a scratch buffer instead.
- *       With this scheme we are going to generate the rule twice, once to know
- *       size using scratch buffer and second to write the rule to the actual
- *       caller supplied buffer which is of required size.
- *
- * Returns:	0 on success, negative on failure
- *
- * caller needs to hold any needed locks to ensure integrity
- *
- */
-int __ipa_generate_rt_hw_rule_v2_6L(enum ipa_ip_type ip,
-		struct ipa_rt_entry *entry, u8 *buf)
-{
-	/* Same implementation as IPAv2 */
-	return __ipa_generate_rt_hw_rule_v2(ip, entry, buf);
-}
-
-/**
- * ipa_get_rt_hw_tbl_size() - returns the size of HW routing table
- * @ip: the ip address family type
- * @hdr_sz: header size
- * @max_rt_idx: maximal index
- *
- * Returns:	size on success, negative on failure
- *
- * caller needs to hold any needed locks to ensure integrity
- *
- * the MSB set in rt_idx_bitmap indicates the size of hdr of routing tbl
- */
-static int ipa_get_rt_hw_tbl_size(enum ipa_ip_type ip, u32 *hdr_sz,
-		int *max_rt_idx)
-{
-	struct ipa_rt_tbl_set *set;
-	struct ipa_rt_tbl *tbl;
-	struct ipa_rt_entry *entry;
-	u32 total_sz = 0;
-	u32 tbl_sz;
-	u32 bitmap = ipa_ctx->rt_idx_bitmap[ip];
-	int highest_bit_set = IPA_RT_TABLE_INDEX_NOT_FOUND;
-	int i;
-	int res;
-
-	*hdr_sz = 0;
-	set = &ipa_ctx->rt_tbl_set[ip];
-
-	for (i = 0; i < IPA_RT_INDEX_BITMAP_SIZE; i++) {
-		if (bitmap & IPA_RT_BIT_MASK)
-			highest_bit_set = i;
-		bitmap >>= 1;
-	}
-
-	*max_rt_idx = highest_bit_set;
-	if (highest_bit_set == IPA_RT_TABLE_INDEX_NOT_FOUND) {
-		IPAERR("no rt tbls present\n");
-		total_sz = IPA_RT_TABLE_WORD_SIZE;
-		*hdr_sz = IPA_RT_TABLE_WORD_SIZE;
-		return total_sz;
-	}
-
-	*hdr_sz = (highest_bit_set + 1) * IPA_RT_TABLE_WORD_SIZE;
-	total_sz += *hdr_sz;
-	list_for_each_entry(tbl, &set->head_rt_tbl_list, link) {
-		tbl_sz = 0;
-		list_for_each_entry(entry, &tbl->head_rt_rule_list, link) {
-			res = ipa_ctx->ctrl->ipa_generate_rt_hw_rule(
-				ip,
-				entry,
-				NULL);
-			if (res) {
-				IPAERR("failed to find HW RT rule size\n");
-				return -EPERM;
-			}
-			tbl_sz += entry->hw_len;
-		}
-
-		if (tbl_sz)
-			tbl->sz = tbl_sz + IPA_RT_TABLE_WORD_SIZE;
-
-		if (tbl->in_sys)
-			continue;
-
-		if (tbl_sz) {
-			/* add the terminator */
-			total_sz += (tbl_sz + IPA_RT_TABLE_WORD_SIZE);
-			/* every rule-set should start at word boundary */
-			total_sz = (total_sz + IPA_RT_ENTRY_MEMORY_ALLIGNMENT) &
-						~IPA_RT_ENTRY_MEMORY_ALLIGNMENT;
-		}
-	}
-
-	IPADBG("RT HW TBL SZ %d HDR SZ %d IP %d\n", total_sz, *hdr_sz, ip);
-
-	return total_sz;
-}
-
-static int ipa_generate_rt_hw_tbl_common(enum ipa_ip_type ip, u8 *base, u8 *hdr,
-		u32 body_ofst, u32 apps_start_idx)
-{
-	struct ipa_rt_tbl *tbl;
-	struct ipa_rt_entry *entry;
-	struct ipa_rt_tbl_set *set;
-	u32 offset;
-	u8 *body;
-	struct ipa_mem_buffer rt_tbl_mem;
-	u8 *rt_tbl_mem_body;
-	int res;
-
-	/* build the rt tbl in the DMA buffer to submit to IPA HW */
-	body = base;
-
-	set = &ipa_ctx->rt_tbl_set[ip];
-	list_for_each_entry(tbl, &set->head_rt_tbl_list, link) {
-		if (!tbl->in_sys) {
-			offset = body - base + body_ofst;
-			if (offset & IPA_RT_ENTRY_MEMORY_ALLIGNMENT) {
-				IPAERR("offset is not word multiple %d\n",
-						offset);
-				goto proc_err;
-			}
-
-			/* convert offset to words from bytes */
-			offset &= ~IPA_RT_ENTRY_MEMORY_ALLIGNMENT;
-			/* rule is at an offset from base */
-			offset |= IPA_RT_BIT_MASK;
-
-			/* update the hdr at the right index */
-			ipa_write_32(offset, hdr +
-					((tbl->idx - apps_start_idx) *
-					 IPA_RT_TABLE_WORD_SIZE));
-
-			/* generate the rule-set */
-			list_for_each_entry(entry, &tbl->head_rt_rule_list,
-					link) {
-				res = ipa_ctx->ctrl->ipa_generate_rt_hw_rule(
-					ip,
-					entry,
-					body);
-				if (res) {
-					IPAERR("failed to gen HW RT rule\n");
-					goto proc_err;
-				}
-				body += entry->hw_len;
-			}
-
-			/* write the rule-set terminator */
-			body = ipa_write_32(0, body);
-			if ((long)body & IPA_RT_ENTRY_MEMORY_ALLIGNMENT)
-				/* advance body to next word boundary */
-				body = body + (IPA_RT_TABLE_WORD_SIZE -
-					      ((long)body &
-					      IPA_RT_ENTRY_MEMORY_ALLIGNMENT));
-		} else {
-			if (tbl->sz == 0) {
-				IPAERR("cannot generate 0 size table\n");
-				goto proc_err;
-			}
-
-			/* allocate memory for the RT tbl */
-			rt_tbl_mem.size = tbl->sz;
-			rt_tbl_mem.base =
-			   dma_alloc_coherent(ipa_ctx->pdev, rt_tbl_mem.size,
-					   &rt_tbl_mem.phys_base, GFP_KERNEL);
-			if (!rt_tbl_mem.base) {
-				IPAERR("fail to alloc DMA buff of size %d\n",
-						rt_tbl_mem.size);
-				WARN_ON(1);
-				goto proc_err;
-			}
-
-			WARN_ON(rt_tbl_mem.phys_base &
-					IPA_RT_ENTRY_MEMORY_ALLIGNMENT);
-			rt_tbl_mem_body = rt_tbl_mem.base;
-			memset(rt_tbl_mem.base, 0, rt_tbl_mem.size);
-			/* update the hdr at the right index */
-			ipa_write_32(rt_tbl_mem.phys_base,
-					hdr + ((tbl->idx - apps_start_idx) *
-					IPA_RT_TABLE_WORD_SIZE));
-			/* generate the rule-set */
-			list_for_each_entry(entry, &tbl->head_rt_rule_list,
-					link) {
-				res = ipa_ctx->ctrl->ipa_generate_rt_hw_rule(
-					ip,
-					entry,
-					rt_tbl_mem_body);
-				if (res) {
-					IPAERR("failed to gen HW RT rule\n");
-					WARN_ON(1);
-					goto rt_table_mem_alloc_failed;
-				}
-				rt_tbl_mem_body += entry->hw_len;
-			}
-
-			/* write the rule-set terminator */
-			rt_tbl_mem_body = ipa_write_32(0, rt_tbl_mem_body);
-
-			if (tbl->curr_mem.phys_base) {
-				WARN_ON(tbl->prev_mem.phys_base);
-				tbl->prev_mem = tbl->curr_mem;
-			}
-			tbl->curr_mem = rt_tbl_mem;
-		}
-	}
-
-	return 0;
-
-rt_table_mem_alloc_failed:
-	dma_free_coherent(ipa_ctx->pdev, rt_tbl_mem.size,
-			  rt_tbl_mem.base, rt_tbl_mem.phys_base);
-proc_err:
-	return -EPERM;
-}
-
-
-/**
- * ipa_generate_rt_hw_tbl() - generates the routing hardware table
- * @ip:	[in] the ip address family type
- * @mem:	[out] buffer to put the filtering table
- *
- * Returns:	0 on success, negative on failure
- */
-static int ipa_generate_rt_hw_tbl_v1_1(enum ipa_ip_type ip,
-		struct ipa_mem_buffer *mem)
-{
-	u32 hdr_sz;
-	u8 *hdr;
-	u8 *body;
-	u8 *base;
-	int max_rt_idx;
-	int i;
-	int res;
-
-	res = ipa_get_rt_hw_tbl_size(ip, &hdr_sz, &max_rt_idx);
-	if (res < 0) {
-		IPAERR("ipa_get_rt_hw_tbl_size failed %d\n", res);
-		goto error;
-	}
-
-	mem->size = res;
-	mem->size = (mem->size + IPA_RT_TABLE_MEMORY_ALLIGNMENT) &
-				~IPA_RT_TABLE_MEMORY_ALLIGNMENT;
-
-	if (mem->size == 0) {
-		IPAERR("rt tbl empty ip=%d\n", ip);
-		goto error;
-	}
-	mem->base = dma_alloc_coherent(ipa_ctx->pdev, mem->size,
-			&mem->phys_base, GFP_KERNEL);
-	if (!mem->base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem->size);
-		goto error;
-	}
-
-	memset(mem->base, 0, mem->size);
-
-	/* build the rt tbl in the DMA buffer to submit to IPA HW */
-	base = hdr = (u8 *)mem->base;
-	body = base + hdr_sz;
-
-	/* setup all indices to point to the empty sys rt tbl */
-	for (i = 0; i <= max_rt_idx; i++)
-		ipa_write_32(ipa_ctx->empty_rt_tbl_mem.phys_base,
-				hdr + (i * IPA_RT_TABLE_WORD_SIZE));
-
-	if (ipa_generate_rt_hw_tbl_common(ip, base, hdr, 0, 0)) {
-		IPAERR("fail to generate RT tbl\n");
-		goto proc_err;
-	}
-
-	return 0;
-
-proc_err:
-	dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base, mem->phys_base);
-	mem->base = NULL;
-error:
-	return -EPERM;
-}
-
-static void __ipa_reap_sys_rt_tbls(enum ipa_ip_type ip)
-{
-	struct ipa_rt_tbl *tbl;
-	struct ipa_rt_tbl *next;
-	struct ipa_rt_tbl_set *set;
-
-	set = &ipa_ctx->rt_tbl_set[ip];
-	list_for_each_entry(tbl, &set->head_rt_tbl_list, link) {
-		if (tbl->prev_mem.phys_base) {
-			IPADBG("reaping rt tbl name=%s ip=%d\n", tbl->name, ip);
-			dma_free_coherent(ipa_ctx->pdev, tbl->prev_mem.size,
-					tbl->prev_mem.base,
-					tbl->prev_mem.phys_base);
-			memset(&tbl->prev_mem, 0, sizeof(tbl->prev_mem));
-		}
-	}
-
-	set = &ipa_ctx->reap_rt_tbl_set[ip];
-	list_for_each_entry_safe(tbl, next, &set->head_rt_tbl_list, link) {
-		list_del(&tbl->link);
-		WARN_ON(tbl->prev_mem.phys_base != 0);
-		if (tbl->curr_mem.phys_base) {
-			IPADBG("reaping sys rt tbl name=%s ip=%d\n", tbl->name,
-					ip);
-			dma_free_coherent(ipa_ctx->pdev, tbl->curr_mem.size,
-					tbl->curr_mem.base,
-					tbl->curr_mem.phys_base);
-			kmem_cache_free(ipa_ctx->rt_tbl_cache, tbl);
-		}
-	}
-}
-
-int __ipa_commit_rt_v1_1(enum ipa_ip_type ip)
-{
-	struct ipa_desc desc = { 0 };
-	struct ipa_mem_buffer *mem;
-	void *cmd;
-	struct ipa_ip_v4_routing_init *v4;
-	struct ipa_ip_v6_routing_init *v6;
-	u16 avail;
-	u16 size;
-
-	mem = kmalloc(sizeof(struct ipa_mem_buffer), GFP_KERNEL);
-	if (!mem) {
-		IPAERR("failed to alloc memory object\n");
-		goto fail_alloc_mem;
-	}
-
-	if (ip == IPA_IP_v4) {
-		avail = ipa_ctx->ip4_rt_tbl_lcl ? IPA_MEM_v1_RAM_V4_RT_SIZE :
-			IPA_MEM_PART(v4_rt_size_ddr);
-		size = sizeof(struct ipa_ip_v4_routing_init);
-	} else {
-		avail = ipa_ctx->ip6_rt_tbl_lcl ? IPA_MEM_v1_RAM_V6_RT_SIZE :
-			IPA_MEM_PART(v6_rt_size_ddr);
-		size = sizeof(struct ipa_ip_v6_routing_init);
-	}
-	cmd = kmalloc(size, GFP_KERNEL);
-	if (!cmd) {
-		IPAERR("failed to alloc immediate command object\n");
-		goto fail_alloc_cmd;
-	}
-
-	if (ipa_generate_rt_hw_tbl_v1_1(ip, mem)) {
-		IPAERR("fail to generate RT HW TBL ip %d\n", ip);
-		goto fail_hw_tbl_gen;
-	}
-
-	if (mem->size > avail) {
-		IPAERR("tbl too big, needed %d avail %d\n", mem->size, avail);
-		goto fail_send_cmd;
-	}
-
-	if (ip == IPA_IP_v4) {
-		v4 = (struct ipa_ip_v4_routing_init *)cmd;
-		desc.opcode = IPA_IP_V4_ROUTING_INIT;
-		v4->ipv4_rules_addr = mem->phys_base;
-		v4->size_ipv4_rules = mem->size;
-		v4->ipv4_addr = IPA_MEM_v1_RAM_V4_RT_OFST;
-		IPADBG("putting Routing IPv4 rules to phys 0x%x",
-				v4->ipv4_addr);
-	} else {
-		v6 = (struct ipa_ip_v6_routing_init *)cmd;
-		desc.opcode = IPA_IP_V6_ROUTING_INIT;
-		v6->ipv6_rules_addr = mem->phys_base;
-		v6->size_ipv6_rules = mem->size;
-		v6->ipv6_addr = IPA_MEM_v1_RAM_V6_RT_OFST;
-		IPADBG("putting Routing IPv6 rules to phys 0x%x",
-				v6->ipv6_addr);
-	}
-
-	desc.pyld = cmd;
-	desc.len = size;
-	desc.type = IPA_IMM_CMD_DESC;
-	IPA_DUMP_BUFF(mem->base, mem->phys_base, mem->size);
-
-	if (ipa_send_cmd(1, &desc)) {
-		IPAERR("fail to send immediate command\n");
-		goto fail_send_cmd;
-	}
-
-	__ipa_reap_sys_rt_tbls(ip);
-	dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base, mem->phys_base);
-	kfree(cmd);
-	kfree(mem);
-
-	return 0;
-
-fail_send_cmd:
-	if (mem->base)
-		dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base,
-				mem->phys_base);
-fail_hw_tbl_gen:
-	kfree(cmd);
-fail_alloc_cmd:
-	kfree(mem);
-fail_alloc_mem:
-	return -EPERM;
-}
-
-static int ipa_generate_rt_hw_tbl_v2(enum ipa_ip_type ip,
-		struct ipa_mem_buffer *mem, struct ipa_mem_buffer *head)
-{
-	u32 hdr_sz;
-	u8 *hdr;
-	u8 *body;
-	u8 *base;
-	int max_rt_idx;
-	int i;
-	u32 *entr;
-	int num_index;
-	u32 body_start_offset;
-	u32 apps_start_idx;
-	int res;
-
-	if (ip == IPA_IP_v4) {
-		num_index = IPA_MEM_PART(v4_apps_rt_index_hi) -
-			IPA_MEM_PART(v4_apps_rt_index_lo) + 1;
-		body_start_offset = IPA_MEM_PART(apps_v4_rt_ofst) -
-			IPA_MEM_PART(v4_rt_ofst);
-		apps_start_idx = IPA_MEM_PART(v4_apps_rt_index_lo);
-	} else {
-		num_index = IPA_MEM_PART(v6_apps_rt_index_hi) -
-			IPA_MEM_PART(v6_apps_rt_index_lo) + 1;
-		body_start_offset = IPA_MEM_PART(apps_v6_rt_ofst) -
-			IPA_MEM_PART(v6_rt_ofst);
-		apps_start_idx = IPA_MEM_PART(v6_apps_rt_index_lo);
-	}
-
-	head->size = num_index * 4;
-	head->base = dma_alloc_coherent(ipa_ctx->pdev, head->size,
-			&head->phys_base, GFP_KERNEL);
-	if (!head->base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", head->size);
-		goto err;
-	}
-	entr = (u32 *)head->base;
-	hdr = (u8 *)head->base;
-	for (i = 1; i <= num_index; i++) {
-		*entr = ipa_ctx->empty_rt_tbl_mem.phys_base;
-		entr++;
-	}
-
-	res = ipa_get_rt_hw_tbl_size(ip, &hdr_sz, &max_rt_idx);
-	if (res < 0) {
-		IPAERR("ipa_get_rt_hw_tbl_size failed %d\n", res);
-		goto base_err;
-	}
-
-	mem->size = res;
-	mem->size -= hdr_sz;
-	mem->size = (mem->size + IPA_RT_TABLE_MEMORY_ALLIGNMENT) &
-				~IPA_RT_TABLE_MEMORY_ALLIGNMENT;
-
-	if (mem->size > 0) {
-		mem->base = dma_alloc_coherent(ipa_ctx->pdev, mem->size,
-				&mem->phys_base, GFP_KERNEL);
-		if (!mem->base) {
-			IPAERR("fail to alloc DMA buff of size %d\n",
-					mem->size);
-			goto base_err;
-		}
-		memset(mem->base, 0, mem->size);
-	}
-
-	/* build the rt tbl in the DMA buffer to submit to IPA HW */
-	body = base = (u8 *)mem->base;
-
-	if (ipa_generate_rt_hw_tbl_common(ip, base, hdr, body_start_offset,
-				apps_start_idx)) {
-		IPAERR("fail to generate RT tbl\n");
-		goto proc_err;
-	}
-
-	return 0;
-
-proc_err:
-	dma_free_coherent(ipa_ctx->pdev, mem->size, mem->base, mem->phys_base);
-base_err:
-	dma_free_coherent(ipa_ctx->pdev, head->size, head->base,
-			head->phys_base);
-err:
-	return -EPERM;
-}
-
-int __ipa_commit_rt_v2(enum ipa_ip_type ip)
-{
-	struct ipa_desc desc[2];
-	struct ipa_mem_buffer body;
-	struct ipa_mem_buffer head;
-	struct ipa_hw_imm_cmd_dma_shared_mem cmd1 = {0};
-	struct ipa_hw_imm_cmd_dma_shared_mem cmd2 = {0};
-	u16 avail;
-	u32 num_modem_rt_index;
-	int rc = 0;
-	u32 local_addr1;
-	u32 local_addr2;
-	bool lcl;
-
-	memset(desc, 0, 2 * sizeof(struct ipa_desc));
-
-	if (ip == IPA_IP_v4) {
-		avail = ipa_ctx->ip4_rt_tbl_lcl ?
-			IPA_MEM_PART(apps_v4_rt_size) :
-			IPA_MEM_PART(v4_rt_size_ddr);
-		num_modem_rt_index =
-			IPA_MEM_PART(v4_modem_rt_index_hi) -
-			IPA_MEM_PART(v4_modem_rt_index_lo) + 1;
-		local_addr1 = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(v4_rt_ofst) +
-			num_modem_rt_index * 4;
-		local_addr2 = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(apps_v4_rt_ofst);
-		lcl = ipa_ctx->ip4_rt_tbl_lcl;
-	} else {
-		avail = ipa_ctx->ip6_rt_tbl_lcl ?
-			IPA_MEM_PART(apps_v6_rt_size) :
-			IPA_MEM_PART(v6_rt_size_ddr);
-		num_modem_rt_index =
-			IPA_MEM_PART(v6_modem_rt_index_hi) -
-			IPA_MEM_PART(v6_modem_rt_index_lo) + 1;
-		local_addr1 = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(v6_rt_ofst) +
-			num_modem_rt_index * 4;
-		local_addr2 = ipa_ctx->smem_restricted_bytes +
-			IPA_MEM_PART(apps_v6_rt_ofst);
-		lcl = ipa_ctx->ip6_rt_tbl_lcl;
-	}
-
-	if (ipa_generate_rt_hw_tbl_v2(ip, &body, &head)) {
-		IPAERR("fail to generate RT HW TBL ip %d\n", ip);
-		rc = -EFAULT;
-		goto fail_gen;
-	}
-
-	if (body.size > avail) {
-		IPAERR("tbl too big, needed %d avail %d\n", body.size, avail);
-		rc = -EFAULT;
-		goto fail_send_cmd;
-	}
-
-	cmd1.size = head.size;
-	cmd1.system_addr = head.phys_base;
-	cmd1.local_addr = local_addr1;
-	desc[0].opcode = IPA_DMA_SHARED_MEM;
-	desc[0].pyld = &cmd1;
-	desc[0].len = sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-	desc[0].type = IPA_IMM_CMD_DESC;
-
-	if (lcl) {
-		cmd2.size = body.size;
-		cmd2.system_addr = body.phys_base;
-		cmd2.local_addr = local_addr2;
-
-		desc[1].opcode = IPA_DMA_SHARED_MEM;
-		desc[1].pyld = &cmd2;
-		desc[1].len = sizeof(struct ipa_hw_imm_cmd_dma_shared_mem);
-		desc[1].type = IPA_IMM_CMD_DESC;
-
-		if (ipa_send_cmd(2, desc)) {
-			IPAERR("fail to send immediate command\n");
-			rc = -EFAULT;
-			goto fail_send_cmd;
-		}
-	} else {
-		if (ipa_send_cmd(1, desc)) {
-			IPAERR("fail to send immediate command\n");
-			rc = -EFAULT;
-			goto fail_send_cmd;
-		}
-	}
-
-	IPADBG("HEAD\n");
-	IPA_DUMP_BUFF(head.base, head.phys_base, head.size);
-	if (body.size) {
-		IPADBG("BODY\n");
-		IPA_DUMP_BUFF(body.base, body.phys_base, body.size);
-	}
-	__ipa_reap_sys_rt_tbls(ip);
-fail_send_cmd:
-	dma_free_coherent(ipa_ctx->pdev, head.size, head.base, head.phys_base);
-	if (body.size)
-		dma_free_coherent(ipa_ctx->pdev, body.size, body.base,
-				body.phys_base);
-fail_gen:
-	return rc;
-}
-
-/**
- * __ipa_find_rt_tbl() - find the routing table
- *			which name is given as parameter
- * @ip:	[in] the ip address family type of the wanted routing table
- * @name:	[in] the name of the wanted routing table
- *
- * Returns: the routing table which name is given as parameter, or NULL if it
- * doesn't exist
- */
-struct ipa_rt_tbl *__ipa_find_rt_tbl(enum ipa_ip_type ip, const char *name)
-{
-	struct ipa_rt_tbl *entry;
-	struct ipa_rt_tbl_set *set;
-
-	set = &ipa_ctx->rt_tbl_set[ip];
-	list_for_each_entry(entry, &set->head_rt_tbl_list, link) {
-		if (!strncmp(name, entry->name, IPA_RESOURCE_NAME_MAX))
-			return entry;
-	}
-
-	return NULL;
-}
-
-/**
- * ipa_query_rt_index() - find the routing table index
- *			which name and ip type are given as parameters
- * @in:	[out] the index of the wanted routing table
- *
- * Returns: the routing table which name is given as parameter, or NULL if it
- * doesn't exist
- */
-int ipa_query_rt_index(struct ipa_ioc_get_rt_tbl_indx *in)
-{
-	struct ipa_rt_tbl *entry;
-
-	if (in->ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	/* check if this table exists */
-	entry = __ipa_find_rt_tbl(in->ip, in->name);
-	if (!entry){
-		mutex_unlock(&ipa_ctx->lock);
-		return -EFAULT;
-	}
-
-	in->idx  = entry->idx;
-	mutex_unlock(&ipa_ctx->lock);
-	return 0;
-}
-
-static struct ipa_rt_tbl *__ipa_add_rt_tbl(enum ipa_ip_type ip,
-		const char *name)
-{
-	struct ipa_rt_tbl *entry;
-	struct ipa_rt_tbl_set *set;
-	int i;
-	int id;
-
-	if (ip >= IPA_IP_MAX || name == NULL) {
-		IPAERR("bad parm\n");
-		goto error;
-	}
-
-	set = &ipa_ctx->rt_tbl_set[ip];
-	/* check if this table exists */
-	entry = __ipa_find_rt_tbl(ip, name);
-	if (!entry) {
-		entry = kmem_cache_zalloc(ipa_ctx->rt_tbl_cache, GFP_KERNEL);
-		if (!entry) {
-			IPAERR("failed to alloc RT tbl object\n");
-			goto error;
-		}
-		/* find a routing tbl index */
-		for (i = 0; i < IPA_RT_INDEX_BITMAP_SIZE; i++) {
-			if (!test_bit(i, &ipa_ctx->rt_idx_bitmap[ip])) {
-				entry->idx = i;
-				set_bit(i, &ipa_ctx->rt_idx_bitmap[ip]);
-				break;
-			}
-		}
-		if (i == IPA_RT_INDEX_BITMAP_SIZE) {
-			IPAERR("not free RT tbl indices left\n");
-			goto fail_rt_idx_alloc;
-		}
-
-		INIT_LIST_HEAD(&entry->head_rt_rule_list);
-		INIT_LIST_HEAD(&entry->link);
-		strlcpy(entry->name, name, IPA_RESOURCE_NAME_MAX);
-		entry->set = set;
-		entry->cookie = IPA_RT_TBL_COOKIE;
-		entry->in_sys = (ip == IPA_IP_v4) ?
-			!ipa_ctx->ip4_rt_tbl_lcl : !ipa_ctx->ip6_rt_tbl_lcl;
-		set->tbl_cnt++;
-		list_add(&entry->link, &set->head_rt_tbl_list);
-
-		IPADBG("add rt tbl idx=%d tbl_cnt=%d ip=%d\n", entry->idx,
-				set->tbl_cnt, ip);
-
-		id = ipa_id_alloc(entry);
-		if (id < 0) {
-			IPAERR("failed to add to tree\n");
-			WARN_ON(1);
-			goto ipa_insert_failed;
-		}
-		entry->id = id;
-	}
-
-	return entry;
-
-ipa_insert_failed:
-	set->tbl_cnt--;
-	list_del(&entry->link);
-fail_rt_idx_alloc:
-	entry->cookie = 0;
-	kmem_cache_free(ipa_ctx->rt_tbl_cache, entry);
-error:
-	return NULL;
-}
-
-static int __ipa_del_rt_tbl(struct ipa_rt_tbl *entry)
-{
-	enum ipa_ip_type ip = IPA_IP_MAX;
-	u32 id;
-
-	if (entry == NULL || (entry->cookie != IPA_RT_TBL_COOKIE)) {
-		IPAERR("bad parms\n");
-		return -EINVAL;
-	}
-	id = entry->id;
-	if (ipa_id_find(id) == NULL) {
-		IPAERR("lookup failed\n");
-		return -EPERM;
-	}
-
-	if (entry->set == &ipa_ctx->rt_tbl_set[IPA_IP_v4])
-		ip = IPA_IP_v4;
-	else if (entry->set == &ipa_ctx->rt_tbl_set[IPA_IP_v6])
-		ip = IPA_IP_v6;
-	else {
-		WARN_ON(1);
-		return -EPERM;
-	}
-
-	if (!entry->in_sys) {
-		list_del(&entry->link);
-		clear_bit(entry->idx, &ipa_ctx->rt_idx_bitmap[ip]);
-		entry->set->tbl_cnt--;
-		IPADBG("del rt tbl_idx=%d tbl_cnt=%d\n", entry->idx,
-				entry->set->tbl_cnt);
-		kmem_cache_free(ipa_ctx->rt_tbl_cache, entry);
-	} else {
-		list_move(&entry->link,
-				&ipa_ctx->reap_rt_tbl_set[ip].head_rt_tbl_list);
-		clear_bit(entry->idx, &ipa_ctx->rt_idx_bitmap[ip]);
-		entry->set->tbl_cnt--;
-		IPADBG("del sys rt tbl_idx=%d tbl_cnt=%d\n", entry->idx,
-				entry->set->tbl_cnt);
-	}
-
-	/* remove the handle from the database */
-	ipa_id_remove(id);
-	return 0;
-}
-
-static int __ipa_add_rt_rule(enum ipa_ip_type ip, const char *name,
-		const struct ipa_rt_rule *rule, u8 at_rear, u32 *rule_hdl)
-{
-	struct ipa_rt_tbl *tbl;
-	struct ipa_rt_entry *entry;
-	struct ipa_hdr_entry *hdr = NULL;
-	struct ipa_hdr_proc_ctx_entry *proc_ctx = NULL;
-	int id;
-
-	if (rule->hdr_hdl && rule->hdr_proc_ctx_hdl) {
-		IPAERR("rule contains both hdr_hdl and hdr_proc_ctx_hdl\n");
-		goto error;
-	}
-
-	if (rule->hdr_hdl) {
-		hdr = ipa_id_find(rule->hdr_hdl);
-		if ((hdr == NULL) || (hdr->cookie != IPA_HDR_COOKIE)) {
-			IPAERR("rt rule does not point to valid hdr\n");
-			goto error;
-		}
-	} else if (rule->hdr_proc_ctx_hdl) {
-		proc_ctx = ipa_id_find(rule->hdr_proc_ctx_hdl);
-		if ((proc_ctx == NULL) ||
-			(proc_ctx->cookie != IPA_PROC_HDR_COOKIE)) {
-			IPAERR("rt rule does not point to valid proc ctx\n");
-			goto error;
-		}
-	}
-
-
-	tbl = __ipa_add_rt_tbl(ip, name);
-	if (tbl == NULL || (tbl->cookie != IPA_RT_TBL_COOKIE)) {
-		IPAERR("bad params\n");
-		goto error;
-	}
-	/*
-	 * do not allow any rules to be added at end of the "default" routing
-	 * tables
-	 */
-	if (!strncmp(tbl->name, IPA_DFLT_RT_TBL_NAME, IPA_RESOURCE_NAME_MAX) &&
-	    (tbl->rule_cnt > 0) && (at_rear != 0)) {
-		IPAERR("cannot add rule at end of tbl rule_cnt=%d at_rear=%d\n",
-		       tbl->rule_cnt, at_rear);
-		goto error;
-	}
-
-	entry = kmem_cache_zalloc(ipa_ctx->rt_rule_cache, GFP_KERNEL);
-	if (!entry) {
-		IPAERR("failed to alloc RT rule object\n");
-		goto error;
-	}
-	INIT_LIST_HEAD(&entry->link);
-	entry->cookie = IPA_RT_RULE_COOKIE;
-	entry->rule = *rule;
-	entry->tbl = tbl;
-	entry->hdr = hdr;
-	entry->proc_ctx = proc_ctx;
-	if (at_rear)
-		list_add_tail(&entry->link, &tbl->head_rt_rule_list);
-	else
-		list_add(&entry->link, &tbl->head_rt_rule_list);
-	tbl->rule_cnt++;
-	if (entry->hdr)
-		entry->hdr->ref_cnt++;
-	else if (entry->proc_ctx)
-		entry->proc_ctx->ref_cnt++;
-	id = ipa_id_alloc(entry);
-	if (id < 0) {
-		IPAERR("failed to add to tree\n");
-		WARN_ON(1);
-		goto ipa_insert_failed;
-	}
-	IPADBG("add rt rule tbl_idx=%d rule_cnt=%d\n", tbl->idx, tbl->rule_cnt);
-	*rule_hdl = id;
-	entry->id = id;
-
-	return 0;
-
-ipa_insert_failed:
-	list_del(&entry->link);
-	kmem_cache_free(ipa_ctx->rt_rule_cache, entry);
-error:
-	return -EPERM;
-}
-
-/**
- * ipa_add_rt_rule() - Add the specified routing rules to SW and optionally
- * commit to IPA HW
- * @rules:	[inout] set of routing rules to add
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_add_rt_rule(struct ipa_ioc_add_rt_rule *rules)
-{
-	int i;
-	int ret;
-
-	if (rules == NULL || rules->num_rules == 0 || rules->ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	for (i = 0; i < rules->num_rules; i++) {
-		if (__ipa_add_rt_rule(rules->ip, rules->rt_tbl_name,
-					&rules->rules[i].rule,
-					rules->rules[i].at_rear,
-					&rules->rules[i].rt_rule_hdl)) {
-			IPAERR("failed to add rt rule %d\n", i);
-			rules->rules[i].status = IPA_RT_STATUS_OF_ADD_FAILED;
-		} else {
-			rules->rules[i].status = 0;
-		}
-	}
-
-	if (rules->commit)
-		if (ipa_ctx->ctrl->ipa_commit_rt(rules->ip)) {
-			ret = -EPERM;
-			goto bail;
-		}
-
-	ret = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-	return ret;
-}
-EXPORT_SYMBOL(ipa_add_rt_rule);
-
-int __ipa_del_rt_rule(u32 rule_hdl)
-{
-	struct ipa_rt_entry *entry;
-	int id;
-
-	entry = ipa_id_find(rule_hdl);
-
-	if (entry == NULL) {
-		IPAERR("lookup failed\n");
-		return -EINVAL;
-	}
-
-	if (entry->cookie != IPA_RT_RULE_COOKIE) {
-		IPAERR("bad params\n");
-		return -EINVAL;
-	}
-
-	if (entry->hdr)
-		__ipa_release_hdr(entry->hdr->id);
-	else if (entry->proc_ctx)
-		__ipa_release_hdr_proc_ctx(entry->proc_ctx->id);
-	list_del(&entry->link);
-	entry->tbl->rule_cnt--;
-	IPADBG("del rt rule tbl_idx=%d rule_cnt=%d\n", entry->tbl->idx,
-			entry->tbl->rule_cnt);
-	if (entry->tbl->rule_cnt == 0 && entry->tbl->ref_cnt == 0) {
-		if (__ipa_del_rt_tbl(entry->tbl))
-			IPAERR("fail to del RT tbl\n");
-	}
-	entry->cookie = 0;
-	id = entry->id;
-	kmem_cache_free(ipa_ctx->rt_rule_cache, entry);
-
-	/* remove the handle from the database */
-	ipa_id_remove(id);
-
-	return 0;
-}
-
-/**
- * ipa_del_rt_rule() - Remove the specified routing rules to SW and optionally
- * commit to IPA HW
- * @hdls:	[inout] set of routing rules to delete
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_del_rt_rule(struct ipa_ioc_del_rt_rule *hdls)
-{
-	int i;
-	int ret;
-
-	if (hdls == NULL || hdls->num_hdls == 0 || hdls->ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	for (i = 0; i < hdls->num_hdls; i++) {
-		if (__ipa_del_rt_rule(hdls->hdl[i].hdl)) {
-			IPAERR("failed to del rt rule %i\n", i);
-			hdls->hdl[i].status = IPA_RT_STATUS_OF_DEL_FAILED;
-		} else {
-			hdls->hdl[i].status = 0;
-		}
-	}
-
-	if (hdls->commit)
-		if (ipa_ctx->ctrl->ipa_commit_rt(hdls->ip)) {
-			ret = -EPERM;
-			goto bail;
-		}
-
-	ret = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-	return ret;
-}
-EXPORT_SYMBOL(ipa_del_rt_rule);
-
-/**
- * ipa_commit_rt_rule() - Commit the current SW routing table of specified type
- * to IPA HW
- * @ip:	The family of routing tables
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_commit_rt(enum ipa_ip_type ip)
-{
-	int ret;
-
-	if (ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	/*
-	 * issue a commit on the filtering module of same IP type since
-	 * filtering rules point to routing tables
-	 */
-	if (ipa_commit_flt(ip))
-		return -EPERM;
-
-	mutex_lock(&ipa_ctx->lock);
-	if (ipa_ctx->ctrl->ipa_commit_rt(ip)) {
-		ret = -EPERM;
-		goto bail;
-	}
-
-	ret = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-	return ret;
-}
-EXPORT_SYMBOL(ipa_commit_rt);
-
-/**
- * ipa_reset_rt() - reset the current SW routing table of specified type
- * (does not commit to HW)
- * @ip:	The family of routing tables
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_reset_rt(enum ipa_ip_type ip)
-{
-	struct ipa_rt_tbl *tbl;
-	struct ipa_rt_tbl *tbl_next;
-	struct ipa_rt_tbl_set *set;
-	struct ipa_rt_entry *rule;
-	struct ipa_rt_entry *rule_next;
-	struct ipa_rt_tbl_set *rset;
-	u32 apps_start_idx;
-	int id;
-
-	if (ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_0) {
-		if (ip == IPA_IP_v4)
-			apps_start_idx = IPA_MEM_PART(v4_apps_rt_index_lo);
-		else
-			apps_start_idx = IPA_MEM_PART(v6_apps_rt_index_lo);
-	} else {
-		apps_start_idx = 0;
-	}
-
-	/*
-	 * issue a reset on the filtering module of same IP type since
-	 * filtering rules point to routing tables
-	 */
-	if (ipa_reset_flt(ip))
-		IPAERR("fail to reset flt ip=%d\n", ip);
-
-	set = &ipa_ctx->rt_tbl_set[ip];
-	rset = &ipa_ctx->reap_rt_tbl_set[ip];
-	mutex_lock(&ipa_ctx->lock);
-	IPADBG("reset rt ip=%d\n", ip);
-	list_for_each_entry_safe(tbl, tbl_next, &set->head_rt_tbl_list, link) {
-		list_for_each_entry_safe(rule, rule_next,
-					 &tbl->head_rt_rule_list, link) {
-			if (ipa_id_find(rule->id) == NULL) {
-				WARN_ON(1);
-				mutex_unlock(&ipa_ctx->lock);
-				return -EFAULT;
-			}
-
-			/*
-			 * for the "default" routing tbl, remove all but the
-			 *  last rule
-			 */
-			if (tbl->idx == apps_start_idx && tbl->rule_cnt == 1)
-				continue;
-
-			list_del(&rule->link);
-			tbl->rule_cnt--;
-			if (rule->hdr)
-				__ipa_release_hdr(rule->hdr->id);
-			else if (rule->proc_ctx)
-				__ipa_release_hdr_proc_ctx(rule->proc_ctx->id);
-			rule->cookie = 0;
-			id = rule->id;
-			kmem_cache_free(ipa_ctx->rt_rule_cache, rule);
-
-			/* remove the handle from the database */
-			ipa_id_remove(id);
-		}
-
-		if (ipa_id_find(tbl->id) == NULL) {
-			WARN_ON(1);
-			mutex_unlock(&ipa_ctx->lock);
-			return -EFAULT;
-		}
-		id = tbl->id;
-
-		/* do not remove the "default" routing tbl which has index 0 */
-		if (tbl->idx != apps_start_idx) {
-			if (!tbl->in_sys) {
-				list_del(&tbl->link);
-				set->tbl_cnt--;
-				clear_bit(tbl->idx,
-					  &ipa_ctx->rt_idx_bitmap[ip]);
-				IPADBG("rst rt tbl_idx=%d tbl_cnt=%d\n",
-						tbl->idx, set->tbl_cnt);
-				kmem_cache_free(ipa_ctx->rt_tbl_cache, tbl);
-			} else {
-				list_move(&tbl->link, &rset->head_rt_tbl_list);
-				clear_bit(tbl->idx,
-					  &ipa_ctx->rt_idx_bitmap[ip]);
-				set->tbl_cnt--;
-				IPADBG("rst sys rt tbl_idx=%d tbl_cnt=%d\n",
-						tbl->idx, set->tbl_cnt);
-			}
-			/* remove the handle from the database */
-			ipa_id_remove(id);
-		}
-	}
-	mutex_unlock(&ipa_ctx->lock);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_reset_rt);
-
-/**
- * ipa_get_rt_tbl() - lookup the specified routing table and return handle if it
- * exists, if lookup succeeds the routing table ref cnt is increased
- * @lookup:	[inout] routing table to lookup and its handle
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- *	Caller should call ipa_put_rt_tbl later if this function succeeds
- */
-int ipa_get_rt_tbl(struct ipa_ioc_get_rt_tbl *lookup)
-{
-	struct ipa_rt_tbl *entry;
-	int result = -EFAULT;
-
-	if (lookup == NULL || lookup->ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-	mutex_lock(&ipa_ctx->lock);
-	entry = __ipa_find_rt_tbl(lookup->ip, lookup->name);
-	if (entry && entry->cookie == IPA_RT_TBL_COOKIE) {
-		entry->ref_cnt++;
-		lookup->hdl = entry->id;
-
-		/* commit for get */
-		if (ipa_ctx->ctrl->ipa_commit_rt(lookup->ip))
-			IPAERR("fail to commit RT tbl\n");
-
-		result = 0;
-	}
-	mutex_unlock(&ipa_ctx->lock);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_get_rt_tbl);
-
-/**
- * ipa_put_rt_tbl() - Release the specified routing table handle
- * @rt_tbl_hdl:	[in] the routing table handle to release
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_put_rt_tbl(u32 rt_tbl_hdl)
-{
-	struct ipa_rt_tbl *entry;
-	enum ipa_ip_type ip = IPA_IP_MAX;
-	int result;
-
-	mutex_lock(&ipa_ctx->lock);
-	entry = ipa_id_find(rt_tbl_hdl);
-	if (entry == NULL) {
-		IPAERR("lookup failed\n");
-		result = -EINVAL;
-		goto ret;
-	}
-
-	if ((entry->cookie != IPA_RT_TBL_COOKIE) || entry->ref_cnt == 0) {
-		IPAERR("bad parms\n");
-		result = -EINVAL;
-		goto ret;
-	}
-
-	if (entry->set == &ipa_ctx->rt_tbl_set[IPA_IP_v4])
-		ip = IPA_IP_v4;
-	else if (entry->set == &ipa_ctx->rt_tbl_set[IPA_IP_v6])
-		ip = IPA_IP_v6;
-	else {
-		WARN_ON(1);
-		result = -EINVAL;
-		goto ret;
-	}
-
-	entry->ref_cnt--;
-	if (entry->ref_cnt == 0 && entry->rule_cnt == 0) {
-		if (__ipa_del_rt_tbl(entry))
-			IPAERR("fail to del RT tbl\n");
-		/* commit for put */
-		if (ipa_ctx->ctrl->ipa_commit_rt(ip))
-			IPAERR("fail to commit RT tbl\n");
-	}
-
-	result = 0;
-
-ret:
-	mutex_unlock(&ipa_ctx->lock);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_put_rt_tbl);
-
-
-static int __ipa_mdfy_rt_rule(struct ipa_rt_rule_mdfy *rtrule)
-{
-	struct ipa_rt_entry *entry;
-	struct ipa_hdr_entry *hdr = NULL;
-
-	if (rtrule->rule.hdr_hdl) {
-		hdr = ipa_id_find(rtrule->rule.hdr_hdl);
-		if ((hdr == NULL) || (hdr->cookie != IPA_HDR_COOKIE)) {
-			IPAERR("rt rule does not point to valid hdr\n");
-			goto error;
-		}
-	}
-
-	entry = ipa_id_find(rtrule->rt_rule_hdl);
-	if (entry == NULL) {
-		IPAERR("lookup failed\n");
-		goto error;
-	}
-
-	if (entry->cookie != IPA_RT_RULE_COOKIE) {
-		IPAERR("bad params\n");
-		goto error;
-	}
-
-	if (entry->hdr)
-		entry->hdr->ref_cnt--;
-
-	entry->rule = rtrule->rule;
-	entry->hdr = hdr;
-
-	if (entry->hdr)
-		entry->hdr->ref_cnt++;
-
-	return 0;
-
-error:
-	return -EPERM;
-}
-
-/**
- * ipa_mdfy_rt_rule() - Modify the specified routing rules in SW and optionally
- * commit to IPA HW
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_mdfy_rt_rule(struct ipa_ioc_mdfy_rt_rule *hdls)
-{
-	int i;
-	int result;
-
-	if (hdls == NULL || hdls->num_rules == 0 || hdls->ip >= IPA_IP_MAX) {
-		IPAERR("bad parm\n");
-		return -EINVAL;
-	}
-
-	mutex_lock(&ipa_ctx->lock);
-	for (i = 0; i < hdls->num_rules; i++) {
-		if (__ipa_mdfy_rt_rule(&hdls->rules[i])) {
-			IPAERR("failed to mdfy rt rule %i\n", i);
-			hdls->rules[i].status = IPA_RT_STATUS_OF_MDFY_FAILED;
-		} else {
-			hdls->rules[i].status = 0;
-		}
-	}
-
-	if (hdls->commit)
-		if (ipa_ctx->ctrl->ipa_commit_rt(hdls->ip)) {
-			result = -EPERM;
-			goto bail;
-		}
-	result = 0;
-bail:
-	mutex_unlock(&ipa_ctx->lock);
-
-	return result;
-}
-EXPORT_SYMBOL(ipa_mdfy_rt_rule);
diff --git a/drivers/platform/msm/ipa/ipa_uc.c b/drivers/platform/msm/ipa/ipa_uc.c
deleted file mode 100644
index a2b224b..00000000
--- a/drivers/platform/msm/ipa/ipa_uc.c
+++ /dev/null
@@ -1,888 +0,0 @@
-/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-#include "ipa_i.h"
-#include <linux/delay.h>
-
-#define IPA_RAM_UC_SMEM_SIZE 128
-#define IPA_HW_INTERFACE_VERSION     0x0111
-#define IPA_PKT_FLUSH_TO_US 100
-#define IPA_UC_POLL_SLEEP_USEC 100
-#define IPA_UC_POLL_MAX_RETRY 10000
-
-/**
- * enum ipa_cpu_2_hw_commands - Values that represent the commands from the CPU
- * IPA_CPU_2_HW_CMD_NO_OP : No operation is required.
- * IPA_CPU_2_HW_CMD_UPDATE_FLAGS : Update SW flags which defines the behavior
- *                                 of HW.
- * IPA_CPU_2_HW_CMD_DEBUG_RUN_TEST : Launch predefined test over HW.
- * IPA_CPU_2_HW_CMD_DEBUG_GET_INFO : Read HW internal debug information.
- * IPA_CPU_2_HW_CMD_ERR_FATAL : CPU instructs HW to perform error fatal
- *                              handling.
- * IPA_CPU_2_HW_CMD_CLK_GATE : CPU instructs HW to goto Clock Gated state.
- * IPA_CPU_2_HW_CMD_CLK_UNGATE : CPU instructs HW to goto Clock Ungated state.
- * IPA_CPU_2_HW_CMD_MEMCPY : CPU instructs HW to do memcopy using QMB.
- * IPA_CPU_2_HW_CMD_RESET_PIPE : Command to reset a pipe - SW WA for a HW bug.
- */
-enum ipa_cpu_2_hw_commands {
-	IPA_CPU_2_HW_CMD_NO_OP                     =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 0),
-	IPA_CPU_2_HW_CMD_UPDATE_FLAGS              =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 1),
-	IPA_CPU_2_HW_CMD_DEBUG_RUN_TEST            =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 2),
-	IPA_CPU_2_HW_CMD_DEBUG_GET_INFO            =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 3),
-	IPA_CPU_2_HW_CMD_ERR_FATAL                 =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 4),
-	IPA_CPU_2_HW_CMD_CLK_GATE                  =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 5),
-	IPA_CPU_2_HW_CMD_CLK_UNGATE                =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 6),
-	IPA_CPU_2_HW_CMD_MEMCPY                    =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 7),
-	IPA_CPU_2_HW_CMD_RESET_PIPE                =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 8),
-	IPA_CPU_2_HW_CMD_UPDATE_HOLB_MONITORING    =
-			FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 9),
-};
-
-/**
- * enum ipa_hw_2_cpu_responses -  Values that represent common HW responses
- * to CPU commands.
- * @IPA_HW_2_CPU_RESPONSE_INIT_COMPLETED : HW shall send this command once
- * boot sequence is completed and HW is ready to serve commands from CPU
- * @IPA_HW_2_CPU_RESPONSE_CMD_COMPLETED: Response to CPU commands
- */
-enum ipa_hw_2_cpu_responses {
-	IPA_HW_2_CPU_RESPONSE_INIT_COMPLETED =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 1),
-	IPA_HW_2_CPU_RESPONSE_CMD_COMPLETED  =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 2),
-};
-
-/**
- * enum ipa_hw_2_cpu_events - Values that represent HW event to be sent to CPU.
- * @IPA_HW_2_CPU_EVENT_ERROR : Event specify a system error is detected by the
- * device
- * @IPA_HW_2_CPU_EVENT_LOG_INFO : Event providing logging specific information
- */
-enum ipa_hw_2_cpu_events {
-	IPA_HW_2_CPU_EVENT_ERROR     =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 1),
-	IPA_HW_2_CPU_EVENT_LOG_INFO  =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 2),
-};
-
-/**
- * enum ipa_hw_errors - Common error types.
- * @IPA_HW_ERROR_NONE : No error persists
- * @IPA_HW_INVALID_DOORBELL_ERROR : Invalid data read from doorbell
- * @IPA_HW_DMA_ERROR : Unexpected DMA error
- * @IPA_HW_FATAL_SYSTEM_ERROR : HW has crashed and requires reset.
- * @IPA_HW_INVALID_OPCODE : Invalid opcode sent
- * @IPA_HW_ZIP_ENGINE_ERROR : ZIP engine error
- */
-enum ipa_hw_errors {
-	IPA_HW_ERROR_NONE              =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 0),
-	IPA_HW_INVALID_DOORBELL_ERROR  =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 1),
-	IPA_HW_DMA_ERROR               =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 2),
-	IPA_HW_FATAL_SYSTEM_ERROR      =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 3),
-	IPA_HW_INVALID_OPCODE          =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 4),
-	IPA_HW_ZIP_ENGINE_ERROR        =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_COMMON, 5)
-};
-
-/**
- * struct IpaHwResetPipeCmdData_t - Structure holding the parameters
- * for IPA_CPU_2_HW_CMD_MEMCPY command.
- *
- * The parameters are passed as immediate params in the shared memory
- */
-struct IpaHwMemCopyData_t  {
-	u32 destination_addr;
-	u32 source_addr;
-	u32 dest_buffer_size;
-	u32 source_buffer_size;
-};
-
-/**
- * union IpaHwResetPipeCmdData_t - Structure holding the parameters
- * for IPA_CPU_2_HW_CMD_RESET_PIPE command.
- * @pipeNum : Pipe number to be reset
- * @direction : 1 - IPA Producer, 0 - IPA Consumer
- * @reserved_02_03 : Reserved
- *
- * The parameters are passed as immediate params in the shared memory
- */
-union IpaHwResetPipeCmdData_t {
-	struct IpaHwResetPipeCmdParams_t {
-		u8     pipeNum;
-		u8     direction;
-		u32    reserved_02_03;
-	} __packed params;
-	u32 raw32b;
-} __packed;
-
-/**
- * union IpaHwmonitorHolbCmdData_t - Structure holding the parameters
- * for IPA_CPU_2_HW_CMD_UPDATE_HOLB_MONITORING command.
- * @monitorPipe : Indication whether to monitor the pipe. 0  Do not Monitor Pipe, 1  Monitor Pipe
- * @pipeNum : Pipe to be monitored/not monitored
- * @reserved_02_03 : Reserved
- *
- * The parameters are passed as immediate params in the shared memory
- */
-union IpaHwmonitorHolbCmdData_t {
-	struct IpaHwmonitorHolbCmdParams_t {
-		u8     monitorPipe;
-		u8     pipeNum;
-		u32    reserved_02_03:16;
-	} __packed params;
-	u32 raw32b;
-} __packed;
-
-
-/**
- * union IpaHwCpuCmdCompletedResponseData_t - Structure holding the parameters
- * for IPA_HW_2_CPU_RESPONSE_CMD_COMPLETED response.
- * @originalCmdOp : The original command opcode
- * @status : 0 for success indication, otherwise failure
- * @reserved : Reserved
- *
- * Parameters are sent as 32b immediate parameters.
- */
-union IpaHwCpuCmdCompletedResponseData_t {
-	struct IpaHwCpuCmdCompletedResponseParams_t {
-		u32 originalCmdOp:8;
-		u32 status:8;
-		u32 reserved:16;
-	} __packed params;
-	u32 raw32b;
-} __packed;
-
-/**
- * union IpaHwErrorEventData_t - HW->CPU Common Events
- * @errorType : Entered when a system error is detected by the HW. Type of
- * error is specified by IPA_HW_ERRORS
- * @reserved : Reserved
- */
-union IpaHwErrorEventData_t {
-	struct IpaHwErrorEventParams_t {
-		u32 errorType:8;
-		u32 reserved:24;
-	} __packed params;
-	u32 raw32b;
-} __packed;
-
-/**
- * union IpaHwUpdateFlagsCmdData_t - Structure holding the parameters for
- * IPA_CPU_2_HW_CMD_UPDATE_FLAGS command
- * @newFlags: SW flags defined the behavior of HW.
- *	This field is expected to be used as bitmask for enum ipa_hw_flags
- */
-union IpaHwUpdateFlagsCmdData_t {
-	struct IpaHwUpdateFlagsCmdParams_t {
-		u32 newFlags;
-	} params;
-	u32 raw32b;
-};
-
-struct ipa_uc_hdlrs uc_hdlrs[IPA_HW_NUM_FEATURES] = { { 0 } };
-
-static inline const char *ipa_hw_error_str(enum ipa_hw_errors err_type)
-{
-	const char *str;
-
-	switch (err_type) {
-	case IPA_HW_ERROR_NONE:
-		str = "IPA_HW_ERROR_NONE";
-		break;
-	case IPA_HW_INVALID_DOORBELL_ERROR:
-		str = "IPA_HW_INVALID_DOORBELL_ERROR";
-		break;
-	case IPA_HW_FATAL_SYSTEM_ERROR:
-		str = "IPA_HW_FATAL_SYSTEM_ERROR";
-		break;
-	case IPA_HW_INVALID_OPCODE:
-		str = "IPA_HW_INVALID_OPCODE";
-		break;
-	case IPA_HW_ZIP_ENGINE_ERROR:
-		str = "IPA_HW_ZIP_ENGINE_ERROR";
-		break;
-	default:
-		str = "INVALID ipa_hw_errors type";
-	}
-
-	return str;
-}
-
-static void ipa_log_evt_hdlr(void)
-{
-	int i;
-
-	if (!ipa_ctx->uc_ctx.uc_event_top_ofst) {
-		ipa_ctx->uc_ctx.uc_event_top_ofst =
-			ipa_ctx->uc_ctx.uc_sram_mmio->eventParams;
-		if (ipa_ctx->uc_ctx.uc_event_top_ofst +
-			sizeof(struct IpaHwEventLogInfoData_t) >=
-			ipa_ctx->ctrl->ipa_reg_base_ofst +
-			IPA_SRAM_DIRECT_ACCESS_N_OFST_v2_0(0) +
-			ipa_ctx->smem_sz) {
-				IPAERR("uc_top 0x%x outside SRAM\n",
-					ipa_ctx->uc_ctx.uc_event_top_ofst);
-				goto bad_uc_top_ofst;
-		}
-
-		ipa_ctx->uc_ctx.uc_event_top_mmio = ioremap(
-			ipa_ctx->ipa_wrapper_base +
-			ipa_ctx->uc_ctx.uc_event_top_ofst,
-			sizeof(struct IpaHwEventLogInfoData_t));
-		if (!ipa_ctx->uc_ctx.uc_event_top_mmio) {
-			IPAERR("fail to ioremap uc top\n");
-			goto bad_uc_top_ofst;
-		}
-
-		for (i = 0; i < IPA_HW_NUM_FEATURES; i++) {
-			if (uc_hdlrs[i].ipa_uc_event_log_info_hdlr)
-				uc_hdlrs[i].ipa_uc_event_log_info_hdlr
-					(ipa_ctx->uc_ctx.uc_event_top_mmio);
-		}
-	} else {
-
-		if (ipa_ctx->uc_ctx.uc_sram_mmio->eventParams !=
-			ipa_ctx->uc_ctx.uc_event_top_ofst) {
-				IPAERR("uc top ofst changed new=%u cur=%u\n",
-					ipa_ctx->uc_ctx.uc_sram_mmio->
-						eventParams,
-					ipa_ctx->uc_ctx.uc_event_top_ofst);
-		}
-	}
-
-	return;
-
-bad_uc_top_ofst:
-	ipa_ctx->uc_ctx.uc_event_top_ofst = 0;
-	return;
-}
-
-/**
- * ipa_uc_state_check() - Check the status of the uC interface
- *
- * Return value: 0 if the uC is loaded, interface is initialized
- *               and there was no recent failure in one of the commands.
- *               A negative value is returned otherwise.
- */
-int ipa_uc_state_check(void)
-{
-	if (!ipa_ctx->uc_ctx.uc_inited) {
-		IPAERR("uC interface not initialized\n");
-		return -EFAULT;
-	}
-
-	if (!ipa_ctx->uc_ctx.uc_loaded) {
-		IPAERR("uC is not loaded\n");
-		return -EFAULT;
-	}
-
-	if (ipa_ctx->uc_ctx.uc_failed) {
-		IPAERR("uC has failed its last command\n");
-		return -EFAULT;
-	}
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_uc_state_check);
-
-/**
- * ipa_uc_loaded_check() - Check the uC has been loaded
- *
- * Return value: 1 if the uC is loaded, 0 otherwise
- */
-int ipa_uc_loaded_check(void)
-{
-	return ipa_ctx->uc_ctx.uc_loaded;
-}
-EXPORT_SYMBOL(ipa_uc_loaded_check);
-
-static void ipa_uc_event_handler(enum ipa_irq_type interrupt,
-				 void *private_data,
-				 void *interrupt_data)
-{
-	union IpaHwErrorEventData_t evt;
-	u8 feature;
-
-	WARN_ON(private_data != ipa_ctx);
-
-	ipa_inc_client_enable_clks();
-
-	IPADBG("uC evt opcode=%u\n",
-		ipa_ctx->uc_ctx.uc_sram_mmio->eventOp);
-
-
-	feature = EXTRACT_UC_FEATURE(ipa_ctx->uc_ctx.uc_sram_mmio->eventOp);
-
-	if (0 > feature || IPA_HW_FEATURE_MAX <= feature) {
-		IPAERR("Invalid feature %u for event %u\n",
-			feature, ipa_ctx->uc_ctx.uc_sram_mmio->eventOp);
-		ipa_dec_client_disable_clks();
-		return;
-	}
-	/* Feature specific handling */
-	if (uc_hdlrs[feature].ipa_uc_event_hdlr)
-		uc_hdlrs[feature].ipa_uc_event_hdlr
-			(ipa_ctx->uc_ctx.uc_sram_mmio);
-
-	/* General handling */
-	if (ipa_ctx->uc_ctx.uc_sram_mmio->eventOp ==
-	    IPA_HW_2_CPU_EVENT_ERROR) {
-		evt.raw32b = ipa_ctx->uc_ctx.uc_sram_mmio->eventParams;
-		IPAERR("uC Error, evt errorType = %s\n",
-			ipa_hw_error_str(evt.params.errorType));
-		ipa_ctx->uc_ctx.uc_failed = true;
-		ipa_ctx->uc_ctx.uc_error_type = evt.params.errorType;
-		if (evt.params.errorType == IPA_HW_ZIP_ENGINE_ERROR) {
-			IPAERR("IPA has encountered a ZIP engine error\n");
-			ipa_ctx->uc_ctx.uc_zip_error = true;
-		}
-		BUG();
-	} else if (ipa_ctx->uc_ctx.uc_sram_mmio->eventOp ==
-		IPA_HW_2_CPU_EVENT_LOG_INFO) {
-			IPADBG("uC evt log info ofst=0x%x\n",
-				ipa_ctx->uc_ctx.uc_sram_mmio->eventParams);
-		ipa_log_evt_hdlr();
-	} else {
-		IPADBG("unsupported uC evt opcode=%u\n",
-				ipa_ctx->uc_ctx.uc_sram_mmio->eventOp);
-	}
-	ipa_dec_client_disable_clks();
-
-}
-
-static int ipa_uc_panic_notifier(struct notifier_block *this,
-		unsigned long event, void *ptr)
-{
-	int result = 0;
-
-	IPADBG("this=%p evt=%lu ptr=%p\n", this, event, ptr);
-
-	result = ipa_uc_state_check();
-	if (result)
-		goto fail;
-
-	if (ipa_inc_client_enable_clks_no_block())
-		goto fail;
-
-	ipa_ctx->uc_ctx.uc_sram_mmio->cmdOp =
-		IPA_CPU_2_HW_CMD_ERR_FATAL;
-	/* ensure write to shared memory is done before triggering uc */
-	wmb();
-	ipa_write_reg(ipa_ctx->mmio, IPA_IRQ_EE_UC_n_OFFS(0), 0x1);
-	/* give uc enough time to save state */
-	udelay(IPA_PKT_FLUSH_TO_US);
-
-	ipa_dec_client_disable_clks();
-	IPADBG("err_fatal issued\n");
-
-fail:
-	return NOTIFY_DONE;
-}
-
-static struct notifier_block ipa_uc_panic_blk = {
-	.notifier_call  = ipa_uc_panic_notifier,
-};
-
-void ipa_register_panic_uc_hdlr(void)
-{
-	atomic_notifier_chain_register(&panic_notifier_list,
-			&ipa_uc_panic_blk);
-}
-
-static void ipa_uc_response_hdlr(enum ipa_irq_type interrupt,
-				void *private_data,
-				void *interrupt_data)
-{
-	union IpaHwCpuCmdCompletedResponseData_t uc_rsp;
-	u8 feature;
-	int res;
-	int i;
-
-	WARN_ON(private_data != ipa_ctx);
-
-	ipa_inc_client_enable_clks();
-	IPADBG("uC rsp opcode=%u\n",
-			ipa_ctx->uc_ctx.uc_sram_mmio->responseOp);
-
-	feature = EXTRACT_UC_FEATURE(ipa_ctx->uc_ctx.uc_sram_mmio->responseOp);
-
-	if (0 > feature || IPA_HW_FEATURE_MAX <= feature) {
-		IPAERR("Invalid feature %u for event %u\n",
-			feature, ipa_ctx->uc_ctx.uc_sram_mmio->eventOp);
-		ipa_dec_client_disable_clks();
-		return;
-	}
-
-	/* Feature specific handling */
-	if (uc_hdlrs[feature].ipa_uc_response_hdlr) {
-		res = uc_hdlrs[feature].ipa_uc_response_hdlr(
-			ipa_ctx->uc_ctx.uc_sram_mmio,
-			&ipa_ctx->uc_ctx.uc_status);
-		if (res == 0) {
-			IPADBG("feature %d specific response handler\n",
-				feature);
-			complete_all(&ipa_ctx->uc_ctx.uc_completion);
-			ipa_dec_client_disable_clks();
-			return;
-		}
-	}
-
-	/* General handling */
-	if (ipa_ctx->uc_ctx.uc_sram_mmio->responseOp ==
-			IPA_HW_2_CPU_RESPONSE_INIT_COMPLETED) {
-		ipa_ctx->uc_ctx.uc_loaded = true;
-		IPAERR("IPA uC loaded\n");
-		/*
-		 * The proxy vote is held until uC is loaded to ensure that
-		 * IPA_HW_2_CPU_RESPONSE_INIT_COMPLETED is received.
-		 */
-		ipa_proxy_clk_unvote();
-		for (i = 0; i < IPA_HW_NUM_FEATURES; i++) {
-			if (uc_hdlrs[i].ipa_uc_loaded_hdlr)
-				uc_hdlrs[i].ipa_uc_loaded_hdlr();
-		}
-		/* Enable holb monitoring on IPA-USB Producer pipe if valid. */
-		ipa_uc_monitor_holb(IPA_CLIENT_USB_CONS, true);
-	} else if (ipa_ctx->uc_ctx.uc_sram_mmio->responseOp ==
-		   IPA_HW_2_CPU_RESPONSE_CMD_COMPLETED) {
-		uc_rsp.raw32b = ipa_ctx->uc_ctx.uc_sram_mmio->responseParams;
-		IPADBG("uC cmd response opcode=%u status=%u\n",
-		       uc_rsp.params.originalCmdOp,
-		       uc_rsp.params.status);
-		if (uc_rsp.params.originalCmdOp ==
-		    ipa_ctx->uc_ctx.pending_cmd) {
-			ipa_ctx->uc_ctx.uc_status = uc_rsp.params.status;
-			complete_all(&ipa_ctx->uc_ctx.uc_completion);
-		} else {
-			IPAERR("Expected cmd=%u rcvd cmd=%u\n",
-			       ipa_ctx->uc_ctx.pending_cmd,
-			       uc_rsp.params.originalCmdOp);
-		}
-	} else {
-		IPAERR("Unsupported uC rsp opcode = %u\n",
-		       ipa_ctx->uc_ctx.uc_sram_mmio->responseOp);
-	}
-	ipa_dec_client_disable_clks();
-}
-
-/**
- * ipa_uc_interface_init() - Initialize the interface with the uC
- *
- * Return value: 0 on success, negative value otherwise
- */
-int ipa_uc_interface_init(void)
-{
-	int result;
-	unsigned long phys_addr;
-
-	if (ipa_ctx->uc_ctx.uc_inited) {
-		IPADBG("uC interface already initialized\n");
-		return 0;
-	}
-
-	mutex_init(&ipa_ctx->uc_ctx.uc_lock);
-
-	if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_5) {
-		phys_addr = ipa_ctx->ipa_wrapper_base +
-			ipa_ctx->ctrl->ipa_reg_base_ofst +
-			IPA_SRAM_SW_FIRST_v2_5;
-	} else {
-		phys_addr = ipa_ctx->ipa_wrapper_base +
-			ipa_ctx->ctrl->ipa_reg_base_ofst +
-			IPA_SRAM_DIRECT_ACCESS_N_OFST_v2_0(
-			ipa_ctx->smem_restricted_bytes / 4);
-	}
-
-	ipa_ctx->uc_ctx.uc_sram_mmio = ioremap(phys_addr,
-					       IPA_RAM_UC_SMEM_SIZE);
-	if (!ipa_ctx->uc_ctx.uc_sram_mmio) {
-		IPAERR("Fail to ioremap IPA uC SRAM\n");
-		result = -ENOMEM;
-		goto remap_fail;
-	}
-
-	result = ipa_add_interrupt_handler(IPA_UC_IRQ_0,
-		ipa_uc_event_handler, true,
-		ipa_ctx);
-	if (result) {
-		IPAERR("Fail to register for UC_IRQ0 rsp interrupt\n");
-		result = -EFAULT;
-		goto irq_fail0;
-	}
-
-	result = ipa_add_interrupt_handler(IPA_UC_IRQ_1,
-		ipa_uc_response_hdlr, true,
-		ipa_ctx);
-	if (result) {
-		IPAERR("fail to register for UC_IRQ1 rsp interrupt\n");
-		result = -EFAULT;
-		goto irq_fail1;
-	}
-
-	ipa_ctx->uc_ctx.uc_inited = true;
-
-	IPADBG("IPA uC interface is initialized\n");
-	return 0;
-
-irq_fail1:
-	ipa_remove_interrupt_handler(IPA_UC_IRQ_0);
-irq_fail0:
-	iounmap(ipa_ctx->uc_ctx.uc_sram_mmio);
-remap_fail:
-	return result;
-}
-EXPORT_SYMBOL(ipa_uc_interface_init);
-
-/**
- * ipa_uc_send_cmd() - Send a command to the uC
- *
- * Note: In case the operation times out (No response from the uC) or
- *       polling maximal amount of retries has reached, the logic
- *       considers it as an invalid state of the uC/IPA, and
- *       issues a kernel panic.
- *
- * Returns: 0 on success.
- *          -EINVAL in case of invalid input.
- *          -EBADF in case uC interface is not initialized /
- *                 or the uC has failed previously.
- *          -EFAULT in case the received status doesn't match
- *                  the expected.
- */
-int ipa_uc_send_cmd(u32 cmd, u32 opcode, u32 expected_status,
-		    bool polling_mode, unsigned long timeout_jiffies)
-{
-	int index;
-	union IpaHwCpuCmdCompletedResponseData_t uc_rsp;
-
-	mutex_lock(&ipa_ctx->uc_ctx.uc_lock);
-
-	if (ipa_uc_state_check()) {
-		IPADBG("uC send command aborted\n");
-		mutex_unlock(&ipa_ctx->uc_ctx.uc_lock);
-		return -EBADF;
-	}
-
-	init_completion(&ipa_ctx->uc_ctx.uc_completion);
-
-	ipa_ctx->uc_ctx.uc_sram_mmio->cmdParams = cmd;
-	ipa_ctx->uc_ctx.uc_sram_mmio->cmdOp = opcode;
-	ipa_ctx->uc_ctx.pending_cmd = opcode;
-
-	ipa_ctx->uc_ctx.uc_sram_mmio->responseOp = 0;
-	ipa_ctx->uc_ctx.uc_sram_mmio->responseParams = 0;
-
-	ipa_ctx->uc_ctx.uc_status = 0;
-
-	/* ensure write to shared memory is done before triggering uc */
-	wmb();
-
-	ipa_write_reg(ipa_ctx->mmio, IPA_IRQ_EE_UC_n_OFFS(0), 0x1);
-
-	if (polling_mode) {
-		for (index = 0; index < IPA_UC_POLL_MAX_RETRY; index++) {
-			if (ipa_ctx->uc_ctx.uc_sram_mmio->responseOp ==
-			    IPA_HW_2_CPU_RESPONSE_CMD_COMPLETED) {
-				uc_rsp.raw32b = ipa_ctx->uc_ctx.uc_sram_mmio->
-						responseParams;
-				if (uc_rsp.params.originalCmdOp ==
-				    ipa_ctx->uc_ctx.pending_cmd) {
-					ipa_ctx->uc_ctx.pending_cmd = -1;
-					break;
-				}
-			}
-			usleep(IPA_UC_POLL_SLEEP_USEC);
-		}
-
-		if (index == IPA_UC_POLL_MAX_RETRY) {
-			IPAERR("uC max polling retries reached\n");
-			if (ipa_ctx->uc_ctx.uc_failed) {
-				IPAERR("uC reported on Error, errorType = %s\n",
-					ipa_hw_error_str(ipa_ctx->
-					uc_ctx.uc_error_type));
-			}
-			mutex_unlock(&ipa_ctx->uc_ctx.uc_lock);
-			BUG();
-			return -EFAULT;
-		}
-	} else {
-		if (wait_for_completion_timeout(&ipa_ctx->uc_ctx.uc_completion,
-			timeout_jiffies) == 0) {
-			IPAERR("uC timed out\n");
-			if (ipa_ctx->uc_ctx.uc_failed) {
-				IPAERR("uC reported on Error, errorType = %s\n",
-					ipa_hw_error_str(ipa_ctx->
-					uc_ctx.uc_error_type));
-			}
-			mutex_unlock(&ipa_ctx->uc_ctx.uc_lock);
-			BUG();
-			return -EFAULT;
-		}
-	}
-
-	if (ipa_ctx->uc_ctx.uc_status != expected_status) {
-		IPAERR("Recevied status %u, Expected status %u\n",
-			ipa_ctx->uc_ctx.uc_status, expected_status);
-		ipa_ctx->uc_ctx.pending_cmd = -1;
-		mutex_unlock(&ipa_ctx->uc_ctx.uc_lock);
-		return -EFAULT;
-	}
-
-	ipa_ctx->uc_ctx.pending_cmd = -1;
-	mutex_unlock(&ipa_ctx->uc_ctx.uc_lock);
-
-	IPADBG("uC cmd %u send succeeded\n", opcode);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_uc_send_cmd);
-
-/**
- * ipa_uc_register_handlers() - Registers event, response and log event
- *                              handlers for a specific feature.Please note
- *                              that currently only one handler can be
- *                              registered per feature.
- *
- * Return value: None
- */
-void ipa_uc_register_handlers(enum ipa_hw_features feature,
-			      struct ipa_uc_hdlrs *hdlrs)
-{
-
-	if (0 > feature || IPA_HW_FEATURE_MAX <= feature) {
-		IPAERR("Feature %u is invalid, not registering hdlrs\n",
-		       feature);
-		return;
-	}
-
-	mutex_lock(&ipa_ctx->uc_ctx.uc_lock);
-	uc_hdlrs[feature] = *hdlrs;
-	mutex_unlock(&ipa_ctx->uc_ctx.uc_lock);
-
-	IPADBG("uC handlers registered for feature %u\n", feature);
-}
-EXPORT_SYMBOL(ipa_uc_register_handlers);
-
-/**
- * ipa_uc_reset_pipe() - reset a BAM pipe using the uC interface
- * @ipa_client: [in] ipa client handle representing the pipe
- *
- * The function uses the uC interface in order to issue a BAM
- * PIPE reset request. The uC makes sure there's no traffic in
- * the TX command queue before issuing the reset.
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_uc_reset_pipe(enum ipa_client_type ipa_client)
-{
-	union IpaHwResetPipeCmdData_t cmd;
-	int ep_idx;
-	int ret;
-
-	ep_idx = ipa_get_ep_mapping(ipa_client);
-	if (ep_idx == -1) {
-		IPAERR("Invalid IPA client\n");
-		return 0;
-	}
-
-	/*
-	 * If the uC interface has not been initialized yet,
-	 * continue with the sequence without resetting the
-	 * pipe.
-	 */
-	if (ipa_uc_state_check()) {
-		IPADBG("uC interface will not be used to reset %s pipe %d\n",
-		       IPA_CLIENT_IS_PROD(ipa_client) ? "CONS" : "PROD",
-		       ep_idx);
-		return 0;
-	}
-
-	/*
-	 * IPA consumer = 0, IPA producer = 1.
-	 * IPA driver concept of PROD/CONS is the opposite of the
-	 * IPA HW concept. Therefore, IPA AP CLIENT PRODUCER = IPA CONSUMER,
-	 * and vice-versa.
-	 */
-	cmd.params.direction = (u8)(IPA_CLIENT_IS_PROD(ipa_client) ? 0 : 1);
-	cmd.params.pipeNum = (u8)ep_idx;
-
-	IPADBG("uC pipe reset on IPA %s pipe %d\n",
-	       IPA_CLIENT_IS_PROD(ipa_client) ? "CONS" : "PROD", ep_idx);
-
-	ret = ipa_uc_send_cmd(cmd.raw32b, IPA_CPU_2_HW_CMD_RESET_PIPE, 0,
-			      false, 10*HZ);
-
-	return ret;
-}
-EXPORT_SYMBOL(ipa_uc_reset_pipe);
-
-/**
- * ipa_uc_monitor_holb() - Enable/Disable holb monitoring of a producer pipe.
- * @ipa_client: [in] ipa client handle representing the pipe
- *
- * The function uses the uC interface in order to disable/enable holb
- * monitoring.
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_uc_monitor_holb(enum ipa_client_type ipa_client, bool enable)
-{
-	union IpaHwmonitorHolbCmdData_t cmd;
-	int ep_idx;
-	int ret;
-
-	/* HOLB monitoring is applicable only to 2.6L. */
-	if (ipa_ctx->ipa_hw_type != IPA_HW_v2_6L) {
-		IPADBG("Not applicable on this target\n");
-		return 0;
-	}
-
-	ep_idx = ipa_get_ep_mapping(ipa_client);
-	if (ep_idx == -1) {
-		IPAERR("Invalid IPA client\n");
-		return 0;
-	}
-
-	/*
-	 * If the uC interface has not been initialized yet,
-	 * continue with the sequence without resetting the
-	 * pipe.
-	 */
-	if (ipa_uc_state_check()) {
-		IPADBG("uC interface will not be used to reset %s pipe %d\n",
-		       IPA_CLIENT_IS_PROD(ipa_client) ? "CONS" : "PROD",
-		       ep_idx);
-		return 0;
-	}
-
-	/*
-	 * IPA consumer = 0, IPA producer = 1.
-	 * IPA driver concept of PROD/CONS is the opposite of the
-	 * IPA HW concept. Therefore, IPA AP CLIENT PRODUCER = IPA CONSUMER,
-	 * and vice-versa.
-	 */
-	cmd.params.monitorPipe = (u8)(enable ? 1 : 0);
-	cmd.params.pipeNum = (u8)ep_idx;
-
-	IPADBG("uC holb monitoring on IPA pipe %d, Enable: %d\n",
-	       ep_idx, enable);
-
-	ret = ipa_uc_send_cmd(cmd.raw32b,
-				IPA_CPU_2_HW_CMD_UPDATE_HOLB_MONITORING, 0,
-				true, 10*HZ);
-
-	return ret;
-}
-EXPORT_SYMBOL(ipa_uc_monitor_holb);
-
-
-/**
- * ipa_uc_notify_clk_state() - notify to uC of clock enable / disable
- * @enabled: true if clock are enabled
- *
- * The function uses the uC interface in order to notify uC before IPA clocks
- * are disabled to make sure uC is not in the middle of operation.
- * Also after clocks are enabled ned to notify uC to start processing.
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_uc_notify_clk_state(bool enabled)
-{
-	u32 opcode;
-
-	/*
-	 * If the uC interface has not been initialized yet,
-	 * don't notify the uC on the enable/disable
-	 */
-	if (ipa_uc_state_check()) {
-		IPADBG("uC interface will not notify the UC on clock state\n");
-		return 0;
-	}
-
-	IPADBG("uC clock %s notification\n", (enabled) ? "UNGATE" : "GATE");
-
-	opcode = (enabled) ? IPA_CPU_2_HW_CMD_CLK_UNGATE :
-			     IPA_CPU_2_HW_CMD_CLK_GATE;
-
-	return ipa_uc_send_cmd(0, opcode, 0, true, 0);
-}
-EXPORT_SYMBOL(ipa_uc_notify_clk_state);
-
-/**
- * ipa_uc_update_hw_flags() - send uC the HW flags to be used
- * @flags: This field is expected to be used as bitmask for enum ipa_hw_flags
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_uc_update_hw_flags(u32 flags)
-{
-	union IpaHwUpdateFlagsCmdData_t cmd;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.params.newFlags = flags;
-	return ipa_uc_send_cmd(cmd.raw32b, IPA_CPU_2_HW_CMD_UPDATE_FLAGS, 0,
-		false, HZ);
-}
-EXPORT_SYMBOL(ipa_uc_update_hw_flags);
-
-/**
- * ipa_uc_memcpy() - Perform a memcpy action using IPA uC
- * @dest: physical address to store the copied data.
- * @src: physical address of the source data to copy.
- * @len: number of bytes to copy.
- *
- * Returns: 0 on success, negative on failure
- */
-int ipa_uc_memcpy(phys_addr_t dest, phys_addr_t src, int len)
-{
-	int res;
-	struct ipa_mem_buffer mem;
-	struct IpaHwMemCopyData_t *cmd;
-
-	IPADBG("dest 0x%pa src 0x%pa len %d\n", &dest, &src, len);
-	mem.size = sizeof(cmd);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-		GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		return -ENOMEM;
-	}
-	cmd = (struct IpaHwMemCopyData_t *)mem.base;
-	memset(cmd, 0, sizeof(*cmd));
-	cmd->destination_addr = dest;
-	cmd->dest_buffer_size = len;
-	cmd->source_addr = src;
-	cmd->source_buffer_size = len;
-	res = ipa_uc_send_cmd((u32)mem.phys_base, IPA_CPU_2_HW_CMD_MEMCPY, 0,
-		true, 10 * HZ);
-	if (res) {
-		IPAERR("ipa_uc_send_cmd failed %d\n", res);
-		goto free_coherent;
-	}
-
-	res = 0;
-free_coherent:
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-	return res;
-}
diff --git a/drivers/platform/msm/ipa/ipa_uc_mhi.c b/drivers/platform/msm/ipa/ipa_uc_mhi.c
deleted file mode 100644
index 1e1a98d..00000000
--- a/drivers/platform/msm/ipa/ipa_uc_mhi.c
+++ /dev/null
@@ -1,948 +0,0 @@
-/* Copyright (c) 2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/ipa.h>
-#include "ipa_i.h"
-
-/* MHI uC interface definitions */
-#define IPA_HW_INTERFACE_MHI_VERSION            0x0004
-
-#define IPA_HW_MAX_NUMBER_OF_CHANNELS	2
-#define IPA_HW_MAX_NUMBER_OF_EVENTRINGS	2
-#define IPA_HW_MAX_CHANNEL_HANDLE	(IPA_HW_MAX_NUMBER_OF_CHANNELS-1)
-
-/**
- * Values that represent the MHI commands from CPU to IPA HW.
- * @IPA_CPU_2_HW_CMD_MHI_INIT: Initialize HW to be ready for MHI processing.
- *	Once operation was completed HW shall respond with
- *	IPA_HW_2_CPU_RESPONSE_CMD_COMPLETED.
- * @IPA_CPU_2_HW_CMD_MHI_INIT_CHANNEL: Initialize specific channel to be ready
- *	to serve MHI transfers. Once initialization was completed HW shall
- *	respond with IPA_HW_2_CPU_RESPONSE_MHI_CHANGE_CHANNEL_STATE.
- *		IPA_HW_MHI_CHANNEL_STATE_ENABLE
- * @IPA_CPU_2_HW_CMD_MHI_UPDATE_MSI: Update MHI MSI interrupts data.
- *	Once operation was completed HW shall respond with
- *	IPA_HW_2_CPU_RESPONSE_CMD_COMPLETED.
- * @IPA_CPU_2_HW_CMD_MHI_CHANGE_CHANNEL_STATE: Change specific channel
- *	processing state following host request. Once operation was completed
- *	HW shall respond with IPA_HW_2_CPU_RESPONSE_MHI_CHANGE_CHANNEL_STATE.
- * @IPA_CPU_2_HW_CMD_MHI_DL_UL_SYNC_INFO: Info related to DL UL syncronization.
- * @IPA_CPU_2_HW_CMD_MHI_STOP_EVENT_UPDATE: Cmd to stop event ring processing.
- */
-enum ipa_cpu_2_hw_mhi_commands {
-	IPA_CPU_2_HW_CMD_MHI_INIT
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 0),
-	IPA_CPU_2_HW_CMD_MHI_INIT_CHANNEL
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 1),
-	IPA_CPU_2_HW_CMD_MHI_UPDATE_MSI
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 2),
-	IPA_CPU_2_HW_CMD_MHI_CHANGE_CHANNEL_STATE
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 3),
-	IPA_CPU_2_HW_CMD_MHI_DL_UL_SYNC_INFO
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 4),
-	IPA_CPU_2_HW_CMD_MHI_STOP_EVENT_UPDATE
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 5)
-};
-
-/**
- * Values that represent MHI related HW responses to CPU commands.
- * @IPA_HW_2_CPU_RESPONSE_MHI_CHANGE_CHANNEL_STATE: Response to
- *	IPA_CPU_2_HW_CMD_MHI_INIT_CHANNEL or
- *	IPA_CPU_2_HW_CMD_MHI_CHANGE_CHANNEL_STATE commands.
- */
-enum ipa_hw_2_cpu_mhi_responses {
-	IPA_HW_2_CPU_RESPONSE_MHI_CHANGE_CHANNEL_STATE
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 0),
-};
-
-/**
- * Values that represent MHI related HW event to be sent to CPU.
- * @IPA_HW_2_CPU_EVENT_MHI_CHANNEL_ERROR: Event specify the device detected an
- *	error in an element from the transfer ring associated with the channel
- * @IPA_HW_2_CPU_EVENT_MHI_CHANNEL_WAKE_UP_REQUEST: Event specify a bam
- *	interrupt was asserted when MHI engine is suspended
- */
-enum ipa_hw_2_cpu_mhi_events {
-	IPA_HW_2_CPU_EVENT_MHI_CHANNEL_ERROR
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 0),
-	IPA_HW_2_CPU_EVENT_MHI_CHANNEL_WAKE_UP_REQUEST
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 1),
-};
-
-/**
- * Channel error types.
- * @IPA_HW_CHANNEL_ERROR_NONE: No error persists.
- * @IPA_HW_CHANNEL_INVALID_RE_ERROR: Invalid Ring Element was detected
- */
-enum ipa_hw_channel_errors {
-	IPA_HW_CHANNEL_ERROR_NONE,
-	IPA_HW_CHANNEL_INVALID_RE_ERROR
-};
-
-/**
- * MHI error types.
- * @IPA_HW_INVALID_MMIO_ERROR: Invalid data read from MMIO space
- * @IPA_HW_INVALID_CHANNEL_ERROR: Invalid data read from channel context array
- * @IPA_HW_INVALID_EVENT_ERROR: Invalid data read from event ring context array
- * @IPA_HW_NO_ED_IN_RING_ERROR: No event descriptors are available to report on
- *	secondary event ring
- * @IPA_HW_LINK_ERROR: Link error
- */
-enum ipa_hw_mhi_errors {
-	IPA_HW_INVALID_MMIO_ERROR
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 0),
-	IPA_HW_INVALID_CHANNEL_ERROR
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 1),
-	IPA_HW_INVALID_EVENT_ERROR
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 2),
-	IPA_HW_NO_ED_IN_RING_ERROR
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 4),
-	IPA_HW_LINK_ERROR
-		= FEATURE_ENUM_VAL(IPA_HW_FEATURE_MHI, 5),
-};
-
-
-/**
- * Structure referring to the common and MHI section of 128B shared memory
- * located in offset zero of SW Partition in IPA SRAM.
- * The shared memory is used for communication between IPA HW and CPU.
- * @common: common section in IPA SRAM
- * @interfaceVersionMhi: The MHI interface version as reported by HW
- * @mhiState: Overall MHI state
- * @reserved_2B: reserved
- * @mhiCnl0State: State of MHI channel 0.
- *	The state carries information regarding the error type.
- *	See IPA_HW_MHI_CHANNEL_STATES.
- * @mhiCnl0State: State of MHI channel 1.
- * @mhiCnl0State: State of MHI channel 2.
- * @mhiCnl0State: State of MHI channel 3
- * @mhiCnl0State: State of MHI channel 4.
- * @mhiCnl0State: State of MHI channel 5.
- * @mhiCnl0State: State of MHI channel 6.
- * @mhiCnl0State: State of MHI channel 7.
- * @reserved_37_34: reserved
- * @reserved_3B_38: reserved
- * @reserved_3F_3C: reserved
- */
-struct IpaHwSharedMemMhiMapping_t {
-	struct IpaHwSharedMemCommonMapping_t common;
-	u16 interfaceVersionMhi;
-	u8 mhiState;
-	u8 reserved_2B;
-	u8 mhiCnl0State;
-	u8 mhiCnl1State;
-	u8 mhiCnl2State;
-	u8 mhiCnl3State;
-	u8 mhiCnl4State;
-	u8 mhiCnl5State;
-	u8 mhiCnl6State;
-	u8 mhiCnl7State;
-	u32 reserved_37_34;
-	u32 reserved_3B_38;
-	u32 reserved_3F_3C;
-};
-
-
-/**
- * Structure holding the parameters for IPA_CPU_2_HW_CMD_MHI_INIT command.
- * Parameters are sent as pointer thus should be reside in address accessible
- * to HW.
- * @msiAddress: The MSI base (in device space) used for asserting the interrupt
- *	(MSI) associated with the event ring
- * mmioBaseAddress: The address (in device space) of MMIO structure in
- *	host space
- * deviceMhiCtrlBaseAddress: Base address of the memory region in the device
- *	address space where the MHI control data structures are allocated by
- *	the host, including channel context array, event context array,
- *	and rings. This value is used for host/device address translation.
- * deviceMhiDataBaseAddress: Base address of the memory region in the device
- *	address space where the MHI data buffers are allocated by the host.
- *	This value is used for host/device address translation.
- * firstChannelIndex: First channel ID. Doorbell 0 is mapped to this channel
- * firstEventRingIndex: First event ring ID. Doorbell 16 is mapped to this
- *	event ring.
- */
-struct IpaHwMhiInitCmdData_t {
-	u32 msiAddress;
-	u32 mmioBaseAddress;
-	u32 deviceMhiCtrlBaseAddress;
-	u32 deviceMhiDataBaseAddress;
-	u32 firstChannelIndex;
-	u32 firstEventRingIndex;
-};
-
-/**
- * Structure holding the parameters for IPA_CPU_2_HW_CMD_MHI_INIT_CHANNEL
- *	command. Parameters are sent as 32b immediate parameters.
- * @hannelHandle: The channel identifier as allocated by driver.
- *	value is within the range 0 to IPA_HW_MAX_CHANNEL_HANDLE
- * @contexArrayIndex: Unique index for channels, between 0 and 255. The index is
- *	used as an index in channel context array structures.
- * @bamPipeId: The BAM pipe number for pipe dedicated for this channel
- * @channelDirection: The direction of the channel as defined in the channel
- *	type field (CHTYPE) in the channel context data structure.
- * @reserved: reserved.
- */
-union IpaHwMhiInitChannelCmdData_t {
-	struct IpaHwMhiInitChannelCmdParams_t {
-		u32 channelHandle:8;
-		u32 contexArrayIndex:8;
-		u32 bamPipeId:6;
-		u32 channelDirection:2;
-		u32 reserved:8;
-	} params;
-	u32 raw32b;
-};
-
-/**
- * Structure holding the parameters for IPA_CPU_2_HW_CMD_MHI_UPDATE_MSI command.
- * @msiAddress_low: The MSI lower base addr (in device space) used for asserting
- *	the interrupt (MSI) associated with the event ring.
- * @msiAddress_hi: The MSI higher base addr (in device space) used for asserting
- *	the interrupt (MSI) associated with the event ring.
- * @msiMask: Mask indicating number of messages assigned by the host to device
- * @msiData: Data Pattern to use when generating the MSI
- */
-struct IpaHwMhiMsiCmdData_t {
-	u32 msiAddress_low;
-	u32 msiAddress_hi;
-	u32 msiMask;
-	u32 msiData;
-};
-
-/**
- * Structure holding the parameters for
- * IPA_CPU_2_HW_CMD_MHI_CHANGE_CHANNEL_STATE command.
- * Parameters are sent as 32b immediate parameters.
- * @requestedState: The requested channel state as was indicated from Host.
- *	Use IPA_HW_MHI_CHANNEL_STATES to specify the requested state
- * @channelHandle: The channel identifier as allocated by driver.
- *	value is within the range 0 to IPA_HW_MAX_CHANNEL_HANDLE
- * @LPTransitionRejected: Indication that low power state transition was
- *	rejected
- * @reserved: reserved
- */
-union IpaHwMhiChangeChannelStateCmdData_t {
-	struct IpaHwMhiChangeChannelStateCmdParams_t {
-		u32 requestedState:8;
-		u32 channelHandle:8;
-		u32 LPTransitionRejected:8;
-		u32 reserved:8;
-	} params;
-	u32 raw32b;
-};
-
-/**
- * Structure holding the parameters for
- *	IPA_CPU_2_HW_CMD_MHI_STOP_EVENT_UPDATE command.
- * Parameters are sent as 32b immediate parameters.
- * @channelHandle: The channel identifier as allocated by driver.
- *	value is within the range 0 to IPA_HW_MAX_CHANNEL_HANDLE
- * @reserved: reserved
- */
-union IpaHwMhiStopEventUpdateData_t {
-	struct IpaHwMhiStopEventUpdateDataParams_t {
-		u32 channelHandle:8;
-		u32 reserved:24;
-	} params;
-	u32 raw32b;
-};
-
-/**
- * Structure holding the parameters for
- *	IPA_HW_2_CPU_RESPONSE_MHI_CHANGE_CHANNEL_STATE response.
- * Parameters are sent as 32b immediate parameters.
- * @state: The new channel state. In case state is not as requested this is
- *	error indication for the last command
- * @channelHandle: The channel identifier
- * @additonalParams: For stop: the number of pending bam descriptors currently
- *	queued
-*/
-union IpaHwMhiChangeChannelStateResponseData_t {
-	struct IpaHwMhiChangeChannelStateResponseParams_t {
-		u32 state:8;
-		u32 channelHandle:8;
-		u32 additonalParams:16;
-	} params;
-	u32 raw32b;
-};
-
-/**
- * Structure holding the parameters for
- *	IPA_HW_2_CPU_EVENT_MHI_CHANNEL_ERROR event.
- * Parameters are sent as 32b immediate parameters.
- * @errorType: Type of error - IPA_HW_CHANNEL_ERRORS
- * @channelHandle: The channel identifier as allocated by driver.
- *	value is within the range 0 to IPA_HW_MAX_CHANNEL_HANDLE
- * @reserved: reserved
- */
-union IpaHwMhiChannelErrorEventData_t {
-	struct IpaHwMhiChannelErrorEventParams_t {
-		u32 errorType:8;
-		u32 channelHandle:8;
-		u32 reserved:16;
-	} params;
-	u32 raw32b;
-};
-
-/**
- * Structure holding the parameters for
- *	IPA_HW_2_CPU_EVENT_MHI_CHANNEL_WAKE_UP_REQUEST event.
- * Parameters are sent as 32b immediate parameters.
- * @channelHandle: The channel identifier as allocated by driver.
- *	value is within the range 0 to IPA_HW_MAX_CHANNEL_HANDLE
- * @reserved: reserved
- */
-union IpaHwMhiChannelWakeupEventData_t {
-	struct IpaHwMhiChannelWakeupEventParams_t {
-		u32 channelHandle:8;
-		u32 reserved:24;
-	} params;
-	u32 raw32b;
-};
-
-/**
- * Structure holding the MHI Common statistics
- * @numULDLSync: Number of times UL activity trigged due to DL activity
- * @numULTimerExpired: Number of times UL Accm Timer expired
- */
-struct IpaHwStatsMhiCmnInfoData_t {
-	u32 numULDLSync;
-	u32 numULTimerExpired;
-	u32 numChEvCtxWpRead;
-	u32 reserved;
-};
-
-/**
- * Structure holding the MHI Channel statistics
- * @doorbellInt: The number of doorbell int
- * @reProccesed: The number of ring elements processed
- * @bamFifoFull: Number of times Bam Fifo got full
- * @bamFifoEmpty: Number of times Bam Fifo got empty
- * @bamFifoUsageHigh: Number of times Bam fifo usage went above 75%
- * @bamFifoUsageLow: Number of times Bam fifo usage went below 25%
- * @bamInt: Number of BAM Interrupts
- * @ringFull: Number of times Transfer Ring got full
- * @ringEmpty: umber of times Transfer Ring got empty
- * @ringUsageHigh: Number of times Transfer Ring usage went above 75%
- * @ringUsageLow: Number of times Transfer Ring usage went below 25%
- * @delayedMsi: Number of times device triggered MSI to host after
- *	Interrupt Moderation Timer expiry
- * @immediateMsi: Number of times device triggered MSI to host immediately
- * @thresholdMsi: Number of times device triggered MSI due to max pending
- *	events threshold reached
- * @numSuspend: Number of times channel was suspended
- * @numResume: Number of times channel was suspended
- * @num_OOB: Number of times we indicated that we are OOB
- * @num_OOB_timer_expiry: Number of times we indicated that we are OOB
- *	after timer expiry
- * @num_OOB_moderation_timer_start: Number of times we started timer after
- *	sending OOB and hitting OOB again before we processed threshold
- *	number of packets
- * @num_db_mode_evt: Number of times we indicated that we are in Doorbell mode
- */
-struct IpaHwStatsMhiCnlInfoData_t {
-	u32 doorbellInt;
-	u32 reProccesed;
-	u32 bamFifoFull;
-	u32 bamFifoEmpty;
-	u32 bamFifoUsageHigh;
-	u32 bamFifoUsageLow;
-	u32 bamInt;
-	u32 ringFull;
-	u32 ringEmpty;
-	u32 ringUsageHigh;
-	u32 ringUsageLow;
-	u32 delayedMsi;
-	u32 immediateMsi;
-	u32 thresholdMsi;
-	u32 numSuspend;
-	u32 numResume;
-	u32 num_OOB;
-	u32 num_OOB_timer_expiry;
-	u32 num_OOB_moderation_timer_start;
-	u32 num_db_mode_evt;
-};
-
-/**
- * Structure holding the MHI statistics
- * @mhiCmnStats: Stats pertaining to MHI
- * @mhiCnlStats: Stats pertaining to each channel
- */
-struct IpaHwStatsMhiInfoData_t {
-	struct IpaHwStatsMhiCmnInfoData_t mhiCmnStats;
-	struct IpaHwStatsMhiCnlInfoData_t mhiCnlStats[
-						IPA_HW_MAX_NUMBER_OF_CHANNELS];
-};
-
-/**
- * Structure holding the MHI Common Config info
- * @isDlUlSyncEnabled: Flag to indicate if DL-UL synchronization is enabled
- * @UlAccmVal: Out Channel(UL) accumulation time in ms when DL UL Sync is
- *	enabled
- * @ulMsiEventThreshold: Threshold at which HW fires MSI to host for UL events
- * @dlMsiEventThreshold: Threshold at which HW fires MSI to host for DL events
- */
-struct IpaHwConfigMhiCmnInfoData_t {
-	u8 isDlUlSyncEnabled;
-	u8 UlAccmVal;
-	u8 ulMsiEventThreshold;
-	u8 dlMsiEventThreshold;
-};
-
-/**
- * Structure holding the parameters for MSI info data
- * @msiAddress_low: The MSI lower base addr (in device space) used for asserting
- *	the interrupt (MSI) associated with the event ring.
- * @msiAddress_hi: The MSI higher base addr (in device space) used for asserting
- *	the interrupt (MSI) associated with the event ring.
- * @msiMask: Mask indicating number of messages assigned by the host to device
- * @msiData: Data Pattern to use when generating the MSI
- */
-struct IpaHwConfigMhiMsiInfoData_t {
-	u32 msiAddress_low;
-	u32 msiAddress_hi;
-	u32 msiMask;
-	u32 msiData;
-};
-
-/**
- * Structure holding the MHI Channel Config info
- * @transferRingSize: The Transfer Ring size in terms of Ring Elements
- * @transferRingIndex: The Transfer Ring channel number as defined by host
- * @eventRingIndex: The Event Ring Index associated with this Transfer Ring
- * @bamPipeIndex: The BAM Pipe associated with this channel
- * @isOutChannel: Indication for the direction of channel
- * @reserved_0: Reserved byte for maintaining 4byte alignment
- * @reserved_1: Reserved byte for maintaining 4byte alignment
- */
-struct IpaHwConfigMhiCnlInfoData_t {
-	u16 transferRingSize;
-	u8  transferRingIndex;
-	u8  eventRingIndex;
-	u8  bamPipeIndex;
-	u8  isOutChannel;
-	u8  reserved_0;
-	u8  reserved_1;
-};
-
-/**
- * Structure holding the MHI Event Config info
- * @msiVec: msi vector to invoke MSI interrupt
- * @intmodtValue: Interrupt moderation timer (in milliseconds)
- * @eventRingSize: The Event Ring size in terms of Ring Elements
- * @eventRingIndex: The Event Ring number as defined by host
- * @reserved_0: Reserved byte for maintaining 4byte alignment
- * @reserved_1: Reserved byte for maintaining 4byte alignment
- * @reserved_2: Reserved byte for maintaining 4byte alignment
- */
-struct IpaHwConfigMhiEventInfoData_t {
-	u32 msiVec;
-	u16 intmodtValue;
-	u16 eventRingSize;
-	u8  eventRingIndex;
-	u8  reserved_0;
-	u8  reserved_1;
-	u8  reserved_2;
-};
-
-/**
- * Structure holding the MHI Config info
- * @mhiCmnCfg: Common Config pertaining to MHI
- * @mhiMsiCfg: Config pertaining to MSI config
- * @mhiCnlCfg: Config pertaining to each channel
- * @mhiEvtCfg: Config pertaining to each event Ring
- */
-struct IpaHwConfigMhiInfoData_t {
-	struct IpaHwConfigMhiCmnInfoData_t mhiCmnCfg;
-	struct IpaHwConfigMhiMsiInfoData_t mhiMsiCfg;
-	struct IpaHwConfigMhiCnlInfoData_t mhiCnlCfg[
-						IPA_HW_MAX_NUMBER_OF_CHANNELS];
-	struct IpaHwConfigMhiEventInfoData_t mhiEvtCfg[
-					IPA_HW_MAX_NUMBER_OF_EVENTRINGS];
-};
-
-
-struct ipa_uc_mhi_ctx {
-	u8 expected_responseOp;
-	u32 expected_responseParams;
-	void (*ready_cb)(void);
-	void (*wakeup_request_cb)(void);
-	u32 mhi_uc_stats_ofst;
-	struct IpaHwStatsMhiInfoData_t *mhi_uc_stats_mmio;
-};
-
-#define PRINT_COMMON_STATS(x) \
-	(nBytes += scnprintf(&dbg_buff[nBytes], size - nBytes, \
-	#x "=0x%x\n", ipa_uc_mhi_ctx->mhi_uc_stats_mmio->mhiCmnStats.x))
-
-#define PRINT_CHANNEL_STATS(ch, x) \
-	(nBytes += scnprintf(&dbg_buff[nBytes], size - nBytes, \
-	#x "=0x%x\n", ipa_uc_mhi_ctx->mhi_uc_stats_mmio->mhiCnlStats[ch].x))
-
-struct ipa_uc_mhi_ctx *ipa_uc_mhi_ctx;
-
-static int ipa_uc_mhi_response_hdlr(struct IpaHwSharedMemCommonMapping_t
-	*uc_sram_mmio, u32 *uc_status)
-{
-	IPADBG("responseOp=%d\n", uc_sram_mmio->responseOp);
-	if (uc_sram_mmio->responseOp == ipa_uc_mhi_ctx->expected_responseOp &&
-	    uc_sram_mmio->responseParams ==
-	    ipa_uc_mhi_ctx->expected_responseParams) {
-		*uc_status = 0;
-		return 0;
-	}
-	return -EINVAL;
-}
-
-static void ipa_uc_mhi_event_hdlr(struct IpaHwSharedMemCommonMapping_t
-	*uc_sram_mmio)
-{
-	if (ipa_ctx->uc_ctx.uc_sram_mmio->eventOp ==
-	    IPA_HW_2_CPU_EVENT_MHI_CHANNEL_ERROR) {
-		union IpaHwMhiChannelErrorEventData_t evt;
-		IPAERR("Channel error\n");
-		evt.raw32b = uc_sram_mmio->eventParams;
-		IPAERR("errorType=%d channelHandle=%d reserved=%d\n",
-			evt.params.errorType, evt.params.channelHandle,
-			evt.params.reserved);
-	} else if (ipa_ctx->uc_ctx.uc_sram_mmio->eventOp ==
-		   IPA_HW_2_CPU_EVENT_MHI_CHANNEL_WAKE_UP_REQUEST) {
-		union IpaHwMhiChannelWakeupEventData_t evt;
-		IPADBG("WakeUp channel request\n");
-		evt.raw32b = uc_sram_mmio->eventParams;
-		IPADBG("channelHandle=%d reserved=%d\n",
-			evt.params.channelHandle, evt.params.reserved);
-		ipa_uc_mhi_ctx->wakeup_request_cb();
-	}
-}
-
-static void ipa_uc_mhi_event_log_info_hdlr(
-	struct IpaHwEventLogInfoData_t *uc_event_top_mmio)
-
-{
-	if ((uc_event_top_mmio->featureMask & (1 << IPA_HW_FEATURE_MHI)) == 0) {
-		IPAERR("MHI feature missing 0x%x\n",
-			uc_event_top_mmio->featureMask);
-		return;
-	}
-
-	if (uc_event_top_mmio->statsInfo.featureInfo[IPA_HW_FEATURE_MHI].
-		params.size != sizeof(struct IpaHwStatsMhiInfoData_t)) {
-		IPAERR("mhi stats sz invalid exp=%zu is=%u\n",
-			sizeof(struct IpaHwStatsMhiInfoData_t),
-			uc_event_top_mmio->statsInfo.
-			featureInfo[IPA_HW_FEATURE_MHI].params.size);
-		return;
-	}
-
-	ipa_uc_mhi_ctx->mhi_uc_stats_ofst = uc_event_top_mmio->
-		statsInfo.baseAddrOffset + uc_event_top_mmio->statsInfo.
-		featureInfo[IPA_HW_FEATURE_MHI].params.offset;
-	IPAERR("MHI stats ofst=0x%x\n", ipa_uc_mhi_ctx->mhi_uc_stats_ofst);
-	if (ipa_uc_mhi_ctx->mhi_uc_stats_ofst +
-		sizeof(struct IpaHwStatsMhiInfoData_t) >=
-		ipa_ctx->ctrl->ipa_reg_base_ofst +
-		IPA_SRAM_DIRECT_ACCESS_N_OFST_v2_0(0) +
-		ipa_ctx->smem_sz) {
-		IPAERR("uc_mhi_stats 0x%x outside SRAM\n",
-			ipa_uc_mhi_ctx->mhi_uc_stats_ofst);
-		return;
-	}
-
-	ipa_uc_mhi_ctx->mhi_uc_stats_mmio =
-		ioremap(ipa_ctx->ipa_wrapper_base +
-		ipa_uc_mhi_ctx->mhi_uc_stats_ofst,
-		sizeof(struct IpaHwStatsMhiInfoData_t));
-	if (!ipa_uc_mhi_ctx->mhi_uc_stats_mmio) {
-		IPAERR("fail to ioremap uc mhi stats\n");
-		return;
-	}
-
-	return;
-}
-
-int ipa_uc_mhi_init(void (*ready_cb)(void), void (*wakeup_request_cb)(void))
-{
-	struct ipa_uc_hdlrs hdlrs;
-
-	if (ipa_uc_mhi_ctx) {
-		IPAERR("Already initialized\n");
-		return -EFAULT;
-	}
-
-	ipa_uc_mhi_ctx = kzalloc(sizeof(*ipa_uc_mhi_ctx), GFP_KERNEL);
-	if (!ipa_uc_mhi_ctx) {
-		IPAERR("no mem\n");
-		return -ENOMEM;
-	}
-
-	ipa_uc_mhi_ctx->ready_cb = ready_cb;
-	ipa_uc_mhi_ctx->wakeup_request_cb = wakeup_request_cb;
-
-	memset(&hdlrs, 0, sizeof(hdlrs));
-	hdlrs.ipa_uc_loaded_hdlr = ipa_uc_mhi_ctx->ready_cb;
-	hdlrs.ipa_uc_response_hdlr = ipa_uc_mhi_response_hdlr;
-	hdlrs.ipa_uc_event_hdlr = ipa_uc_mhi_event_hdlr;
-	hdlrs.ipa_uc_event_log_info_hdlr = ipa_uc_mhi_event_log_info_hdlr;
-	ipa_uc_register_handlers(IPA_HW_FEATURE_MHI, &hdlrs);
-
-	IPADBG("Done\n");
-	return 0;
-}
-
-int ipa_uc_mhi_init_engine(struct ipa_mhi_msi_info *msi, u32 mmio_addr,
-	u32 host_ctrl_addr, u32 host_data_addr, u32 first_ch_idx,
-	u32 first_evt_idx)
-{
-	int res;
-	struct ipa_mem_buffer mem;
-	struct IpaHwMhiInitCmdData_t *init_cmd_data;
-	struct IpaHwMhiMsiCmdData_t *msi_cmd;
-
-	if (!ipa_uc_mhi_ctx) {
-		IPAERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	ipa_inc_client_enable_clks();
-
-	res = ipa_uc_update_hw_flags(0);
-	if (res) {
-		IPAERR("ipa_uc_update_hw_flags failed %d\n", res);
-		goto disable_clks;
-	}
-
-	mem.size = sizeof(*init_cmd_data);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-		GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		res = -ENOMEM;
-		goto disable_clks;
-	}
-	memset(mem.base, 0, mem.size);
-	init_cmd_data = (struct IpaHwMhiInitCmdData_t *)mem.base;
-	init_cmd_data->msiAddress = msi->addr_low;
-	init_cmd_data->mmioBaseAddress = mmio_addr;
-	init_cmd_data->deviceMhiCtrlBaseAddress = host_ctrl_addr;
-	init_cmd_data->deviceMhiDataBaseAddress = host_data_addr;
-	init_cmd_data->firstChannelIndex = first_ch_idx;
-	init_cmd_data->firstEventRingIndex = first_evt_idx;
-	res = ipa_uc_send_cmd((u32)mem.phys_base, IPA_CPU_2_HW_CMD_MHI_INIT, 0,
-		false, HZ);
-	if (res) {
-		IPAERR("ipa_uc_send_cmd failed %d\n", res);
-		dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base,
-			mem.phys_base);
-		goto disable_clks;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-
-	mem.size = sizeof(*msi_cmd);
-	mem.base = dma_alloc_coherent(ipa_ctx->pdev, mem.size, &mem.phys_base,
-		GFP_KERNEL);
-	if (!mem.base) {
-		IPAERR("fail to alloc DMA buff of size %d\n", mem.size);
-		res = -ENOMEM;
-		goto disable_clks;
-	}
-
-	msi_cmd = (struct IpaHwMhiMsiCmdData_t *)mem.base;
-	msi_cmd->msiAddress_hi = msi->addr_hi;
-	msi_cmd->msiAddress_low = msi->addr_low;
-	msi_cmd->msiData = msi->data;
-	msi_cmd->msiMask = msi->mask;
-	res = ipa_uc_send_cmd((u32)mem.phys_base,
-		IPA_CPU_2_HW_CMD_MHI_UPDATE_MSI, 0, false, HZ);
-	if (res) {
-		IPAERR("ipa_uc_send_cmd failed %d\n", res);
-		dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base,
-			mem.phys_base);
-		goto disable_clks;
-	}
-
-	dma_free_coherent(ipa_ctx->pdev, mem.size, mem.base, mem.phys_base);
-
-	res = 0;
-
-disable_clks:
-	ipa_dec_client_disable_clks();
-	return res;
-
-}
-
-int ipa_uc_mhi_init_channel(int ipa_ep_idx, int channelHandle,
-	int contexArrayIndex, int channelDirection)
-
-{
-	int res;
-	union IpaHwMhiInitChannelCmdData_t init_cmd;
-	union IpaHwMhiChangeChannelStateResponseData_t uc_rsp;
-
-	if (!ipa_uc_mhi_ctx) {
-		IPAERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	if (ipa_ep_idx < 0  || ipa_ep_idx >= ipa_ctx->ipa_num_pipes) {
-		IPAERR("Invalid ipa_ep_idx.\n");
-		return -EINVAL;
-	}
-
-	ipa_inc_client_enable_clks();
-
-	memset(&uc_rsp, 0, sizeof(uc_rsp));
-	uc_rsp.params.state = IPA_HW_MHI_CHANNEL_STATE_RUN;
-	uc_rsp.params.channelHandle = channelHandle;
-	ipa_uc_mhi_ctx->expected_responseOp =
-		IPA_HW_2_CPU_RESPONSE_MHI_CHANGE_CHANNEL_STATE;
-	ipa_uc_mhi_ctx->expected_responseParams = uc_rsp.raw32b;
-
-	memset(&init_cmd, 0, sizeof(init_cmd));
-	init_cmd.params.channelHandle = channelHandle;
-	init_cmd.params.contexArrayIndex = contexArrayIndex;
-	init_cmd.params.bamPipeId = ipa_ep_idx;
-	init_cmd.params.channelDirection = channelDirection;
-
-	res = ipa_uc_send_cmd(init_cmd.raw32b,
-		IPA_CPU_2_HW_CMD_MHI_INIT_CHANNEL, 0, false, HZ);
-	if (res) {
-		IPAERR("ipa_uc_send_cmd failed %d\n", res);
-		goto disable_clks;
-	}
-
-	res = 0;
-
-disable_clks:
-	ipa_dec_client_disable_clks();
-	return res;
-}
-
-
-int ipa_uc_mhi_reset_channel(int channelHandle)
-{
-	union IpaHwMhiChangeChannelStateCmdData_t cmd;
-	union IpaHwMhiChangeChannelStateResponseData_t uc_rsp;
-	int res;
-
-	if (!ipa_uc_mhi_ctx) {
-		IPAERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	ipa_inc_client_enable_clks();
-
-	memset(&uc_rsp, 0, sizeof(uc_rsp));
-	uc_rsp.params.state = IPA_HW_MHI_CHANNEL_STATE_DISABLE;
-	uc_rsp.params.channelHandle = channelHandle;
-	ipa_uc_mhi_ctx->expected_responseOp =
-		IPA_HW_2_CPU_RESPONSE_MHI_CHANGE_CHANNEL_STATE;
-	ipa_uc_mhi_ctx->expected_responseParams = uc_rsp.raw32b;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.params.requestedState = IPA_HW_MHI_CHANNEL_STATE_DISABLE;
-	cmd.params.channelHandle = channelHandle;
-	res = ipa_uc_send_cmd(cmd.raw32b,
-		IPA_CPU_2_HW_CMD_MHI_CHANGE_CHANNEL_STATE, 0, false, HZ);
-	if (res) {
-		IPAERR("ipa_uc_send_cmd failed %d\n", res);
-		goto disable_clks;
-	}
-
-	res = 0;
-
-disable_clks:
-	ipa_dec_client_disable_clks();
-	return res;
-}
-
-int ipa_uc_mhi_suspend_channel(int channelHandle)
-{
-	union IpaHwMhiChangeChannelStateCmdData_t cmd;
-	union IpaHwMhiChangeChannelStateResponseData_t uc_rsp;
-	int res;
-
-	if (!ipa_uc_mhi_ctx) {
-		IPAERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	ipa_inc_client_enable_clks();
-
-	memset(&uc_rsp, 0, sizeof(uc_rsp));
-	uc_rsp.params.state = IPA_HW_MHI_CHANNEL_STATE_SUSPEND;
-	uc_rsp.params.channelHandle = channelHandle;
-	ipa_uc_mhi_ctx->expected_responseOp =
-		IPA_HW_2_CPU_RESPONSE_MHI_CHANGE_CHANNEL_STATE;
-	ipa_uc_mhi_ctx->expected_responseParams = uc_rsp.raw32b;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.params.requestedState = IPA_HW_MHI_CHANNEL_STATE_SUSPEND;
-	cmd.params.channelHandle = channelHandle;
-	res = ipa_uc_send_cmd(cmd.raw32b,
-		IPA_CPU_2_HW_CMD_MHI_CHANGE_CHANNEL_STATE, 0, false, HZ);
-	if (res) {
-		IPAERR("ipa_uc_send_cmd failed %d\n", res);
-		goto disable_clks;
-	}
-
-	res = 0;
-
-disable_clks:
-	ipa_dec_client_disable_clks();
-	return res;
-}
-
-int ipa_uc_mhi_resume_channel(int channelHandle, bool LPTransitionRejected)
-{
-	union IpaHwMhiChangeChannelStateCmdData_t cmd;
-	union IpaHwMhiChangeChannelStateResponseData_t uc_rsp;
-	int res;
-
-	if (!ipa_uc_mhi_ctx) {
-		IPAERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	ipa_inc_client_enable_clks();
-
-	memset(&uc_rsp, 0, sizeof(uc_rsp));
-	uc_rsp.params.state = IPA_HW_MHI_CHANNEL_STATE_RUN;
-	uc_rsp.params.channelHandle = channelHandle;
-	ipa_uc_mhi_ctx->expected_responseOp =
-		IPA_HW_2_CPU_RESPONSE_MHI_CHANGE_CHANNEL_STATE;
-	ipa_uc_mhi_ctx->expected_responseParams = uc_rsp.raw32b;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.params.requestedState = IPA_HW_MHI_CHANNEL_STATE_RUN;
-	cmd.params.channelHandle = channelHandle;
-	cmd.params.LPTransitionRejected = LPTransitionRejected;
-	res = ipa_uc_send_cmd(cmd.raw32b,
-		IPA_CPU_2_HW_CMD_MHI_CHANGE_CHANNEL_STATE, 0, false, HZ);
-	if (res) {
-		IPAERR("ipa_uc_send_cmd failed %d\n", res);
-		goto disable_clks;
-	}
-
-	res = 0;
-
-disable_clks:
-	ipa_dec_client_disable_clks();
-	return res;
-}
-
-int ipa_uc_mhi_stop_event_update_channel(int channelHandle)
-{
-	union IpaHwMhiStopEventUpdateData_t cmd;
-	int res;
-
-	if (!ipa_uc_mhi_ctx) {
-		IPAERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	ipa_inc_client_enable_clks();
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.params.channelHandle = channelHandle;
-
-	ipa_uc_mhi_ctx->expected_responseOp =
-		IPA_CPU_2_HW_CMD_MHI_STOP_EVENT_UPDATE;
-	ipa_uc_mhi_ctx->expected_responseParams = cmd.raw32b;
-
-	res = ipa_uc_send_cmd(cmd.raw32b,
-		IPA_CPU_2_HW_CMD_MHI_STOP_EVENT_UPDATE, 0, false, HZ);
-	if (res) {
-		IPAERR("ipa_uc_send_cmd failed %d\n", res);
-		goto disable_clks;
-	}
-
-	res = 0;
-disable_clks:
-	ipa_dec_client_disable_clks();
-	return res;
-}
-
-int ipa_uc_mhi_send_dl_ul_sync_info(union IpaHwMhiDlUlSyncCmdData_t cmd)
-{
-	int res;
-
-	if (!ipa_uc_mhi_ctx) {
-		IPAERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	IPADBG("isDlUlSyncEnabled=0x%x UlAccmVal=0x%x\n",
-		cmd.params.isDlUlSyncEnabled, cmd.params.UlAccmVal);
-	IPADBG("ulMsiEventThreshold=0x%x dlMsiEventThreshold=0x%x\n",
-		cmd.params.ulMsiEventThreshold, cmd.params.dlMsiEventThreshold);
-
-	ipa_inc_client_enable_clks();
-
-	res = ipa_uc_send_cmd(cmd.raw32b,
-		IPA_CPU_2_HW_CMD_MHI_DL_UL_SYNC_INFO, 0, false, HZ);
-	if (res) {
-		IPAERR("ipa_uc_send_cmd failed %d\n", res);
-		goto disable_clks;
-	}
-
-	res = 0;
-disable_clks:
-	ipa_dec_client_disable_clks();
-	return res;
-}
-
-int ipa_uc_mhi_print_stats(char *dbg_buff, int size)
-{
-	int nBytes = 0;
-	int i;
-
-	if (!ipa_uc_mhi_ctx->mhi_uc_stats_mmio) {
-		IPAERR("MHI uc stats is not valid\n");
-		return 0;
-	}
-
-	nBytes += scnprintf(&dbg_buff[nBytes], size - nBytes,
-		"Common Stats:\n");
-	PRINT_COMMON_STATS(numULDLSync);
-	PRINT_COMMON_STATS(numULTimerExpired);
-	PRINT_COMMON_STATS(numChEvCtxWpRead);
-
-	for (i = 0; i < IPA_HW_MAX_NUMBER_OF_CHANNELS; i++) {
-		nBytes += scnprintf(&dbg_buff[nBytes], size - nBytes,
-			"Channel %d Stats:\n", i);
-		PRINT_CHANNEL_STATS(i, doorbellInt);
-		PRINT_CHANNEL_STATS(i, reProccesed);
-		PRINT_CHANNEL_STATS(i, bamFifoFull);
-		PRINT_CHANNEL_STATS(i, bamFifoEmpty);
-		PRINT_CHANNEL_STATS(i, bamFifoUsageHigh);
-		PRINT_CHANNEL_STATS(i, bamFifoUsageLow);
-		PRINT_CHANNEL_STATS(i, bamInt);
-		PRINT_CHANNEL_STATS(i, ringFull);
-		PRINT_CHANNEL_STATS(i, ringEmpty);
-		PRINT_CHANNEL_STATS(i, ringUsageHigh);
-		PRINT_CHANNEL_STATS(i, ringUsageLow);
-		PRINT_CHANNEL_STATS(i, delayedMsi);
-		PRINT_CHANNEL_STATS(i, immediateMsi);
-		PRINT_CHANNEL_STATS(i, thresholdMsi);
-		PRINT_CHANNEL_STATS(i, numSuspend);
-		PRINT_CHANNEL_STATS(i, numResume);
-		PRINT_CHANNEL_STATS(i, num_OOB);
-		PRINT_CHANNEL_STATS(i, num_OOB_timer_expiry);
-		PRINT_CHANNEL_STATS(i, num_OOB_moderation_timer_start);
-		PRINT_CHANNEL_STATS(i, num_db_mode_evt);
-	}
-
-	return nBytes;
-}
diff --git a/drivers/platform/msm/ipa/ipa_uc_wdi.c b/drivers/platform/msm/ipa/ipa_uc_wdi.c
deleted file mode 100644
index 71b48bb4..00000000
--- a/drivers/platform/msm/ipa/ipa_uc_wdi.c
+++ /dev/null
@@ -1,1617 +0,0 @@
-/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-#include "ipa_i.h"
-#include <linux/dmapool.h>
-#include <linux/delay.h>
-
-#define IPA_HOLB_TMR_DIS 0x0
-
-#define IPA_HW_INTERFACE_WDI_VERSION 0x0001
-#define IPA_HW_WDI_RX_MBOX_START_INDEX 48
-#define IPA_HW_WDI_TX_MBOX_START_INDEX 50
-#define IPA_WDI_RING_ALIGNMENT 8
-
-#define IPA_WDI_CONNECTED BIT(0)
-#define IPA_WDI_ENABLED BIT(1)
-#define IPA_WDI_RESUMED BIT(2)
-#define IPA_UC_POLL_SLEEP_USEC 100
-
-#define IPA_WDI_RX_RING_RES	0
-#define IPA_WDI_RX_RING_RP_RES	1
-#define IPA_WDI_TX_RING_RES	2
-#define IPA_WDI_CE_RING_RES	3
-#define IPA_WDI_CE_DB_RES	4
-#define IPA_WDI_MAX_RES		5
-
-struct ipa_wdi_res {
-	struct ipa_wdi_buffer_info *res;
-	unsigned int nents;
-	bool valid;
-};
-
-static struct ipa_wdi_res wdi_res[IPA_WDI_MAX_RES];
-
-static void ipa_uc_wdi_loaded_handler(void);
-
-/**
- * enum ipa_hw_2_cpu_wdi_events - Values that represent HW event to be sent to CPU.
- * @IPA_HW_2_CPU_EVENT_WDI_ERROR : Event to specify that HW detected an error
- * in WDI
- */
-enum ipa_hw_2_cpu_wdi_events {
-	IPA_HW_2_CPU_EVENT_WDI_ERROR =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 0),
-};
-
-/**
- * enum ipa_hw_wdi_channel_states - Values that represent WDI channel state
- * machine.
- * @IPA_HW_WDI_CHANNEL_STATE_INITED_DISABLED : Channel is initialized but
- * disabled
- * @IPA_HW_WDI_CHANNEL_STATE_ENABLED_SUSPEND : Channel is enabled but in
- * suspended state
- * @IPA_HW_WDI_CHANNEL_STATE_RUNNING : Channel is running. Entered after
- * SET_UP_COMMAND is processed successfully
- * @IPA_HW_WDI_CHANNEL_STATE_ERROR : Channel is in error state
- * @IPA_HW_WDI_CHANNEL_STATE_INVALID : Invalid state. Shall not be in use in
- * operational scenario
- *
- * These states apply to both Tx and Rx paths. These do not reflect the
- * sub-state the state machine may be in.
- */
-enum ipa_hw_wdi_channel_states {
-	IPA_HW_WDI_CHANNEL_STATE_INITED_DISABLED = 1,
-	IPA_HW_WDI_CHANNEL_STATE_ENABLED_SUSPEND = 2,
-	IPA_HW_WDI_CHANNEL_STATE_RUNNING         = 3,
-	IPA_HW_WDI_CHANNEL_STATE_ERROR           = 4,
-	IPA_HW_WDI_CHANNEL_STATE_INVALID         = 0xFF
-};
-
-/**
- * enum ipa_cpu_2_hw_commands -  Values that represent the WDI commands from CPU
- * @IPA_CPU_2_HW_CMD_WDI_TX_SET_UP : Command to set up WDI Tx Path
- * @IPA_CPU_2_HW_CMD_WDI_RX_SET_UP : Command to set up WDI Rx Path
- * @IPA_CPU_2_HW_CMD_WDI_RX_EXT_CFG : Provide extended config info for Rx path
- * @IPA_CPU_2_HW_CMD_WDI_CH_ENABLE : Command to enable a channel
- * @IPA_CPU_2_HW_CMD_WDI_CH_DISABLE : Command to disable a channel
- * @IPA_CPU_2_HW_CMD_WDI_CH_SUSPEND : Command to suspend a channel
- * @IPA_CPU_2_HW_CMD_WDI_CH_RESUME : Command to resume a channel
- * @IPA_CPU_2_HW_CMD_WDI_TEAR_DOWN : Command to tear down WDI Tx/ Rx Path
- */
-enum ipa_cpu_2_hw_wdi_commands {
-	IPA_CPU_2_HW_CMD_WDI_TX_SET_UP  =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 0),
-	IPA_CPU_2_HW_CMD_WDI_RX_SET_UP  =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 1),
-	IPA_CPU_2_HW_CMD_WDI_RX_EXT_CFG =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 2),
-	IPA_CPU_2_HW_CMD_WDI_CH_ENABLE  =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 3),
-	IPA_CPU_2_HW_CMD_WDI_CH_DISABLE =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 4),
-	IPA_CPU_2_HW_CMD_WDI_CH_SUSPEND =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 5),
-	IPA_CPU_2_HW_CMD_WDI_CH_RESUME  =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 6),
-	IPA_CPU_2_HW_CMD_WDI_TEAR_DOWN  =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 7),
-};
-
-/**
- * enum ipa_hw_2_cpu_cmd_resp_status -  Values that represent WDI related
- * command response status to be sent to CPU.
- */
-enum ipa_hw_2_cpu_cmd_resp_status {
-	IPA_HW_2_CPU_WDI_CMD_STATUS_SUCCESS            =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 0),
-	IPA_HW_2_CPU_MAX_WDI_TX_CHANNELS               =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 1),
-	IPA_HW_2_CPU_WDI_CE_RING_OVERRUN_POSSIBILITY   =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 2),
-	IPA_HW_2_CPU_WDI_CE_RING_SET_UP_FAILURE        =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 3),
-	IPA_HW_2_CPU_WDI_CE_RING_PARAMS_UNALIGNED      =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 4),
-	IPA_HW_2_CPU_WDI_COMP_RING_OVERRUN_POSSIBILITY =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 5),
-	IPA_HW_2_CPU_WDI_COMP_RING_SET_UP_FAILURE      =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 6),
-	IPA_HW_2_CPU_WDI_COMP_RING_PARAMS_UNALIGNED    =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 7),
-	IPA_HW_2_CPU_WDI_UNKNOWN_TX_CHANNEL            =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 8),
-	IPA_HW_2_CPU_WDI_TX_INVALID_FSM_TRANSITION     =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 9),
-	IPA_HW_2_CPU_WDI_TX_FSM_TRANSITION_ERROR       =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 10),
-	IPA_HW_2_CPU_MAX_WDI_RX_CHANNELS               =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 11),
-	IPA_HW_2_CPU_WDI_RX_RING_PARAMS_UNALIGNED      =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 12),
-	IPA_HW_2_CPU_WDI_RX_RING_SET_UP_FAILURE        =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 13),
-	IPA_HW_2_CPU_WDI_UNKNOWN_RX_CHANNEL            =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 14),
-	IPA_HW_2_CPU_WDI_RX_INVALID_FSM_TRANSITION     =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 15),
-	IPA_HW_2_CPU_WDI_RX_FSM_TRANSITION_ERROR       =
-		FEATURE_ENUM_VAL(IPA_HW_FEATURE_WDI, 16),
-};
-
-/**
- * enum ipa_hw_wdi_errors - WDI specific error types.
- * @IPA_HW_WDI_ERROR_NONE : No error persists
- * @IPA_HW_WDI_CHANNEL_ERROR : Error is specific to channel
- */
-enum ipa_hw_wdi_errors {
-	IPA_HW_WDI_ERROR_NONE    = 0,
-	IPA_HW_WDI_CHANNEL_ERROR = 1
-};
-
-/**
- * enum ipa_hw_wdi_ch_errors = List of WDI Channel error types. This is present
- * in the event param.
- * @IPA_HW_WDI_CH_ERR_NONE : No error persists
- * @IPA_HW_WDI_TX_COMP_RING_WP_UPDATE_FAIL : Write pointer update failed in Tx
- * Completion ring
- * @IPA_HW_WDI_TX_FSM_ERROR : Error in the state machine transition
- * @IPA_HW_WDI_TX_COMP_RE_FETCH_FAIL : Error while calculating num RE to bring
- * @IPA_HW_WDI_CH_ERR_RESERVED : Reserved - Not available for CPU to use
-*/
-enum ipa_hw_wdi_ch_errors {
-	IPA_HW_WDI_CH_ERR_NONE                 = 0,
-	IPA_HW_WDI_TX_COMP_RING_WP_UPDATE_FAIL = 1,
-	IPA_HW_WDI_TX_FSM_ERROR                = 2,
-	IPA_HW_WDI_TX_COMP_RE_FETCH_FAIL       = 3,
-	IPA_HW_WDI_CH_ERR_RESERVED             = 0xFF
-};
-
-/**
- * struct IpaHwSharedMemWdiMapping_t  - Structure referring to the common and
- * WDI section of 128B shared memory located in offset zero of SW Partition in
- * IPA SRAM.
- *
- * The shared memory is used for communication between IPA HW and CPU.
- */
-struct IpaHwSharedMemWdiMapping_t {
-	struct IpaHwSharedMemCommonMapping_t common;
-	u32 reserved_2B_28;
-	u32 reserved_2F_2C;
-	u32 reserved_33_30;
-	u32 reserved_37_34;
-	u32 reserved_3B_38;
-	u32 reserved_3F_3C;
-	u16 interfaceVersionWdi;
-	u16 reserved_43_42;
-	u8  wdi_tx_ch_0_state;
-	u8  wdi_rx_ch_0_state;
-	u16 reserved_47_46;
-} __packed;
-
-/**
- * struct IpaHwWdiTxSetUpCmdData_t - Structure holding the parameters for
- * IPA_CPU_2_HW_CMD_WDI_TX_SET_UP command.
- * @comp_ring_base_pa : This is the physical address of the base of the Tx
- * completion ring
- * @comp_ring_size : This is the size of the Tx completion ring
- * @reserved_comp_ring : Reserved field for expansion of Completion ring params
- * @ce_ring_base_pa : This is the physical address of the base of the Copy
- * Engine Source Ring
- * @ce_ring_size : Copy Engine Ring size
- * @reserved_ce_ring : Reserved field for expansion of CE ring params
- * @ce_ring_doorbell_pa : This is the physical address of the doorbell that the
- * IPA uC has to write into to trigger the copy engine
- * @num_tx_buffers : Number of pkt buffers allocated. The size of the CE ring
- * and the Tx completion ring has to be atleast ( num_tx_buffers + 1)
- * @ipa_pipe_number : This is the IPA pipe number that has to be used for the
- * Tx path
- * @reserved : Reserved field
- *
- * Parameters are sent as pointer thus should be reside in address accessible
- * to HW
- */
-struct IpaHwWdiTxSetUpCmdData_t {
-	u32 comp_ring_base_pa;
-	u16 comp_ring_size;
-	u16 reserved_comp_ring;
-	u32 ce_ring_base_pa;
-	u16 ce_ring_size;
-	u16 reserved_ce_ring;
-	u32 ce_ring_doorbell_pa;
-	u16 num_tx_buffers;
-	u8  ipa_pipe_number;
-	u8  reserved;
-} __packed;
-
-/**
- * struct IpaHwWdiRxSetUpCmdData_t -  Structure holding the parameters for
- * IPA_CPU_2_HW_CMD_WDI_RX_SET_UP command.
- * @rx_ring_base_pa : This is the physical address of the base of the Rx ring
- * (containing Rx buffers)
- * @rx_ring_size : This is the size of the Rx ring
- * @rx_ring_rp_pa : This is the physical address of the location through which
- * IPA uc is expected to communicate about the Read pointer into the Rx Ring
- * @ipa_pipe_number : This is the IPA pipe number that has to be used for the
- * Rx path
- *
- * Parameters are sent as pointer thus should be reside in address accessible
- * to HW
-*/
-struct IpaHwWdiRxSetUpCmdData_t {
-	u32 rx_ring_base_pa;
-	u32 rx_ring_size;
-	u32 rx_ring_rp_pa;
-	u8  ipa_pipe_number;
-} __packed;
-
-/**
- * union IpaHwWdiRxExtCfgCmdData_t - Structure holding the parameters for
- * IPA_CPU_2_HW_CMD_WDI_RX_EXT_CFG command.
- * @ipa_pipe_number : The IPA pipe number for which this config is passed
- * @qmap_id : QMAP ID to be set in the metadata register
- * @reserved : Reserved
- *
- * The parameters are passed as immediate params in the shared memory
-*/
-union IpaHwWdiRxExtCfgCmdData_t {
-	struct IpaHwWdiRxExtCfgCmdParams_t {
-		u32 ipa_pipe_number:8;
-		u32 qmap_id:8;
-		u32 reserved:16;
-	} __packed params;
-	u32 raw32b;
-} __packed;
-
-/**
- * union IpaHwWdiCommonChCmdData_t -  Structure holding the parameters for
- * IPA_CPU_2_HW_CMD_WDI_TEAR_DOWN,
- * IPA_CPU_2_HW_CMD_WDI_CH_ENABLE,
- * IPA_CPU_2_HW_CMD_WDI_CH_DISABLE,
- * IPA_CPU_2_HW_CMD_WDI_CH_SUSPEND,
- * IPA_CPU_2_HW_CMD_WDI_CH_RESUME command.
- * @ipa_pipe_number :  The IPA pipe number. This could be Tx or an Rx pipe
- * @reserved : Reserved
- *
- * The parameters are passed as immediate params in the shared memory
- */
-union IpaHwWdiCommonChCmdData_t {
-	struct IpaHwWdiCommonChCmdParams_t {
-		u32 ipa_pipe_number:8;
-		u32 reserved:24;
-	} __packed params;
-	u32 raw32b;
-} __packed;
-
-/**
- * union IpaHwWdiErrorEventData_t - parameters for IPA_HW_2_CPU_EVENT_WDI_ERROR
- * event.
- * @wdi_error_type : The IPA pipe number to be torn down. This could be Tx or
- * an Rx pipe
- * @reserved : Reserved
- * @ipa_pipe_number : IPA pipe number on which error has happened. Applicable
- * only if error type indicates channel error
- * @wdi_ch_err_type : Information about the channel error (if available)
- *
- * The parameters are passed as immediate params in the shared memory
- */
-union IpaHwWdiErrorEventData_t {
-	struct IpaHwWdiErrorEventParams_t {
-		u32 wdi_error_type:8;
-		u32 reserved:8;
-		u32 ipa_pipe_number:8;
-		u32 wdi_ch_err_type:8;
-	} __packed params;
-	u32 raw32b;
-} __packed;
-
-static void ipa_uc_wdi_event_log_info_handler(
-struct IpaHwEventLogInfoData_t *uc_event_top_mmio)
-
-{
-	if ((uc_event_top_mmio->featureMask & (1 << IPA_HW_FEATURE_WDI)) == 0) {
-		IPAERR("WDI feature missing 0x%x\n",
-			uc_event_top_mmio->featureMask);
-		return;
-	}
-
-	if (uc_event_top_mmio->statsInfo.featureInfo[IPA_HW_FEATURE_WDI].
-		params.size != sizeof(struct IpaHwStatsWDIInfoData_t)) {
-			IPAERR("wdi stats sz invalid exp=%zu is=%u\n",
-				sizeof(struct IpaHwStatsWDIInfoData_t),
-				uc_event_top_mmio->statsInfo.
-				featureInfo[IPA_HW_FEATURE_WDI].params.size);
-			return;
-	}
-
-	ipa_ctx->uc_wdi_ctx.wdi_uc_stats_ofst = uc_event_top_mmio->
-		statsInfo.baseAddrOffset + uc_event_top_mmio->statsInfo.
-		featureInfo[IPA_HW_FEATURE_WDI].params.offset;
-	IPAERR("WDI stats ofst=0x%x\n", ipa_ctx->uc_wdi_ctx.wdi_uc_stats_ofst);
-	if (ipa_ctx->uc_wdi_ctx.wdi_uc_stats_ofst +
-		sizeof(struct IpaHwStatsWDIInfoData_t) >=
-		ipa_ctx->ctrl->ipa_reg_base_ofst +
-		IPA_SRAM_DIRECT_ACCESS_N_OFST_v2_0(0) +
-		ipa_ctx->smem_sz) {
-			IPAERR("uc_wdi_stats 0x%x outside SRAM\n",
-				ipa_ctx->uc_wdi_ctx.wdi_uc_stats_ofst);
-			return;
-	}
-
-	ipa_ctx->uc_wdi_ctx.wdi_uc_stats_mmio =
-		ioremap(ipa_ctx->ipa_wrapper_base +
-		ipa_ctx->uc_wdi_ctx.wdi_uc_stats_ofst,
-		sizeof(struct IpaHwStatsWDIInfoData_t));
-	if (!ipa_ctx->uc_wdi_ctx.wdi_uc_stats_mmio) {
-		IPAERR("fail to ioremap uc wdi stats\n");
-		return;
-	}
-
-	return;
-}
-
-static void ipa_uc_wdi_event_handler(struct IpaHwSharedMemCommonMapping_t
-				     *uc_sram_mmio)
-
-{
-	union IpaHwWdiErrorEventData_t wdi_evt;
-	struct IpaHwSharedMemWdiMapping_t *wdi_sram_mmio_ext;
-
-	if (uc_sram_mmio->eventOp ==
-		IPA_HW_2_CPU_EVENT_WDI_ERROR) {
-			wdi_evt.raw32b = uc_sram_mmio->eventParams;
-			IPADBG("uC WDI evt errType=%u pipe=%d cherrType=%u\n",
-				wdi_evt.params.wdi_error_type,
-				wdi_evt.params.ipa_pipe_number,
-				wdi_evt.params.wdi_ch_err_type);
-			wdi_sram_mmio_ext =
-				(struct IpaHwSharedMemWdiMapping_t *)
-				uc_sram_mmio;
-			IPADBG("tx_ch_state=%u rx_ch_state=%u\n",
-				wdi_sram_mmio_ext->wdi_tx_ch_0_state,
-				wdi_sram_mmio_ext->wdi_rx_ch_0_state);
-	}
-}
-
-/**
- * ipa_get_wdi_stats() - Query WDI statistics from uc
- * @stats:	[inout] stats blob from client populated by driver
- *
- * Returns:	0 on success, negative on failure
- *
- * @note Cannot be called from atomic context
- *
- */
-int ipa_get_wdi_stats(struct IpaHwStatsWDIInfoData_t *stats)
-{
-#define TX_STATS(y) stats->tx_ch_stats.y = \
-	ipa_ctx->uc_wdi_ctx.wdi_uc_stats_mmio->tx_ch_stats.y
-#define RX_STATS(y) stats->rx_ch_stats.y = \
-	ipa_ctx->uc_wdi_ctx.wdi_uc_stats_mmio->rx_ch_stats.y
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (!stats || !ipa_ctx->uc_wdi_ctx.wdi_uc_stats_mmio) {
-		IPAERR("bad parms stats=%p wdi_stats=%p\n",
-			stats,
-			ipa_ctx->uc_wdi_ctx.wdi_uc_stats_mmio);
-		return -EINVAL;
-	}
-
-	ipa_inc_client_enable_clks();
-
-	TX_STATS(num_pkts_processed);
-	TX_STATS(copy_engine_doorbell_value);
-	TX_STATS(num_db_fired);
-	TX_STATS(tx_comp_ring_stats.ringFull);
-	TX_STATS(tx_comp_ring_stats.ringEmpty);
-	TX_STATS(tx_comp_ring_stats.ringUsageHigh);
-	TX_STATS(tx_comp_ring_stats.ringUsageLow);
-	TX_STATS(tx_comp_ring_stats.RingUtilCount);
-	TX_STATS(bam_stats.bamFifoFull);
-	TX_STATS(bam_stats.bamFifoEmpty);
-	TX_STATS(bam_stats.bamFifoUsageHigh);
-	TX_STATS(bam_stats.bamFifoUsageLow);
-	TX_STATS(bam_stats.bamUtilCount);
-	TX_STATS(num_db);
-	TX_STATS(num_unexpected_db);
-	TX_STATS(num_bam_int_handled);
-	TX_STATS(num_bam_int_in_non_runnning_state);
-	TX_STATS(num_qmb_int_handled);
-	TX_STATS(num_bam_int_handled_while_wait_for_bam);
-
-	RX_STATS(max_outstanding_pkts);
-	RX_STATS(num_pkts_processed);
-	RX_STATS(rx_ring_rp_value);
-	RX_STATS(rx_ind_ring_stats.ringFull);
-	RX_STATS(rx_ind_ring_stats.ringEmpty);
-	RX_STATS(rx_ind_ring_stats.ringUsageHigh);
-	RX_STATS(rx_ind_ring_stats.ringUsageLow);
-	RX_STATS(rx_ind_ring_stats.RingUtilCount);
-	RX_STATS(bam_stats.bamFifoFull);
-	RX_STATS(bam_stats.bamFifoEmpty);
-	RX_STATS(bam_stats.bamFifoUsageHigh);
-	RX_STATS(bam_stats.bamFifoUsageLow);
-	RX_STATS(bam_stats.bamUtilCount);
-	RX_STATS(num_bam_int_handled);
-	RX_STATS(num_db);
-	RX_STATS(num_unexpected_db);
-	RX_STATS(num_pkts_in_dis_uninit_state);
-	RX_STATS(reserved1);
-	RX_STATS(reserved2);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_get_wdi_stats);
-
-int ipa_wdi_init(void)
-{
-	struct ipa_uc_hdlrs uc_wdi_cbs = { 0 };
-
-	uc_wdi_cbs.ipa_uc_event_hdlr = ipa_uc_wdi_event_handler;
-	uc_wdi_cbs.ipa_uc_event_log_info_hdlr =
-		ipa_uc_wdi_event_log_info_handler;
-	uc_wdi_cbs.ipa_uc_loaded_hdlr =
-		ipa_uc_wdi_loaded_handler;
-
-	ipa_uc_register_handlers(IPA_HW_FEATURE_WDI, &uc_wdi_cbs);
-
-	return 0;
-}
-
-static int ipa_create_uc_smmu_mapping_pa(phys_addr_t pa, size_t len,
-		bool device, unsigned long *iova)
-{
-	struct ipa_smmu_cb_ctx *cb = ipa_get_uc_smmu_ctx();
-	unsigned long va = roundup(cb->next_addr, PAGE_SIZE);
-	int prot = IOMMU_READ | IOMMU_WRITE;
-	size_t true_len = roundup(len + pa - rounddown(pa, PAGE_SIZE),
-			PAGE_SIZE);
-	int ret;
-
-	if (!cb->valid) {
-		IPAERR("No SMMU CB setup\n");
-		return -EINVAL;
-	}
-
-	ret = iommu_map(cb->mapping->domain, va, rounddown(pa, PAGE_SIZE),
-			true_len,
-			device ? (prot | IOMMU_DEVICE) : prot);
-	if (ret) {
-		IPAERR("iommu map failed for pa=%pa len=%zu\n", &pa, true_len);
-		return -EINVAL;
-	}
-
-	ipa_ctx->wdi_map_cnt++;
-	cb->next_addr = va + true_len;
-	*iova = va + pa - rounddown(pa, PAGE_SIZE);
-	return 0;
-}
-
-static int ipa_create_uc_smmu_mapping_sgt(struct sg_table *sgt,
-		unsigned long *iova)
-{
-	struct ipa_smmu_cb_ctx *cb = ipa_get_uc_smmu_ctx();
-	unsigned long va = roundup(cb->next_addr, PAGE_SIZE);
-	int prot = IOMMU_READ | IOMMU_WRITE;
-	int ret;
-	int i;
-	struct scatterlist *sg;
-	unsigned long start_iova = va;
-	phys_addr_t phys;
-	size_t len;
-	int count = 0;
-
-	if (!cb->valid) {
-		IPAERR("No SMMU CB setup\n");
-		return -EINVAL;
-	}
-
-	for_each_sg(sgt->sgl, sg, sgt->nents, i) {
-		phys = page_to_phys(sg_page(sg));
-		len = PAGE_ALIGN(sg->offset + sg->length);
-
-		ret = iommu_map(cb->mapping->domain, va, phys, len, prot);
-		if (ret) {
-			IPAERR("iommu map failed for pa=%pa len=%zu\n",
-					&phys, len);
-			goto bad_mapping;
-		}
-		va += len;
-		ipa_ctx->wdi_map_cnt++;
-		count++;
-	}
-	cb->next_addr = va;
-	*iova = start_iova;
-
-	return 0;
-
-bad_mapping:
-	for_each_sg(sgt->sgl, sg, count, i)
-		iommu_unmap(cb->mapping->domain, sg_dma_address(sg),
-				sg_dma_len(sg));
-	return -EINVAL;
-}
-
-static void ipa_release_uc_smmu_mappings(enum ipa_client_type client)
-{
-	struct ipa_smmu_cb_ctx *cb = ipa_get_uc_smmu_ctx();
-	int i;
-	int j;
-	int start;
-	int end;
-
-	if (IPA_CLIENT_IS_CONS(client)) {
-		start = IPA_WDI_TX_RING_RES;
-		end = IPA_WDI_CE_DB_RES;
-	} else {
-		start = IPA_WDI_RX_RING_RES;
-		end = IPA_WDI_RX_RING_RP_RES;
-	}
-
-	for (i = start; i <= end; i++) {
-		if (wdi_res[i].valid) {
-			for (j = 0; j < wdi_res[i].nents; j++) {
-				iommu_unmap(cb->mapping->domain,
-					wdi_res[i].res[j].iova,
-					wdi_res[i].res[j].size);
-				ipa_ctx->wdi_map_cnt--;
-			}
-			kfree(wdi_res[i].res);
-			wdi_res[i].valid = false;
-		}
-	}
-
-	if (ipa_ctx->wdi_map_cnt == 0)
-		cb->next_addr = IPA_SMMU_UC_VA_END;
-
-}
-
-static void ipa_save_uc_smmu_mapping_pa(int res_idx, phys_addr_t pa,
-		unsigned long iova, size_t len)
-{
-	IPADBG("--res_idx=%d pa=0x%pa iova=0x%lx sz=0x%zx\n", res_idx,
-			&pa, iova, len);
-	wdi_res[res_idx].res = kzalloc(sizeof(struct ipa_wdi_res), GFP_KERNEL);
-	if (!wdi_res[res_idx].res)
-		BUG();
-	wdi_res[res_idx].nents = 1;
-	wdi_res[res_idx].valid = true;
-	wdi_res[res_idx].res->pa = rounddown(pa, PAGE_SIZE);
-	wdi_res[res_idx].res->iova = rounddown(iova, PAGE_SIZE);
-	wdi_res[res_idx].res->size = roundup(len + pa - rounddown(pa,
-				PAGE_SIZE), PAGE_SIZE);
-	IPADBG("res_idx=%d pa=0x%pa iova=0x%lx sz=0x%zx\n", res_idx,
-			&wdi_res[res_idx].res->pa, wdi_res[res_idx].res->iova,
-			wdi_res[res_idx].res->size);
-}
-
-static void ipa_save_uc_smmu_mapping_sgt(int res_idx, struct sg_table *sgt,
-		unsigned long iova)
-{
-	int i;
-	struct scatterlist *sg;
-	unsigned long curr_iova = iova;
-
-	wdi_res[res_idx].res = kcalloc(sgt->nents, sizeof(struct ipa_wdi_res),
-			GFP_KERNEL);
-	if (!wdi_res[res_idx].res)
-		BUG();
-	wdi_res[res_idx].nents = sgt->nents;
-	wdi_res[res_idx].valid = true;
-	for_each_sg(sgt->sgl, sg, sgt->nents, i) {
-		wdi_res[res_idx].res[i].pa = page_to_phys(sg_page(sg));
-		wdi_res[res_idx].res[i].iova = curr_iova;
-		wdi_res[res_idx].res[i].size = PAGE_ALIGN(sg->offset +
-				sg->length);
-		IPADBG("res_idx=%d pa=0x%pa iova=0x%lx sz=0x%zx\n", res_idx,
-			&wdi_res[res_idx].res[i].pa,
-			wdi_res[res_idx].res[i].iova,
-			wdi_res[res_idx].res[i].size);
-		curr_iova += wdi_res[res_idx].res[i].size;
-	}
-}
-
-static int ipa_create_uc_smmu_mapping(int res_idx, bool wlan_smmu_en,
-		phys_addr_t pa, struct sg_table *sgt, size_t len, bool device,
-		unsigned long *iova)
-{
-	/* support for SMMU on WLAN but no SMMU on IPA */
-	if (wlan_smmu_en && !ipa_ctx->smmu_present) {
-		IPAERR("Unsupported SMMU pairing\n");
-		return -EINVAL;
-	}
-
-	/* legacy: no SMMUs on either end */
-	if (!wlan_smmu_en && !ipa_ctx->smmu_present) {
-		*iova = pa;
-		return 0;
-	}
-
-	/* no SMMU on WLAN but SMMU on IPA */
-	if (!wlan_smmu_en && ipa_ctx->smmu_present) {
-		if (ipa_create_uc_smmu_mapping_pa(pa, len,
-			(res_idx == IPA_WDI_CE_DB_RES) ? true : false, iova)) {
-			IPAERR("Fail to create mapping res %d\n", res_idx);
-			return -EFAULT;
-		}
-		ipa_save_uc_smmu_mapping_pa(res_idx, pa, *iova, len);
-		return 0;
-	}
-
-	/* SMMU on WLAN and SMMU on IPA */
-	if (wlan_smmu_en && ipa_ctx->smmu_present) {
-		switch (res_idx) {
-		case IPA_WDI_RX_RING_RP_RES:
-		case IPA_WDI_CE_DB_RES:
-			if (ipa_create_uc_smmu_mapping_pa(pa, len,
-				(res_idx == IPA_WDI_CE_DB_RES) ? true : false,
-				iova)) {
-				IPAERR("Fail to create mapping res %d\n",
-						res_idx);
-				return -EFAULT;
-			}
-			ipa_save_uc_smmu_mapping_pa(res_idx, pa, *iova, len);
-			break;
-		case IPA_WDI_RX_RING_RES:
-		case IPA_WDI_TX_RING_RES:
-		case IPA_WDI_CE_RING_RES:
-			if (ipa_create_uc_smmu_mapping_sgt(sgt, iova)) {
-				IPAERR("Fail to create mapping res %d\n",
-						res_idx);
-				return -EFAULT;
-			}
-			ipa_save_uc_smmu_mapping_sgt(res_idx, sgt, *iova);
-			break;
-		default:
-			BUG();
-		}
-	}
-
-	return 0;
-}
-
-/**
- * ipa_connect_wdi_pipe() - WDI client connect
- * @in:	[in] input parameters from client
- * @out: [out] output params to client
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_connect_wdi_pipe(struct ipa_wdi_in_params *in,
-		struct ipa_wdi_out_params *out)
-{
-	int ipa_ep_idx;
-	int result = -EFAULT;
-	struct ipa_ep_context *ep;
-	struct ipa_mem_buffer cmd;
-	struct IpaHwWdiTxSetUpCmdData_t *tx;
-	struct IpaHwWdiRxSetUpCmdData_t *rx;
-	struct ipa_ep_cfg_ctrl ep_cfg_ctrl;
-	unsigned long va;
-	phys_addr_t pa;
-	u32 len;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (in == NULL || out == NULL || in->sys.client >= IPA_CLIENT_MAX) {
-		IPAERR("bad parm. in=%p out=%p\n", in, out);
-		if (in)
-			IPAERR("client = %d\n", in->sys.client);
-		return -EINVAL;
-	}
-
-	if (IPA_CLIENT_IS_CONS(in->sys.client)) {
-		if (in->u.dl.comp_ring_base_pa % IPA_WDI_RING_ALIGNMENT ||
-			in->u.dl.ce_ring_base_pa % IPA_WDI_RING_ALIGNMENT) {
-			IPAERR("alignment failure on TX\n");
-			return -EINVAL;
-		}
-	} else {
-		if (in->u.ul.rdy_ring_base_pa % IPA_WDI_RING_ALIGNMENT) {
-			IPAERR("alignment failure on RX\n");
-			return -EINVAL;
-		}
-	}
-
-	result = ipa_uc_state_check();
-	if (result)
-		return result;
-
-	ipa_ep_idx = ipa_get_ep_mapping(in->sys.client);
-	if (ipa_ep_idx == -1) {
-		IPAERR("fail to alloc EP.\n");
-		goto fail;
-	}
-
-	ep = &ipa_ctx->ep[ipa_ep_idx];
-
-	if (ep->valid) {
-		IPAERR("EP already allocated.\n");
-		goto fail;
-	}
-
-	memset(&ipa_ctx->ep[ipa_ep_idx], 0, sizeof(struct ipa_ep_context));
-	ipa_inc_client_enable_clks();
-
-	IPADBG("client=%d ep=%d\n", in->sys.client, ipa_ep_idx);
-	if (IPA_CLIENT_IS_CONS(in->sys.client)) {
-		cmd.size = sizeof(*tx);
-		IPADBG("comp_ring_base_pa=0x%pa\n",
-				&in->u.dl.comp_ring_base_pa);
-		IPADBG("comp_ring_size=%d\n", in->u.dl.comp_ring_size);
-		IPADBG("ce_ring_base_pa=0x%pa\n", &in->u.dl.ce_ring_base_pa);
-		IPADBG("ce_ring_size=%d\n", in->u.dl.ce_ring_size);
-		IPADBG("ce_ring_doorbell_pa=0x%pa\n",
-				&in->u.dl.ce_door_bell_pa);
-		IPADBG("num_tx_buffers=%d\n", in->u.dl.num_tx_buffers);
-	} else {
-		cmd.size = sizeof(*rx);
-		IPADBG("rx_ring_base_pa=0x%pa\n", &in->u.ul.rdy_ring_base_pa);
-		IPADBG("rx_ring_size=%d\n", in->u.ul.rdy_ring_size);
-		IPADBG("rx_ring_rp_pa=0x%pa\n", &in->u.ul.rdy_ring_rp_pa);
-	}
-
-	cmd.base = dma_alloc_coherent(ipa_ctx->uc_pdev, cmd.size,
-			&cmd.phys_base, GFP_KERNEL);
-	if (cmd.base == NULL) {
-		IPAERR("fail to get DMA memory.\n");
-		result = -ENOMEM;
-		goto dma_alloc_fail;
-	}
-
-	if (IPA_CLIENT_IS_CONS(in->sys.client)) {
-		tx = (struct IpaHwWdiTxSetUpCmdData_t *)cmd.base;
-
-		len = in->smmu_enabled ? in->u.dl_smmu.comp_ring_size :
-			in->u.dl.comp_ring_size;
-		IPADBG("TX ring smmu_en=%d ring_size=%d %d\n", in->smmu_enabled,
-				in->u.dl_smmu.comp_ring_size,
-				in->u.dl.comp_ring_size);
-		if (ipa_create_uc_smmu_mapping(IPA_WDI_TX_RING_RES,
-					in->smmu_enabled,
-					in->u.dl.comp_ring_base_pa,
-					&in->u.dl_smmu.comp_ring,
-					len,
-					false,
-					&va)) {
-				IPAERR("fail to create uc mapping TX ring.\n");
-				result = -ENOMEM;
-				goto uc_timeout;
-		}
-		tx->comp_ring_base_pa = va;
-		tx->comp_ring_size = len;
-
-		len = in->smmu_enabled ? in->u.dl_smmu.ce_ring_size :
-			in->u.dl.ce_ring_size;
-		IPADBG("TX CE ring smmu_en=%d ring_size=%d %d\n",
-				in->smmu_enabled,
-				in->u.dl_smmu.ce_ring_size,
-				in->u.dl.ce_ring_size);
-		if (ipa_create_uc_smmu_mapping(IPA_WDI_CE_RING_RES,
-					in->smmu_enabled,
-					in->u.dl.ce_ring_base_pa,
-					&in->u.dl_smmu.ce_ring,
-					len,
-					false,
-					&va)) {
-				IPAERR("fail to create uc mapping CE ring.\n");
-				result = -ENOMEM;
-				goto uc_timeout;
-		}
-		tx->ce_ring_base_pa = va;
-		tx->ce_ring_size = len;
-
-		pa = in->smmu_enabled ? in->u.dl_smmu.ce_door_bell_pa :
-			in->u.dl.ce_door_bell_pa;
-		if (ipa_create_uc_smmu_mapping(IPA_WDI_CE_DB_RES,
-					in->smmu_enabled,
-					pa,
-					NULL,
-					4,
-					true,
-					&va)) {
-				IPAERR("fail to create uc mapping CE DB.\n");
-				result = -ENOMEM;
-				goto uc_timeout;
-		}
-		tx->ce_ring_doorbell_pa = va;
-
-		tx->num_tx_buffers = in->u.dl.num_tx_buffers;
-		tx->ipa_pipe_number = ipa_ep_idx;
-		if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_5) {
-				out->uc_door_bell_pa =
-				 ipa_ctx->ipa_wrapper_base +
-				   IPA_REG_BASE_OFST_v2_5 +
-				   IPA_UC_MAILBOX_m_n_OFFS_v2_5(
-				    IPA_HW_WDI_TX_MBOX_START_INDEX/32,
-				    IPA_HW_WDI_TX_MBOX_START_INDEX % 32);
-		} else {
-				out->uc_door_bell_pa =
-				 ipa_ctx->ipa_wrapper_base +
-				   IPA_REG_BASE_OFST_v2_0 +
-				   IPA_UC_MAILBOX_m_n_OFFS(
-				    IPA_HW_WDI_TX_MBOX_START_INDEX/32,
-				    IPA_HW_WDI_TX_MBOX_START_INDEX % 32);
-		}
-	} else {
-		rx = (struct IpaHwWdiRxSetUpCmdData_t *)cmd.base;
-
-		len = in->smmu_enabled ? in->u.ul_smmu.rdy_ring_size :
-			in->u.ul.rdy_ring_size;
-		IPADBG("RX ring smmu_en=%d ring_size=%d %d\n", in->smmu_enabled,
-				in->u.ul_smmu.rdy_ring_size,
-				in->u.ul.rdy_ring_size);
-		if (ipa_create_uc_smmu_mapping(IPA_WDI_RX_RING_RES,
-					in->smmu_enabled,
-					in->u.ul.rdy_ring_base_pa,
-					&in->u.ul_smmu.rdy_ring,
-					len,
-					false,
-					&va)) {
-				IPAERR("fail to create uc mapping RX ring.\n");
-				result = -ENOMEM;
-				goto uc_timeout;
-		}
-		rx->rx_ring_base_pa = va;
-		rx->rx_ring_size = len;
-
-		pa = in->smmu_enabled ? in->u.ul_smmu.rdy_ring_rp_pa :
-			in->u.ul.rdy_ring_rp_pa;
-		if (ipa_create_uc_smmu_mapping(IPA_WDI_RX_RING_RP_RES,
-					in->smmu_enabled,
-					pa,
-					NULL,
-					4,
-					false,
-					&va)) {
-				IPAERR("fail to create uc mapping RX rng RP\n");
-				result = -ENOMEM;
-				goto uc_timeout;
-		}
-		rx->rx_ring_rp_pa = va;
-
-		rx->ipa_pipe_number = ipa_ep_idx;
-		if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_5) {
-				out->uc_door_bell_pa =
-				 ipa_ctx->ipa_wrapper_base +
-				   IPA_REG_BASE_OFST_v2_5 +
-				   IPA_UC_MAILBOX_m_n_OFFS_v2_5(
-				    IPA_HW_WDI_RX_MBOX_START_INDEX/32,
-				    IPA_HW_WDI_RX_MBOX_START_INDEX % 32);
-		} else {
-				out->uc_door_bell_pa =
-				 ipa_ctx->ipa_wrapper_base +
-				   IPA_REG_BASE_OFST_v2_0 +
-				   IPA_UC_MAILBOX_m_n_OFFS(
-				    IPA_HW_WDI_RX_MBOX_START_INDEX/32,
-				    IPA_HW_WDI_RX_MBOX_START_INDEX % 32);
-		}
-	}
-
-	ep->valid = 1;
-	ep->client = in->sys.client;
-	ep->keep_ipa_awake = in->sys.keep_ipa_awake;
-	result = ipa_disable_data_path(ipa_ep_idx);
-	if (result) {
-		IPAERR("disable data path failed res=%d clnt=%d.\n", result,
-			ipa_ep_idx);
-		goto uc_timeout;
-	}
-	if (IPA_CLIENT_IS_PROD(in->sys.client)) {
-		memset(&ep_cfg_ctrl, 0 , sizeof(struct ipa_ep_cfg_ctrl));
-		ep_cfg_ctrl.ipa_ep_delay = true;
-		ipa_cfg_ep_ctrl(ipa_ep_idx, &ep_cfg_ctrl);
-	}
-
-	result = ipa_uc_send_cmd((u32)(cmd.phys_base),
-				IPA_CLIENT_IS_CONS(in->sys.client) ?
-				IPA_CPU_2_HW_CMD_WDI_TX_SET_UP :
-				IPA_CPU_2_HW_CMD_WDI_RX_SET_UP,
-				IPA_HW_2_CPU_WDI_CMD_STATUS_SUCCESS,
-				false, 10*HZ);
-
-	if (result) {
-		result = -EFAULT;
-		goto uc_timeout;
-	}
-
-	ep->skip_ep_cfg = in->sys.skip_ep_cfg;
-	ep->client_notify = in->sys.notify;
-	ep->priv = in->sys.priv;
-
-	if (!ep->skip_ep_cfg) {
-		if (ipa_cfg_ep(ipa_ep_idx, &in->sys.ipa_ep_cfg)) {
-			IPAERR("fail to configure EP.\n");
-			goto ipa_cfg_ep_fail;
-		}
-		IPADBG("ep configuration successful\n");
-	} else {
-		IPADBG("Skipping endpoint configuration.\n");
-	}
-
-	out->clnt_hdl = ipa_ep_idx;
-
-	if (!ep->skip_ep_cfg && IPA_CLIENT_IS_PROD(in->sys.client))
-		ipa_install_dflt_flt_rules(ipa_ep_idx);
-
-	if (!ep->keep_ipa_awake)
-		ipa_dec_client_disable_clks();
-
-	dma_free_coherent(ipa_ctx->uc_pdev, cmd.size, cmd.base, cmd.phys_base);
-	ep->wdi_state |= IPA_WDI_CONNECTED;
-	IPADBG("client %d (ep: %d) connected\n", in->sys.client, ipa_ep_idx);
-
-	return 0;
-
-ipa_cfg_ep_fail:
-	memset(&ipa_ctx->ep[ipa_ep_idx], 0, sizeof(struct ipa_ep_context));
-uc_timeout:
-	ipa_release_uc_smmu_mappings(in->sys.client);
-	dma_free_coherent(ipa_ctx->uc_pdev, cmd.size, cmd.base, cmd.phys_base);
-dma_alloc_fail:
-	ipa_dec_client_disable_clks();
-fail:
-	return result;
-}
-EXPORT_SYMBOL(ipa_connect_wdi_pipe);
-
-
-/**
- * ipa_disconnect_wdi_pipe() - WDI client disconnect
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_disconnect_wdi_pipe(u32 clnt_hdl)
-{
-	int result = 0;
-	struct ipa_ep_context *ep;
-	union IpaHwWdiCommonChCmdData_t tear;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	result = ipa_uc_state_check();
-	if (result)
-		return result;
-
-	IPADBG("ep=%d\n", clnt_hdl);
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (ep->wdi_state != IPA_WDI_CONNECTED) {
-		IPAERR("WDI channel bad state %d\n", ep->wdi_state);
-		return -EFAULT;
-	}
-
-	if (!ep->keep_ipa_awake)
-		ipa_inc_client_enable_clks();
-
-	tear.params.ipa_pipe_number = clnt_hdl;
-
-	result = ipa_uc_send_cmd(tear.raw32b,
-				IPA_CPU_2_HW_CMD_WDI_TEAR_DOWN,
-				IPA_HW_2_CPU_WDI_CMD_STATUS_SUCCESS,
-				false, 10*HZ);
-
-	if (result) {
-		result = -EFAULT;
-		goto uc_timeout;
-	}
-
-	ipa_delete_dflt_flt_rules(clnt_hdl);
-	ipa_release_uc_smmu_mappings(ep->client);
-
-	memset(&ipa_ctx->ep[clnt_hdl], 0, sizeof(struct ipa_ep_context));
-	ipa_dec_client_disable_clks();
-
-	IPADBG("client (ep: %d) disconnected\n", clnt_hdl);
-
-uc_timeout:
-	return result;
-}
-EXPORT_SYMBOL(ipa_disconnect_wdi_pipe);
-
-/**
- * ipa_enable_wdi_pipe() - WDI client enable
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_enable_wdi_pipe(u32 clnt_hdl)
-{
-	int result = 0;
-	struct ipa_ep_context *ep;
-	union IpaHwWdiCommonChCmdData_t enable;
-	struct ipa_ep_cfg_holb holb_cfg;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	result = ipa_uc_state_check();
-	if (result)
-		return result;
-
-	IPADBG("ep=%d\n", clnt_hdl);
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (ep->wdi_state != IPA_WDI_CONNECTED) {
-		IPAERR("WDI channel bad state %d\n", ep->wdi_state);
-		return -EFAULT;
-	}
-
-	ipa_inc_client_enable_clks();
-	enable.params.ipa_pipe_number = clnt_hdl;
-
-	result = ipa_uc_send_cmd(enable.raw32b,
-		IPA_CPU_2_HW_CMD_WDI_CH_ENABLE,
-		IPA_HW_2_CPU_WDI_CMD_STATUS_SUCCESS,
-		false, 10*HZ);
-
-	if (result) {
-		result = -EFAULT;
-		goto uc_timeout;
-	}
-
-	if (IPA_CLIENT_IS_CONS(ep->client)) {
-		memset(&holb_cfg, 0 , sizeof(holb_cfg));
-		holb_cfg.en = IPA_HOLB_TMR_DIS;
-		holb_cfg.tmr_val = 0;
-		result = ipa_cfg_ep_holb(clnt_hdl, &holb_cfg);
-	}
-
-	ipa_dec_client_disable_clks();
-	ep->wdi_state |= IPA_WDI_ENABLED;
-	IPADBG("client (ep: %d) enabled\n", clnt_hdl);
-
-uc_timeout:
-	return result;
-}
-EXPORT_SYMBOL(ipa_enable_wdi_pipe);
-
-/**
- * ipa_disable_wdi_pipe() - WDI client disable
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_disable_wdi_pipe(u32 clnt_hdl)
-{
-	int result = 0;
-	struct ipa_ep_context *ep;
-	union IpaHwWdiCommonChCmdData_t disable;
-	struct ipa_ep_cfg_ctrl ep_cfg_ctrl;
-	u32 prod_hdl;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	result = ipa_uc_state_check();
-	if (result)
-		return result;
-
-	IPADBG("ep=%d\n", clnt_hdl);
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (ep->wdi_state != (IPA_WDI_CONNECTED | IPA_WDI_ENABLED)) {
-		IPAERR("WDI channel bad state %d\n", ep->wdi_state);
-		return -EFAULT;
-	}
-
-	ipa_inc_client_enable_clks();
-
-	result = ipa_disable_data_path(clnt_hdl);
-	if (result) {
-		IPAERR("disable data path failed res=%d clnt=%d.\n", result,
-			clnt_hdl);
-		result = -EPERM;
-		goto uc_timeout;
-	}
-
-	/**
-	 * To avoid data stall during continuous SAP on/off before
-	 * setting delay to IPA Consumer pipe, remove delay and enable
-	 * holb on IPA Producer pipe
-	 */
-	if (IPA_CLIENT_IS_PROD(ep->client)) {
-		memset(&ep_cfg_ctrl, 0 , sizeof(struct ipa_ep_cfg_ctrl));
-		ipa_cfg_ep_ctrl(clnt_hdl, &ep_cfg_ctrl);
-
-		prod_hdl = ipa_get_ep_mapping(IPA_CLIENT_WLAN1_CONS);
-		if (ipa_ctx->ep[prod_hdl].valid == 1) {
-			result = ipa_disable_data_path(prod_hdl);
-			if (result) {
-				IPAERR("disable data path failed\n");
-				IPAERR("res=%d clnt=%d\n",
-					result, prod_hdl);
-				result = -EPERM;
-				goto uc_timeout;
-			}
-		}
-		usleep(IPA_UC_POLL_SLEEP_USEC * IPA_UC_POLL_SLEEP_USEC);
-	}
-
-	disable.params.ipa_pipe_number = clnt_hdl;
-
-	result = ipa_uc_send_cmd(disable.raw32b,
-		IPA_CPU_2_HW_CMD_WDI_CH_DISABLE,
-		IPA_HW_2_CPU_WDI_CMD_STATUS_SUCCESS,
-		false, 10*HZ);
-
-	if (result) {
-		result = -EFAULT;
-		goto uc_timeout;
-	}
-
-	/* Set the delay after disabling IPA Producer pipe */
-	if (IPA_CLIENT_IS_PROD(ep->client)) {
-		memset(&ep_cfg_ctrl, 0, sizeof(struct ipa_ep_cfg_ctrl));
-		ep_cfg_ctrl.ipa_ep_delay = true;
-		ipa_cfg_ep_ctrl(clnt_hdl, &ep_cfg_ctrl);
-	}
-
-	ipa_dec_client_disable_clks();
-	ep->wdi_state &= ~IPA_WDI_ENABLED;
-	IPADBG("client (ep: %d) disabled\n", clnt_hdl);
-
-uc_timeout:
-	return result;
-}
-EXPORT_SYMBOL(ipa_disable_wdi_pipe);
-
-/**
- * ipa_resume_wdi_pipe() - WDI client resume
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_resume_wdi_pipe(u32 clnt_hdl)
-{
-	int result = 0;
-	struct ipa_ep_context *ep;
-	union IpaHwWdiCommonChCmdData_t resume;
-	struct ipa_ep_cfg_ctrl ep_cfg_ctrl;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	result = ipa_uc_state_check();
-	if (result)
-		return result;
-
-	IPADBG("ep=%d\n", clnt_hdl);
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (ep->wdi_state != (IPA_WDI_CONNECTED | IPA_WDI_ENABLED)) {
-		IPAERR("WDI channel bad state %d\n", ep->wdi_state);
-		return -EFAULT;
-	}
-
-	ipa_inc_client_enable_clks();
-	resume.params.ipa_pipe_number = clnt_hdl;
-
-	result = ipa_uc_send_cmd(resume.raw32b,
-		IPA_CPU_2_HW_CMD_WDI_CH_RESUME,
-		IPA_HW_2_CPU_WDI_CMD_STATUS_SUCCESS,
-		false, 10*HZ);
-
-	if (result) {
-		result = -EFAULT;
-		goto uc_timeout;
-	}
-
-	memset(&ep_cfg_ctrl, 0 , sizeof(struct ipa_ep_cfg_ctrl));
-	result = ipa_cfg_ep_ctrl(clnt_hdl, &ep_cfg_ctrl);
-	if (result)
-		IPAERR("client (ep: %d) fail un-susp/delay result=%d\n",
-				clnt_hdl, result);
-	else
-		IPADBG("client (ep: %d) un-susp/delay\n", clnt_hdl);
-
-	ep->wdi_state |= IPA_WDI_RESUMED;
-	IPADBG("client (ep: %d) resumed\n", clnt_hdl);
-
-uc_timeout:
-	return result;
-}
-EXPORT_SYMBOL(ipa_resume_wdi_pipe);
-
-/**
- * ipa_suspend_wdi_pipe() - WDI client suspend
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_suspend_wdi_pipe(u32 clnt_hdl)
-{
-	int result = 0;
-	struct ipa_ep_context *ep;
-	union IpaHwWdiCommonChCmdData_t suspend;
-	struct ipa_ep_cfg_ctrl ep_cfg_ctrl;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	result = ipa_uc_state_check();
-	if (result)
-		return result;
-
-	IPADBG("ep=%d\n", clnt_hdl);
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (ep->wdi_state != (IPA_WDI_CONNECTED | IPA_WDI_ENABLED |
-				IPA_WDI_RESUMED)) {
-		IPAERR("WDI channel bad state %d\n", ep->wdi_state);
-		return -EFAULT;
-	}
-
-	suspend.params.ipa_pipe_number = clnt_hdl;
-
-	if (IPA_CLIENT_IS_PROD(ep->client)) {
-		IPADBG("Post suspend event first for IPA Producer\n");
-		IPADBG("Client: %d clnt_hdl: %d\n", ep->client, clnt_hdl);
-		result = ipa_uc_send_cmd(suspend.raw32b,
-			IPA_CPU_2_HW_CMD_WDI_CH_SUSPEND,
-			IPA_HW_2_CPU_WDI_CMD_STATUS_SUCCESS,
-			false, 10*HZ);
-
-		if (result) {
-			result = -EFAULT;
-			goto uc_timeout;
-		}
-	}
-
-	memset(&ep_cfg_ctrl, 0 , sizeof(struct ipa_ep_cfg_ctrl));
-	if (IPA_CLIENT_IS_CONS(ep->client)) {
-		ep_cfg_ctrl.ipa_ep_suspend = true;
-		result = ipa_cfg_ep_ctrl(clnt_hdl, &ep_cfg_ctrl);
-		if (result)
-			IPAERR("client (ep: %d) failed to suspend result=%d\n",
-					clnt_hdl, result);
-		else
-			IPADBG("client (ep: %d) suspended\n", clnt_hdl);
-	} else {
-		ep_cfg_ctrl.ipa_ep_delay = true;
-		result = ipa_cfg_ep_ctrl(clnt_hdl, &ep_cfg_ctrl);
-		if (result)
-			IPAERR("client (ep: %d) failed to delay result=%d\n",
-					clnt_hdl, result);
-		else
-			IPADBG("client (ep: %d) delayed\n", clnt_hdl);
-	}
-
-	if (IPA_CLIENT_IS_CONS(ep->client)) {
-		result = ipa_uc_send_cmd(suspend.raw32b,
-			IPA_CPU_2_HW_CMD_WDI_CH_SUSPEND,
-			IPA_HW_2_CPU_WDI_CMD_STATUS_SUCCESS,
-			false, 10*HZ);
-
-		if (result) {
-			result = -EFAULT;
-			goto uc_timeout;
-		}
-	}
-
-	ipa_ctx->tag_process_before_gating = true;
-	ipa_dec_client_disable_clks();
-	ep->wdi_state &= ~IPA_WDI_RESUMED;
-	IPADBG("client (ep: %d) suspended\n", clnt_hdl);
-
-uc_timeout:
-	return result;
-}
-EXPORT_SYMBOL(ipa_suspend_wdi_pipe);
-
-int ipa_write_qmapid_wdi_pipe(u32 clnt_hdl, u8 qmap_id)
-{
-	int result = 0;
-	struct ipa_ep_context *ep;
-	union IpaHwWdiRxExtCfgCmdData_t qmap;
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	result = ipa_uc_state_check();
-	if (result)
-		return result;
-
-	IPADBG("ep=%d\n", clnt_hdl);
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	if (!(ep->wdi_state & IPA_WDI_CONNECTED)) {
-		IPAERR("WDI channel bad state %d\n", ep->wdi_state);
-		return -EFAULT;
-	}
-
-	ipa_inc_client_enable_clks();
-	qmap.params.ipa_pipe_number = clnt_hdl;
-	qmap.params.qmap_id = qmap_id;
-
-	result = ipa_uc_send_cmd(qmap.raw32b,
-		IPA_CPU_2_HW_CMD_WDI_RX_EXT_CFG,
-		IPA_HW_2_CPU_WDI_CMD_STATUS_SUCCESS,
-		false, 10*HZ);
-
-	if (result) {
-		result = -EFAULT;
-		goto uc_timeout;
-	}
-
-	ipa_dec_client_disable_clks();
-
-	IPADBG("client (ep: %d) qmap_id %d updated\n", clnt_hdl, qmap_id);
-
-uc_timeout:
-	return result;
-}
-
-/**
- * ipa_uc_reg_rdyCB() - To register uC
- * ready CB if uC not ready
- * @inout:	[in/out] input/ouput parameters
- * from/to client
- *
- * Returns:	0 on success, negative on failure
- *
- */
-int ipa_uc_reg_rdyCB(
-	struct ipa_wdi_uc_ready_params *inout)
-{
-	int result = 0;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (inout == NULL) {
-		IPAERR("bad parm. inout=%p ", inout);
-		return -EINVAL;
-	}
-
-	result = ipa_uc_state_check();
-	if (result) {
-		inout->is_uC_ready = false;
-		ipa_ctx->uc_wdi_ctx.uc_ready_cb = inout->notify;
-		ipa_ctx->uc_wdi_ctx.priv = inout->priv;
-	} else {
-		inout->is_uC_ready = true;
-	}
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_uc_reg_rdyCB);
-
-/**
- * ipa_uc_dereg_rdyCB() - To de-register uC ready CB
- *
- * Returns:	0 on success, negative on failure
- *
- */
-int ipa_uc_dereg_rdyCB(void)
-{
-	ipa_ctx->uc_wdi_ctx.uc_ready_cb = NULL;
-	ipa_ctx->uc_wdi_ctx.priv = NULL;
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_uc_dereg_rdyCB);
-
-/**
- * ipa_uc_wdi_get_dbpa() - To retrieve
- * doorbell physical address of wlan pipes
- * @param:  [in/out] input/ouput parameters
- *          from/to client
- *
- * Returns:	0 on success, negative on failure
- *
- */
-int ipa_uc_wdi_get_dbpa(
-	struct ipa_wdi_db_params *param)
-{
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (param == NULL || param->client >= IPA_CLIENT_MAX) {
-		IPAERR("bad parm. param=%p ", param);
-		if (param)
-			IPAERR("client = %d\n", param->client);
-		return -EINVAL;
-	}
-
-	if (IPA_CLIENT_IS_CONS(param->client)) {
-		if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_5) {
-				param->uc_door_bell_pa =
-				 ipa_ctx->ipa_wrapper_base +
-					IPA_REG_BASE_OFST_v2_5 +
-				   IPA_UC_MAILBOX_m_n_OFFS_v2_5(
-				    IPA_HW_WDI_TX_MBOX_START_INDEX/32,
-				    IPA_HW_WDI_TX_MBOX_START_INDEX % 32);
-		} else {
-				param->uc_door_bell_pa =
-				 ipa_ctx->ipa_wrapper_base +
-					IPA_REG_BASE_OFST_v2_0 +
-				   IPA_UC_MAILBOX_m_n_OFFS(
-				    IPA_HW_WDI_TX_MBOX_START_INDEX/32,
-				    IPA_HW_WDI_TX_MBOX_START_INDEX % 32);
-		}
-	} else {
-		if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_5) {
-				param->uc_door_bell_pa =
-				 ipa_ctx->ipa_wrapper_base +
-					IPA_REG_BASE_OFST_v2_5 +
-				   IPA_UC_MAILBOX_m_n_OFFS_v2_5(
-				    IPA_HW_WDI_RX_MBOX_START_INDEX/32,
-				    IPA_HW_WDI_RX_MBOX_START_INDEX % 32);
-		} else {
-				param->uc_door_bell_pa =
-				 ipa_ctx->ipa_wrapper_base +
-					IPA_REG_BASE_OFST_v2_0 +
-				   IPA_UC_MAILBOX_m_n_OFFS(
-				    IPA_HW_WDI_RX_MBOX_START_INDEX/32,
-				    IPA_HW_WDI_RX_MBOX_START_INDEX % 32);
-		}
-	}
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_uc_wdi_get_dbpa);
-
-static void ipa_uc_wdi_loaded_handler(void)
-{
-	if (!ipa_ctx) {
-		IPAERR("IPA ctx is null\n");
-		return;
-	}
-
-	if (ipa_ctx->uc_wdi_ctx.uc_ready_cb) {
-		ipa_ctx->uc_wdi_ctx.uc_ready_cb(
-			ipa_ctx->uc_wdi_ctx.priv);
-
-		ipa_ctx->uc_wdi_ctx.uc_ready_cb =
-			NULL;
-		ipa_ctx->uc_wdi_ctx.priv = NULL;
-	}
-
-	return;
-}
-
-int ipa_create_wdi_mapping(u32 num_buffers, struct ipa_wdi_buffer_info *info)
-{
-	struct ipa_smmu_cb_ctx *cb = ipa_get_wlan_smmu_ctx();
-	int i;
-	int ret = 0;
-	int prot = IOMMU_READ | IOMMU_WRITE;
-
-	if (!info) {
-		IPAERR("info = %p\n", info);
-		return -EINVAL;
-	}
-
-	if (!cb->valid) {
-		IPAERR("No SMMU CB setup\n");
-		return -EINVAL;
-	}
-
-	for (i = 0; i < num_buffers; i++) {
-		IPADBG("i=%d pa=0x%pa iova=0x%lx sz=0x%zx\n", i,
-			&info[i].pa, info[i].iova, info[i].size);
-		info[i].result = iommu_map(cb->iommu,
-			rounddown(info[i].iova, PAGE_SIZE),
-			rounddown(info[i].pa, PAGE_SIZE),
-			roundup(info[i].size + info[i].pa -
-				rounddown(info[i].pa, PAGE_SIZE), PAGE_SIZE),
-			prot);
-	}
-
-	return ret;
-}
-EXPORT_SYMBOL(ipa_create_wdi_mapping);
-
-int ipa_release_wdi_mapping(u32 num_buffers, struct ipa_wdi_buffer_info *info)
-{
-	struct ipa_smmu_cb_ctx *cb = ipa_get_wlan_smmu_ctx();
-	int i;
-	int ret = 0;
-
-	if (!info) {
-		IPAERR("info = %p\n", info);
-		return -EINVAL;
-	}
-
-	if (!cb->valid) {
-		IPAERR("No SMMU CB setup\n");
-		return -EINVAL;
-	}
-
-	for (i = 0; i < num_buffers; i++) {
-		IPADBG("i=%d pa=0x%pa iova=0x%lx sz=0x%zx\n", i,
-			&info[i].pa, info[i].iova, info[i].size);
-		info[i].result = iommu_unmap(cb->iommu,
-			rounddown(info[i].iova, PAGE_SIZE),
-			roundup(info[i].size + info[i].pa -
-				rounddown(info[i].pa, PAGE_SIZE), PAGE_SIZE));
-	}
-
-	return ret;
-}
-EXPORT_SYMBOL(ipa_release_wdi_mapping);
diff --git a/drivers/platform/msm/ipa/ipa_utils.c b/drivers/platform/msm/ipa/ipa_utils.c
deleted file mode 100644
index e458f495..00000000
--- a/drivers/platform/msm/ipa/ipa_utils.c
+++ /dev/null
@@ -1,5008 +0,0 @@
-/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <net/ip.h>
-#include <linux/genalloc.h>	/* gen_pool_alloc() */
-#include <linux/io.h>
-#include <linux/ratelimit.h>
-#include <linux/msm-bus.h>
-#include <linux/msm-bus-board.h>
-#include "ipa_i.h"
-
-#define IPA_V1_CLK_RATE (92.31 * 1000 * 1000UL)
-#define IPA_V1_1_CLK_RATE (100 * 1000 * 1000UL)
-#define IPA_V2_0_CLK_RATE_SVS (75 * 1000 * 1000UL)
-#define IPA_V2_0_CLK_RATE_NOMINAL (150 * 1000 * 1000UL)
-#define IPA_V2_0_CLK_RATE_TURBO (200 * 1000 * 1000UL)
-#define IPA_V1_MAX_HOLB_TMR_VAL (512 - 1)
-#define IPA_V2_0_MAX_HOLB_TMR_VAL (65536 - 1)
-#define IPA_V2_5_MAX_HOLB_TMR_VAL (4294967296 - 1)
-#define IPA_V2_6L_MAX_HOLB_TMR_VAL IPA_V2_5_MAX_HOLB_TMR_VAL
-
-#define IPA_V2_0_BW_THRESHOLD_TURBO_MBPS (1000)
-#define IPA_V2_0_BW_THRESHOLD_NOMINAL_MBPS (600)
-
-/* Max pipes + ICs for TAG process */
-#define IPA_TAG_MAX_DESC (IPA_MAX_NUM_PIPES + 6)
-
-#define IPA_TAG_SLEEP_MIN_USEC (1000)
-#define IPA_TAG_SLEEP_MAX_USEC (2000)
-#define IPA_FORCE_CLOSE_TAG_PROCESS_TIMEOUT (10 * HZ)
-#define IPA_BCR_REG_VAL (0x001FFF7F)
-#define IPA_AGGR_GRAN_MIN (1)
-#define IPA_AGGR_GRAN_MAX (32)
-#define IPA_EOT_COAL_GRAN_MIN (1)
-#define IPA_EOT_COAL_GRAN_MAX (16)
-
-#define IPA_AGGR_BYTE_LIMIT (\
-		IPA_ENDP_INIT_AGGR_N_AGGR_BYTE_LIMIT_BMSK >> \
-		IPA_ENDP_INIT_AGGR_N_AGGR_BYTE_LIMIT_SHFT)
-#define IPA_AGGR_PKT_LIMIT (\
-		IPA_ENDP_INIT_AGGR_n_AGGR_PKT_LIMIT_BMSK >> \
-		IPA_ENDP_INIT_AGGR_n_AGGR_PKT_LIMIT_SHFT)
-
-static const int ipa_ofst_meq32[] = { IPA_OFFSET_MEQ32_0,
-					IPA_OFFSET_MEQ32_1, -1 };
-static const int ipa_ofst_meq128[] = { IPA_OFFSET_MEQ128_0,
-					IPA_OFFSET_MEQ128_1, -1 };
-static const int ipa_ihl_ofst_rng16[] = { IPA_IHL_OFFSET_RANGE16_0,
-					IPA_IHL_OFFSET_RANGE16_1, -1 };
-static const int ipa_ihl_ofst_meq32[] = { IPA_IHL_OFFSET_MEQ32_0,
-					IPA_IHL_OFFSET_MEQ32_1, -1 };
-#define IPA_1_1 (0)
-#define IPA_2_0 (1)
-#define IPA_2_6L (2)
-
-#define INVALID_EP_MAPPING_INDEX (-1)
-
-static const int ep_mapping[3][IPA_CLIENT_MAX] = {
-	[IPA_1_1][IPA_CLIENT_HSIC1_PROD]         = 19,
-	[IPA_1_1][IPA_CLIENT_WLAN1_PROD]         = -1,
-	[IPA_1_1][IPA_CLIENT_HSIC2_PROD]         = 12,
-	[IPA_1_1][IPA_CLIENT_USB2_PROD]          = 12,
-	[IPA_1_1][IPA_CLIENT_HSIC3_PROD]         = 13,
-	[IPA_1_1][IPA_CLIENT_USB3_PROD]          = 13,
-	[IPA_1_1][IPA_CLIENT_HSIC4_PROD]         =  0,
-	[IPA_1_1][IPA_CLIENT_USB4_PROD]          =  0,
-	[IPA_1_1][IPA_CLIENT_HSIC5_PROD]         = -1,
-	[IPA_1_1][IPA_CLIENT_USB_PROD]           = 11,
-	[IPA_1_1][IPA_CLIENT_A5_WLAN_AMPDU_PROD] = 15,
-	[IPA_1_1][IPA_CLIENT_A2_EMBEDDED_PROD]   =  8,
-	[IPA_1_1][IPA_CLIENT_A2_TETHERED_PROD]   =  6,
-	[IPA_1_1][IPA_CLIENT_APPS_LAN_WAN_PROD]  =  2,
-	[IPA_1_1][IPA_CLIENT_APPS_CMD_PROD]      =  1,
-	[IPA_1_1][IPA_CLIENT_ODU_PROD]           = -1,
-	[IPA_1_1][IPA_CLIENT_MHI_PROD]           = -1,
-	[IPA_1_1][IPA_CLIENT_Q6_LAN_PROD]        =  5,
-	[IPA_1_1][IPA_CLIENT_Q6_CMD_PROD]        = -1,
-
-	[IPA_1_1][IPA_CLIENT_HSIC1_CONS]         = 14,
-	[IPA_1_1][IPA_CLIENT_WLAN1_CONS]         = -1,
-	[IPA_1_1][IPA_CLIENT_HSIC2_CONS]         = 16,
-	[IPA_1_1][IPA_CLIENT_USB2_CONS]          = 16,
-	[IPA_1_1][IPA_CLIENT_WLAN2_CONS]         = -1,
-	[IPA_1_1][IPA_CLIENT_HSIC3_CONS]         = 17,
-	[IPA_1_1][IPA_CLIENT_USB3_CONS]          = 17,
-	[IPA_1_1][IPA_CLIENT_WLAN3_CONS]         = -1,
-	[IPA_1_1][IPA_CLIENT_HSIC4_CONS]         = 18,
-	[IPA_1_1][IPA_CLIENT_USB4_CONS]          = 18,
-	[IPA_1_1][IPA_CLIENT_WLAN4_CONS]         = -1,
-	[IPA_1_1][IPA_CLIENT_HSIC5_CONS]         = -1,
-	[IPA_1_1][IPA_CLIENT_USB_CONS]           = 10,
-	[IPA_1_1][IPA_CLIENT_USB_DPL_CONS]       = -1,
-	[IPA_1_1][IPA_CLIENT_A2_EMBEDDED_CONS]   =  9,
-	[IPA_1_1][IPA_CLIENT_A2_TETHERED_CONS]   =  7,
-	[IPA_1_1][IPA_CLIENT_A5_LAN_WAN_CONS]    =  3,
-	[IPA_1_1][IPA_CLIENT_APPS_LAN_CONS]      = -1,
-	[IPA_1_1][IPA_CLIENT_APPS_WAN_CONS]      = -1,
-	[IPA_1_1][IPA_CLIENT_ODU_EMB_CONS]       = -1,
-	[IPA_1_1][IPA_CLIENT_ODU_TETH_CONS]      = -1,
-	[IPA_1_1][IPA_CLIENT_MHI_CONS]           = -1,
-	[IPA_1_1][IPA_CLIENT_Q6_LAN_CONS]        =  4,
-	[IPA_1_1][IPA_CLIENT_Q6_WAN_CONS]        = -1,
-
-
-	[IPA_2_0][IPA_CLIENT_HSIC1_PROD]         = 12,
-	[IPA_2_0][IPA_CLIENT_WLAN1_PROD]         = 18,
-	[IPA_2_0][IPA_CLIENT_HSIC2_PROD]         = -1,
-	[IPA_2_0][IPA_CLIENT_USB2_PROD]          = 12,
-	[IPA_2_0][IPA_CLIENT_HSIC3_PROD]         = -1,
-	[IPA_2_0][IPA_CLIENT_USB3_PROD]          = 13,
-	[IPA_2_0][IPA_CLIENT_HSIC4_PROD]         = -1,
-	[IPA_2_0][IPA_CLIENT_USB4_PROD]          =  0,
-	[IPA_2_0][IPA_CLIENT_HSIC5_PROD]         = -1,
-	[IPA_2_0][IPA_CLIENT_USB_PROD]           = 11,
-	[IPA_2_0][IPA_CLIENT_A5_WLAN_AMPDU_PROD] = -1,
-	[IPA_2_0][IPA_CLIENT_A2_EMBEDDED_PROD]   = -1,
-	[IPA_2_0][IPA_CLIENT_A2_TETHERED_PROD]   = -1,
-	[IPA_2_0][IPA_CLIENT_APPS_LAN_WAN_PROD]  =  4,
-	[IPA_2_0][IPA_CLIENT_APPS_CMD_PROD]      =  3,
-	[IPA_2_0][IPA_CLIENT_ODU_PROD]           = 12,
-	[IPA_2_0][IPA_CLIENT_MHI_PROD]           = 18,
-	[IPA_2_0][IPA_CLIENT_Q6_LAN_PROD]        =  6,
-	[IPA_2_0][IPA_CLIENT_Q6_CMD_PROD]        =  7,
-	[IPA_2_0][IPA_CLIENT_Q6_DECOMP_PROD]     = -1,
-	[IPA_2_0][IPA_CLIENT_Q6_DECOMP2_PROD]    = -1,
-	[IPA_2_0][IPA_CLIENT_MEMCPY_DMA_SYNC_PROD]
-						 =  12,
-	[IPA_2_0][IPA_CLIENT_MEMCPY_DMA_ASYNC_PROD]
-						 =  19,
-	/* Only for test purpose */
-	[IPA_2_0][IPA_CLIENT_TEST_PROD]          = 19,
-	[IPA_2_0][IPA_CLIENT_TEST1_PROD]         = 19,
-	[IPA_2_0][IPA_CLIENT_TEST2_PROD]         = 12,
-	[IPA_2_0][IPA_CLIENT_TEST3_PROD]         = 11,
-	[IPA_2_0][IPA_CLIENT_TEST4_PROD]         =  0,
-
-	[IPA_2_0][IPA_CLIENT_HSIC1_CONS]         = 13,
-	[IPA_2_0][IPA_CLIENT_WLAN1_CONS]         = 17,
-	[IPA_2_0][IPA_CLIENT_HSIC2_CONS]         = -1,
-	[IPA_2_0][IPA_CLIENT_USB2_CONS]          = -1,
-	[IPA_2_0][IPA_CLIENT_WLAN2_CONS]         = 16,
-	[IPA_2_0][IPA_CLIENT_HSIC3_CONS]         = -1,
-	[IPA_2_0][IPA_CLIENT_USB3_CONS]          = -1,
-	[IPA_2_0][IPA_CLIENT_WLAN3_CONS]         = 14,
-	[IPA_2_0][IPA_CLIENT_HSIC4_CONS]         = -1,
-	[IPA_2_0][IPA_CLIENT_USB4_CONS]          = -1,
-	[IPA_2_0][IPA_CLIENT_WLAN4_CONS]         = 19,
-	[IPA_2_0][IPA_CLIENT_HSIC5_CONS]         = -1,
-	[IPA_2_0][IPA_CLIENT_USB_CONS]           = 15,
-	[IPA_2_0][IPA_CLIENT_USB_DPL_CONS]       =  0,
-	[IPA_2_0][IPA_CLIENT_A2_EMBEDDED_CONS]   = -1,
-	[IPA_2_0][IPA_CLIENT_A2_TETHERED_CONS]   = -1,
-	[IPA_2_0][IPA_CLIENT_A5_LAN_WAN_CONS]    = -1,
-	[IPA_2_0][IPA_CLIENT_APPS_LAN_CONS]      =  2,
-	[IPA_2_0][IPA_CLIENT_APPS_WAN_CONS]      =  5,
-	[IPA_2_0][IPA_CLIENT_ODU_EMB_CONS]       = 13,
-	[IPA_2_0][IPA_CLIENT_ODU_TETH_CONS]      =  1,
-	[IPA_2_0][IPA_CLIENT_MHI_CONS]           = 17,
-	[IPA_2_0][IPA_CLIENT_Q6_LAN_CONS]        =  8,
-	[IPA_2_0][IPA_CLIENT_Q6_WAN_CONS]        =  9,
-	[IPA_2_0][IPA_CLIENT_Q6_DUN_CONS]        = 10,
-	[IPA_2_0][IPA_CLIENT_Q6_DECOMP_CONS]     = -1,
-	[IPA_2_0][IPA_CLIENT_Q6_DECOMP2_CONS]    = -1,
-	[IPA_2_0][IPA_CLIENT_MEMCPY_DMA_SYNC_CONS]
-						 =  13,
-	[IPA_2_0][IPA_CLIENT_MEMCPY_DMA_ASYNC_CONS]
-						 =  16,
-	/* Only for test purpose */
-	[IPA_2_0][IPA_CLIENT_TEST_CONS]          = 1,
-	[IPA_2_0][IPA_CLIENT_TEST1_CONS]         = 1,
-	[IPA_2_0][IPA_CLIENT_TEST2_CONS]         = 16,
-	[IPA_2_0][IPA_CLIENT_TEST3_CONS]         = 13,
-	[IPA_2_0][IPA_CLIENT_TEST4_CONS]         = 15,
-
-
-	[IPA_2_6L][IPA_CLIENT_HSIC1_PROD]         = -1,
-	[IPA_2_6L][IPA_CLIENT_WLAN1_PROD]         = -1,
-	[IPA_2_6L][IPA_CLIENT_HSIC2_PROD]         = -1,
-	[IPA_2_6L][IPA_CLIENT_USB2_PROD]          = -1,
-	[IPA_2_6L][IPA_CLIENT_HSIC3_PROD]         = -1,
-	[IPA_2_6L][IPA_CLIENT_USB3_PROD]          = -1,
-	[IPA_2_6L][IPA_CLIENT_HSIC4_PROD]         = -1,
-	[IPA_2_6L][IPA_CLIENT_USB4_PROD]          = -1,
-	[IPA_2_6L][IPA_CLIENT_HSIC5_PROD]         = -1,
-	[IPA_2_6L][IPA_CLIENT_USB_PROD]           =  1,
-	[IPA_2_6L][IPA_CLIENT_A5_WLAN_AMPDU_PROD] = -1,
-	[IPA_2_6L][IPA_CLIENT_A2_EMBEDDED_PROD]   = -1,
-	[IPA_2_6L][IPA_CLIENT_A2_TETHERED_PROD]   = -1,
-	[IPA_2_6L][IPA_CLIENT_APPS_LAN_WAN_PROD]  =  4,
-	[IPA_2_6L][IPA_CLIENT_APPS_CMD_PROD]      =  3,
-	[IPA_2_6L][IPA_CLIENT_ODU_PROD]           = -1,
-	[IPA_2_6L][IPA_CLIENT_MHI_PROD]           = -1,
-	[IPA_2_6L][IPA_CLIENT_Q6_LAN_PROD]        =  6,
-	[IPA_2_6L][IPA_CLIENT_Q6_CMD_PROD]        =  7,
-	[IPA_2_6L][IPA_CLIENT_Q6_DECOMP_PROD]     = 11,
-	[IPA_2_6L][IPA_CLIENT_Q6_DECOMP2_PROD]    = 13,
-	[IPA_2_6L][IPA_CLIENT_MEMCPY_DMA_SYNC_PROD]
-						 =  -1,
-	[IPA_2_6L][IPA_CLIENT_MEMCPY_DMA_ASYNC_PROD]
-						 =  -1,
-
-	/* Only for test purpose */
-	[IPA_2_6L][IPA_CLIENT_TEST_PROD]          = 11,
-	[IPA_2_6L][IPA_CLIENT_TEST1_PROD]         = 11,
-	[IPA_2_6L][IPA_CLIENT_TEST2_PROD]         = 12,
-	[IPA_2_6L][IPA_CLIENT_TEST3_PROD]         = 13,
-	[IPA_2_6L][IPA_CLIENT_TEST4_PROD]         = 14,
-
-	[IPA_2_6L][IPA_CLIENT_HSIC1_CONS]         = -1,
-	[IPA_2_6L][IPA_CLIENT_WLAN1_CONS]         = -1,
-	[IPA_2_6L][IPA_CLIENT_HSIC2_CONS]         = -1,
-	[IPA_2_6L][IPA_CLIENT_USB2_CONS]          = -1,
-	[IPA_2_6L][IPA_CLIENT_WLAN2_CONS]         = -1,
-	[IPA_2_6L][IPA_CLIENT_HSIC3_CONS]         = -1,
-	[IPA_2_6L][IPA_CLIENT_USB3_CONS]          = -1,
-	[IPA_2_6L][IPA_CLIENT_WLAN3_CONS]         = -1,
-	[IPA_2_6L][IPA_CLIENT_HSIC4_CONS]         = -1,
-	[IPA_2_6L][IPA_CLIENT_USB4_CONS]          = -1,
-	[IPA_2_6L][IPA_CLIENT_WLAN4_CONS]         = -1,
-	[IPA_2_6L][IPA_CLIENT_HSIC5_CONS]         = -1,
-	[IPA_2_6L][IPA_CLIENT_USB_CONS]           =  0,
-	[IPA_2_6L][IPA_CLIENT_USB_DPL_CONS]       = 10,
-	[IPA_2_6L][IPA_CLIENT_A2_EMBEDDED_CONS]   = -1,
-	[IPA_2_6L][IPA_CLIENT_A2_TETHERED_CONS]   = -1,
-	[IPA_2_6L][IPA_CLIENT_A5_LAN_WAN_CONS]    = -1,
-	[IPA_2_6L][IPA_CLIENT_APPS_LAN_CONS]      =  2,
-	[IPA_2_6L][IPA_CLIENT_APPS_WAN_CONS]      =  5,
-	[IPA_2_6L][IPA_CLIENT_ODU_EMB_CONS]       = -1,
-	[IPA_2_6L][IPA_CLIENT_ODU_TETH_CONS]      = -1,
-	[IPA_2_6L][IPA_CLIENT_MHI_CONS]           = -1,
-	[IPA_2_6L][IPA_CLIENT_Q6_LAN_CONS]        =  8,
-	[IPA_2_6L][IPA_CLIENT_Q6_WAN_CONS]        =  9,
-	[IPA_2_6L][IPA_CLIENT_Q6_DUN_CONS]        = -1,
-	[IPA_2_6L][IPA_CLIENT_Q6_DECOMP_CONS]     = 12,
-	[IPA_2_6L][IPA_CLIENT_Q6_DECOMP2_CONS]    = 14,
-	[IPA_2_6L][IPA_CLIENT_MEMCPY_DMA_SYNC_CONS]
-						 =  -1,
-	[IPA_2_6L][IPA_CLIENT_MEMCPY_DMA_ASYNC_CONS]
-						 =  -1,
-	/* Only for test purpose */
-	[IPA_2_6L][IPA_CLIENT_TEST_CONS]          = 15,
-	[IPA_2_6L][IPA_CLIENT_TEST1_CONS]         = 15,
-	[IPA_2_6L][IPA_CLIENT_TEST2_CONS]         = 0,
-	[IPA_2_6L][IPA_CLIENT_TEST3_CONS]         = 1,
-	[IPA_2_6L][IPA_CLIENT_TEST4_CONS]         = 10,
-};
-
-static struct msm_bus_vectors ipa_init_vectors_v1_1[]  = {
-	{
-		.src = MSM_BUS_MASTER_IPA,
-		.dst = MSM_BUS_SLAVE_EBI_CH0,
-		.ab = 0,
-		.ib = 0,
-	},
-	{
-		.src = MSM_BUS_MASTER_BAM_DMA,
-		.dst = MSM_BUS_SLAVE_EBI_CH0,
-		.ab = 0,
-		.ib = 0,
-	},
-	{
-		.src = MSM_BUS_MASTER_BAM_DMA,
-		.dst = MSM_BUS_SLAVE_OCIMEM,
-		.ab = 0,
-		.ib = 0,
-	},
-};
-
-static struct msm_bus_vectors ipa_init_vectors_v2_0[]  = {
-	{
-		.src = MSM_BUS_MASTER_IPA,
-		.dst = MSM_BUS_SLAVE_EBI_CH0,
-		.ab = 0,
-		.ib = 0,
-	},
-	{
-		.src = MSM_BUS_MASTER_IPA,
-		.dst = MSM_BUS_SLAVE_OCIMEM,
-		.ab = 0,
-		.ib = 0,
-	},
-};
-
-static struct msm_bus_vectors ipa_max_perf_vectors_v1_1[]  = {
-	{
-		.src = MSM_BUS_MASTER_IPA,
-		.dst = MSM_BUS_SLAVE_EBI_CH0,
-		.ab = 50000000,
-		.ib = 960000000,
-	},
-	{
-		.src = MSM_BUS_MASTER_BAM_DMA,
-		.dst = MSM_BUS_SLAVE_EBI_CH0,
-		.ab = 50000000,
-		.ib = 960000000,
-	},
-	{
-		.src = MSM_BUS_MASTER_BAM_DMA,
-		.dst = MSM_BUS_SLAVE_OCIMEM,
-		.ab = 50000000,
-		.ib = 960000000,
-	},
-};
-
-static struct msm_bus_vectors ipa_nominal_perf_vectors_v2_0[]  = {
-	{
-		.src = MSM_BUS_MASTER_IPA,
-		.dst = MSM_BUS_SLAVE_EBI_CH0,
-		.ab = 100000000,
-		.ib = 1300000000,
-	},
-	{
-		.src = MSM_BUS_MASTER_IPA,
-		.dst = MSM_BUS_SLAVE_OCIMEM,
-		.ab = 100000000,
-		.ib = 1300000000,
-	},
-};
-
-static struct msm_bus_paths ipa_usecases_v1_1[]  = {
-	{
-		ARRAY_SIZE(ipa_init_vectors_v1_1),
-		ipa_init_vectors_v1_1,
-	},
-	{
-		ARRAY_SIZE(ipa_max_perf_vectors_v1_1),
-		ipa_max_perf_vectors_v1_1,
-	},
-};
-
-static struct msm_bus_paths ipa_usecases_v2_0[]  = {
-	{
-		ARRAY_SIZE(ipa_init_vectors_v2_0),
-		ipa_init_vectors_v2_0,
-	},
-	{
-		ARRAY_SIZE(ipa_nominal_perf_vectors_v2_0),
-		ipa_nominal_perf_vectors_v2_0,
-	},
-};
-
-static struct msm_bus_scale_pdata ipa_bus_client_pdata_v1_1 = {
-	ipa_usecases_v1_1,
-	ARRAY_SIZE(ipa_usecases_v1_1),
-	.name = "ipa",
-};
-
-static struct msm_bus_scale_pdata ipa_bus_client_pdata_v2_0 = {
-	ipa_usecases_v2_0,
-	ARRAY_SIZE(ipa_usecases_v2_0),
-	.name = "ipa",
-};
-
-void ipa_active_clients_lock(void)
-{
-	unsigned long flags;
-
-	mutex_lock(&ipa_ctx->ipa_active_clients.mutex);
-	spin_lock_irqsave(&ipa_ctx->ipa_active_clients.spinlock, flags);
-	ipa_ctx->ipa_active_clients.mutex_locked = true;
-	spin_unlock_irqrestore(&ipa_ctx->ipa_active_clients.spinlock, flags);
-}
-
-int ipa_active_clients_trylock(unsigned long *flags)
-{
-	spin_lock_irqsave(&ipa_ctx->ipa_active_clients.spinlock, *flags);
-	if (ipa_ctx->ipa_active_clients.mutex_locked) {
-		spin_unlock_irqrestore(&ipa_ctx->ipa_active_clients.spinlock,
-					 *flags);
-		return 0;
-	}
-
-	return 1;
-}
-
-void ipa_active_clients_trylock_unlock(unsigned long *flags)
-{
-	spin_unlock_irqrestore(&ipa_ctx->ipa_active_clients.spinlock, *flags);
-}
-
-void ipa_active_clients_unlock(void)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&ipa_ctx->ipa_active_clients.spinlock, flags);
-	ipa_ctx->ipa_active_clients.mutex_locked = false;
-	spin_unlock_irqrestore(&ipa_ctx->ipa_active_clients.spinlock, flags);
-	mutex_unlock(&ipa_ctx->ipa_active_clients.mutex);
-}
-
-/**
- * ipa_get_clients_from_rm_resource() - get IPA clients which are related to an
- * IPA_RM resource
- *
- * @resource: [IN] IPA Resource Manager resource
- * @clients: [OUT] Empty array which will contain the list of clients. The
- *         caller must initialize this array.
- *
- * Return codes: 0 on success, negative on failure.
- */
-int ipa_get_clients_from_rm_resource(
-	enum ipa_rm_resource_name resource,
-	struct ipa_client_names *clients)
-{
-	int i = 0;
-
-	if (resource < 0 ||
-	    resource >= IPA_RM_RESOURCE_MAX ||
-	    !clients) {
-		IPAERR("Bad parameters\n");
-		return -EINVAL;
-	}
-
-	switch (resource) {
-	case IPA_RM_RESOURCE_USB_CONS:
-		clients->names[i++] = IPA_CLIENT_USB_CONS;
-		break;
-	case IPA_RM_RESOURCE_HSIC_CONS:
-		clients->names[i++] = IPA_CLIENT_HSIC1_CONS;
-		break;
-	case IPA_RM_RESOURCE_WLAN_CONS:
-		clients->names[i++] = IPA_CLIENT_WLAN1_CONS;
-		clients->names[i++] = IPA_CLIENT_WLAN2_CONS;
-		clients->names[i++] = IPA_CLIENT_WLAN3_CONS;
-		clients->names[i++] = IPA_CLIENT_WLAN4_CONS;
-		break;
-	case IPA_RM_RESOURCE_MHI_CONS:
-		clients->names[i++] = IPA_CLIENT_MHI_CONS;
-		break;
-	case IPA_RM_RESOURCE_USB_PROD:
-		clients->names[i++] = IPA_CLIENT_USB_PROD;
-		break;
-	case IPA_RM_RESOURCE_HSIC_PROD:
-		clients->names[i++] = IPA_CLIENT_HSIC1_PROD;
-		break;
-	case IPA_RM_RESOURCE_MHI_PROD:
-		clients->names[i++] = IPA_CLIENT_MHI_PROD;
-		break;
-	default:
-		break;
-	}
-	clients->length = i;
-
-	return 0;
-}
-
-/**
- * ipa_should_pipe_be_suspended() - returns true when the client's pipe should
- * be suspended during a power save scenario. False otherwise.
- *
- * @client: [IN] IPA client
- */
-bool ipa_should_pipe_be_suspended(enum ipa_client_type client)
-{
-	struct ipa_ep_context *ep;
-	int ipa_ep_idx;
-
-	ipa_ep_idx = ipa_get_ep_mapping(client);
-	if (ipa_ep_idx == -1) {
-		IPAERR("Invalid client.\n");
-		WARN_ON(1);
-		return false;
-	}
-
-	ep = &ipa_ctx->ep[ipa_ep_idx];
-
-	if (ep->keep_ipa_awake)
-		return false;
-
-	if (client == IPA_CLIENT_USB_CONS   ||
-	    client == IPA_CLIENT_MHI_CONS   ||
-	    client == IPA_CLIENT_HSIC1_CONS ||
-	    client == IPA_CLIENT_WLAN1_CONS ||
-	    client == IPA_CLIENT_WLAN2_CONS ||
-	    client == IPA_CLIENT_WLAN3_CONS ||
-	    client == IPA_CLIENT_WLAN4_CONS)
-		return true;
-
-	return false;
-}
-
-/**
- * ipa_suspend_resource_sync() - suspend client endpoints related to the IPA_RM
- * resource and decrement active clients counter, which may result in clock
- * gating of IPA clocks.
- *
- * @resource: [IN] IPA Resource Manager resource
- *
- * Return codes: 0 on success, negative on failure.
- */
-int ipa_suspend_resource_sync(enum ipa_rm_resource_name resource)
-{
-	struct ipa_client_names clients;
-	int res;
-	int index;
-	struct ipa_ep_cfg_ctrl suspend;
-	enum ipa_client_type client;
-	int ipa_ep_idx;
-	bool pipe_suspended = false;
-
-	memset(&clients, 0, sizeof(clients));
-	res = ipa_get_clients_from_rm_resource(resource, &clients);
-	if (res) {
-		IPAERR("Bad params.\n");
-		return res;
-	}
-
-	for (index = 0; index < clients.length; index++) {
-		client = clients.names[index];
-		ipa_ep_idx = ipa_get_ep_mapping(client);
-		if (ipa_ep_idx == -1) {
-			IPAERR("Invalid client.\n");
-			res = -EINVAL;
-			continue;
-		}
-		ipa_ctx->resume_on_connect[client] = false;
-		if (ipa_ctx->ep[ipa_ep_idx].client == client &&
-		    ipa_should_pipe_be_suspended(client)) {
-			if (ipa_ctx->ep[ipa_ep_idx].valid) {
-				/* suspend endpoint */
-				memset(&suspend, 0, sizeof(suspend));
-				suspend.ipa_ep_suspend = true;
-				ipa_cfg_ep_ctrl(ipa_ep_idx, &suspend);
-				pipe_suspended = true;
-			}
-		}
-	}
-	/* Sleep ~1 msec */
-	if (pipe_suspended)
-		usleep_range(1000, 2000);
-
-	/* before gating IPA clocks do TAG process */
-	ipa_ctx->tag_process_before_gating = true;
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-
-/**
- * ipa_suspend_resource_no_block() - suspend client endpoints related to the
- * IPA_RM resource and decrement active clients counter. This function is
- * guaranteed to avoid sleeping.
- *
- * @resource: [IN] IPA Resource Manager resource
- *
- * Return codes: 0 on success, negative on failure.
- */
-int ipa_suspend_resource_no_block(enum ipa_rm_resource_name resource)
-{
-	int res;
-	struct ipa_client_names clients;
-	int index;
-	enum ipa_client_type client;
-	struct ipa_ep_cfg_ctrl suspend;
-	int ipa_ep_idx;
-	unsigned long flags;
-
-	if (ipa_active_clients_trylock(&flags) == 0)
-		return -EPERM;
-	if (ipa_ctx->ipa_active_clients.cnt == 1) {
-		res = -EPERM;
-		goto bail;
-	}
-
-	memset(&clients, 0, sizeof(clients));
-	res = ipa_get_clients_from_rm_resource(resource, &clients);
-	if (res) {
-		IPAERR("ipa_get_clients_from_rm_resource() failed, name = %d.\n"
-		       , resource);
-		goto bail;
-	}
-
-	for (index = 0; index < clients.length; index++) {
-		client = clients.names[index];
-		ipa_ep_idx = ipa_get_ep_mapping(client);
-		if (ipa_ep_idx == -1) {
-			IPAERR("Invalid client.\n");
-			res = -EINVAL;
-			continue;
-		}
-		ipa_ctx->resume_on_connect[client] = false;
-		if (ipa_ctx->ep[ipa_ep_idx].client == client &&
-		    ipa_should_pipe_be_suspended(client)) {
-			if (ipa_ctx->ep[ipa_ep_idx].valid) {
-				/* suspend endpoint */
-				memset(&suspend, 0, sizeof(suspend));
-				suspend.ipa_ep_suspend = true;
-				ipa_cfg_ep_ctrl(ipa_ep_idx, &suspend);
-			}
-		}
-	}
-
-	if (res == 0) {
-		ipa_ctx->ipa_active_clients.cnt--;
-		IPADBG("active clients = %d\n",
-		       ipa_ctx->ipa_active_clients.cnt);
-	}
-bail:
-	ipa_active_clients_trylock_unlock(&flags);
-
-	return res;
-}
-
-/**
- * ipa_resume_resource() - resume client endpoints related to the IPA_RM
- * resource.
- *
- * @resource: [IN] IPA Resource Manager resource
- *
- * Return codes: 0 on success, negative on failure.
- */
-int ipa_resume_resource(enum ipa_rm_resource_name resource)
-{
-
-	struct ipa_client_names clients;
-	int res;
-	int index;
-	struct ipa_ep_cfg_ctrl suspend;
-	enum ipa_client_type client;
-	int ipa_ep_idx;
-
-	memset(&clients, 0, sizeof(clients));
-	res = ipa_get_clients_from_rm_resource(resource, &clients);
-	if (res) {
-		IPAERR("ipa_get_clients_from_rm_resource() failed.\n");
-		return res;
-	}
-
-	for (index = 0; index < clients.length; index++) {
-		client = clients.names[index];
-		ipa_ep_idx = ipa_get_ep_mapping(client);
-		if (ipa_ep_idx == -1) {
-			IPAERR("Invalid client.\n");
-			res = -EINVAL;
-			continue;
-		}
-		/*
-		 * The related ep, will be resumed on connect
-		 * while its resource is granted
-		 */
-		ipa_ctx->resume_on_connect[client] = true;
-		IPADBG("%d will be resumed on connect.\n", client);
-		if (ipa_ctx->ep[ipa_ep_idx].client == client &&
-		    ipa_should_pipe_be_suspended(client)) {
-			if (ipa_ctx->ep[ipa_ep_idx].valid) {
-				memset(&suspend, 0, sizeof(suspend));
-				suspend.ipa_ep_suspend = false;
-				ipa_cfg_ep_ctrl(ipa_ep_idx, &suspend);
-			}
-		}
-	}
-
-	return res;
-}
-
-/* read how much SRAM is available for SW use
- * In case of IPAv2.0 this will also supply an offset from
- * which we can start write
- */
-void _ipa_sram_settings_read_v1_1(void)
-{
-	ipa_ctx->smem_restricted_bytes = 0;
-	ipa_ctx->smem_sz = ipa_read_reg(ipa_ctx->mmio,
-			IPA_SHARED_MEM_SIZE_OFST_v1_1);
-	ipa_ctx->smem_reqd_sz = IPA_MEM_v1_RAM_END_OFST;
-	ipa_ctx->hdr_tbl_lcl = 1;
-	ipa_ctx->ip4_rt_tbl_lcl = 0;
-	ipa_ctx->ip6_rt_tbl_lcl = 0;
-	ipa_ctx->ip4_flt_tbl_lcl = 1;
-	ipa_ctx->ip6_flt_tbl_lcl = 1;
-}
-
-void _ipa_sram_settings_read_v2_0(void)
-{
-	ipa_ctx->smem_restricted_bytes = ipa_read_reg_field(ipa_ctx->mmio,
-			IPA_SHARED_MEM_SIZE_OFST_v2_0,
-			IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_BMSK_v2_0,
-			IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_SHFT_v2_0);
-	ipa_ctx->smem_sz = ipa_read_reg_field(ipa_ctx->mmio,
-			IPA_SHARED_MEM_SIZE_OFST_v2_0,
-			IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_BMSK_v2_0,
-			IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_SHFT_v2_0);
-	ipa_ctx->smem_reqd_sz = IPA_MEM_PART(end_ofst);
-	ipa_ctx->hdr_tbl_lcl = 0;
-	ipa_ctx->ip4_rt_tbl_lcl = 0;
-	ipa_ctx->ip6_rt_tbl_lcl = 0;
-	ipa_ctx->ip4_flt_tbl_lcl = 0;
-	ipa_ctx->ip6_flt_tbl_lcl = 0;
-}
-
-void _ipa_sram_settings_read_v2_5(void)
-{
-	ipa_ctx->smem_restricted_bytes = ipa_read_reg_field(ipa_ctx->mmio,
-		IPA_SHARED_MEM_SIZE_OFST_v2_0,
-		IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_BMSK_v2_0,
-		IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_SHFT_v2_0);
-	ipa_ctx->smem_sz = ipa_read_reg_field(ipa_ctx->mmio,
-		IPA_SHARED_MEM_SIZE_OFST_v2_0,
-		IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_BMSK_v2_0,
-		IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_SHFT_v2_0);
-	ipa_ctx->smem_reqd_sz = IPA_MEM_PART(end_ofst);
-	ipa_ctx->hdr_tbl_lcl = 0;
-	ipa_ctx->hdr_proc_ctx_tbl_lcl = 1;
-
-	/*
-	 * when proc ctx table is located in internal memory,
-	 * modem entries resides first.
-	 */
-	if (ipa_ctx->hdr_proc_ctx_tbl_lcl) {
-		ipa_ctx->hdr_proc_ctx_tbl.start_offset =
-			IPA_MEM_PART(modem_hdr_proc_ctx_size);
-	}
-	ipa_ctx->ip4_rt_tbl_lcl = 0;
-	ipa_ctx->ip6_rt_tbl_lcl = 0;
-	ipa_ctx->ip4_flt_tbl_lcl = 0;
-	ipa_ctx->ip6_flt_tbl_lcl = 0;
-}
-
-void _ipa_sram_settings_read_v2_6L(void)
-{
-	ipa_ctx->smem_restricted_bytes = ipa_read_reg_field(ipa_ctx->mmio,
-		IPA_SHARED_MEM_SIZE_OFST_v2_0,
-		IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_BMSK_v2_0,
-		IPA_SHARED_MEM_SIZE_SHARED_MEM_BADDR_SHFT_v2_0);
-	ipa_ctx->smem_sz = ipa_read_reg_field(ipa_ctx->mmio,
-		IPA_SHARED_MEM_SIZE_OFST_v2_0,
-		IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_BMSK_v2_0,
-		IPA_SHARED_MEM_SIZE_SHARED_MEM_SIZE_SHFT_v2_0);
-	ipa_ctx->smem_reqd_sz = IPA_MEM_PART(end_ofst);
-	ipa_ctx->hdr_tbl_lcl = 0;
-	ipa_ctx->ip4_rt_tbl_lcl = 0;
-	ipa_ctx->ip6_rt_tbl_lcl = 0;
-	ipa_ctx->ip4_flt_tbl_lcl = 0;
-	ipa_ctx->ip6_flt_tbl_lcl = 0;
-}
-
-void _ipa_cfg_route_v1_1(struct ipa_route *route)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, route->route_dis,
-			IPA_ROUTE_ROUTE_DIS_SHFT,
-			IPA_ROUTE_ROUTE_DIS_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, route->route_def_pipe,
-			IPA_ROUTE_ROUTE_DEF_PIPE_SHFT,
-			IPA_ROUTE_ROUTE_DEF_PIPE_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, route->route_def_hdr_table,
-			IPA_ROUTE_ROUTE_DEF_HDR_TABLE_SHFT,
-			IPA_ROUTE_ROUTE_DEF_HDR_TABLE_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, route->route_def_hdr_ofst,
-			IPA_ROUTE_ROUTE_DEF_HDR_OFST_SHFT,
-			IPA_ROUTE_ROUTE_DEF_HDR_OFST_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio, IPA_ROUTE_OFST_v1_1, reg_val);
-}
-
-void _ipa_cfg_route_v2_0(struct ipa_route *route)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, route->route_dis,
-			IPA_ROUTE_ROUTE_DIS_SHFT,
-			IPA_ROUTE_ROUTE_DIS_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, route->route_def_pipe,
-			IPA_ROUTE_ROUTE_DEF_PIPE_SHFT,
-			IPA_ROUTE_ROUTE_DEF_PIPE_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, route->route_def_hdr_table,
-			IPA_ROUTE_ROUTE_DEF_HDR_TABLE_SHFT,
-			IPA_ROUTE_ROUTE_DEF_HDR_TABLE_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, route->route_def_hdr_ofst,
-			IPA_ROUTE_ROUTE_DEF_HDR_OFST_SHFT,
-			IPA_ROUTE_ROUTE_DEF_HDR_OFST_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, route->route_frag_def_pipe,
-			IPA_ROUTE_ROUTE_FRAG_DEF_PIPE_SHFT,
-			IPA_ROUTE_ROUTE_FRAG_DEF_PIPE_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio, IPA_ROUTE_OFST_v1_1, reg_val);
-}
-
-/**
- * ipa_cfg_route() - configure IPA route
- * @route: IPA route
- *
- * Return codes:
- * 0: success
- */
-int ipa_cfg_route(struct ipa_route *route)
-{
-
-	IPADBG("disable_route_block=%d, default_pipe=%d, default_hdr_tbl=%d\n",
-		route->route_dis,
-		route->route_def_pipe,
-		route->route_def_hdr_table);
-	IPADBG("default_hdr_ofst=%d, default_frag_pipe=%d\n",
-		route->route_def_hdr_ofst,
-		route->route_frag_def_pipe);
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_route(route);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-
-/**
- * ipa_cfg_filter() - configure filter
- * @disable: disable value
- *
- * Return codes:
- * 0: success
- */
-int ipa_cfg_filter(u32 disable)
-{
-	u32 ipa_filter_ofst = IPA_FILTER_OFST_v1_1;
-
-	ipa_inc_client_enable_clks();
-	ipa_write_reg(ipa_ctx->mmio, ipa_filter_ofst,
-			IPA_SETFIELD(!disable,
-					IPA_FILTER_FILTER_EN_SHFT,
-					IPA_FILTER_FILTER_EN_BMSK));
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-
-/**
- * ipa_init_hw() - initialize HW
- *
- * Return codes:
- * 0: success
- */
-int ipa_init_hw(void)
-{
-	u32 ipa_version = 0;
-
-	/* do soft reset of IPA */
-	ipa_write_reg(ipa_ctx->mmio, IPA_COMP_SW_RESET_OFST, 1);
-	ipa_write_reg(ipa_ctx->mmio, IPA_COMP_SW_RESET_OFST, 0);
-
-	/* enable IPA */
-	ipa_write_reg(ipa_ctx->mmio, IPA_COMP_CFG_OFST, 1);
-
-	/* Read IPA version and make sure we have access to the registers */
-	ipa_version = ipa_read_reg(ipa_ctx->mmio, IPA_VERSION_OFST);
-	if (ipa_version == 0)
-		return -EFAULT;
-
-	if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_5) {
-		/* set ipa_bcr to 0xFFFFFFFF for using new IPA behavior */
-		ipa_write_reg(ipa_ctx->mmio, IPA_BCR_OFST, IPA_BCR_REG_VAL);
-	}
-	return 0;
-}
-
-/**
- * ipa_get_ep_mapping() - provide endpoint mapping
- * @client: client type
- *
- * Return value: endpoint mapping or -1 if unmapped/invalid
- */
-int ipa_get_ep_mapping(enum ipa_client_type client)
-{
-	u8 hw_type_index = IPA_1_1;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return INVALID_EP_MAPPING_INDEX;
-	}
-
-	if (client >= IPA_CLIENT_MAX || client < 0) {
-		IPAERR("Bad client number! client =%d\n", client);
-		return INVALID_EP_MAPPING_INDEX;
-	}
-
-	switch (ipa_ctx->ipa_hw_type) {
-	case IPA_HW_v2_0:
-	case IPA_HW_v2_5:
-		hw_type_index = IPA_2_0;
-		break;
-	case IPA_HW_v2_6L:
-		hw_type_index = IPA_2_6L;
-		break;
-	default:
-		hw_type_index = IPA_1_1;
-		break;
-	}
-
-	return ep_mapping[hw_type_index][client];
-}
-EXPORT_SYMBOL(ipa_get_ep_mapping);
-
-/* ipa_set_client() - provide client mapping
- * @client: client type
- *
- * Return value: none
- */
-void ipa_set_client(int index, enum ipacm_client_enum client, bool uplink)
-{
-	if (client >= IPACM_CLIENT_MAX || client < IPACM_CLIENT_USB) {
-		IPAERR("Bad client number! client =%d\n", client);
-	} else if (index >= IPA_MAX_NUM_PIPES || index < 0) {
-		IPAERR("Bad pipe index! index =%d\n", index);
-	} else {
-		ipa_ctx->ipacm_client[index].client_enum = client;
-		ipa_ctx->ipacm_client[index].uplink = uplink;
-	}
-}
-EXPORT_SYMBOL(ipa_set_client);
-
-/**
- * ipa_get_client() - provide client mapping
- * @client: client type
- *
- * Return value: none
- */
-enum ipacm_client_enum ipa_get_client(int pipe_idx)
-{
-	if (pipe_idx >= IPA_MAX_NUM_PIPES || pipe_idx < 0) {
-		IPAERR("Bad pipe index! pipe_idx =%d\n", pipe_idx);
-		return IPACM_CLIENT_MAX;
-	} else {
-		return ipa_ctx->ipacm_client[pipe_idx].client_enum;
-	}
-}
-EXPORT_SYMBOL(ipa_get_client);
-
-/**
- * ipa_get_client_uplink() - provide client mapping
- * @client: client type
- *
- * Return value: none
- */
-bool ipa_get_client_uplink(int pipe_idx)
-{
-	return ipa_ctx->ipacm_client[pipe_idx].uplink;
-}
-EXPORT_SYMBOL(ipa_get_client_uplink);
-
-/**
- * ipa_get_rm_resource_from_ep() - get the IPA_RM resource which is related to
- * the supplied pipe index.
- *
- * @pipe_idx:
- *
- * Return value: IPA_RM resource related to the pipe, -1 if a resource was not
- * found.
- */
-enum ipa_rm_resource_name ipa_get_rm_resource_from_ep(int pipe_idx)
-{
-	int i;
-	int j;
-	enum ipa_client_type client;
-	struct ipa_client_names clients;
-	bool found = false;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (pipe_idx >= ipa_ctx->ipa_num_pipes || pipe_idx < 0) {
-		IPAERR("Bad pipe index!\n");
-		return -EINVAL;
-	}
-
-	client = ipa_ctx->ep[pipe_idx].client;
-
-	for (i = 0; i < IPA_RM_RESOURCE_MAX; i++) {
-		memset(&clients, 0, sizeof(clients));
-		ipa_get_clients_from_rm_resource(i, &clients);
-		for (j = 0; j < clients.length; j++) {
-			if (clients.names[j] == client) {
-				found = true;
-				break;
-			}
-		}
-		if (found)
-			break;
-	}
-
-	if (!found)
-		return -EFAULT;
-
-	return i;
-}
-
-/**
- * ipa_get_client_mapping() - provide client mapping
- * @pipe_idx: IPA end-point number
- *
- * Return value: client mapping
- */
-enum ipa_client_type ipa_get_client_mapping(int pipe_idx)
-{
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (pipe_idx >= ipa_ctx->ipa_num_pipes || pipe_idx < 0) {
-		IPAERR("Bad pipe index!\n");
-		return -EINVAL;
-	}
-
-	return ipa_ctx->ep[pipe_idx].client;
-}
-
-/**
- * ipa_write_32() - convert 32 bit value to byte array
- * @w: 32 bit integer
- * @dest: byte array
- *
- * Return value: converted value
- */
-u8 *ipa_write_32(u32 w, u8 *dest)
-{
-	*dest++ = (u8)((w) & 0xFF);
-	*dest++ = (u8)((w >> 8) & 0xFF);
-	*dest++ = (u8)((w >> 16) & 0xFF);
-	*dest++ = (u8)((w >> 24) & 0xFF);
-
-	return dest;
-}
-
-/**
- * ipa_write_16() - convert 16 bit value to byte array
- * @hw: 16 bit integer
- * @dest: byte array
- *
- * Return value: converted value
- */
-u8 *ipa_write_16(u16 hw, u8 *dest)
-{
-	*dest++ = (u8)((hw) & 0xFF);
-	*dest++ = (u8)((hw >> 8) & 0xFF);
-
-	return dest;
-}
-
-/**
- * ipa_write_8() - convert 8 bit value to byte array
- * @hw: 8 bit integer
- * @dest: byte array
- *
- * Return value: converted value
- */
-u8 *ipa_write_8(u8 b, u8 *dest)
-{
-	*dest++ = (b) & 0xFF;
-
-	return dest;
-}
-
-/**
- * ipa_pad_to_32() - pad byte array to 32 bit value
- * @dest: byte array
- *
- * Return value: padded value
- */
-u8 *ipa_pad_to_32(u8 *dest)
-{
-	int i = (long)dest & 0x3;
-	int j;
-
-	if (i)
-		for (j = 0; j < (4 - i); j++)
-			*dest++ = 0;
-
-	return dest;
-}
-
-void ipa_generate_mac_addr_hw_rule(u8 **buf, u8 hdr_mac_addr_offset,
-	const uint8_t mac_addr_mask[ETH_ALEN],
-	const uint8_t mac_addr[ETH_ALEN])
-{
-	*buf = ipa_write_8(hdr_mac_addr_offset, *buf);
-
-	/* MAC addr mask copied as little endian each 4 bytes */
-	*buf = ipa_write_8(mac_addr_mask[3], *buf);
-	*buf = ipa_write_8(mac_addr_mask[2], *buf);
-	*buf = ipa_write_8(mac_addr_mask[1], *buf);
-	*buf = ipa_write_8(mac_addr_mask[0], *buf);
-	*buf = ipa_write_16(0, *buf);
-	*buf = ipa_write_8(mac_addr_mask[5], *buf);
-	*buf = ipa_write_8(mac_addr_mask[4], *buf);
-	*buf = ipa_write_32(0, *buf);
-	*buf = ipa_write_32(0, *buf);
-
-	/* MAC addr copied as little endian each 4 bytes */
-	*buf = ipa_write_8(mac_addr[3], *buf);
-	*buf = ipa_write_8(mac_addr[2], *buf);
-	*buf = ipa_write_8(mac_addr[1], *buf);
-	*buf = ipa_write_8(mac_addr[0], *buf);
-	*buf = ipa_write_16(0, *buf);
-	*buf = ipa_write_8(mac_addr[5], *buf);
-	*buf = ipa_write_8(mac_addr[4], *buf);
-	*buf = ipa_write_32(0, *buf);
-	*buf = ipa_write_32(0, *buf);
-	*buf = ipa_pad_to_32(*buf);
-}
-
-/**
- * ipa_generate_hw_rule() - generate HW rule
- * @ip: IP address type
- * @attrib: IPA rule attribute
- * @buf: output buffer
- * @en_rule: rule
- *
- * Return codes:
- * 0: success
- * -EPERM: wrong input
- */
-int ipa_generate_hw_rule(enum ipa_ip_type ip,
-	const struct ipa_rule_attrib *attrib, u8 **buf, u16 *en_rule)
-{
-	u8 ofst_meq32 = 0;
-	u8 ihl_ofst_rng16 = 0;
-	u8 ihl_ofst_meq32 = 0;
-	u8 ofst_meq128 = 0;
-
-	if (ip == IPA_IP_v4) {
-
-		/* error check */
-		if (attrib->attrib_mask & IPA_FLT_NEXT_HDR ||
-		    attrib->attrib_mask & IPA_FLT_TC || attrib->attrib_mask &
-		    IPA_FLT_FLOW_LABEL) {
-			IPAERR("v6 attrib's specified for v4 rule\n");
-			return -EPERM;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TOS) {
-			*en_rule |= IPA_TOS_EQ;
-			*buf = ipa_write_8(attrib->u.v4.tos, *buf);
-			*buf = ipa_pad_to_32(*buf);
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TOS_MASKED) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			/* 0 => offset of TOS in v4 header */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_32((attrib->tos_mask << 16), *buf);
-			*buf = ipa_write_32((attrib->tos_value << 16), *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_PROTOCOL) {
-			*en_rule |= IPA_PROTOCOL_EQ;
-			*buf = ipa_write_8(attrib->u.v4.protocol, *buf);
-			*buf = ipa_pad_to_32(*buf);
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_ADDR) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			/* 12 => offset of src ip in v4 header */
-			*buf = ipa_write_8(12, *buf);
-			*buf = ipa_write_32(attrib->u.v4.src_addr_mask, *buf);
-			*buf = ipa_write_32(attrib->u.v4.src_addr, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_ADDR) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			/* 16 => offset of dst ip in v4 header */
-			*buf = ipa_write_8(16, *buf);
-			*buf = ipa_write_32(attrib->u.v4.dst_addr_mask, *buf);
-			*buf = ipa_write_32(attrib->u.v4.dst_addr, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_ETHER_TYPE) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			/* -2 => offset of ether type in L2 hdr */
-			*buf = ipa_write_8((u8)-2, *buf);
-			*buf = ipa_write_16(0, *buf);
-			*buf = ipa_write_16(htons(attrib->ether_type), *buf);
-			*buf = ipa_write_16(0, *buf);
-			*buf = ipa_write_16(htons(attrib->ether_type), *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_PORT_RANGE) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			if (attrib->src_port_hi < attrib->src_port_lo) {
-				IPAERR("bad src port range param\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			/* 0  => offset of src port after v4 header */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_16(attrib->src_port_hi, *buf);
-			*buf = ipa_write_16(attrib->src_port_lo, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_PORT_RANGE) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			if (attrib->dst_port_hi < attrib->dst_port_lo) {
-				IPAERR("bad dst port range param\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			/* 2  => offset of dst port after v4 header */
-			*buf = ipa_write_8(2, *buf);
-			*buf = ipa_write_16(attrib->dst_port_hi, *buf);
-			*buf = ipa_write_16(attrib->dst_port_lo, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TYPE) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			/* 0  => offset of type after v4 header */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_32(0xFF, *buf);
-			*buf = ipa_write_32(attrib->type, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_CODE) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			/* 1  => offset of code after v4 header */
-			*buf = ipa_write_8(1, *buf);
-			*buf = ipa_write_32(0xFF, *buf);
-			*buf = ipa_write_32(attrib->code, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SPI) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			/* 0  => offset of SPI after v4 header FIXME */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_32(0xFFFFFFFF, *buf);
-			*buf = ipa_write_32(attrib->spi, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_PORT) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			/* 0  => offset of src port after v4 header */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_16(attrib->src_port, *buf);
-			*buf = ipa_write_16(attrib->src_port, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_PORT) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			/* 2  => offset of dst port after v4 header */
-			*buf = ipa_write_8(2, *buf);
-			*buf = ipa_write_16(attrib->dst_port, *buf);
-			*buf = ipa_write_16(attrib->dst_port, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_ETHER_II) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -14 => offset of dst mac addr in Ethernet II hdr */
-			ipa_generate_mac_addr_hw_rule(
-				buf,
-				-14,
-				attrib->dst_mac_addr_mask,
-				attrib->dst_mac_addr);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_ETHER_II) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -8 => offset of src mac addr in Ethernet II hdr */
-			ipa_generate_mac_addr_hw_rule(
-				buf,
-				-8,
-				attrib->src_mac_addr_mask,
-				attrib->src_mac_addr);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_802_3) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -22 => offset of dst mac addr in 802.3 hdr */
-			ipa_generate_mac_addr_hw_rule(
-				buf,
-				-22,
-				attrib->dst_mac_addr_mask,
-				attrib->dst_mac_addr);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_802_3) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -16 => offset of src mac addr in 802.3 hdr */
-			ipa_generate_mac_addr_hw_rule(
-				buf,
-				-16,
-				attrib->src_mac_addr_mask,
-				attrib->src_mac_addr);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_META_DATA) {
-			*en_rule |= IPA_METADATA_COMPARE;
-			*buf = ipa_write_8(0, *buf);    /* offset, reserved */
-			*buf = ipa_write_32(attrib->meta_data_mask, *buf);
-			*buf = ipa_write_32(attrib->meta_data, *buf);
-			*buf = ipa_pad_to_32(*buf);
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_FRAGMENT) {
-			*en_rule |= IPA_IS_FRAG;
-			*buf = ipa_pad_to_32(*buf);
-		}
-	} else if (ip == IPA_IP_v6) {
-
-		/* v6 code below assumes no extension headers TODO: fix this */
-
-		/* error check */
-		if (attrib->attrib_mask & IPA_FLT_TOS ||
-		    attrib->attrib_mask & IPA_FLT_PROTOCOL) {
-			IPAERR("v4 attrib's specified for v6 rule\n");
-			return -EPERM;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_NEXT_HDR) {
-			*en_rule |= IPA_PROTOCOL_EQ;
-			*buf = ipa_write_8(attrib->u.v6.next_hdr, *buf);
-			*buf = ipa_pad_to_32(*buf);
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_ETHER_TYPE) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			/* -2 => offset of ether type in L2 hdr */
-			*buf = ipa_write_8((u8)-2, *buf);
-			*buf = ipa_write_16(0, *buf);
-			*buf = ipa_write_16(htons(attrib->ether_type), *buf);
-			*buf = ipa_write_16(0, *buf);
-			*buf = ipa_write_16(htons(attrib->ether_type), *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TYPE) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			/* 0  => offset of type after v6 header */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_32(0xFF, *buf);
-			*buf = ipa_write_32(attrib->type, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_CODE) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			/* 1  => offset of code after v6 header */
-			*buf = ipa_write_8(1, *buf);
-			*buf = ipa_write_32(0xFF, *buf);
-			*buf = ipa_write_32(attrib->code, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SPI) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			/* 0  => offset of SPI after v6 header FIXME */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_32(0xFFFFFFFF, *buf);
-			*buf = ipa_write_32(attrib->spi, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_PORT) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			/* 0  => offset of src port after v6 header */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_16(attrib->src_port, *buf);
-			*buf = ipa_write_16(attrib->src_port, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_PORT) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			/* 2  => offset of dst port after v6 header */
-			*buf = ipa_write_8(2, *buf);
-			*buf = ipa_write_16(attrib->dst_port, *buf);
-			*buf = ipa_write_16(attrib->dst_port, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_PORT_RANGE) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			if (attrib->src_port_hi < attrib->src_port_lo) {
-				IPAERR("bad src port range param\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			/* 0  => offset of src port after v6 header */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_16(attrib->src_port_hi, *buf);
-			*buf = ipa_write_16(attrib->src_port_lo, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_PORT_RANGE) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			if (attrib->dst_port_hi < attrib->dst_port_lo) {
-				IPAERR("bad dst port range param\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			/* 2  => offset of dst port after v6 header */
-			*buf = ipa_write_8(2, *buf);
-			*buf = ipa_write_16(attrib->dst_port_hi, *buf);
-			*buf = ipa_write_16(attrib->dst_port_lo, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_ADDR) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-			/* 8 => offset of src ip in v6 header */
-			*buf = ipa_write_8(8, *buf);
-			*buf = ipa_write_32(attrib->u.v6.src_addr_mask[0],
-					*buf);
-			*buf = ipa_write_32(attrib->u.v6.src_addr_mask[1],
-					*buf);
-			*buf = ipa_write_32(attrib->u.v6.src_addr_mask[2],
-					*buf);
-			*buf = ipa_write_32(attrib->u.v6.src_addr_mask[3],
-					*buf);
-			*buf = ipa_write_32(attrib->u.v6.src_addr[0], *buf);
-			*buf = ipa_write_32(attrib->u.v6.src_addr[1], *buf);
-			*buf = ipa_write_32(attrib->u.v6.src_addr[2], *buf);
-			*buf = ipa_write_32(attrib->u.v6.src_addr[3], *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_ADDR) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-			/* 24 => offset of dst ip in v6 header */
-			*buf = ipa_write_8(24, *buf);
-			*buf = ipa_write_32(attrib->u.v6.dst_addr_mask[0],
-					*buf);
-			*buf = ipa_write_32(attrib->u.v6.dst_addr_mask[1],
-					*buf);
-			*buf = ipa_write_32(attrib->u.v6.dst_addr_mask[2],
-					*buf);
-			*buf = ipa_write_32(attrib->u.v6.dst_addr_mask[3],
-					*buf);
-			*buf = ipa_write_32(attrib->u.v6.dst_addr[0], *buf);
-			*buf = ipa_write_32(attrib->u.v6.dst_addr[1], *buf);
-			*buf = ipa_write_32(attrib->u.v6.dst_addr[2], *buf);
-			*buf = ipa_write_32(attrib->u.v6.dst_addr[3], *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TC) {
-			*en_rule |= IPA_FLT_TC;
-			*buf = ipa_write_8(attrib->u.v6.tc, *buf);
-			*buf = ipa_pad_to_32(*buf);
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TOS_MASKED) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-			/* 0 => offset of TOS in v6 header */
-			*buf = ipa_write_8(0, *buf);
-			*buf = ipa_write_32((attrib->tos_mask << 20), *buf);
-			*buf = ipa_write_32(0, *buf);
-			*buf = ipa_write_32(0, *buf);
-			*buf = ipa_write_32(0, *buf);
-
-			*buf = ipa_write_32((attrib->tos_value << 20), *buf);
-			*buf = ipa_write_32(0, *buf);
-			*buf = ipa_write_32(0, *buf);
-			*buf = ipa_write_32(0, *buf);
-			*buf = ipa_pad_to_32(*buf);
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_ETHER_II) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -14 => offset of dst mac addr in Ethernet II hdr */
-			ipa_generate_mac_addr_hw_rule(
-				buf,
-				-14,
-				attrib->dst_mac_addr_mask,
-				attrib->dst_mac_addr);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_ETHER_II) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -8 => offset of src mac addr in Ethernet II hdr */
-			ipa_generate_mac_addr_hw_rule(
-				buf,
-				-8,
-				attrib->src_mac_addr_mask,
-				attrib->src_mac_addr);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_802_3) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -22 => offset of dst mac addr in 802.3 hdr */
-			ipa_generate_mac_addr_hw_rule(
-				buf,
-				-22,
-				attrib->dst_mac_addr_mask,
-				attrib->dst_mac_addr);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_802_3) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -16 => offset of src mac addr in 802.3 hdr */
-			ipa_generate_mac_addr_hw_rule(
-				buf,
-				-16,
-				attrib->src_mac_addr_mask,
-				attrib->src_mac_addr);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_FLOW_LABEL) {
-			*en_rule |= IPA_FLT_FLOW_LABEL;
-			 /* FIXME FL is only 20 bits */
-			*buf = ipa_write_32(attrib->u.v6.flow_label, *buf);
-			*buf = ipa_pad_to_32(*buf);
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_META_DATA) {
-			*en_rule |= IPA_METADATA_COMPARE;
-			*buf = ipa_write_8(0, *buf);    /* offset, reserved */
-			*buf = ipa_write_32(attrib->meta_data_mask, *buf);
-			*buf = ipa_write_32(attrib->meta_data, *buf);
-			*buf = ipa_pad_to_32(*buf);
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_FRAGMENT) {
-			*en_rule |= IPA_IS_FRAG;
-			*buf = ipa_pad_to_32(*buf);
-		}
-	} else {
-		IPAERR("unsupported ip %d\n", ip);
-		return -EPERM;
-	}
-
-	/*
-	 * default "rule" means no attributes set -> map to
-	 * OFFSET_MEQ32_0 with mask of 0 and val of 0 and offset 0
-	 */
-	if (attrib->attrib_mask == 0) {
-		if (ipa_ofst_meq32[ofst_meq32] == -1) {
-			IPAERR("ran out of meq32 eq\n");
-			return -EPERM;
-		}
-		*en_rule |= ipa_ofst_meq32[ofst_meq32];
-		*buf = ipa_write_8(0, *buf);    /* offset */
-		*buf = ipa_write_32(0, *buf);   /* mask */
-		*buf = ipa_write_32(0, *buf);   /* val */
-		*buf = ipa_pad_to_32(*buf);
-		ofst_meq32++;
-	}
-
-	return 0;
-}
-
-void ipa_generate_flt_mac_addr_eq(struct ipa_ipfltri_rule_eq *eq_atrb,
-	u8 hdr_mac_addr_offset,	const uint8_t mac_addr_mask[ETH_ALEN],
-	const uint8_t mac_addr[ETH_ALEN], u8 ofst_meq128)
-{
-	eq_atrb->offset_meq_128[ofst_meq128].offset = hdr_mac_addr_offset;
-	eq_atrb->offset_meq_128[ofst_meq128].mask[0] = mac_addr_mask[3];
-	eq_atrb->offset_meq_128[ofst_meq128].mask[1] = mac_addr_mask[2];
-	eq_atrb->offset_meq_128[ofst_meq128].mask[2] = mac_addr_mask[1];
-	eq_atrb->offset_meq_128[ofst_meq128].mask[3] = mac_addr_mask[0];
-	eq_atrb->offset_meq_128[ofst_meq128].mask[4] = 0;
-	eq_atrb->offset_meq_128[ofst_meq128].mask[5] = 0;
-	eq_atrb->offset_meq_128[ofst_meq128].mask[6] = mac_addr_mask[5];
-	eq_atrb->offset_meq_128[ofst_meq128].mask[7] = mac_addr_mask[4];
-	memset(eq_atrb->offset_meq_128[ofst_meq128].mask + 8, 0, 8);
-	eq_atrb->offset_meq_128[ofst_meq128].value[0] =	mac_addr[3];
-	eq_atrb->offset_meq_128[ofst_meq128].value[1] =	mac_addr[2];
-	eq_atrb->offset_meq_128[ofst_meq128].value[2] =	mac_addr[1];
-	eq_atrb->offset_meq_128[ofst_meq128].value[3] =	mac_addr[0];
-	eq_atrb->offset_meq_128[ofst_meq128].value[4] = 0;
-	eq_atrb->offset_meq_128[ofst_meq128].value[5] = 0;
-	eq_atrb->offset_meq_128[ofst_meq128].value[6] =	mac_addr[5];
-	eq_atrb->offset_meq_128[ofst_meq128].value[7] =	mac_addr[4];
-	memset(eq_atrb->offset_meq_128[ofst_meq128].value + 8, 0, 8);
-}
-
-int ipa_generate_flt_eq(enum ipa_ip_type ip,
-		const struct ipa_rule_attrib *attrib,
-		struct ipa_ipfltri_rule_eq *eq_atrb)
-{
-	u8 ofst_meq32 = 0;
-	u8 ihl_ofst_rng16 = 0;
-	u8 ihl_ofst_meq32 = 0;
-	u8 ofst_meq128 = 0;
-	u16 eq_bitmap = 0;
-	u16 *en_rule = &eq_bitmap;
-
-	if (ip == IPA_IP_v4) {
-
-		/* error check */
-		if (attrib->attrib_mask & IPA_FLT_NEXT_HDR ||
-		    attrib->attrib_mask & IPA_FLT_TC || attrib->attrib_mask &
-		    IPA_FLT_FLOW_LABEL) {
-			IPAERR("v6 attrib's specified for v4 rule\n");
-			return -EPERM;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TOS) {
-			*en_rule |= IPA_TOS_EQ;
-			eq_atrb->tos_eq_present = 1;
-			eq_atrb->tos_eq = attrib->u.v4.tos;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TOS_MASKED) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			eq_atrb->offset_meq_32[ofst_meq32].offset = 0;
-			eq_atrb->offset_meq_32[ofst_meq32].mask =
-				attrib->tos_mask << 16;
-			eq_atrb->offset_meq_32[ofst_meq32].value =
-				attrib->tos_value << 16;
-			ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_PROTOCOL) {
-			*en_rule |= IPA_PROTOCOL_EQ;
-			eq_atrb->protocol_eq_present = 1;
-			eq_atrb->protocol_eq = attrib->u.v4.protocol;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_ADDR) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			eq_atrb->offset_meq_32[ofst_meq32].offset = 12;
-			eq_atrb->offset_meq_32[ofst_meq32].mask =
-				attrib->u.v4.src_addr_mask;
-			eq_atrb->offset_meq_32[ofst_meq32].value =
-				attrib->u.v4.src_addr;
-			ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_ADDR) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			eq_atrb->offset_meq_32[ofst_meq32].offset = 16;
-			eq_atrb->offset_meq_32[ofst_meq32].mask =
-				attrib->u.v4.dst_addr_mask;
-			eq_atrb->offset_meq_32[ofst_meq32].value =
-				attrib->u.v4.dst_addr;
-			ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_PORT_RANGE) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			if (attrib->src_port_hi < attrib->src_port_lo) {
-				IPAERR("bad src port range param\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].offset = 0;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_low
-				= attrib->src_port_lo;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_high
-				= attrib->src_port_hi;
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_PORT_RANGE) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			if (attrib->dst_port_hi < attrib->dst_port_lo) {
-				IPAERR("bad dst port range param\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].offset = 2;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_low
-				= attrib->dst_port_lo;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_high
-				= attrib->dst_port_hi;
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TYPE) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].offset = 0;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].mask = 0xFF;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].value =
-				attrib->type;
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_CODE) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].offset = 1;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].mask = 0xFF;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].value =
-				attrib->code;
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SPI) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].offset = 0;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].mask =
-				0xFFFFFFFF;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].value =
-				attrib->spi;
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_PORT) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].offset = 0;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_low
-				= attrib->src_port;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_high
-				= attrib->src_port;
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_PORT) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].offset = 2;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_low
-				= attrib->dst_port;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_high
-				= attrib->dst_port;
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_META_DATA) {
-			*en_rule |= IPA_METADATA_COMPARE;
-			eq_atrb->metadata_meq32_present = 1;
-			eq_atrb->metadata_meq32.offset = 0;
-			eq_atrb->metadata_meq32.mask = attrib->meta_data_mask;
-			eq_atrb->metadata_meq32.value = attrib->meta_data;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_FRAGMENT) {
-			*en_rule |= IPA_IS_FRAG;
-			eq_atrb->ipv4_frag_eq_present = 1;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_ETHER_II) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -14 => offset of dst mac addr in Ethernet II hdr */
-			ipa_generate_flt_mac_addr_eq(eq_atrb, -14,
-				attrib->dst_mac_addr_mask, attrib->dst_mac_addr,
-				ofst_meq128);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_ETHER_II) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -8 => offset of src mac addr in Ethernet II hdr */
-			ipa_generate_flt_mac_addr_eq(eq_atrb, -8,
-				attrib->src_mac_addr_mask, attrib->src_mac_addr,
-				ofst_meq128);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_802_3) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -22 => offset of dst mac addr in 802.3 hdr */
-			ipa_generate_flt_mac_addr_eq(eq_atrb, -22,
-				attrib->dst_mac_addr_mask, attrib->dst_mac_addr,
-				ofst_meq128);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_802_3) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -16 => offset of src mac addr in 802.3 hdr */
-			ipa_generate_flt_mac_addr_eq(eq_atrb, -16,
-				attrib->src_mac_addr_mask, attrib->src_mac_addr,
-				ofst_meq128);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_ETHER_TYPE) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			eq_atrb->offset_meq_32[ofst_meq32].offset = -2;
-			eq_atrb->offset_meq_32[ofst_meq32].mask =
-				htons(attrib->ether_type);
-			eq_atrb->offset_meq_32[ofst_meq32].value =
-				htons(attrib->ether_type);
-			ofst_meq32++;
-		}
-	} else if (ip == IPA_IP_v6) {
-
-		/* v6 code below assumes no extension headers TODO: fix this */
-
-		/* error check */
-		if (attrib->attrib_mask & IPA_FLT_TOS ||
-		    attrib->attrib_mask & IPA_FLT_PROTOCOL) {
-			IPAERR("v4 attrib's specified for v6 rule\n");
-			return -EPERM;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_NEXT_HDR) {
-			*en_rule |= IPA_PROTOCOL_EQ;
-			eq_atrb->protocol_eq_present = 1;
-			eq_atrb->protocol_eq = attrib->u.v6.next_hdr;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TYPE) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].offset = 0;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].mask = 0xFF;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].value =
-				attrib->type;
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_CODE) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].offset = 1;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].mask = 0xFF;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].value =
-				attrib->code;
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SPI) {
-			if (ipa_ihl_ofst_meq32[ihl_ofst_meq32] == -1) {
-				IPAERR("ran out of ihl_meq32 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_meq32[ihl_ofst_meq32];
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].offset = 0;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].mask =
-				0xFFFFFFFF;
-			eq_atrb->ihl_offset_meq_32[ihl_ofst_meq32].value =
-				attrib->spi;
-			ihl_ofst_meq32++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_PORT) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].offset = 0;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_low
-				= attrib->src_port;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_high
-				= attrib->src_port;
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_PORT) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].offset = 2;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_low
-				= attrib->dst_port;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_high
-				= attrib->dst_port;
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_PORT_RANGE) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			if (attrib->src_port_hi < attrib->src_port_lo) {
-				IPAERR("bad src port range param\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].offset = 0;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_low
-				= attrib->src_port_lo;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_high
-				= attrib->src_port_hi;
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_PORT_RANGE) {
-			if (ipa_ihl_ofst_rng16[ihl_ofst_rng16] == -1) {
-				IPAERR("ran out of ihl_rng16 eq\n");
-				return -EPERM;
-			}
-			if (attrib->dst_port_hi < attrib->dst_port_lo) {
-				IPAERR("bad dst port range param\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ihl_ofst_rng16[ihl_ofst_rng16];
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].offset = 2;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_low
-				= attrib->dst_port_lo;
-			eq_atrb->ihl_offset_range_16[ihl_ofst_rng16].range_high
-				= attrib->dst_port_hi;
-			ihl_ofst_rng16++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_SRC_ADDR) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-			eq_atrb->offset_meq_128[ofst_meq128].offset = 8;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 0)
-				= attrib->u.v6.src_addr_mask[0];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 4)
-				= attrib->u.v6.src_addr_mask[1];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 8)
-				= attrib->u.v6.src_addr_mask[2];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 12)
-				= attrib->u.v6.src_addr_mask[3];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value + 0)
-				= attrib->u.v6.src_addr[0];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value + 4)
-				= attrib->u.v6.src_addr[1];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value + 8)
-				= attrib->u.v6.src_addr[2];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value +
-					12) = attrib->u.v6.src_addr[3];
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_DST_ADDR) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-			eq_atrb->offset_meq_128[ofst_meq128].offset = 24;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 0)
-				= attrib->u.v6.dst_addr_mask[0];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 4)
-				= attrib->u.v6.dst_addr_mask[1];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 8)
-				= attrib->u.v6.dst_addr_mask[2];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 12)
-				= attrib->u.v6.dst_addr_mask[3];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value + 0)
-				= attrib->u.v6.dst_addr[0];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value + 4)
-				= attrib->u.v6.dst_addr[1];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value + 8)
-				= attrib->u.v6.dst_addr[2];
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value +
-					12) = attrib->u.v6.dst_addr[3];
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TC) {
-			*en_rule |= IPA_FLT_TC;
-			eq_atrb->tc_eq_present = 1;
-			eq_atrb->tc_eq = attrib->u.v6.tc;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_TOS_MASKED) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-			eq_atrb->offset_meq_128[ofst_meq128].offset = 0;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 0)
-				= attrib->tos_mask << 20;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 4)
-				= 0;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 8)
-				= 0;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].mask + 12)
-				= 0;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value + 0)
-				= attrib->tos_value << 20;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value + 4)
-				= 0;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value + 8)
-				= 0;
-			*(u32 *)(eq_atrb->offset_meq_128[ofst_meq128].value +
-					12) = 0;
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_FLOW_LABEL) {
-			*en_rule |= IPA_FLT_FLOW_LABEL;
-			eq_atrb->fl_eq_present = 1;
-			eq_atrb->fl_eq = attrib->u.v6.flow_label;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_META_DATA) {
-			*en_rule |= IPA_METADATA_COMPARE;
-			eq_atrb->metadata_meq32_present = 1;
-			eq_atrb->metadata_meq32.offset = 0;
-			eq_atrb->metadata_meq32.mask = attrib->meta_data_mask;
-			eq_atrb->metadata_meq32.value = attrib->meta_data;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_FRAGMENT) {
-			*en_rule |= IPA_IS_FRAG;
-			eq_atrb->ipv4_frag_eq_present = 1;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_ETHER_II) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -14 => offset of dst mac addr in Ethernet II hdr */
-			ipa_generate_flt_mac_addr_eq(eq_atrb, -14,
-				attrib->dst_mac_addr_mask, attrib->dst_mac_addr,
-				ofst_meq128);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_ETHER_II) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -8 => offset of src mac addr in Ethernet II hdr */
-			ipa_generate_flt_mac_addr_eq(eq_atrb, -8,
-				attrib->src_mac_addr_mask, attrib->src_mac_addr,
-				ofst_meq128);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_DST_ADDR_802_3) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -22 => offset of dst mac addr in 802.3 hdr */
-			ipa_generate_flt_mac_addr_eq(eq_atrb, -22,
-				attrib->dst_mac_addr_mask, attrib->dst_mac_addr,
-				ofst_meq128);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_SRC_ADDR_802_3) {
-			if (ipa_ofst_meq128[ofst_meq128] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq128[ofst_meq128];
-
-			/* -16 => offset of src mac addr in 802.3 hdr */
-			ipa_generate_flt_mac_addr_eq(eq_atrb, -16,
-				attrib->src_mac_addr_mask, attrib->src_mac_addr,
-				ofst_meq128);
-
-			ofst_meq128++;
-		}
-
-		if (attrib->attrib_mask & IPA_FLT_MAC_ETHER_TYPE) {
-			if (ipa_ofst_meq32[ofst_meq32] == -1) {
-				IPAERR("ran out of meq128 eq\n");
-				return -EPERM;
-			}
-			*en_rule |= ipa_ofst_meq32[ofst_meq32];
-			eq_atrb->offset_meq_32[ofst_meq32].offset = -2;
-			eq_atrb->offset_meq_32[ofst_meq32].mask =
-				htons(attrib->ether_type);
-			eq_atrb->offset_meq_32[ofst_meq32].value =
-				htons(attrib->ether_type);
-			ofst_meq32++;
-		}
-
-	} else {
-		IPAERR("unsupported ip %d\n", ip);
-		return -EPERM;
-	}
-
-	/*
-	 * default "rule" means no attributes set -> map to
-	 * OFFSET_MEQ32_0 with mask of 0 and val of 0 and offset 0
-	 */
-	if (attrib->attrib_mask == 0) {
-		if (ipa_ofst_meq32[ofst_meq32] == -1) {
-			IPAERR("ran out of meq32 eq\n");
-			return -EPERM;
-		}
-		*en_rule |= ipa_ofst_meq32[ofst_meq32];
-		eq_atrb->offset_meq_32[ofst_meq32].offset = 0;
-		eq_atrb->offset_meq_32[ofst_meq32].mask = 0;
-		eq_atrb->offset_meq_32[ofst_meq32].value = 0;
-		ofst_meq32++;
-	}
-
-	eq_atrb->rule_eq_bitmap = *en_rule;
-	eq_atrb->num_offset_meq_32 = ofst_meq32;
-	eq_atrb->num_ihl_offset_range_16 = ihl_ofst_rng16;
-	eq_atrb->num_ihl_offset_meq_32 = ihl_ofst_meq32;
-	eq_atrb->num_offset_meq_128 = ofst_meq128;
-
-	return 0;
-}
-
-/**
- * ipa_cfg_ep - IPA end-point configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * This includes nat, header, mode, aggregation and route settings and is a one
- * shot API to configure the IPA end-point fully
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep(u32 clnt_hdl, const struct ipa_ep_cfg *ipa_ep_cfg)
-{
-	int result = -EINVAL;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ipa_ep_cfg == NULL) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	result = ipa_cfg_ep_hdr(clnt_hdl, &ipa_ep_cfg->hdr);
-	if (result)
-		return result;
-
-	result = ipa_cfg_ep_hdr_ext(clnt_hdl, &ipa_ep_cfg->hdr_ext);
-	if (result)
-		return result;
-
-	result = ipa_cfg_ep_aggr(clnt_hdl, &ipa_ep_cfg->aggr);
-	if (result)
-		return result;
-
-	result = ipa_cfg_ep_cfg(clnt_hdl, &ipa_ep_cfg->cfg);
-	if (result)
-		return result;
-
-	if (IPA_CLIENT_IS_PROD(ipa_ctx->ep[clnt_hdl].client)) {
-		result = ipa_cfg_ep_nat(clnt_hdl, &ipa_ep_cfg->nat);
-		if (result)
-			return result;
-
-		result = ipa_cfg_ep_mode(clnt_hdl, &ipa_ep_cfg->mode);
-		if (result)
-			return result;
-
-		result = ipa_cfg_ep_route(clnt_hdl, &ipa_ep_cfg->route);
-		if (result)
-			return result;
-
-		result = ipa_cfg_ep_deaggr(clnt_hdl, &ipa_ep_cfg->deaggr);
-		if (result)
-			return result;
-	} else {
-		result = ipa_cfg_ep_metadata_mask(clnt_hdl,
-				&ipa_ep_cfg->metadata_mask);
-		if (result)
-			return result;
-	}
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep);
-
-const char *ipa_get_nat_en_str(enum ipa_nat_en_type nat_en)
-{
-	switch (nat_en) {
-	case (IPA_BYPASS_NAT):
-		return "NAT disabled";
-	case (IPA_SRC_NAT):
-		return "Source NAT";
-	case (IPA_DST_NAT):
-		return "Dst NAT";
-	}
-
-	return "undefined";
-}
-
-void _ipa_cfg_ep_nat_v1_1(u32 clnt_hdl,
-		const struct ipa_ep_cfg_nat *ep_nat)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_nat->nat_en,
-			IPA_ENDP_INIT_NAT_N_NAT_EN_SHFT,
-			IPA_ENDP_INIT_NAT_N_NAT_EN_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_NAT_N_OFST_v1_1(clnt_hdl),
-			reg_val);
-}
-
-void _ipa_cfg_ep_nat_v2_0(u32 clnt_hdl,
-		const struct ipa_ep_cfg_nat *ep_nat)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_nat->nat_en,
-			IPA_ENDP_INIT_NAT_N_NAT_EN_SHFT,
-			IPA_ENDP_INIT_NAT_N_NAT_EN_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_NAT_N_OFST_v2_0(clnt_hdl),
-			reg_val);
-}
-
-/**
- * ipa_cfg_ep_nat() - IPA end-point NAT configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_nat(u32 clnt_hdl, const struct ipa_ep_cfg_nat *ep_nat)
-{
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ep_nat == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-					clnt_hdl,
-					ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	if (IPA_CLIENT_IS_CONS(ipa_ctx->ep[clnt_hdl].client)) {
-		IPAERR("NAT does not apply to IPA out EP %d\n", clnt_hdl);
-		return -EINVAL;
-	}
-
-	IPADBG("pipe=%d, nat_en=%d(%s)\n",
-			clnt_hdl,
-			ep_nat->nat_en,
-			ipa_get_nat_en_str(ep_nat->nat_en));
-
-	/* copy over EP cfg */
-	ipa_ctx->ep[clnt_hdl].cfg.nat = *ep_nat;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_nat(clnt_hdl, ep_nat);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_nat);
-
-static void _ipa_cfg_ep_status_v1_1(u32 clnt_hdl,
-				const struct ipa_ep_cfg_status *ep_status)
-{
-	IPADBG("Not supported for version 1.1\n");
-}
-
-static void _ipa_cfg_ep_status_v2_0(u32 clnt_hdl,
-		const struct ipa_ep_cfg_status *ep_status)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_status->status_en,
-			IPA_ENDP_STATUS_n_STATUS_EN_SHFT,
-			IPA_ENDP_STATUS_n_STATUS_EN_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_status->status_ep,
-			IPA_ENDP_STATUS_n_STATUS_ENDP_SHFT,
-			IPA_ENDP_STATUS_n_STATUS_ENDP_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_STATUS_n_OFST(clnt_hdl),
-			reg_val);
-}
-
-/**
- * ipa_cfg_ep_status() - IPA end-point status configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_status(u32 clnt_hdl, const struct ipa_ep_cfg_status *ep_status)
-{
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ep_status == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-					clnt_hdl,
-					ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	IPADBG("pipe=%d, status_en=%d status_ep=%d\n",
-			clnt_hdl,
-			ep_status->status_en,
-			ep_status->status_ep);
-
-	/* copy over EP cfg */
-	ipa_ctx->ep[clnt_hdl].status = *ep_status;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_status(clnt_hdl, ep_status);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-
-static void _ipa_cfg_ep_cfg_v1_1(u32 clnt_hdl,
-				const struct ipa_ep_cfg_cfg *cfg)
-{
-	IPADBG("Not supported for version 1.1\n");
-}
-
-static void _ipa_cfg_ep_cfg_v2_0(u32 clnt_hdl,
-		const struct ipa_ep_cfg_cfg *cfg)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, cfg->frag_offload_en,
-			IPA_ENDP_INIT_CFG_n_FRAG_OFFLOAD_EN_SHFT,
-			IPA_ENDP_INIT_CFG_n_FRAG_OFFLOAD_EN_BMSK);
-	IPA_SETFIELD_IN_REG(reg_val, cfg->cs_offload_en,
-			IPA_ENDP_INIT_CFG_n_CS_OFFLOAD_EN_SHFT,
-			IPA_ENDP_INIT_CFG_n_CS_OFFLOAD_EN_BMSK);
-	IPA_SETFIELD_IN_REG(reg_val, cfg->cs_metadata_hdr_offset,
-			IPA_ENDP_INIT_CFG_n_CS_METADATA_HDR_OFFSET_SHFT,
-			IPA_ENDP_INIT_CFG_n_CS_METADATA_HDR_OFFSET_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio, IPA_ENDP_INIT_CFG_n_OFST(clnt_hdl),
-			reg_val);
-}
-
-/**
- * ipa_cfg_ep_cfg() - IPA end-point cfg configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_cfg(u32 clnt_hdl, const struct ipa_ep_cfg_cfg *cfg)
-{
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || cfg == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-					clnt_hdl,
-					ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	IPADBG("pipe=%d, frag_ofld_en=%d cs_ofld_en=%d mdata_hdr_ofst=%d\n",
-			clnt_hdl,
-			cfg->frag_offload_en,
-			cfg->cs_offload_en,
-			cfg->cs_metadata_hdr_offset);
-
-	/* copy over EP cfg */
-	ipa_ctx->ep[clnt_hdl].cfg.cfg = *cfg;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_cfg(clnt_hdl, cfg);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_cfg);
-
-static void _ipa_cfg_ep_metadata_mask_v1_1(u32 clnt_hdl,
-			const struct ipa_ep_cfg_metadata_mask *metadata_mask)
-{
-	IPADBG("Not supported for version 1.1\n");
-}
-
-static void _ipa_cfg_ep_metadata_mask_v2_0(u32 clnt_hdl,
-		const struct ipa_ep_cfg_metadata_mask *metadata_mask)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, metadata_mask->metadata_mask,
-			IPA_ENDP_INIT_HDR_METADATA_MASK_n_METADATA_MASK_SHFT,
-			IPA_ENDP_INIT_HDR_METADATA_MASK_n_METADATA_MASK_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HDR_METADATA_MASK_n_OFST(clnt_hdl),
-			reg_val);
-}
-
-/**
- * ipa_cfg_ep_metadata_mask() - IPA end-point meta-data mask configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_metadata_mask(u32 clnt_hdl, const struct ipa_ep_cfg_metadata_mask
-		*metadata_mask)
-{
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || metadata_mask == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-					clnt_hdl,
-					ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	IPADBG("pipe=%d, metadata_mask=0x%x\n",
-			clnt_hdl,
-			metadata_mask->metadata_mask);
-
-	/* copy over EP cfg */
-	ipa_ctx->ep[clnt_hdl].cfg.metadata_mask = *metadata_mask;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_metadata_mask(clnt_hdl, metadata_mask);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_metadata_mask);
-
-void _ipa_cfg_ep_hdr_v1_1(u32 pipe_number,
-		const struct ipa_ep_cfg_hdr *ep_hdr)
-{
-	u32 val = 0;
-
-	val = IPA_SETFIELD(ep_hdr->hdr_len,
-		   IPA_ENDP_INIT_HDR_N_HDR_LEN_SHFT,
-		   IPA_ENDP_INIT_HDR_N_HDR_LEN_BMSK) |
-	      IPA_SETFIELD(ep_hdr->hdr_ofst_metadata_valid,
-		   IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_VALID_SHFT,
-		   IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_VALID_BMSK) |
-	      IPA_SETFIELD(ep_hdr->hdr_ofst_metadata,
-		   IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_SHFT,
-		   IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_BMSK) |
-	      IPA_SETFIELD(ep_hdr->hdr_additional_const_len,
-		   IPA_ENDP_INIT_HDR_N_HDR_ADDITIONAL_CONST_LEN_SHFT,
-		   IPA_ENDP_INIT_HDR_N_HDR_ADDITIONAL_CONST_LEN_BMSK) |
-	      IPA_SETFIELD(ep_hdr->hdr_ofst_pkt_size_valid,
-		   IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_VALID_SHFT,
-		   IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_VALID_BMSK) |
-	      IPA_SETFIELD(ep_hdr->hdr_ofst_pkt_size,
-		   IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_SHFT,
-		   IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_BMSK) |
-	      IPA_SETFIELD(ep_hdr->hdr_a5_mux,
-		   IPA_ENDP_INIT_HDR_N_HDR_A5_MUX_SHFT,
-		   IPA_ENDP_INIT_HDR_N_HDR_A5_MUX_BMSK);
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HDR_N_OFST_v1_1(pipe_number), val);
-}
-
-void _ipa_cfg_ep_hdr_v2_0(u32 pipe_number,
-		const struct ipa_ep_cfg_hdr *ep_hdr)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr->hdr_metadata_reg_valid,
-			IPA_ENDP_INIT_HDR_N_HDR_METADATA_REG_VALID_SHFT_v2,
-			IPA_ENDP_INIT_HDR_N_HDR_METADATA_REG_VALID_BMSK_v2);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr->hdr_remove_additional,
-			IPA_ENDP_INIT_HDR_N_HDR_LEN_INC_DEAGG_HDR_SHFT_v2,
-			IPA_ENDP_INIT_HDR_N_HDR_LEN_INC_DEAGG_HDR_BMSK_v2);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr->hdr_a5_mux,
-			IPA_ENDP_INIT_HDR_N_HDR_A5_MUX_SHFT,
-			IPA_ENDP_INIT_HDR_N_HDR_A5_MUX_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr->hdr_ofst_pkt_size,
-			IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_SHFT,
-			IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr->hdr_ofst_pkt_size_valid,
-			IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_VALID_SHFT,
-			IPA_ENDP_INIT_HDR_N_HDR_OFST_PKT_SIZE_VALID_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr->hdr_additional_const_len,
-			IPA_ENDP_INIT_HDR_N_HDR_ADDITIONAL_CONST_LEN_SHFT,
-			IPA_ENDP_INIT_HDR_N_HDR_ADDITIONAL_CONST_LEN_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr->hdr_ofst_metadata,
-			IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_SHFT,
-			IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr->hdr_ofst_metadata_valid,
-			IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_VALID_SHFT,
-			IPA_ENDP_INIT_HDR_N_HDR_OFST_METADATA_VALID_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr->hdr_len,
-			IPA_ENDP_INIT_HDR_N_HDR_LEN_SHFT,
-			IPA_ENDP_INIT_HDR_N_HDR_LEN_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HDR_N_OFST_v2_0(pipe_number), reg_val);
-}
-
-/**
- * ipa_cfg_ep_hdr() -  IPA end-point header configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_hdr(u32 clnt_hdl, const struct ipa_ep_cfg_hdr *ep_hdr)
-{
-	struct ipa_ep_context *ep;
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ep_hdr == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-				clnt_hdl, ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-	IPADBG("pipe=%d remove_additional=%d, a5_mux=%d, ofst_pkt_size=0x%x\n",
-		clnt_hdl,
-		ep_hdr->hdr_remove_additional,
-		ep_hdr->hdr_a5_mux,
-		ep_hdr->hdr_ofst_pkt_size);
-
-	IPADBG("ofst_pkt_size_valid=%d, additional_const_len=0x%x\n",
-		ep_hdr->hdr_ofst_pkt_size_valid,
-		ep_hdr->hdr_additional_const_len);
-
-	IPADBG("ofst_metadata=0x%x, ofst_metadata_valid=%d, len=0x%x",
-		ep_hdr->hdr_ofst_metadata,
-		ep_hdr->hdr_ofst_metadata_valid,
-		ep_hdr->hdr_len);
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	/* copy over EP cfg */
-	ep->cfg.hdr = *ep_hdr;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_hdr(clnt_hdl, &ep->cfg.hdr);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_hdr);
-
-static int _ipa_cfg_ep_hdr_ext_v1_1(u32 clnt_hdl,
-				const struct ipa_ep_cfg_hdr_ext *ep_hdr)
-{
-	IPADBG("Not supported for version 1.1\n");
-	return 0;
-}
-
-static int _ipa_cfg_ep_hdr_ext(u32 clnt_hdl,
-		const struct ipa_ep_cfg_hdr_ext *ep_hdr_ext, u32 reg_val)
-{
-	u8 hdr_endianess = ep_hdr_ext->hdr_little_endian ? 0 : 1;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr_ext->hdr_total_len_or_pad_offset,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_OFFSET_SHFT,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_OFFSET_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr_ext->hdr_payload_len_inc_padding,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_PAYLOAD_LEN_INC_PADDING_SHFT,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_PAYLOAD_LEN_INC_PADDING_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr_ext->hdr_total_len_or_pad,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_SHFT,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr_ext->hdr_total_len_or_pad_valid,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_VALID_SHFT,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_TOTAL_LEN_OR_PAD_VALID_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, hdr_endianess,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_ENDIANESS_SHFT,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_ENDIANESS_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-		IPA_ENDP_INIT_HDR_EXT_n_OFST_v2_0(clnt_hdl), reg_val);
-
-	return 0;
-}
-
-static int _ipa_cfg_ep_hdr_ext_v2_0(u32 clnt_hdl,
-				const struct ipa_ep_cfg_hdr_ext *ep_hdr_ext)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr_ext->hdr_pad_to_alignment,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_PAD_TO_ALIGNMENT_SHFT,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_PAD_TO_ALIGNMENT_BMSK_v2_0);
-
-	return _ipa_cfg_ep_hdr_ext(clnt_hdl, ep_hdr_ext, reg_val);
-}
-
-static int _ipa_cfg_ep_hdr_ext_v2_5(u32 clnt_hdl,
-				const struct ipa_ep_cfg_hdr_ext *ep_hdr_ext)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr_ext->hdr_pad_to_alignment,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_PAD_TO_ALIGNMENT_SHFT,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_PAD_TO_ALIGNMENT_BMSK_v2_5);
-
-	return _ipa_cfg_ep_hdr_ext(clnt_hdl, ep_hdr_ext, reg_val);
-
-}
-
-static int _ipa_cfg_ep_hdr_ext_v2_6L(u32 clnt_hdl,
-				const struct ipa_ep_cfg_hdr_ext *ep_hdr_ext)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_hdr_ext->hdr_pad_to_alignment,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_PAD_TO_ALIGNMENT_SHFT,
-		IPA_ENDP_INIT_HDR_EXT_n_HDR_PAD_TO_ALIGNMENT_BMSK_v2_5);
-
-	return _ipa_cfg_ep_hdr_ext(clnt_hdl, ep_hdr_ext, reg_val);
-
-}
-
-/**
- * ipa_cfg_ep_hdr_ext() -  IPA end-point extended header configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ep_hdr_ext:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_hdr_ext(u32 clnt_hdl,
-		       const struct ipa_ep_cfg_hdr_ext *ep_hdr_ext)
-{
-	struct ipa_ep_context *ep;
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ep_hdr_ext == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-				clnt_hdl, ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	IPADBG("pipe=%d hdr_pad_to_alignment=%d\n",
-		clnt_hdl,
-		ep_hdr_ext->hdr_pad_to_alignment);
-
-	IPADBG("hdr_total_len_or_pad_offset=%d\n",
-		ep_hdr_ext->hdr_total_len_or_pad_offset);
-
-	IPADBG("hdr_payload_len_inc_padding=%d hdr_total_len_or_pad=%d\n",
-		ep_hdr_ext->hdr_payload_len_inc_padding,
-		ep_hdr_ext->hdr_total_len_or_pad);
-
-	IPADBG("hdr_total_len_or_pad_valid=%d hdr_little_endian=%d\n",
-		ep_hdr_ext->hdr_total_len_or_pad_valid,
-		ep_hdr_ext->hdr_little_endian);
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	/* copy over EP cfg */
-	ep->cfg.hdr_ext = *ep_hdr_ext;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_hdr_ext(clnt_hdl, &ep->cfg.hdr_ext);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_hdr_ext);
-
-/**
- * ipa_cfg_ep_hdr() -  IPA end-point Control configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg_ctrl:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_cfg_ep_ctrl(u32 clnt_hdl, const struct ipa_ep_cfg_ctrl *ep_ctrl)
-{
-	u32 reg_val = 0;
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes || ep_ctrl == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d\n", clnt_hdl);
-		return -EINVAL;
-	}
-
-	IPADBG("pipe=%d ep_suspend=%d, ep_delay=%d\n",
-		clnt_hdl,
-		ep_ctrl->ipa_ep_suspend,
-		ep_ctrl->ipa_ep_delay);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_ctrl->ipa_ep_suspend,
-		IPA_ENDP_INIT_CTRL_N_ENDP_SUSPEND_SHFT,
-		IPA_ENDP_INIT_CTRL_N_ENDP_SUSPEND_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_ctrl->ipa_ep_delay,
-			IPA_ENDP_INIT_CTRL_N_ENDP_DELAY_SHFT,
-			IPA_ENDP_INIT_CTRL_N_ENDP_DELAY_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-		IPA_ENDP_INIT_CTRL_N_OFST(clnt_hdl), reg_val);
-
-	return 0;
-
-}
-EXPORT_SYMBOL(ipa_cfg_ep_ctrl);
-
-/**
- * ipa_cfg_aggr_cntr_granularity() - granularity of the AGGR timer configuration
- * @aggr_granularity:     [in] defines the granularity of AGGR timers
- *			  number of units of 1/32msec
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_cfg_aggr_cntr_granularity(u8 aggr_granularity)
-{
-	u32 reg_val = 0;
-
-	if (aggr_granularity <= IPA_AGGR_GRAN_MIN ||
-			aggr_granularity > IPA_AGGR_GRAN_MAX) {
-		IPAERR("bad param, aggr_granularity = %d\n",
-				aggr_granularity);
-		return -EINVAL;
-	}
-	IPADBG("aggr_granularity=%d\n", aggr_granularity);
-
-	reg_val = ipa_read_reg(ipa_ctx->mmio, IPA_COUNTER_CFG_OFST);
-	reg_val = (reg_val & ~IPA_COUNTER_CFG_AGGR_GRAN_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, aggr_granularity - 1,
-			IPA_COUNTER_CFG_AGGR_GRAN_SHFT,
-			IPA_COUNTER_CFG_AGGR_GRAN_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_COUNTER_CFG_OFST, reg_val);
-
-	return 0;
-
-}
-EXPORT_SYMBOL(ipa_cfg_aggr_cntr_granularity);
-
-/**
- * ipa_cfg_eot_coal_cntr_granularity() - granularity of EOT_COAL timer
- *					 configuration
- * @eot_coal_granularity: defines the granularity of EOT_COAL timers
- *			  number of units of 1/32msec
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_cfg_eot_coal_cntr_granularity(u8 eot_coal_granularity)
-{
-	u32 reg_val = 0;
-
-	if (eot_coal_granularity <= IPA_EOT_COAL_GRAN_MIN ||
-			eot_coal_granularity > IPA_EOT_COAL_GRAN_MAX) {
-		IPAERR("bad parm, eot_coal_granularity = %d\n",
-				eot_coal_granularity);
-		return -EINVAL;
-	}
-	IPADBG("eot_coal_granularity=%d\n", eot_coal_granularity);
-
-	reg_val = ipa_read_reg(ipa_ctx->mmio, IPA_COUNTER_CFG_OFST);
-	reg_val = (reg_val & ~IPA_COUNTER_CFG_EOT_COAL_GRAN_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, eot_coal_granularity - 1,
-			IPA_COUNTER_CFG_EOT_COAL_GRAN_SHFT,
-			IPA_COUNTER_CFG_EOT_COAL_GRAN_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_COUNTER_CFG_OFST, reg_val);
-
-	return 0;
-
-}
-EXPORT_SYMBOL(ipa_cfg_eot_coal_cntr_granularity);
-
-const char *ipa_get_mode_type_str(enum ipa_mode_type mode)
-{
-	switch (mode) {
-	case (IPA_BASIC):
-		return "Basic";
-	case (IPA_ENABLE_FRAMING_HDLC):
-		return "HDLC framing";
-	case (IPA_ENABLE_DEFRAMING_HDLC):
-		return "HDLC de-framing";
-	case (IPA_DMA):
-		return "DMA";
-	}
-
-	return "undefined";
-}
-
-void _ipa_cfg_ep_mode_v1_1(u32 pipe_number, u32 dst_pipe_number,
-		const struct ipa_ep_cfg_mode *ep_mode)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_mode->mode,
-			IPA_ENDP_INIT_MODE_N_MODE_SHFT,
-			IPA_ENDP_INIT_MODE_N_MODE_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, dst_pipe_number,
-			IPA_ENDP_INIT_MODE_N_DEST_PIPE_INDEX_SHFT_v1_1,
-			IPA_ENDP_INIT_MODE_N_DEST_PIPE_INDEX_BMSK_v1_1);
-
-		ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_MODE_N_OFST_v1_1(pipe_number), reg_val);
-}
-
-void _ipa_cfg_ep_mode_v2_0(u32 pipe_number, u32 dst_pipe_number,
-		const struct ipa_ep_cfg_mode *ep_mode)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_mode->mode,
-			IPA_ENDP_INIT_MODE_N_MODE_SHFT,
-			IPA_ENDP_INIT_MODE_N_MODE_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, dst_pipe_number,
-			IPA_ENDP_INIT_MODE_N_DEST_PIPE_INDEX_SHFT_v2_0,
-			IPA_ENDP_INIT_MODE_N_DEST_PIPE_INDEX_BMSK_v2_0);
-
-		ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_MODE_N_OFST_v2_0(pipe_number), reg_val);
-}
-
-/**
- * ipa_cfg_ep_mode() - IPA end-point mode configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_mode(u32 clnt_hdl, const struct ipa_ep_cfg_mode *ep_mode)
-{
-	int ep;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ep_mode == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-					clnt_hdl, ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	if (IPA_CLIENT_IS_CONS(ipa_ctx->ep[clnt_hdl].client)) {
-		IPAERR("MODE does not apply to IPA out EP %d\n", clnt_hdl);
-		return -EINVAL;
-	}
-
-	ep = ipa_get_ep_mapping(ep_mode->dst);
-	if (ep == -1 && ep_mode->mode == IPA_DMA) {
-		IPAERR("dst %d does not exist\n", ep_mode->dst);
-		return -EINVAL;
-	}
-
-	WARN_ON(ep_mode->mode == IPA_DMA && IPA_CLIENT_IS_PROD(ep_mode->dst));
-
-	if (!IPA_CLIENT_IS_CONS(ep_mode->dst))
-		ep = ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_CONS);
-
-	IPADBG("pipe=%d mode=%d(%s), dst_client_number=%d",
-			clnt_hdl,
-			ep_mode->mode,
-			ipa_get_mode_type_str(ep_mode->mode),
-			ep_mode->dst);
-
-	/* copy over EP cfg */
-	ipa_ctx->ep[clnt_hdl].cfg.mode = *ep_mode;
-	ipa_ctx->ep[clnt_hdl].dst_pipe_index = ep;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_mode(clnt_hdl,
-			ipa_ctx->ep[clnt_hdl].dst_pipe_index,
-			ep_mode);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_mode);
-
-const char *get_aggr_enable_str(enum ipa_aggr_en_type aggr_en)
-{
-	switch (aggr_en) {
-	case (IPA_BYPASS_AGGR):
-			return "no aggregation";
-	case (IPA_ENABLE_AGGR):
-			return "aggregation enabled";
-	case (IPA_ENABLE_DEAGGR):
-		return "de-aggregation enabled";
-	}
-
-	return "undefined";
-}
-
-const char *get_aggr_type_str(enum ipa_aggr_type aggr_type)
-{
-	switch (aggr_type) {
-	case (IPA_MBIM_16):
-			return "MBIM_16";
-	case (IPA_HDLC):
-		return "HDLC";
-	case (IPA_TLP):
-			return "TLP";
-	case (IPA_RNDIS):
-			return "RNDIS";
-	case (IPA_GENERIC):
-			return "GENERIC";
-	case (IPA_QCMAP):
-			return "QCMAP";
-	}
-	return "undefined";
-}
-
-void _ipa_cfg_ep_aggr_v1_1(u32 pipe_number,
-		const struct ipa_ep_cfg_aggr *ep_aggr)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_aggr->aggr_en,
-			IPA_ENDP_INIT_AGGR_N_AGGR_EN_SHFT,
-			IPA_ENDP_INIT_AGGR_N_AGGR_EN_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_aggr->aggr,
-			IPA_ENDP_INIT_AGGR_N_AGGR_TYPE_SHFT,
-			IPA_ENDP_INIT_AGGR_N_AGGR_TYPE_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_aggr->aggr_byte_limit,
-			IPA_ENDP_INIT_AGGR_N_AGGR_BYTE_LIMIT_SHFT,
-			IPA_ENDP_INIT_AGGR_N_AGGR_BYTE_LIMIT_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_aggr->aggr_time_limit,
-			IPA_ENDP_INIT_AGGR_N_AGGR_TIME_LIMIT_SHFT,
-			IPA_ENDP_INIT_AGGR_N_AGGR_TIME_LIMIT_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_AGGR_N_OFST_v1_1(pipe_number), reg_val);
-}
-
-void _ipa_cfg_ep_aggr_v2_0(u32 pipe_number,
-		const struct ipa_ep_cfg_aggr *ep_aggr)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_aggr->aggr_en,
-			IPA_ENDP_INIT_AGGR_N_AGGR_EN_SHFT,
-			IPA_ENDP_INIT_AGGR_N_AGGR_EN_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_aggr->aggr,
-			IPA_ENDP_INIT_AGGR_N_AGGR_TYPE_SHFT,
-			IPA_ENDP_INIT_AGGR_N_AGGR_TYPE_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_aggr->aggr_byte_limit,
-			IPA_ENDP_INIT_AGGR_N_AGGR_BYTE_LIMIT_SHFT,
-			IPA_ENDP_INIT_AGGR_N_AGGR_BYTE_LIMIT_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_aggr->aggr_time_limit,
-			IPA_ENDP_INIT_AGGR_N_AGGR_TIME_LIMIT_SHFT,
-			IPA_ENDP_INIT_AGGR_N_AGGR_TIME_LIMIT_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_aggr->aggr_pkt_limit,
-			IPA_ENDP_INIT_AGGR_n_AGGR_PKT_LIMIT_SHFT,
-			IPA_ENDP_INIT_AGGR_n_AGGR_PKT_LIMIT_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_AGGR_N_OFST_v2_0(pipe_number), reg_val);
-}
-
-/**
- * ipa_cfg_ep_aggr() - IPA end-point aggregation configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_aggr(u32 clnt_hdl, const struct ipa_ep_cfg_aggr *ep_aggr)
-{
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ep_aggr == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-			clnt_hdl, ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	IPADBG("pipe=%d en=%d(%s), type=%d(%s), byte_limit=%d, time_limit=%d\n",
-			clnt_hdl,
-			ep_aggr->aggr_en,
-			get_aggr_enable_str(ep_aggr->aggr_en),
-			ep_aggr->aggr,
-			get_aggr_type_str(ep_aggr->aggr),
-			ep_aggr->aggr_byte_limit,
-			ep_aggr->aggr_time_limit);
-
-	/* copy over EP cfg */
-	ipa_ctx->ep[clnt_hdl].cfg.aggr = *ep_aggr;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_aggr(clnt_hdl, ep_aggr);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_aggr);
-
-void _ipa_cfg_ep_route_v1_1(u32 pipe_index, u32 rt_tbl_index)
-{
-	int reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, rt_tbl_index,
-			IPA_ENDP_INIT_ROUTE_N_ROUTE_TABLE_INDEX_SHFT,
-			IPA_ENDP_INIT_ROUTE_N_ROUTE_TABLE_INDEX_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_ROUTE_N_OFST_v1_1(pipe_index),
-			reg_val);
-}
-
-void _ipa_cfg_ep_route_v2_0(u32 pipe_index, u32 rt_tbl_index)
-{
-	int reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, rt_tbl_index,
-			IPA_ENDP_INIT_ROUTE_N_ROUTE_TABLE_INDEX_SHFT,
-			IPA_ENDP_INIT_ROUTE_N_ROUTE_TABLE_INDEX_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_ROUTE_N_OFST_v2_0(pipe_index),
-			reg_val);
-}
-
-/**
- * ipa_cfg_ep_route() - IPA end-point routing configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_route(u32 clnt_hdl, const struct ipa_ep_cfg_route *ep_route)
-{
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ep_route == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-			clnt_hdl, ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	if (IPA_CLIENT_IS_CONS(ipa_ctx->ep[clnt_hdl].client)) {
-		IPAERR("ROUTE does not apply to IPA out EP %d\n",
-				clnt_hdl);
-		return -EINVAL;
-	}
-
-	/*
-	 * if DMA mode was configured previously for this EP, return with
-	 * success
-	 */
-	if (ipa_ctx->ep[clnt_hdl].cfg.mode.mode == IPA_DMA) {
-		IPADBG("DMA enabled for ep %d, dst pipe is part of DMA\n",
-				clnt_hdl);
-		return 0;
-	}
-
-	if (ep_route->rt_tbl_hdl)
-		IPAERR("client specified non-zero RT TBL hdl - ignore it\n");
-
-	IPADBG("pipe=%d, rt_tbl_hdl=%d\n",
-			clnt_hdl,
-			ep_route->rt_tbl_hdl);
-
-	/* always use "default" routing table when programming EP ROUTE reg */
-	if (ipa_ctx->ipa_hw_type >= IPA_HW_v2_0)
-		ipa_ctx->ep[clnt_hdl].rt_tbl_idx =
-			IPA_MEM_PART(v4_apps_rt_index_lo);
-	else
-		ipa_ctx->ep[clnt_hdl].rt_tbl_idx = 0;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_route(clnt_hdl,
-			ipa_ctx->ep[clnt_hdl].rt_tbl_idx);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_route);
-
-void _ipa_cfg_ep_holb_v1_1(u32 pipe_number,
-			const struct ipa_ep_cfg_holb *ep_holb)
-{
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HOL_BLOCK_EN_N_OFST_v1_1(pipe_number),
-			ep_holb->en);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_OFST_v1_1(pipe_number),
-			(u16)ep_holb->tmr_val);
-}
-
-void _ipa_cfg_ep_holb_v2_0(u32 pipe_number,
-			const struct ipa_ep_cfg_holb *ep_holb)
-{
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HOL_BLOCK_EN_N_OFST_v2_0(pipe_number),
-			ep_holb->en);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_OFST_v2_0(pipe_number),
-			(u16)ep_holb->tmr_val);
-}
-
-void _ipa_cfg_ep_holb_v2_5(u32 pipe_number,
-			const struct ipa_ep_cfg_holb *ep_holb)
-{
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HOL_BLOCK_EN_N_OFST_v2_0(pipe_number),
-			ep_holb->en);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_OFST_v2_0(pipe_number),
-			ep_holb->tmr_val);
-}
-
-void _ipa_cfg_ep_holb_v2_6L(u32 pipe_number,
-			const struct ipa_ep_cfg_holb *ep_holb)
-{
-	ipa_write_reg(ipa_ctx->mmio,
-		IPA_ENDP_INIT_HOL_BLOCK_EN_N_OFST_v2_0(pipe_number),
-		ep_holb->en);
-
-	ipa_write_reg(ipa_ctx->mmio,
-		IPA_ENDP_INIT_HOL_BLOCK_TIMER_N_OFST_v2_0(pipe_number),
-		ep_holb->tmr_val);
-}
-
-/**
- * ipa_cfg_ep_holb() - IPA end-point holb configuration
- *
- * If an IPA producer pipe is full, IPA HW by default will block
- * indefinitely till space opens up. During this time no packets
- * including those from unrelated pipes will be processed. Enabling
- * HOLB means IPA HW will be allowed to drop packets as/when needed
- * and indefinite blocking is avoided.
- *
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_cfg_ep_holb(u32 clnt_hdl, const struct ipa_ep_cfg_holb *ep_holb)
-{
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ep_holb == NULL ||
-	    ep_holb->tmr_val > ipa_ctx->ctrl->max_holb_tmr_val ||
-	    ep_holb->en > 1) {
-		IPAERR("bad parm.\n");
-		return -EINVAL;
-	}
-
-	if (IPA_CLIENT_IS_PROD(ipa_ctx->ep[clnt_hdl].client)) {
-		IPAERR("HOLB does not apply to IPA in EP %d\n", clnt_hdl);
-		return -EINVAL;
-	}
-
-	if (!ipa_ctx->ctrl->ipa_cfg_ep_holb) {
-		IPAERR("HOLB is not supported for this IPA core\n");
-		return -EINVAL;
-	}
-
-	ipa_ctx->ep[clnt_hdl].holb = *ep_holb;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_holb(clnt_hdl, ep_holb);
-
-	ipa_dec_client_disable_clks();
-
-	IPADBG("cfg holb %u ep=%d tmr=%d\n", ep_holb->en, clnt_hdl,
-				ep_holb->tmr_val);
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_holb);
-
-/**
- * ipa_cfg_ep_holb_by_client() - IPA end-point holb configuration
- *
- * Wrapper function for ipa_cfg_ep_holb() with client name instead of
- * client handle. This function is used for clients that does not have
- * client handle.
- *
- * @client:	[in] client name
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_cfg_ep_holb_by_client(enum ipa_client_type client,
-				const struct ipa_ep_cfg_holb *ep_holb)
-{
-	return ipa_cfg_ep_holb(ipa_get_ep_mapping(client), ep_holb);
-}
-EXPORT_SYMBOL(ipa_cfg_ep_holb_by_client);
-
-static int _ipa_cfg_ep_deaggr_v1_1(u32 clnt_hdl,
-				const struct ipa_ep_cfg_deaggr *ep_deaggr)
-{
-	IPADBG("Not supported for version 1.1\n");
-	return 0;
-}
-
-static int _ipa_cfg_ep_deaggr_v2_0(u32 clnt_hdl,
-				   const struct ipa_ep_cfg_deaggr *ep_deaggr)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_deaggr->deaggr_hdr_len,
-		IPA_ENDP_INIT_DEAGGR_n_DEAGGR_HDR_LEN_SHFT,
-		IPA_ENDP_INIT_DEAGGR_n_DEAGGR_HDR_LEN_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_deaggr->packet_offset_valid,
-		IPA_ENDP_INIT_DEAGGR_n_PACKET_OFFSET_VALID_SHFT,
-		IPA_ENDP_INIT_DEAGGR_n_PACKET_OFFSET_VALID_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_deaggr->packet_offset_location,
-		IPA_ENDP_INIT_DEAGGR_n_PACKET_OFFSET_LOCATION_SHFT,
-		IPA_ENDP_INIT_DEAGGR_n_PACKET_OFFSET_LOCATION_BMSK);
-
-	IPA_SETFIELD_IN_REG(reg_val, ep_deaggr->max_packet_len,
-		IPA_ENDP_INIT_DEAGGR_n_MAX_PACKET_LEN_SHFT,
-		IPA_ENDP_INIT_DEAGGR_n_MAX_PACKET_LEN_BMSK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-		IPA_ENDP_INIT_DEAGGR_n_OFST_v2_0(clnt_hdl), reg_val);
-
-	return 0;
-}
-
-/**
- * ipa_cfg_ep_deaggr() -  IPA end-point deaggregation configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ep_deaggr:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_deaggr(u32 clnt_hdl,
-			const struct ipa_ep_cfg_deaggr *ep_deaggr)
-{
-	struct ipa_ep_context *ep;
-
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-	    ipa_ctx->ep[clnt_hdl].valid == 0 || ep_deaggr == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-				clnt_hdl, ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	IPADBG("pipe=%d deaggr_hdr_len=%d\n",
-		clnt_hdl,
-		ep_deaggr->deaggr_hdr_len);
-
-	IPADBG("packet_offset_valid=%d\n",
-		ep_deaggr->packet_offset_valid);
-
-	IPADBG("packet_offset_location=%d max_packet_len=%d\n",
-		ep_deaggr->packet_offset_location,
-		ep_deaggr->max_packet_len);
-
-	ep = &ipa_ctx->ep[clnt_hdl];
-
-	/* copy over EP cfg */
-	ep->cfg.deaggr = *ep_deaggr;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_deaggr(clnt_hdl, &ep->cfg.deaggr);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_deaggr);
-
-static void _ipa_cfg_ep_metadata_v1_1(u32 pipe_number,
-					const struct ipa_ep_cfg_metadata *meta)
-{
-	IPADBG("Not supported for version 1.1\n");
-}
-
-static void _ipa_cfg_ep_metadata_v2_0(u32 pipe_number,
-					const struct ipa_ep_cfg_metadata *meta)
-{
-	u32 reg_val = 0;
-
-	IPA_SETFIELD_IN_REG(reg_val, meta->qmap_id,
-			IPA_ENDP_INIT_HDR_METADATA_n_MUX_ID_SHFT,
-			IPA_ENDP_INIT_HDR_METADATA_n_MUX_ID_BMASK);
-
-	ipa_write_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_HDR_METADATA_n_OFST(pipe_number),
-			reg_val);
-}
-
-/**
- * ipa_cfg_ep_metadata() - IPA end-point metadata configuration
- * @clnt_hdl:	[in] opaque client handle assigned by IPA to client
- * @ipa_ep_cfg:	[in] IPA end-point configuration params
- *
- * Returns:	0 on success, negative on failure
- *
- * Note:	Should not be called from atomic context
- */
-int ipa_cfg_ep_metadata(u32 clnt_hdl, const struct ipa_ep_cfg_metadata *ep_md)
-{
-	if (clnt_hdl >= ipa_ctx->ipa_num_pipes ||
-		ipa_ctx->ep[clnt_hdl].valid == 0 || ep_md == NULL) {
-		IPAERR("bad parm, clnt_hdl = %d , ep_valid = %d\n",
-					clnt_hdl, ipa_ctx->ep[clnt_hdl].valid);
-		return -EINVAL;
-	}
-
-	IPADBG("pipe=%d, mux id=%d\n", clnt_hdl, ep_md->qmap_id);
-
-	/* copy over EP cfg */
-	ipa_ctx->ep[clnt_hdl].cfg.meta = *ep_md;
-
-	ipa_inc_client_enable_clks();
-
-	ipa_ctx->ctrl->ipa_cfg_ep_metadata(clnt_hdl, ep_md);
-	ipa_ctx->ep[clnt_hdl].cfg.hdr.hdr_metadata_reg_valid = 1;
-	ipa_ctx->ctrl->ipa_cfg_ep_hdr(clnt_hdl, &ipa_ctx->ep[clnt_hdl].cfg.hdr);
-
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_cfg_ep_metadata);
-
-int ipa_write_qmap_id(struct ipa_ioc_write_qmapid *param_in)
-{
-	struct ipa_ep_cfg_metadata meta;
-	struct ipa_ep_context *ep;
-	int ipa_ep_idx;
-	int result = -EINVAL;
-
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return -EINVAL;
-	}
-
-	if (param_in->client  >= IPA_CLIENT_MAX) {
-		IPAERR("bad parm client:%d\n", param_in->client);
-		goto fail;
-	}
-
-	ipa_ep_idx = ipa_get_ep_mapping(param_in->client);
-	if (ipa_ep_idx == -1) {
-		IPAERR("Invalid client.\n");
-		goto fail;
-	}
-
-	ep = &ipa_ctx->ep[ipa_ep_idx];
-	if (!ep->valid) {
-		IPAERR("EP not allocated.\n");
-		goto fail;
-	}
-
-	meta.qmap_id = param_in->qmap_id;
-	if (param_in->client == IPA_CLIENT_USB_PROD ||
-	    param_in->client == IPA_CLIENT_HSIC1_PROD ||
-	    param_in->client == IPA_CLIENT_ODU_PROD) {
-		result = ipa_cfg_ep_metadata(ipa_ep_idx, &meta);
-	} else if (param_in->client == IPA_CLIENT_WLAN1_PROD) {
-		ipa_ctx->ep[ipa_ep_idx].cfg.meta = meta;
-		result = ipa_write_qmapid_wdi_pipe(ipa_ep_idx, meta.qmap_id);
-		if (result)
-			IPAERR("qmap_id %d write failed on ep=%d\n",
-					meta.qmap_id, ipa_ep_idx);
-		result = 0;
-	}
-
-fail:
-	return result;
-}
-
-/**
- * ipa_dump_buff_internal() - dumps buffer for debug purposes
- * @base: buffer base address
- * @phy_base: buffer physical base address
- * @size: size of the buffer
- */
-void ipa_dump_buff_internal(void *base, dma_addr_t phy_base, u32 size)
-{
-	int i;
-	u32 *cur = (u32 *)base;
-	u8 *byt;
-	IPADBG("system phys addr=%pa len=%u\n", &phy_base, size);
-	for (i = 0; i < size / 4; i++) {
-		byt = (u8 *)(cur + i);
-		IPADBG("%2d %08x   %02x %02x %02x %02x\n", i, *(cur + i),
-				byt[0], byt[1], byt[2], byt[3]);
-	}
-	IPADBG("END\n");
-}
-
-/**
- * ipa_pipe_mem_init() - initialize the pipe memory
- * @start_ofst: start offset
- * @size: size
- *
- * Return value:
- * 0: success
- * -ENOMEM: no memory
- */
-int ipa_pipe_mem_init(u32 start_ofst, u32 size)
-{
-	int res;
-	u32 aligned_start_ofst;
-	u32 aligned_size;
-	struct gen_pool *pool;
-
-	if (!size) {
-		IPAERR("no IPA pipe memory allocated\n");
-		goto fail;
-	}
-
-	aligned_start_ofst = IPA_HW_TABLE_ALIGNMENT(start_ofst);
-	aligned_size = size - (aligned_start_ofst - start_ofst);
-
-	IPADBG("start_ofst=%u aligned_start_ofst=%u size=%u aligned_size=%u\n",
-	       start_ofst, aligned_start_ofst, size, aligned_size);
-
-	/* allocation order of 8 i.e. 128 bytes, global pool */
-	pool = gen_pool_create(8, -1);
-	if (!pool) {
-		IPAERR("Failed to create a new memory pool.\n");
-		goto fail;
-	}
-
-	res = gen_pool_add(pool, aligned_start_ofst, aligned_size, -1);
-	if (res) {
-		IPAERR("Failed to add memory to IPA pipe pool\n");
-		goto err_pool_add;
-	}
-
-	ipa_ctx->pipe_mem_pool = pool;
-	return 0;
-
-err_pool_add:
-	gen_pool_destroy(pool);
-fail:
-	return -ENOMEM;
-}
-
-/**
- * ipa_pipe_mem_alloc() - allocate pipe memory
- * @ofst: offset
- * @size: size
- *
- * Return value:
- * 0: success
- */
-int ipa_pipe_mem_alloc(u32 *ofst, u32 size)
-{
-	u32 vaddr;
-	int res = -1;
-
-	if (!ipa_ctx->pipe_mem_pool || !size) {
-		IPAERR("failed size=%u pipe_mem_pool=%p\n", size,
-				ipa_ctx->pipe_mem_pool);
-		return res;
-	}
-
-	vaddr = gen_pool_alloc(ipa_ctx->pipe_mem_pool, size);
-
-	if (vaddr) {
-		*ofst = vaddr;
-		res = 0;
-		IPADBG("size=%u ofst=%u\n", size, vaddr);
-	} else {
-		IPAERR("size=%u failed\n", size);
-	}
-
-	return res;
-}
-
-/**
- * ipa_pipe_mem_free() - free pipe memory
- * @ofst: offset
- * @size: size
- *
- * Return value:
- * 0: success
- */
-int ipa_pipe_mem_free(u32 ofst, u32 size)
-{
-	IPADBG("size=%u ofst=%u\n", size, ofst);
-	if (ipa_ctx->pipe_mem_pool && size)
-		gen_pool_free(ipa_ctx->pipe_mem_pool, ofst, size);
-	return 0;
-}
-
-/**
- * ipa_set_aggr_mode() - Set the aggregation mode which is a global setting
- * @mode:	[in] the desired aggregation mode for e.g. straight MBIM, QCNCM,
- * etc
- *
- * Returns:	0 on success
- */
-int ipa_set_aggr_mode(enum ipa_aggr_mode mode)
-{
-	u32 reg_val;
-
-	ipa_inc_client_enable_clks();
-	reg_val = ipa_read_reg(ipa_ctx->mmio, IPA_QCNCM_OFST);
-	ipa_write_reg(ipa_ctx->mmio, IPA_QCNCM_OFST, (mode & 0x1) |
-			(reg_val & 0xfffffffe));
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_set_aggr_mode);
-
-/**
- * ipa_set_qcncm_ndp_sig() - Set the NDP signature used for QCNCM aggregation
- * mode
- * @sig:	[in] the first 3 bytes of QCNCM NDP signature (expected to be
- * "QND")
- *
- * Set the NDP signature used for QCNCM aggregation mode. The fourth byte
- * (expected to be 'P') needs to be set using the header addition mechanism
- *
- * Returns:	0 on success, negative on failure
- */
-int ipa_set_qcncm_ndp_sig(char sig[3])
-{
-	u32 reg_val;
-
-	if (sig == NULL) {
-		IPAERR("bad argument for ipa_set_qcncm_ndp_sig/n");
-		return -EINVAL;
-	}
-	ipa_inc_client_enable_clks();
-	reg_val = ipa_read_reg(ipa_ctx->mmio, IPA_QCNCM_OFST);
-	ipa_write_reg(ipa_ctx->mmio, IPA_QCNCM_OFST, sig[0] << 20 |
-			(sig[1] << 12) | (sig[2] << 4) |
-			(reg_val & 0xf000000f));
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_set_qcncm_ndp_sig);
-
-/**
- * ipa_set_single_ndp_per_mbim() - Enable/disable single NDP per MBIM frame
- * configuration
- * @enable:	[in] true for single NDP/MBIM; false otherwise
- *
- * Returns:	0 on success
- */
-int ipa_set_single_ndp_per_mbim(bool enable)
-{
-	u32 reg_val;
-
-	ipa_inc_client_enable_clks();
-	reg_val = ipa_read_reg(ipa_ctx->mmio, IPA_SINGLE_NDP_MODE_OFST);
-	ipa_write_reg(ipa_ctx->mmio, IPA_SINGLE_NDP_MODE_OFST,
-			(enable & 0x1) | (reg_val & 0xfffffffe));
-	ipa_dec_client_disable_clks();
-
-	return 0;
-}
-EXPORT_SYMBOL(ipa_set_single_ndp_per_mbim);
-
-/**
- * ipa_set_hw_timer_fix_for_mbim_aggr() - Enable/disable HW timer fix
- * for MBIM aggregation.
- * @enable:	[in] true for enable HW fix; false otherwise
- *
- * Returns:	0 on success
- */
-int ipa_set_hw_timer_fix_for_mbim_aggr(bool enable)
-{
-	u32 reg_val;
-	ipa_inc_client_enable_clks();
-	reg_val = ipa_read_reg(ipa_ctx->mmio, IPA_AGGREGATION_SPARE_REG_1_OFST);
-	ipa_write_reg(ipa_ctx->mmio, IPA_AGGREGATION_SPARE_REG_1_OFST,
-		(enable << IPA_AGGREGATION_HW_TIMER_FIX_MBIM_AGGR_SHFT) |
-		(reg_val & ~IPA_AGGREGATION_HW_TIMER_FIX_MBIM_AGGR_BMSK));
-	ipa_dec_client_disable_clks();
-	return 0;
-}
-EXPORT_SYMBOL(ipa_set_hw_timer_fix_for_mbim_aggr);
-
-/**
- * ipa_straddle_boundary() - Checks whether a memory buffer straddles a boundary
- * @start: start address of the memory buffer
- * @end: end address of the memory buffer
- * @boundary: boundary
- *
- * Return value:
- * 1: if the interval [start, end] straddles boundary
- * 0: otherwise
- */
-int ipa_straddle_boundary(u32 start, u32 end, u32 boundary)
-{
-	u32 next_start;
-	u32 prev_end;
-
-	IPADBG("start=%u end=%u boundary=%u\n", start, end, boundary);
-
-	next_start = (start + (boundary - 1)) & ~(boundary - 1);
-	prev_end = ((end + (boundary - 1)) & ~(boundary - 1)) - boundary;
-
-	while (next_start < prev_end)
-		next_start += boundary;
-
-	if (next_start == prev_end)
-		return 1;
-	else
-		return 0;
-}
-
-/**
- * ipa_bam_reg_dump() - Dump selected BAM registers for IPA and DMA-BAM
- *
- * Function is rate limited to avoid flooding kernel log buffer
- */
-void ipa_bam_reg_dump(void)
-{
-	static DEFINE_RATELIMIT_STATE(_rs, 500*HZ, 1);
-	if (__ratelimit(&_rs)) {
-		ipa_inc_client_enable_clks();
-		pr_err("IPA BAM START\n");
-		if (ipa_ctx->ipa_hw_type < IPA_HW_v2_0) {
-			sps_get_bam_debug_info(ipa_ctx->bam_handle, 5,
-			511950, 0, 0);
-			sps_get_bam_debug_info(ipa_ctx->bam_handle, 93, 0,
-			0, 0);
-		} else {
-			sps_get_bam_debug_info(ipa_ctx->bam_handle, 93,
-			(SPS_BAM_PIPE(ipa_get_ep_mapping(IPA_CLIENT_USB_CONS))
-			|
-			SPS_BAM_PIPE(ipa_get_ep_mapping(IPA_CLIENT_USB_PROD))),
-			0, 2);
-		}
-		ipa_dec_client_disable_clks();
-	}
-}
-EXPORT_SYMBOL(ipa_bam_reg_dump);
-
-static void ipa_init_mem_partition_v2(void)
-{
-	IPADBG("Memory partition IPA 2\n");
-	IPA_MEM_PART(nat_ofst) = IPA_RAM_NAT_OFST;
-	IPA_MEM_PART(nat_size) = IPA_RAM_NAT_SIZE;
-	IPADBG("NAT OFST 0x%x SIZE 0x%x\n", IPA_MEM_PART(nat_ofst),
-		IPA_MEM_PART(nat_size));
-
-	IPA_MEM_PART(ofst_start) = IPA_MEM_v2_RAM_OFST_START;
-	IPADBG("RAM OFST 0x%x\n", IPA_MEM_PART(ofst_start));
-
-	IPA_MEM_PART(v4_flt_ofst) = IPA_MEM_v2_RAM_V4_FLT_OFST;
-	IPA_MEM_PART(v4_flt_size) = IPA_MEM_v2_RAM_V4_FLT_SIZE;
-	IPA_MEM_PART(v4_flt_size_ddr) = IPA_MEM_RAM_V4_FLT_SIZE_DDR;
-	IPADBG("V4 FLT OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(v4_flt_ofst), IPA_MEM_PART(v4_flt_size),
-		IPA_MEM_PART(v4_flt_size_ddr));
-
-	IPA_MEM_PART(v6_flt_ofst) = IPA_MEM_v2_RAM_V6_FLT_OFST;
-	IPA_MEM_PART(v6_flt_size) = IPA_MEM_v2_RAM_V6_FLT_SIZE;
-	IPA_MEM_PART(v6_flt_size_ddr) = IPA_MEM_RAM_V6_FLT_SIZE_DDR;
-	IPADBG("V6 FLT OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(v6_flt_ofst), IPA_MEM_PART(v6_flt_size),
-		IPA_MEM_PART(v6_flt_size_ddr));
-
-	IPA_MEM_PART(v4_rt_ofst) = IPA_MEM_v2_RAM_V4_RT_OFST;
-	IPADBG("V4 RT OFST 0x%x\n", IPA_MEM_PART(v4_rt_ofst));
-
-	IPA_MEM_PART(v4_num_index) = IPA_MEM_v2_RAM_V4_NUM_INDEX;
-	IPADBG("V4 RT NUM INDEX 0x%x\n", IPA_MEM_PART(v4_num_index));
-
-	IPA_MEM_PART(v4_modem_rt_index_lo) = IPA_MEM_v2_V4_MODEM_RT_INDEX_LO;
-	IPA_MEM_PART(v4_modem_rt_index_hi) = IPA_MEM_v2_V4_MODEM_RT_INDEX_HI;
-	IPADBG("V4 RT MODEM INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v4_modem_rt_index_lo),
-		IPA_MEM_PART(v4_modem_rt_index_hi));
-
-	IPA_MEM_PART(v4_apps_rt_index_lo) = IPA_MEM_v2_V4_APPS_RT_INDEX_LO;
-	IPA_MEM_PART(v4_apps_rt_index_hi) = IPA_MEM_v2_V4_APPS_RT_INDEX_HI;
-	IPADBG("V4 RT APPS INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v4_apps_rt_index_lo),
-		IPA_MEM_PART(v4_apps_rt_index_hi));
-
-	IPA_MEM_PART(v4_rt_size) = IPA_MEM_v2_RAM_V4_RT_SIZE;
-	IPA_MEM_PART(v4_rt_size_ddr) = IPA_MEM_RAM_V4_RT_SIZE_DDR;
-	IPADBG("V4 RT SIZE 0x%x DDR SIZE 0x%x\n", IPA_MEM_PART(v4_rt_size),
-		IPA_MEM_PART(v4_rt_size_ddr));
-
-	IPA_MEM_PART(v6_rt_ofst) = IPA_MEM_v2_RAM_V6_RT_OFST;
-	IPADBG("V6 RT OFST 0x%x\n", IPA_MEM_PART(v6_rt_ofst));
-
-	IPA_MEM_PART(v6_num_index) = IPA_MEM_v2_RAM_V6_NUM_INDEX;
-	IPADBG("V6 RT NUM INDEX 0x%x\n", IPA_MEM_PART(v6_num_index));
-
-	IPA_MEM_PART(v6_modem_rt_index_lo) = IPA_MEM_v2_V6_MODEM_RT_INDEX_LO;
-	IPA_MEM_PART(v6_modem_rt_index_hi) = IPA_MEM_v2_V6_MODEM_RT_INDEX_HI;
-	IPADBG("V6 RT MODEM INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v6_modem_rt_index_lo),
-		IPA_MEM_PART(v6_modem_rt_index_hi));
-
-	IPA_MEM_PART(v6_apps_rt_index_lo) = IPA_MEM_v2_V6_APPS_RT_INDEX_LO;
-	IPA_MEM_PART(v6_apps_rt_index_hi) = IPA_MEM_v2_V6_APPS_RT_INDEX_HI;
-	IPADBG("V6 RT APPS INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v6_apps_rt_index_lo),
-		IPA_MEM_PART(v6_apps_rt_index_hi));
-
-	IPA_MEM_PART(v6_rt_size) = IPA_MEM_v2_RAM_V6_RT_SIZE;
-	IPA_MEM_PART(v6_rt_size_ddr) = IPA_MEM_RAM_V6_RT_SIZE_DDR;
-	IPADBG("V6 RT SIZE 0x%x DDR SIZE 0x%x\n", IPA_MEM_PART(v6_rt_size),
-		IPA_MEM_PART(v6_rt_size_ddr));
-
-	IPA_MEM_PART(modem_hdr_ofst) = IPA_MEM_v2_RAM_MODEM_HDR_OFST;
-	IPA_MEM_PART(modem_hdr_size) = IPA_MEM_v2_RAM_MODEM_HDR_SIZE;
-	IPADBG("MODEM HDR OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(modem_hdr_ofst), IPA_MEM_PART(modem_hdr_size));
-
-	IPA_MEM_PART(apps_hdr_ofst) = IPA_MEM_v2_RAM_APPS_HDR_OFST;
-	IPA_MEM_PART(apps_hdr_size) = IPA_MEM_v2_RAM_APPS_HDR_SIZE;
-	IPA_MEM_PART(apps_hdr_size_ddr) = IPA_MEM_v2_RAM_HDR_SIZE_DDR;
-	IPADBG("APPS HDR OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(apps_hdr_ofst), IPA_MEM_PART(apps_hdr_size),
-		IPA_MEM_PART(apps_hdr_size_ddr));
-
-	IPA_MEM_PART(modem_ofst) = IPA_MEM_v2_RAM_MODEM_OFST;
-	IPA_MEM_PART(modem_size) = IPA_MEM_v2_RAM_MODEM_SIZE;
-	IPADBG("MODEM OFST 0x%x SIZE 0x%x\n", IPA_MEM_PART(modem_ofst),
-		IPA_MEM_PART(modem_size));
-
-	IPA_MEM_PART(apps_v4_flt_ofst) = IPA_MEM_v2_RAM_APPS_V4_FLT_OFST;
-	IPA_MEM_PART(apps_v4_flt_size) = IPA_MEM_v2_RAM_APPS_V4_FLT_SIZE;
-	IPADBG("V4 APPS FLT OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(apps_v4_flt_ofst), IPA_MEM_PART(apps_v4_flt_size));
-
-	IPA_MEM_PART(apps_v6_flt_ofst) = IPA_MEM_v2_RAM_APPS_V6_FLT_OFST;
-	IPA_MEM_PART(apps_v6_flt_size) = IPA_MEM_v2_RAM_APPS_V6_FLT_SIZE;
-	IPADBG("V6 APPS FLT OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(apps_v6_flt_ofst), IPA_MEM_PART(apps_v6_flt_size));
-
-	IPA_MEM_PART(uc_info_ofst) = IPA_MEM_v2_RAM_UC_INFO_OFST;
-	IPA_MEM_PART(uc_info_size) = IPA_MEM_v2_RAM_UC_INFO_SIZE;
-	IPADBG("V6 UC INFO OFST 0x%x SIZE 0x%x\n", IPA_MEM_PART(uc_info_ofst),
-		IPA_MEM_PART(uc_info_size));
-
-	IPA_MEM_PART(end_ofst) = IPA_MEM_v2_RAM_END_OFST;
-	IPA_MEM_PART(apps_v4_rt_ofst) = IPA_MEM_v2_RAM_APPS_V4_RT_OFST;
-	IPA_MEM_PART(apps_v4_rt_size) = IPA_MEM_v2_RAM_APPS_V4_RT_SIZE;
-	IPA_MEM_PART(apps_v6_rt_ofst) = IPA_MEM_v2_RAM_APPS_V6_RT_OFST;
-	IPA_MEM_PART(apps_v6_rt_size) = IPA_MEM_v2_RAM_APPS_V6_RT_SIZE;
-}
-
-static void ipa_init_mem_partition_v2_5(void)
-{
-	IPADBG("Memory partition IPA 2.5\n");
-	IPA_MEM_PART(nat_ofst) = IPA_RAM_NAT_OFST;
-	IPA_MEM_PART(nat_size) = IPA_RAM_NAT_SIZE;
-	IPADBG("NAT OFST 0x%x SIZE 0x%x\n", IPA_MEM_PART(nat_ofst),
-		IPA_MEM_PART(nat_size));
-
-	IPA_MEM_PART(uc_info_ofst) = IPA_MEM_v2_5_RAM_UC_INFO_OFST;
-	IPA_MEM_PART(uc_info_size) = IPA_MEM_v2_5_RAM_UC_INFO_SIZE;
-	IPADBG("UC INFO OFST 0x%x SIZE 0x%x\n", IPA_MEM_PART(uc_info_ofst),
-		IPA_MEM_PART(uc_info_size));
-
-	IPA_MEM_PART(ofst_start) = IPA_MEM_v2_5_RAM_OFST_START;
-	IPADBG("RAM OFST 0x%x\n", IPA_MEM_PART(ofst_start));
-
-	IPA_MEM_PART(v4_flt_ofst) = IPA_MEM_v2_5_RAM_V4_FLT_OFST;
-	IPA_MEM_PART(v4_flt_size) = IPA_MEM_v2_5_RAM_V4_FLT_SIZE;
-	IPA_MEM_PART(v4_flt_size_ddr) = IPA_MEM_RAM_V4_FLT_SIZE_DDR;
-	IPADBG("V4 FLT OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(v4_flt_ofst), IPA_MEM_PART(v4_flt_size),
-		IPA_MEM_PART(v4_flt_size_ddr));
-
-	IPA_MEM_PART(v6_flt_ofst) = IPA_MEM_v2_5_RAM_V6_FLT_OFST;
-	IPA_MEM_PART(v6_flt_size) = IPA_MEM_v2_5_RAM_V6_FLT_SIZE;
-	IPA_MEM_PART(v6_flt_size_ddr) = IPA_MEM_RAM_V6_FLT_SIZE_DDR;
-	IPADBG("V6 FLT OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(v6_flt_ofst), IPA_MEM_PART(v6_flt_size),
-		IPA_MEM_PART(v6_flt_size_ddr));
-
-	IPA_MEM_PART(v4_rt_ofst) = IPA_MEM_v2_5_RAM_V4_RT_OFST;
-	IPADBG("V4 RT OFST 0x%x\n", IPA_MEM_PART(v4_rt_ofst));
-
-	IPA_MEM_PART(v4_num_index) = IPA_MEM_v2_5_RAM_V4_NUM_INDEX;
-	IPADBG("V4 RT NUM INDEX 0x%x\n", IPA_MEM_PART(v4_num_index));
-
-	IPA_MEM_PART(v4_modem_rt_index_lo) = IPA_MEM_v2_5_V4_MODEM_RT_INDEX_LO;
-	IPA_MEM_PART(v4_modem_rt_index_hi) = IPA_MEM_v2_5_V4_MODEM_RT_INDEX_HI;
-	IPADBG("V4 RT MODEM INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v4_modem_rt_index_lo),
-		IPA_MEM_PART(v4_modem_rt_index_hi));
-
-	IPA_MEM_PART(v4_apps_rt_index_lo) = IPA_MEM_v2_5_V4_APPS_RT_INDEX_LO;
-	IPA_MEM_PART(v4_apps_rt_index_hi) = IPA_MEM_v2_5_V4_APPS_RT_INDEX_HI;
-	IPADBG("V4 RT APPS INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v4_apps_rt_index_lo),
-		IPA_MEM_PART(v4_apps_rt_index_hi));
-
-	IPA_MEM_PART(v4_rt_size) = IPA_MEM_v2_5_RAM_V4_RT_SIZE;
-	IPA_MEM_PART(v4_rt_size_ddr) = IPA_MEM_RAM_V4_RT_SIZE_DDR;
-	IPADBG("V4 RT SIZE 0x%x DDR SIZE 0x%x\n", IPA_MEM_PART(v4_rt_size),
-		IPA_MEM_PART(v4_rt_size_ddr));
-
-	IPA_MEM_PART(v6_rt_ofst) = IPA_MEM_v2_5_RAM_V6_RT_OFST;
-	IPADBG("V6 RT OFST 0x%x\n", IPA_MEM_PART(v6_rt_ofst));
-
-	IPA_MEM_PART(v6_num_index) = IPA_MEM_v2_5_RAM_V6_NUM_INDEX;
-	IPADBG("V6 RT NUM INDEX 0x%x\n", IPA_MEM_PART(v6_num_index));
-
-	IPA_MEM_PART(v6_modem_rt_index_lo) = IPA_MEM_v2_5_V6_MODEM_RT_INDEX_LO;
-	IPA_MEM_PART(v6_modem_rt_index_hi) = IPA_MEM_v2_5_V6_MODEM_RT_INDEX_HI;
-	IPADBG("V6 RT MODEM INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v6_modem_rt_index_lo),
-		IPA_MEM_PART(v6_modem_rt_index_hi));
-
-	IPA_MEM_PART(v6_apps_rt_index_lo) = IPA_MEM_v2_5_V6_APPS_RT_INDEX_LO;
-	IPA_MEM_PART(v6_apps_rt_index_hi) = IPA_MEM_v2_5_V6_APPS_RT_INDEX_HI;
-	IPADBG("V6 RT APPS INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v6_apps_rt_index_lo),
-		IPA_MEM_PART(v6_apps_rt_index_hi));
-
-	IPA_MEM_PART(v6_rt_size) = IPA_MEM_v2_5_RAM_V6_RT_SIZE;
-	IPA_MEM_PART(v6_rt_size_ddr) = IPA_MEM_RAM_V6_RT_SIZE_DDR;
-	IPADBG("V6 RT SIZE 0x%x DDR SIZE 0x%x\n", IPA_MEM_PART(v6_rt_size),
-		IPA_MEM_PART(v6_rt_size_ddr));
-
-	IPA_MEM_PART(modem_hdr_ofst) = IPA_MEM_v2_5_RAM_MODEM_HDR_OFST;
-	IPA_MEM_PART(modem_hdr_size) = IPA_MEM_v2_5_RAM_MODEM_HDR_SIZE;
-	IPADBG("MODEM HDR OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(modem_hdr_ofst), IPA_MEM_PART(modem_hdr_size));
-
-	IPA_MEM_PART(apps_hdr_ofst) = IPA_MEM_v2_5_RAM_APPS_HDR_OFST;
-	IPA_MEM_PART(apps_hdr_size) = IPA_MEM_v2_5_RAM_APPS_HDR_SIZE;
-	IPA_MEM_PART(apps_hdr_size_ddr) = IPA_MEM_v2_5_RAM_HDR_SIZE_DDR;
-	IPADBG("APPS HDR OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(apps_hdr_ofst), IPA_MEM_PART(apps_hdr_size),
-		IPA_MEM_PART(apps_hdr_size_ddr));
-
-	IPA_MEM_PART(modem_hdr_proc_ctx_ofst) =
-		IPA_MEM_v2_5_RAM_MODEM_HDR_PROC_CTX_OFST;
-	IPA_MEM_PART(modem_hdr_proc_ctx_size) =
-		IPA_MEM_v2_5_RAM_MODEM_HDR_PROC_CTX_SIZE;
-	IPADBG("MODEM HDR PROC CTX OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(modem_hdr_proc_ctx_ofst),
-		IPA_MEM_PART(modem_hdr_proc_ctx_size));
-
-	IPA_MEM_PART(apps_hdr_proc_ctx_ofst) =
-		IPA_MEM_v2_5_RAM_APPS_HDR_PROC_CTX_OFST;
-	IPA_MEM_PART(apps_hdr_proc_ctx_size) =
-		IPA_MEM_v2_5_RAM_APPS_HDR_PROC_CTX_SIZE;
-	IPA_MEM_PART(apps_hdr_proc_ctx_size_ddr) =
-		IPA_MEM_RAM_HDR_PROC_CTX_SIZE_DDR;
-	IPADBG("APPS HDR PROC CTX OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(apps_hdr_proc_ctx_ofst),
-		IPA_MEM_PART(apps_hdr_proc_ctx_size),
-		IPA_MEM_PART(apps_hdr_proc_ctx_size_ddr));
-
-	IPA_MEM_PART(modem_ofst) = IPA_MEM_v2_5_RAM_MODEM_OFST;
-	IPA_MEM_PART(modem_size) = IPA_MEM_v2_5_RAM_MODEM_SIZE;
-	IPADBG("MODEM OFST 0x%x SIZE 0x%x\n", IPA_MEM_PART(modem_ofst),
-		IPA_MEM_PART(modem_size));
-
-	IPA_MEM_PART(apps_v4_flt_ofst) = IPA_MEM_v2_5_RAM_APPS_V4_FLT_OFST;
-	IPA_MEM_PART(apps_v4_flt_size) = IPA_MEM_v2_5_RAM_APPS_V4_FLT_SIZE;
-	IPADBG("V4 APPS FLT OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(apps_v4_flt_ofst), IPA_MEM_PART(apps_v4_flt_size));
-
-	IPA_MEM_PART(apps_v6_flt_ofst) = IPA_MEM_v2_5_RAM_APPS_V6_FLT_OFST;
-	IPA_MEM_PART(apps_v6_flt_size) = IPA_MEM_v2_5_RAM_APPS_V6_FLT_SIZE;
-	IPADBG("V6 APPS FLT OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(apps_v6_flt_ofst), IPA_MEM_PART(apps_v6_flt_size));
-
-	IPA_MEM_PART(end_ofst) = IPA_MEM_v2_5_RAM_END_OFST;
-	IPA_MEM_PART(apps_v4_rt_ofst) = IPA_MEM_v2_5_RAM_APPS_V4_RT_OFST;
-	IPA_MEM_PART(apps_v4_rt_size) = IPA_MEM_v2_5_RAM_APPS_V4_RT_SIZE;
-	IPA_MEM_PART(apps_v6_rt_ofst) = IPA_MEM_v2_5_RAM_APPS_V6_RT_OFST;
-	IPA_MEM_PART(apps_v6_rt_size) = IPA_MEM_v2_5_RAM_APPS_V6_RT_SIZE;
-}
-
-static void ipa_init_mem_partition_v2_6L(void)
-{
-	IPADBG("Memory partition IPA 2.6Lite\n");
-	IPA_MEM_PART(nat_ofst) = IPA_RAM_NAT_OFST;
-	IPA_MEM_PART(nat_size) = IPA_RAM_NAT_SIZE;
-	IPADBG("NAT OFST 0x%x SIZE 0x%x\n", IPA_MEM_PART(nat_ofst),
-		IPA_MEM_PART(nat_size));
-
-	IPA_MEM_PART(uc_info_ofst) = IPA_MEM_v2_6L_RAM_UC_INFO_OFST;
-	IPA_MEM_PART(uc_info_size) = IPA_MEM_v2_6L_RAM_UC_INFO_SIZE;
-	IPADBG("V6 UC INFO OFST 0x%x SIZE 0x%x\n", IPA_MEM_PART(uc_info_ofst),
-		IPA_MEM_PART(uc_info_size));
-
-	IPA_MEM_PART(ofst_start) = IPA_MEM_v2_6L_RAM_OFST_START;
-	IPADBG("RAM OFST 0x%x\n", IPA_MEM_PART(ofst_start));
-
-	IPA_MEM_PART(v4_flt_ofst) = IPA_MEM_v2_6L_RAM_V4_FLT_OFST;
-	IPA_MEM_PART(v4_flt_size) = IPA_MEM_v2_6L_RAM_V4_FLT_SIZE;
-	IPA_MEM_PART(v4_flt_size_ddr) = IPA_MEM_RAM_V4_FLT_SIZE_DDR;
-	IPADBG("V4 FLT OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(v4_flt_ofst), IPA_MEM_PART(v4_flt_size),
-		IPA_MEM_PART(v4_flt_size_ddr));
-
-	IPA_MEM_PART(v6_flt_ofst) = IPA_MEM_v2_6L_RAM_V6_FLT_OFST;
-	IPA_MEM_PART(v6_flt_size) = IPA_MEM_v2_6L_RAM_V6_FLT_SIZE;
-	IPA_MEM_PART(v6_flt_size_ddr) = IPA_MEM_RAM_V6_FLT_SIZE_DDR;
-	IPADBG("V6 FLT OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(v6_flt_ofst), IPA_MEM_PART(v6_flt_size),
-		IPA_MEM_PART(v6_flt_size_ddr));
-
-	IPA_MEM_PART(v4_rt_ofst) = IPA_MEM_v2_6L_RAM_V4_RT_OFST;
-	IPADBG("V4 RT OFST 0x%x\n", IPA_MEM_PART(v4_rt_ofst));
-
-	IPA_MEM_PART(v4_num_index) = IPA_MEM_v2_6L_RAM_V4_NUM_INDEX;
-	IPADBG("V4 RT NUM INDEX 0x%x\n", IPA_MEM_PART(v4_num_index));
-
-	IPA_MEM_PART(v4_modem_rt_index_lo) = IPA_MEM_v2_6L_V4_MODEM_RT_INDEX_LO;
-	IPA_MEM_PART(v4_modem_rt_index_hi) = IPA_MEM_v2_6L_V4_MODEM_RT_INDEX_HI;
-	IPADBG("V4 RT MODEM INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v4_modem_rt_index_lo),
-		IPA_MEM_PART(v4_modem_rt_index_hi));
-
-	IPA_MEM_PART(v4_apps_rt_index_lo) = IPA_MEM_v2_6L_V4_APPS_RT_INDEX_LO;
-	IPA_MEM_PART(v4_apps_rt_index_hi) = IPA_MEM_v2_6L_V4_APPS_RT_INDEX_HI;
-	IPADBG("V4 RT APPS INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v4_apps_rt_index_lo),
-		IPA_MEM_PART(v4_apps_rt_index_hi));
-
-	IPA_MEM_PART(v4_rt_size) = IPA_MEM_v2_6L_RAM_V4_RT_SIZE;
-	IPA_MEM_PART(v4_rt_size_ddr) = IPA_MEM_RAM_V4_RT_SIZE_DDR;
-	IPADBG("V4 RT SIZE 0x%x DDR SIZE 0x%x\n", IPA_MEM_PART(v4_rt_size),
-		IPA_MEM_PART(v4_rt_size_ddr));
-
-	IPA_MEM_PART(v6_rt_ofst) = IPA_MEM_v2_6L_RAM_V6_RT_OFST;
-	IPADBG("V6 RT OFST 0x%x\n", IPA_MEM_PART(v6_rt_ofst));
-
-	IPA_MEM_PART(v6_num_index) = IPA_MEM_v2_6L_RAM_V6_NUM_INDEX;
-	IPADBG("V6 RT NUM INDEX 0x%x\n", IPA_MEM_PART(v6_num_index));
-
-	IPA_MEM_PART(v6_modem_rt_index_lo) = IPA_MEM_v2_6L_V6_MODEM_RT_INDEX_LO;
-	IPA_MEM_PART(v6_modem_rt_index_hi) = IPA_MEM_v2_6L_V6_MODEM_RT_INDEX_HI;
-	IPADBG("V6 RT MODEM INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v6_modem_rt_index_lo),
-		IPA_MEM_PART(v6_modem_rt_index_hi));
-
-	IPA_MEM_PART(v6_apps_rt_index_lo) = IPA_MEM_v2_6L_V6_APPS_RT_INDEX_LO;
-	IPA_MEM_PART(v6_apps_rt_index_hi) = IPA_MEM_v2_6L_V6_APPS_RT_INDEX_HI;
-	IPADBG("V6 RT APPS INDEXES 0x%x - 0x%x\n",
-		IPA_MEM_PART(v6_apps_rt_index_lo),
-		IPA_MEM_PART(v6_apps_rt_index_hi));
-
-	IPA_MEM_PART(v6_rt_size) = IPA_MEM_v2_6L_RAM_V6_RT_SIZE;
-	IPA_MEM_PART(v6_rt_size_ddr) = IPA_MEM_RAM_V6_RT_SIZE_DDR;
-	IPADBG("V6 RT SIZE 0x%x DDR SIZE 0x%x\n", IPA_MEM_PART(v6_rt_size),
-		IPA_MEM_PART(v6_rt_size_ddr));
-
-	IPA_MEM_PART(modem_hdr_ofst) = IPA_MEM_v2_6L_RAM_MODEM_HDR_OFST;
-	IPA_MEM_PART(modem_hdr_size) = IPA_MEM_v2_6L_RAM_MODEM_HDR_SIZE;
-	IPADBG("MODEM HDR OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(modem_hdr_ofst), IPA_MEM_PART(modem_hdr_size));
-
-	IPA_MEM_PART(apps_hdr_ofst) = IPA_MEM_v2_6L_RAM_APPS_HDR_OFST;
-	IPA_MEM_PART(apps_hdr_size) = IPA_MEM_v2_6L_RAM_APPS_HDR_SIZE;
-	IPA_MEM_PART(apps_hdr_size_ddr) = IPA_MEM_v2_6L_RAM_HDR_SIZE_DDR;
-	IPADBG("APPS HDR OFST 0x%x SIZE 0x%x DDR SIZE 0x%x\n",
-		IPA_MEM_PART(apps_hdr_ofst), IPA_MEM_PART(apps_hdr_size),
-		IPA_MEM_PART(apps_hdr_size_ddr));
-
-	IPA_MEM_PART(modem_comp_decomp_ofst) =
-		IPA_MEM_v2_6L_RAM_MODEM_COMP_DECOMP_OFST;
-	IPA_MEM_PART(modem_comp_decomp_size) =
-		IPA_MEM_v2_6L_RAM_MODEM_COMP_DECOMP_SIZE;
-	IPADBG("MODEM COMP DECOMP OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(modem_comp_decomp_ofst),
-		IPA_MEM_PART(modem_comp_decomp_size));
-
-	IPA_MEM_PART(modem_ofst) = IPA_MEM_v2_6L_RAM_MODEM_OFST;
-	IPA_MEM_PART(modem_size) = IPA_MEM_v2_6L_RAM_MODEM_SIZE;
-	IPADBG("MODEM OFST 0x%x SIZE 0x%x\n", IPA_MEM_PART(modem_ofst),
-		IPA_MEM_PART(modem_size));
-
-	IPA_MEM_PART(apps_v4_flt_ofst) = IPA_MEM_v2_6L_RAM_APPS_V4_FLT_OFST;
-	IPA_MEM_PART(apps_v4_flt_size) = IPA_MEM_v2_6L_RAM_APPS_V4_FLT_SIZE;
-	IPADBG("V4 APPS FLT OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(apps_v4_flt_ofst), IPA_MEM_PART(apps_v4_flt_size));
-
-	IPA_MEM_PART(apps_v6_flt_ofst) = IPA_MEM_v2_6L_RAM_APPS_V6_FLT_OFST;
-	IPA_MEM_PART(apps_v6_flt_size) = IPA_MEM_v2_6L_RAM_APPS_V6_FLT_SIZE;
-	IPADBG("V6 APPS FLT OFST 0x%x SIZE 0x%x\n",
-		IPA_MEM_PART(apps_v6_flt_ofst), IPA_MEM_PART(apps_v6_flt_size));
-
-	IPA_MEM_PART(end_ofst) = IPA_MEM_v2_6L_RAM_END_OFST;
-	IPA_MEM_PART(apps_v4_rt_ofst) = IPA_MEM_v2_6L_RAM_APPS_V4_RT_OFST;
-	IPA_MEM_PART(apps_v4_rt_size) = IPA_MEM_v2_6L_RAM_APPS_V4_RT_SIZE;
-	IPA_MEM_PART(apps_v6_rt_ofst) = IPA_MEM_v2_6L_RAM_APPS_V6_RT_OFST;
-	IPA_MEM_PART(apps_v6_rt_size) = IPA_MEM_v2_6L_RAM_APPS_V6_RT_SIZE;
-}
-
-/**
- * ipa_controller_shared_static_bind() - set the appropriate shared methods for
- * for IPA HW version 2.0, 2.5, 2.6 and 2.6L
- *
- *  @ctrl: data structure which holds the function pointers
- */
-void ipa_controller_shared_static_bind(struct ipa_controller *ctrl)
-{
-	ctrl->ipa_init_rt4 = _ipa_init_rt4_v2;
-	ctrl->ipa_init_rt6 = _ipa_init_rt6_v2;
-	ctrl->ipa_init_flt4 = _ipa_init_flt4_v2;
-	ctrl->ipa_init_flt6 = _ipa_init_flt6_v2;
-	ctrl->ipa_cfg_ep_hdr = _ipa_cfg_ep_hdr_v2_0;
-	ctrl->ipa_cfg_ep_nat = _ipa_cfg_ep_nat_v2_0;
-	ctrl->ipa_cfg_ep_aggr = _ipa_cfg_ep_aggr_v2_0;
-	ctrl->ipa_cfg_ep_deaggr = _ipa_cfg_ep_deaggr_v2_0;
-	ctrl->ipa_cfg_ep_mode = _ipa_cfg_ep_mode_v2_0;
-	ctrl->ipa_cfg_ep_route = _ipa_cfg_ep_route_v2_0;
-	ctrl->ipa_cfg_route = _ipa_cfg_route_v2_0;
-	ctrl->ipa_cfg_ep_status = _ipa_cfg_ep_status_v2_0;
-	ctrl->ipa_cfg_ep_cfg = _ipa_cfg_ep_cfg_v2_0;
-	ctrl->ipa_cfg_ep_metadata_mask = _ipa_cfg_ep_metadata_mask_v2_0;
-	ctrl->ipa_clk_rate_turbo = IPA_V2_0_CLK_RATE_TURBO;
-	ctrl->ipa_clk_rate_nominal = IPA_V2_0_CLK_RATE_NOMINAL;
-	ctrl->ipa_clk_rate_svs = IPA_V2_0_CLK_RATE_SVS;
-	ctrl->ipa_read_gen_reg = _ipa_read_gen_reg_v2_0;
-	ctrl->ipa_read_ep_reg = _ipa_read_ep_reg_v2_0;
-	ctrl->ipa_write_dbg_cnt = _ipa_write_dbg_cnt_v2_0;
-	ctrl->ipa_read_dbg_cnt = _ipa_read_dbg_cnt_v2_0;
-	ctrl->ipa_commit_flt = __ipa_commit_flt_v2;
-	ctrl->ipa_commit_rt = __ipa_commit_rt_v2;
-	ctrl->ipa_commit_hdr = __ipa_commit_hdr_v2;
-	ctrl->ipa_enable_clks = _ipa_enable_clks_v2_0;
-	ctrl->ipa_disable_clks = _ipa_disable_clks_v2_0;
-	ctrl->msm_bus_data_ptr = &ipa_bus_client_pdata_v2_0;
-	ctrl->ipa_cfg_ep_metadata = _ipa_cfg_ep_metadata_v2_0;
-	ctrl->clock_scaling_bw_threshold_nominal =
-		IPA_V2_0_BW_THRESHOLD_NOMINAL_MBPS;
-	ctrl->clock_scaling_bw_threshold_turbo =
-		IPA_V2_0_BW_THRESHOLD_TURBO_MBPS;
-}
-
-/**
- * ipa_ctrl_static_bind() - set the appropriate methods for
- *  IPA Driver based on the HW version
- *
- *  @ctrl: data structure which holds the function pointers
- *  @hw_type: the HW type in use
- *
- *  This function can avoid the runtime assignment by using C99 special
- *  struct initialization - hard decision... time.vs.mem
- */
-int ipa_controller_static_bind(struct ipa_controller *ctrl,
-		enum ipa_hw_type hw_type)
-{
-	switch (hw_type) {
-	case (IPA_HW_v1_1):
-		ipa_init_mem_partition_v2();
-		ctrl->ipa_sram_read_settings = _ipa_sram_settings_read_v1_1;
-		ctrl->ipa_cfg_ep_hdr = _ipa_cfg_ep_hdr_v1_1;
-		ctrl->ipa_cfg_ep_hdr_ext = _ipa_cfg_ep_hdr_ext_v1_1;
-		ctrl->ipa_cfg_ep_aggr = _ipa_cfg_ep_aggr_v1_1;
-		ctrl->ipa_cfg_ep_deaggr = _ipa_cfg_ep_deaggr_v1_1;
-		ctrl->ipa_cfg_ep_nat = _ipa_cfg_ep_nat_v1_1;
-		ctrl->ipa_cfg_ep_mode = _ipa_cfg_ep_mode_v1_1;
-		ctrl->ipa_cfg_ep_route = _ipa_cfg_ep_route_v1_1;
-		ctrl->ipa_cfg_ep_holb = _ipa_cfg_ep_holb_v1_1;
-		ctrl->ipa_cfg_route = _ipa_cfg_route_v1_1;
-		ctrl->ipa_cfg_ep_status = _ipa_cfg_ep_status_v1_1;
-		ctrl->ipa_cfg_ep_cfg = _ipa_cfg_ep_cfg_v1_1;
-		ctrl->ipa_cfg_ep_metadata_mask = _ipa_cfg_ep_metadata_mask_v1_1;
-		ctrl->ipa_clk_rate_turbo = IPA_V1_1_CLK_RATE;
-		ctrl->ipa_clk_rate_nominal = IPA_V1_1_CLK_RATE;
-		ctrl->ipa_clk_rate_svs = IPA_V1_1_CLK_RATE;
-		ctrl->ipa_read_gen_reg = _ipa_read_gen_reg_v1_1;
-		ctrl->ipa_read_ep_reg = _ipa_read_ep_reg_v1_1;
-		ctrl->ipa_write_dbg_cnt = _ipa_write_dbg_cnt_v1_1;
-		ctrl->ipa_read_dbg_cnt = _ipa_read_dbg_cnt_v1_1;
-		ctrl->ipa_commit_flt = __ipa_commit_flt_v1_1;
-		ctrl->ipa_commit_rt = __ipa_commit_rt_v1_1;
-		ctrl->ipa_commit_hdr = __ipa_commit_hdr_v1_1;
-		ctrl->ipa_enable_clks = _ipa_enable_clks_v1_1;
-		ctrl->ipa_disable_clks = _ipa_disable_clks_v1_1;
-		ctrl->msm_bus_data_ptr = &ipa_bus_client_pdata_v1_1;
-		ctrl->ipa_cfg_ep_metadata = _ipa_cfg_ep_metadata_v1_1;
-		ctrl->ipa_reg_base_ofst = IPA_REG_BASE_OFST_v2_0;
-		ctrl->max_holb_tmr_val = IPA_V1_MAX_HOLB_TMR_VAL;
-		break;
-	case (IPA_HW_v2_0):
-		ipa_init_mem_partition_v2();
-		ipa_controller_shared_static_bind(ctrl);
-		ctrl->ipa_cfg_ep_holb = _ipa_cfg_ep_holb_v2_0;
-		ctrl->ipa_reg_base_ofst = IPA_REG_BASE_OFST_v2_0;
-		ctrl->max_holb_tmr_val = IPA_V2_0_MAX_HOLB_TMR_VAL;
-		ctrl->ipa_cfg_ep_hdr_ext = _ipa_cfg_ep_hdr_ext_v2_0;
-		ctrl->ipa_sram_read_settings = _ipa_sram_settings_read_v2_0;
-		ctrl->ipa_init_sram = _ipa_init_sram_v2;
-		ctrl->ipa_init_hdr = _ipa_init_hdr_v2;
-		ctrl->ipa_commit_hdr = __ipa_commit_hdr_v2;
-		ctrl->ipa_generate_rt_hw_rule = __ipa_generate_rt_hw_rule_v2;
-		break;
-	case (IPA_HW_v2_5):
-		ipa_init_mem_partition_v2_5();
-		ipa_controller_shared_static_bind(ctrl);
-		ctrl->ipa_cfg_ep_holb = _ipa_cfg_ep_holb_v2_5;
-		ctrl->ipa_reg_base_ofst = IPA_REG_BASE_OFST_v2_5;
-		ctrl->max_holb_tmr_val = IPA_V2_5_MAX_HOLB_TMR_VAL;
-		ctrl->ipa_cfg_ep_hdr_ext = _ipa_cfg_ep_hdr_ext_v2_5;
-		ctrl->ipa_sram_read_settings = _ipa_sram_settings_read_v2_5;
-		ctrl->ipa_init_sram = _ipa_init_sram_v2_5;
-		ctrl->ipa_init_hdr = _ipa_init_hdr_v2_5;
-		ctrl->ipa_commit_hdr = __ipa_commit_hdr_v2_5;
-		ctrl->ipa_generate_rt_hw_rule = __ipa_generate_rt_hw_rule_v2_5;
-		break;
-	case (IPA_HW_v2_6L):
-		ipa_init_mem_partition_v2_6L();
-		ipa_controller_shared_static_bind(ctrl);
-		ctrl->ipa_cfg_ep_holb = _ipa_cfg_ep_holb_v2_6L;
-		ctrl->ipa_reg_base_ofst = IPA_REG_BASE_OFST_v2_6L;
-		ctrl->max_holb_tmr_val = IPA_V2_6L_MAX_HOLB_TMR_VAL;
-		ctrl->ipa_cfg_ep_hdr_ext = _ipa_cfg_ep_hdr_ext_v2_6L;
-		ctrl->ipa_sram_read_settings = _ipa_sram_settings_read_v2_6L;
-		ctrl->ipa_init_sram = _ipa_init_sram_v2_6L;
-		ctrl->ipa_init_hdr = _ipa_init_hdr_v2_6L;
-		ctrl->ipa_commit_hdr = __ipa_commit_hdr_v2_6L;
-		ctrl->ipa_generate_rt_hw_rule = __ipa_generate_rt_hw_rule_v2_6L;
-		break;
-	default:
-		return -EPERM;
-	}
-
-	return 0;
-}
-
-void ipa_skb_recycle(struct sk_buff *skb)
-{
-	struct skb_shared_info *shinfo;
-
-	shinfo = skb_shinfo(skb);
-	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
-	atomic_set(&shinfo->dataref, 1);
-
-	memset(skb, 0, offsetof(struct sk_buff, tail));
-	skb->data = skb->head + NET_SKB_PAD;
-	skb_reset_tail_pointer(skb);
-}
-
-int ipa_id_alloc(void *ptr)
-{
-	int id;
-
-	idr_preload(GFP_KERNEL);
-	spin_lock(&ipa_ctx->idr_lock);
-	id = idr_alloc(&ipa_ctx->ipa_idr, ptr, 0, 0, GFP_NOWAIT);
-	spin_unlock(&ipa_ctx->idr_lock);
-	idr_preload_end();
-
-	return id;
-}
-
-void *ipa_id_find(u32 id)
-{
-	void *ptr;
-
-	spin_lock(&ipa_ctx->idr_lock);
-	ptr = idr_find(&ipa_ctx->ipa_idr, id);
-	spin_unlock(&ipa_ctx->idr_lock);
-
-	return ptr;
-}
-
-void ipa_id_remove(u32 id)
-{
-	spin_lock(&ipa_ctx->idr_lock);
-	idr_remove(&ipa_ctx->ipa_idr, id);
-	spin_unlock(&ipa_ctx->idr_lock);
-}
-
-static void ipa_tag_free_buf(void *user1, int user2)
-{
-	kfree(user1);
-}
-
-static void ipa_tag_free_skb(void *user1, int user2)
-{
-	dev_kfree_skb_any((struct sk_buff *)user1);
-}
-
-#define REQUIRED_TAG_PROCESS_DESCRIPTORS 4
-
-/* ipa_tag_process() - Initiates a tag process. Incorporates the input
- * descriptors
- *
- * @desc:	descriptors with commands for IC
- * @desc_size:	amount of descriptors in the above variable
- *
- * Note: The descriptors are copied (if there's room), the client needs to
- * free his descriptors afterwards
- *
- * Return: 0 or negative in case of failure
- */
-int ipa_tag_process(struct ipa_desc desc[],
-	int descs_num,
-	unsigned long timeout)
-{
-	struct ipa_sys_context *sys;
-	struct ipa_desc *tag_desc;
-	int desc_idx = 0;
-	struct ipa_ip_packet_init *pkt_init;
-	struct ipa_register_write *reg_write_nop;
-	struct ipa_ip_packet_tag_status *status;
-	int i;
-	struct sk_buff *dummy_skb;
-	int res;
-	struct ipa_tag_completion *comp;
-	int ep_idx;
-
-	/* Not enough room for the required descriptors for the tag process */
-	if (IPA_TAG_MAX_DESC - descs_num < REQUIRED_TAG_PROCESS_DESCRIPTORS) {
-		IPAERR("up to %d descriptors are allowed (received %d)\n",
-		       IPA_TAG_MAX_DESC - REQUIRED_TAG_PROCESS_DESCRIPTORS,
-		       descs_num);
-		return -ENOMEM;
-	}
-
-	ep_idx = ipa_get_ep_mapping(IPA_CLIENT_APPS_CMD_PROD);
-	if (-1 == ep_idx) {
-		IPAERR("Client %u is not mapped\n",
-			IPA_CLIENT_APPS_CMD_PROD);
-		return -EFAULT;
-	}
-	sys = ipa_ctx->ep[ep_idx].sys;
-
-	tag_desc = kzalloc(sizeof(*tag_desc) * IPA_TAG_MAX_DESC, GFP_KERNEL);
-	if (!tag_desc) {
-		IPAERR("failed to allocate memory\n");
-		res = -ENOMEM;
-		goto fail_alloc_desc;
-	}
-
-	/* IP_PACKET_INIT IC for tag status to be sent to apps */
-	pkt_init = kzalloc(sizeof(*pkt_init), GFP_KERNEL);
-	if (!pkt_init) {
-		IPAERR("failed to allocate memory\n");
-		res = -ENOMEM;
-		goto fail_alloc_pkt_init;
-	}
-
-	pkt_init->destination_pipe_index =
-		ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_CONS);
-
-	tag_desc[desc_idx].opcode = IPA_IP_PACKET_INIT;
-	tag_desc[desc_idx].pyld = pkt_init;
-	tag_desc[desc_idx].len = sizeof(*pkt_init);
-	tag_desc[desc_idx].type = IPA_IMM_CMD_DESC;
-	tag_desc[desc_idx].callback = ipa_tag_free_buf;
-	tag_desc[desc_idx].user1 = pkt_init;
-	desc_idx++;
-
-	/* NO-OP IC for ensuring that IPA pipeline is empty */
-	reg_write_nop = kzalloc(sizeof(*reg_write_nop), GFP_KERNEL);
-	if (!reg_write_nop) {
-		IPAERR("no mem\n");
-		res = -ENOMEM;
-		goto fail_free_desc;
-	}
-
-	reg_write_nop->skip_pipeline_clear = 0;
-	reg_write_nop->value_mask = 0x0;
-
-	tag_desc[desc_idx].opcode = IPA_REGISTER_WRITE;
-	tag_desc[desc_idx].pyld = reg_write_nop;
-	tag_desc[desc_idx].len = sizeof(*reg_write_nop);
-	tag_desc[desc_idx].type = IPA_IMM_CMD_DESC;
-	tag_desc[desc_idx].callback = ipa_tag_free_buf;
-	tag_desc[desc_idx].user1 = reg_write_nop;
-	desc_idx++;
-
-	/* status IC */
-	status = kzalloc(sizeof(*status), GFP_KERNEL);
-	if (!status) {
-		IPAERR("no mem\n");
-		res = -ENOMEM;
-		goto fail_free_desc;
-	}
-
-	status->tag_f_2 = IPA_COOKIE;
-
-	tag_desc[desc_idx].opcode = IPA_IP_PACKET_TAG_STATUS;
-	tag_desc[desc_idx].pyld = status;
-	tag_desc[desc_idx].len = sizeof(*status);
-	tag_desc[desc_idx].type = IPA_IMM_CMD_DESC;
-	tag_desc[desc_idx].callback = ipa_tag_free_buf;
-	tag_desc[desc_idx].user1 = status;
-	desc_idx++;
-
-	/* Copy the required descriptors from the client now */
-	if (desc) {
-		memcpy(&(tag_desc[desc_idx]), desc, descs_num *
-			sizeof(struct ipa_desc));
-		desc_idx += descs_num;
-	}
-
-	comp = kzalloc(sizeof(*comp), GFP_KERNEL);
-	if (!comp) {
-		IPAERR("no mem\n");
-		res = -ENOMEM;
-		goto fail_free_desc;
-	}
-	init_completion(&comp->comp);
-
-	/* completion needs to be released from both here and rx handler */
-	atomic_set(&comp->cnt, 2);
-
-	/* dummy packet to send to IPA. packet payload is a completion object */
-	dummy_skb = alloc_skb(sizeof(comp), GFP_KERNEL);
-	if (!dummy_skb) {
-		IPAERR("failed to allocate memory\n");
-		res = -ENOMEM;
-		goto fail_free_skb;
-	}
-
-	memcpy(skb_put(dummy_skb, sizeof(comp)), &comp, sizeof(comp));
-
-	tag_desc[desc_idx].pyld = dummy_skb->data;
-	tag_desc[desc_idx].len = dummy_skb->len;
-	tag_desc[desc_idx].type = IPA_DATA_DESC_SKB;
-	tag_desc[desc_idx].callback = ipa_tag_free_skb;
-	tag_desc[desc_idx].user1 = dummy_skb;
-	desc_idx++;
-
-	/* send all descriptors to IPA with single EOT */
-	res = ipa_send(sys, desc_idx, tag_desc, true);
-	if (res) {
-		IPAERR("failed to send TAG packets %d\n", res);
-		res = -ENOMEM;
-		goto fail_send;
-	}
-	kfree(tag_desc);
-	tag_desc = NULL;
-
-	IPADBG("waiting for TAG response\n");
-	res = wait_for_completion_timeout(&comp->comp, timeout);
-	if (res == 0) {
-		IPAERR("timeout (%lu msec) on waiting for TAG response\n",
-			timeout);
-		WARN_ON(1);
-		if (atomic_dec_return(&comp->cnt) == 0)
-			kfree(comp);
-		return -ETIME;
-	}
-
-	IPADBG("TAG response arrived!\n");
-	if (atomic_dec_return(&comp->cnt) == 0)
-		kfree(comp);
-
-	/* sleep for short period to ensure IPA wrote all packets to BAM */
-	usleep_range(IPA_TAG_SLEEP_MIN_USEC, IPA_TAG_SLEEP_MAX_USEC);
-
-	return 0;
-
-fail_send:
-	dev_kfree_skb_any(dummy_skb);
-	desc_idx--;
-fail_free_skb:
-	kfree(comp);
-fail_free_desc:
-	/*
-	 * Free only the first descriptors allocated here.
-	 * [pkt_init, status, nop]
-	 * The user is responsible to free his allocations
-	 * in case of failure.
-	 * The min is required because we may fail during
-	 * of the initial allocations above
-	 */
-	for (i = 0; i < min(REQUIRED_TAG_PROCESS_DESCRIPTORS-1, desc_idx); i++)
-		kfree(tag_desc[i].user1);
-
-fail_alloc_pkt_init:
-	kfree(tag_desc);
-fail_alloc_desc:
-	return res;
-}
-
-/**
- * ipa_tag_generate_force_close_desc() - generate descriptors for force close
- *					 immediate command
- *
- * @desc: descriptors for IC
- * @desc_size: desc array size
- * @start_pipe: first pipe to close aggregation
- * @end_pipe: last (non-inclusive) pipe to close aggregation
- *
- * Return: number of descriptors written or negative in case of failure
- */
-static int ipa_tag_generate_force_close_desc(struct ipa_desc desc[],
-	int desc_size, int start_pipe, int end_pipe)
-{
-	int i;
-	u32 aggr_init;
-	int desc_idx = 0;
-	int res;
-	struct ipa_register_write *reg_write_agg_close;
-
-	for (i = start_pipe; i < end_pipe; i++) {
-		aggr_init = ipa_read_reg(ipa_ctx->mmio,
-			IPA_ENDP_INIT_AGGR_N_OFST_v2_0(i));
-		if (((aggr_init & IPA_ENDP_INIT_AGGR_N_AGGR_EN_BMSK) >>
-			IPA_ENDP_INIT_AGGR_N_AGGR_EN_SHFT) != IPA_ENABLE_AGGR)
-			continue;
-		IPADBG("Force close ep: %d\n", i);
-		if (desc_idx + 1 > desc_size) {
-			IPAERR("Internal error - no descriptors\n");
-			res = -EFAULT;
-			goto fail_no_desc;
-		}
-
-		reg_write_agg_close = kzalloc(sizeof(*reg_write_agg_close),
-			GFP_KERNEL);
-		if (!reg_write_agg_close) {
-			IPAERR("no mem\n");
-			res = -ENOMEM;
-			goto fail_alloc_reg_write_agg_close;
-		}
-
-		reg_write_agg_close->skip_pipeline_clear = 0;
-		reg_write_agg_close->offset = IPA_ENDP_INIT_AGGR_N_OFST_v2_0(i);
-		reg_write_agg_close->value =
-			(1 & IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_BMSK) <<
-			IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_SHFT;
-		reg_write_agg_close->value_mask =
-			IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_BMSK <<
-			IPA_ENDP_INIT_AGGR_n_AGGR_FORCE_CLOSE_SHFT;
-
-		desc[desc_idx].opcode = IPA_REGISTER_WRITE;
-		desc[desc_idx].pyld = reg_write_agg_close;
-		desc[desc_idx].len = sizeof(*reg_write_agg_close);
-		desc[desc_idx].type = IPA_IMM_CMD_DESC;
-		desc[desc_idx].callback = ipa_tag_free_buf;
-		desc[desc_idx].user1 = reg_write_agg_close;
-		desc_idx++;
-	}
-
-	return desc_idx;
-
-fail_alloc_reg_write_agg_close:
-	for (i = 0; i < desc_idx; i++)
-		kfree(desc[desc_idx].user1);
-fail_no_desc:
-	return res;
-}
-
-/**
- * ipa_tag_aggr_force_close() - Force close aggregation
- *
- * @pipe_num: pipe number or -1 for all pipes
- */
-int ipa_tag_aggr_force_close(int pipe_num)
-{
-	struct ipa_desc *desc;
-	int res = -1;
-	int start_pipe;
-	int end_pipe;
-	int num_descs;
-	int num_aggr_descs;
-
-	if (pipe_num < -1 || pipe_num >= (int)ipa_ctx->ipa_num_pipes) {
-		IPAERR("Invalid pipe number %d\n", pipe_num);
-		return -EINVAL;
-	}
-
-	if (pipe_num == -1) {
-		start_pipe = 0;
-		end_pipe = ipa_ctx->ipa_num_pipes;
-	} else {
-		start_pipe = pipe_num;
-		end_pipe = pipe_num + 1;
-	}
-
-	num_descs = end_pipe - start_pipe;
-
-	desc = kzalloc(sizeof(*desc) * num_descs, GFP_KERNEL);
-	if (!desc) {
-		IPAERR("no mem\n");
-		return -ENOMEM;
-	}
-
-	/* Force close aggregation on all valid pipes with aggregation */
-	num_aggr_descs = ipa_tag_generate_force_close_desc(desc, num_descs,
-						start_pipe, end_pipe);
-	if (num_aggr_descs < 0) {
-		IPAERR("ipa_tag_generate_force_close_desc failed %d\n",
-			num_aggr_descs);
-		goto fail_free_desc;
-	}
-
-	res = ipa_tag_process(desc, num_aggr_descs,
-			      IPA_FORCE_CLOSE_TAG_PROCESS_TIMEOUT);
-
-fail_free_desc:
-	kfree(desc);
-
-	return res;
-}
-
-/**
- * ipa_is_ready() - check if IPA module was initialized
- * successfully
- *
- * Return value: true for yes; false for no
- */
-bool ipa_is_ready(void)
-{
-	return (ipa_ctx != NULL) ? true : false;
-}
-EXPORT_SYMBOL(ipa_is_ready);
-
-/**
- * ipa_is_client_handle_valid() - check if IPA client handle is valid handle
- *
- * Return value: true for yes; false for no
- */
-bool ipa_is_client_handle_valid(u32 clnt_hdl)
-{
-	if (unlikely(!ipa_ctx)) {
-		IPAERR("IPA driver was not initialized\n");
-		return false;
-	}
-
-	if (clnt_hdl >= 0 && clnt_hdl < ipa_ctx->ipa_num_pipes)
-		return true;
-	return false;
-}
-EXPORT_SYMBOL(ipa_is_client_handle_valid);
-
-/**
- * ipa_proxy_clk_unvote() - called to remove IPA clock proxy vote
- *
- * Return value: none
- */
-void ipa_proxy_clk_unvote(void)
-{
-	if (ipa_is_ready() && ipa_ctx->q6_proxy_clk_vote_valid) {
-		ipa_dec_client_disable_clks();
-		ipa_ctx->q6_proxy_clk_vote_valid = false;
-	}
-}
-EXPORT_SYMBOL(ipa_proxy_clk_unvote);
-
-/**
- * ipa_proxy_clk_vote() - called to add IPA clock proxy vote
- *
- * Return value: none
- */
-void ipa_proxy_clk_vote(void)
-{
-	if (ipa_is_ready() && !ipa_ctx->q6_proxy_clk_vote_valid) {
-		ipa_inc_client_enable_clks();
-		ipa_ctx->q6_proxy_clk_vote_valid = true;
-	}
-}
-EXPORT_SYMBOL(ipa_proxy_clk_vote);
-
-/**
- * ipa_get_hw_type() - Return IPA HW version
- *
- * Return value: enum ipa_hw_type
- */
-enum ipa_hw_type ipa_get_hw_type(void)
-{
-	if (ipa_ctx)
-		return ipa_ctx->ipa_hw_type;
-	else
-		return IPA_HW_None;
-}
-EXPORT_SYMBOL(ipa_get_hw_type);
-
-/**
- * ipa_get_smem_restr_bytes()- Return IPA smem restricted bytes
- *
- * Return value: u16 - number of IPA smem restricted bytes
- */
-u16 ipa_get_smem_restr_bytes(void)
-{
-	if (ipa_ctx) {
-		return ipa_ctx->smem_restricted_bytes;
-	} else {
-		IPAERR("IPA driver was not initialized\n");
-		return 0;
-	}
-}
-EXPORT_SYMBOL(ipa_get_smem_restr_bytes);
-
-/**
- * ipa_get_modem_cfg_emb_pipe_flt()- Return ipa_ctx->modem_cfg_emb_pipe_flt
- *
- * Return value: true if modem configures embedded pipe flt, false otherwise
- */
-bool ipa_get_modem_cfg_emb_pipe_flt(void)
-{
-	if (ipa_ctx) {
-		return ipa_ctx->modem_cfg_emb_pipe_flt;
-	} else {
-		IPAERR("IPA driver has not been initialized\n");
-		return false;
-	}
-}
-EXPORT_SYMBOL(ipa_get_modem_cfg_emb_pipe_flt);
-
-u32 ipa_get_num_pipes(void)
-{
-	if (ipa_ctx->ipa_hw_type == IPA_HW_v2_6L)
-		return ipa_read_reg(ipa_ctx->mmio, IPA_ENABLED_PIPES_OFST);
-	else
-		return IPA_MAX_NUM_PIPES;
-}
-EXPORT_SYMBOL(ipa_get_num_pipes);
-
-/**
- * ipa_disable_apps_wan_cons_deaggr()- set ipa_ctx->ipa_client_apps_wan_cons_agg_gro
- *
- * Return value: 0 or negative in case of failure
- */
-int ipa_disable_apps_wan_cons_deaggr(uint32_t agg_size, uint32_t agg_count)
-{
-	int res = -1;
-
-	/* checking if IPA-HW can support */
-	if ((agg_size >> 10) >
-		IPA_AGGR_BYTE_LIMIT) {
-		IPAWANERR("IPA-AGG byte limit %d\n",
-		IPA_AGGR_BYTE_LIMIT);
-		IPAWANERR("exceed aggr_byte_limit\n");
-		return res;
-		}
-	if (agg_count >
-		IPA_AGGR_PKT_LIMIT) {
-		IPAWANERR("IPA-AGG pkt limit %d\n",
-		IPA_AGGR_PKT_LIMIT);
-		IPAWANERR("exceed aggr_pkt_limit\n");
-		return res;
-	}
-
-	if (ipa_ctx) {
-		ipa_ctx->ipa_client_apps_wan_cons_agg_gro = true;
-		return 0;
-	}
-	return res;
-}
-EXPORT_SYMBOL(ipa_disable_apps_wan_cons_deaggr);
-
-/**
- * ipa_get_sys_yellow_wm()- Return yellow WM value for IPA SYS pipes.
- *
- * Return value: IPA_YELLOW_MARKER_SYS_CFG_OFST register if IPA_HW_v2.6L,
- *               0 otherwise.
- */
-u32 ipa_get_sys_yellow_wm(void)
-{
-	if (ipa_ctx->ipa_hw_type == IPA_HW_v2_6L)
-		return ipa_read_reg(ipa_ctx->mmio,
-			IPA_YELLOW_MARKER_SYS_CFG_OFST);
-	else
-		return 0;
-}
-EXPORT_SYMBOL(ipa_get_sys_yellow_wm);
-
-void ipa_suspend_apps_pipes(bool suspend)
-{
-	struct ipa_ep_cfg_ctrl cfg;
-	int ipa_ep_idx;
-	u32 lan_empty = 0, wan_empty = 0;
-	int ret;
-	struct sps_event_notify notify;
-	struct ipa_ep_context *ep;
-
-	memset(&cfg, 0, sizeof(cfg));
-	cfg.ipa_ep_suspend = suspend;
-
-	ipa_ep_idx = ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_CONS);
-	ep = &ipa_ctx->ep[ipa_ep_idx];
-	if (ep->valid) {
-		ipa_cfg_ep_ctrl(ipa_ep_idx, &cfg);
-		/* Check if the pipes are empty. */
-		ret = sps_is_pipe_empty(ep->ep_hdl, &lan_empty);
-		if (ret) {
-			IPAERR("%s: sps_is_pipe_empty failed with %d\n",
-				__func__, ret);
-		}
-		if (!lan_empty) {
-			IPADBG("LAN Cons is not-empty. Enter poll mode.\n");
-			notify.user = ep->sys;
-			notify.event_id = SPS_EVENT_EOT;
-			if (ep->sys->sps_callback)
-				ep->sys->sps_callback(&notify);
-		}
-	}
-
-	ipa_ep_idx = ipa_get_ep_mapping(IPA_CLIENT_APPS_WAN_CONS);
-	/* Considering the case for SSR. */
-	if (ipa_ep_idx == -1) {
-		IPADBG("Invalid client.\n");
-		return;
-	}
-	ep = &ipa_ctx->ep[ipa_ep_idx];
-	if (ep->valid) {
-		ipa_cfg_ep_ctrl(ipa_ep_idx, &cfg);
-		/* Check if the pipes are empty. */
-		ret = sps_is_pipe_empty(ep->ep_hdl, &wan_empty);
-		if (ret) {
-			IPAERR("%s: sps_is_pipe_empty failed with %d\n",
-				__func__, ret);
-		}
-		if (!wan_empty) {
-			IPADBG("WAN Cons is not-empty. Enter poll mode.\n");
-			notify.user = ep->sys;
-			notify.event_id = SPS_EVENT_EOT;
-			if (ep->sys->sps_callback)
-				ep->sys->sps_callback(&notify);
-		}
-	}
-}
diff --git a/drivers/platform/msm/ipa/odu_bridge.c b/drivers/platform/msm/ipa/odu_bridge.c
deleted file mode 100644
index 8d99e18..00000000
--- a/drivers/platform/msm/ipa/odu_bridge.c
+++ /dev/null
@@ -1,1212 +0,0 @@
-/* Copyright (c) 2014-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/debugfs.h>
-#include <linux/export.h>
-#include <linux/fs.h>
-#include <linux/if_ether.h>
-#include <linux/ioctl.h>
-#include <linux/kernel.h>
-#include <linux/module.h>
-#include <linux/msm_ipa.h>
-#include <linux/mutex.h>
-#include <linux/skbuff.h>
-#include <linux/types.h>
-#include <linux/ipv6.h>
-#include <net/addrconf.h>
-#include <linux/ipa.h>
-#include "ipa_i.h"
-
-#define ODU_BRIDGE_DRV_NAME "odu_ipa_bridge"
-
-#define ODU_BRIDGE_DBG(fmt, args...) \
-	pr_debug(ODU_BRIDGE_DRV_NAME " %s:%d " fmt, \
-		 __func__, __LINE__, ## args)
-#define ODU_BRIDGE_ERR(fmt, args...) \
-	pr_err(ODU_BRIDGE_DRV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-#define ODU_BRIDGE_FUNC_ENTRY() \
-	ODU_BRIDGE_DBG("ENTRY\n")
-#define ODU_BRIDGE_FUNC_EXIT() \
-	ODU_BRIDGE_DBG("EXIT\n")
-
-
-#define ODU_BRIDGE_IS_QMI_ADDR(daddr) \
-	(memcmp(&(daddr), &odu_bridge_ctx->llv6_addr, sizeof((daddr))) \
-		== 0)
-
-#define ODU_BRIDGE_IPV4_HDR_NAME "odu_br_ipv4"
-#define ODU_BRIDGE_IPV6_HDR_NAME "odu_br_ipv6"
-
-#define IPA_ODU_SYS_DESC_FIFO_SZ 0x800
-
-#define NULL_CHECK(ptr) \
-	do { \
-		if (!(ptr)) { \
-			ODU_BRIDGE_ERR("null pointer %s\n", #ptr); \
-			return -EINVAL; \
-		} \
-	} \
-	while (0)
-
-
-#ifdef CONFIG_COMPAT
-#define ODU_BRIDGE_IOC_SET_LLV6_ADDR32 _IOW(ODU_BRIDGE_IOC_MAGIC, \
-				ODU_BRIDGE_IOCTL_SET_LLV6_ADDR, \
-				compat_uptr_t)
-#endif
-
-/**
- * struct stats - driver statistics, viewable using debugfs
- * @num_ul_packets: number of packets bridged in uplink direction
- * @num_dl_packets: number of packets bridged in downink direction
- * bridge
- * @num_lan_packets: number of packets bridged to APPS on bridge mode
- */
-struct stats {
-	u64 num_ul_packets;
-	u64 num_dl_packets;
-	u64 num_lan_packets;
-};
-
-/**
- * struct odu_bridge_ctx - ODU bridge driver context information
- * @class: kernel class pointer
- * @dev_num: kernel device number
- * @dev: kernel device struct pointer
- * @cdev: kernel character device struct
- * @netdev_name: network interface name
- * @device_ethaddr: network interface ethernet address
- * @priv: client's private data. to be used in client's callbacks
- * @tx_dp_notify: client callback for handling IPA ODU_PROD callback
- * @send_dl_skb: client callback for sending skb in downlink direction
- * @stats: statistics, how many packets were transmitted using the SW bridge
- * @is_conencted: is bridge connected ?
- * @mode: ODU mode (router/bridge)
- * @lock: for the initialization, connect and disconnect synchronization
- * @llv6_addr: link local IPv6 address of ODU network interface
- * @odu_br_ipv4_hdr_hdl: handle for partial ipv4 ethernet header
- * @odu_br_ipv6_hdr_hdl: handle for partial ipv6 ethernet header
- * @odu_prod_hdl: handle for IPA_CLIENT_ODU_PROD pipe
- * @odu_emb_cons_hdl: handle for IPA_CLIENT_ODU_EMB_CONS pipe
- * @odu_teth_cons_hdl: handle for IPA_CLIENT_ODU_TETH_CONS pipe
- */
-struct odu_bridge_ctx {
-	struct class *class;
-	dev_t dev_num;
-	struct device *dev;
-	struct cdev cdev;
-	char netdev_name[IPA_RESOURCE_NAME_MAX];
-	u8 device_ethaddr[ETH_ALEN];
-	void *priv;
-	ipa_notify_cb tx_dp_notify;
-	int (*send_dl_skb)(void *priv, struct sk_buff *skb);
-	struct stats stats;
-	bool is_connected;
-	enum odu_bridge_mode mode;
-	struct mutex lock;
-	struct in6_addr llv6_addr;
-	uint32_t odu_br_ipv4_hdr_hdl;
-	uint32_t odu_br_ipv6_hdr_hdl;
-	u32 odu_prod_hdl;
-	u32 odu_emb_cons_hdl;
-	u32 odu_teth_cons_hdl;
-	u32 ipa_sys_desc_size;
-};
-static struct odu_bridge_ctx *odu_bridge_ctx;
-
-#ifdef CONFIG_DEBUG_FS
-#define ODU_MAX_MSG_LEN 512
-static char dbg_buff[ODU_MAX_MSG_LEN];
-#endif
-
-static void odu_bridge_emb_cons_cb(void *priv, enum ipa_dp_evt_type evt,
-	unsigned long data)
-{
-	ODU_BRIDGE_FUNC_ENTRY();
-	if (evt != IPA_RECEIVE) {
-		ODU_BRIDGE_ERR("unexpected event\n");
-		WARN_ON(1);
-		return;
-	}
-	odu_bridge_ctx->send_dl_skb(priv, (struct sk_buff *)data);
-	odu_bridge_ctx->stats.num_dl_packets++;
-	ODU_BRIDGE_FUNC_EXIT();
-}
-
-static void odu_bridge_teth_cons_cb(void *priv, enum ipa_dp_evt_type evt,
-	unsigned long data)
-{
-	struct ipv6hdr *ipv6hdr;
-	struct sk_buff *skb = (struct sk_buff *)data;
-	struct sk_buff *skb_copied;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-	if (evt != IPA_RECEIVE) {
-		ODU_BRIDGE_ERR("unexpected event\n");
-		WARN_ON(1);
-		return;
-	}
-
-	ipv6hdr = (struct ipv6hdr *)(skb->data + ETH_HLEN);
-	if (ipv6hdr->version == 6 &&
-	    ipv6_addr_is_multicast(&ipv6hdr->daddr)) {
-		ODU_BRIDGE_DBG("Multicast pkt, send to APPS and adapter\n");
-		skb_copied = skb_clone(skb, GFP_KERNEL);
-		if (skb_copied) {
-			odu_bridge_ctx->tx_dp_notify(odu_bridge_ctx->priv,
-						IPA_RECEIVE,
-						(unsigned long) skb_copied);
-			odu_bridge_ctx->stats.num_lan_packets++;
-		} else {
-			ODU_BRIDGE_ERR("No memory\n");
-		}
-	}
-
-	odu_bridge_ctx->send_dl_skb(priv, skb);
-	odu_bridge_ctx->stats.num_dl_packets++;
-	ODU_BRIDGE_FUNC_EXIT();
-}
-
-static int odu_bridge_connect_router(void)
-{
-	struct ipa_sys_connect_params odu_prod_params;
-	struct ipa_sys_connect_params odu_emb_cons_params;
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	memset(&odu_prod_params, 0, sizeof(odu_prod_params));
-	memset(&odu_emb_cons_params, 0, sizeof(odu_emb_cons_params));
-
-	/* configure RX (ODU->IPA) EP */
-	odu_prod_params.client = IPA_CLIENT_ODU_PROD;
-	odu_prod_params.ipa_ep_cfg.hdr.hdr_len = ETH_HLEN;
-	odu_prod_params.ipa_ep_cfg.nat.nat_en = IPA_SRC_NAT;
-	odu_prod_params.desc_fifo_sz = odu_bridge_ctx->ipa_sys_desc_size;
-	odu_prod_params.priv = odu_bridge_ctx->priv;
-	odu_prod_params.notify = odu_bridge_ctx->tx_dp_notify;
-	odu_prod_params.keep_ipa_awake = true;
-	res = ipa_setup_sys_pipe(&odu_prod_params,
-		&odu_bridge_ctx->odu_prod_hdl);
-	if (res) {
-		ODU_BRIDGE_ERR("fail to setup sys pipe ODU_PROD %d\n", res);
-		goto fail_odu_prod;
-	}
-
-	/* configure TX (IPA->ODU) EP */
-	odu_emb_cons_params.client = IPA_CLIENT_ODU_EMB_CONS;
-	odu_emb_cons_params.ipa_ep_cfg.hdr.hdr_len = ETH_HLEN;
-	odu_emb_cons_params.ipa_ep_cfg.nat.nat_en = IPA_BYPASS_NAT;
-	odu_emb_cons_params.desc_fifo_sz = odu_bridge_ctx->ipa_sys_desc_size;
-	odu_emb_cons_params.priv = odu_bridge_ctx->priv;
-	odu_emb_cons_params.notify = odu_bridge_emb_cons_cb;
-	odu_emb_cons_params.keep_ipa_awake = true;
-	res = ipa_setup_sys_pipe(&odu_emb_cons_params,
-		&odu_bridge_ctx->odu_emb_cons_hdl);
-	if (res) {
-		ODU_BRIDGE_ERR("fail to setup sys pipe ODU_EMB_CONS %d\n", res);
-		goto fail_odu_emb_cons;
-	}
-
-	ODU_BRIDGE_DBG("odu_prod_hdl = %d, odu_emb_cons_hdl = %d\n",
-		odu_bridge_ctx->odu_prod_hdl, odu_bridge_ctx->odu_emb_cons_hdl);
-
-	ODU_BRIDGE_FUNC_EXIT();
-
-	return 0;
-
-fail_odu_emb_cons:
-	ipa_teardown_sys_pipe(odu_bridge_ctx->odu_prod_hdl);
-	odu_bridge_ctx->odu_prod_hdl = 0;
-fail_odu_prod:
-	return res;
-}
-
-static int odu_bridge_connect_bridge(void)
-{
-	struct ipa_sys_connect_params odu_prod_params;
-	struct ipa_sys_connect_params odu_emb_cons_params;
-	struct ipa_sys_connect_params odu_teth_cons_params;
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	memset(&odu_prod_params, 0, sizeof(odu_prod_params));
-	memset(&odu_emb_cons_params, 0, sizeof(odu_emb_cons_params));
-
-	/* Build IPA Resource manager dependency graph */
-	ODU_BRIDGE_DBG("build dependency graph\n");
-	res = ipa_rm_add_dependency(IPA_RM_RESOURCE_ODU_ADAPT_PROD,
-					IPA_RM_RESOURCE_Q6_CONS);
-	if (res && res != -EINPROGRESS) {
-		ODU_BRIDGE_ERR("ipa_rm_add_dependency() failed\n");
-		goto fail_add_dependency_1;
-	}
-
-	res = ipa_rm_add_dependency(IPA_RM_RESOURCE_Q6_PROD,
-					IPA_RM_RESOURCE_ODU_ADAPT_CONS);
-	if (res && res != -EINPROGRESS) {
-		ODU_BRIDGE_ERR("ipa_rm_add_dependency() failed\n");
-		goto fail_add_dependency_2;
-	}
-
-	/* configure RX (ODU->IPA) EP */
-	odu_prod_params.client = IPA_CLIENT_ODU_PROD;
-	odu_prod_params.desc_fifo_sz = IPA_ODU_SYS_DESC_FIFO_SZ;
-	odu_prod_params.priv = odu_bridge_ctx->priv;
-	odu_prod_params.notify = odu_bridge_ctx->tx_dp_notify;
-	odu_prod_params.keep_ipa_awake = true;
-	odu_prod_params.skip_ep_cfg = true;
-	res = ipa_setup_sys_pipe(&odu_prod_params,
-		&odu_bridge_ctx->odu_prod_hdl);
-	if (res) {
-		ODU_BRIDGE_ERR("fail to setup sys pipe ODU_PROD %d\n", res);
-		goto fail_odu_prod;
-	}
-
-	/* configure TX tethered (IPA->ODU) EP */
-	odu_teth_cons_params.client = IPA_CLIENT_ODU_TETH_CONS;
-	odu_teth_cons_params.desc_fifo_sz = IPA_ODU_SYS_DESC_FIFO_SZ;
-	odu_teth_cons_params.priv = odu_bridge_ctx->priv;
-	odu_teth_cons_params.notify = odu_bridge_teth_cons_cb;
-	odu_teth_cons_params.keep_ipa_awake = true;
-	odu_teth_cons_params.skip_ep_cfg = true;
-	res = ipa_setup_sys_pipe(&odu_teth_cons_params,
-		&odu_bridge_ctx->odu_teth_cons_hdl);
-	if (res) {
-		ODU_BRIDGE_ERR("fail to setup sys pipe ODU_TETH_CONS %d\n",
-				res);
-		goto fail_odu_teth_cons;
-	}
-
-	/* configure TX embedded(IPA->ODU) EP */
-	odu_emb_cons_params.client = IPA_CLIENT_ODU_EMB_CONS;
-	odu_emb_cons_params.ipa_ep_cfg.hdr.hdr_len = ETH_HLEN;
-	odu_emb_cons_params.ipa_ep_cfg.nat.nat_en = IPA_BYPASS_NAT;
-	odu_emb_cons_params.desc_fifo_sz = IPA_ODU_SYS_DESC_FIFO_SZ;
-	odu_emb_cons_params.priv = odu_bridge_ctx->priv;
-	odu_emb_cons_params.notify = odu_bridge_emb_cons_cb;
-	odu_emb_cons_params.keep_ipa_awake = true;
-	res = ipa_setup_sys_pipe(&odu_emb_cons_params,
-		&odu_bridge_ctx->odu_emb_cons_hdl);
-	if (res) {
-		ODU_BRIDGE_ERR("fail to setup sys pipe ODU_EMB_CONS %d\n", res);
-		goto fail_odu_emb_cons;
-	}
-
-	ODU_BRIDGE_DBG("odu_prod_hdl = %d, odu_emb_cons_hdl = %d\n",
-		odu_bridge_ctx->odu_prod_hdl, odu_bridge_ctx->odu_emb_cons_hdl);
-	ODU_BRIDGE_DBG("odu_teth_cons_hdl = %d\n",
-		odu_bridge_ctx->odu_teth_cons_hdl);
-
-	ODU_BRIDGE_FUNC_EXIT();
-
-	return 0;
-
-fail_odu_emb_cons:
-	ipa_teardown_sys_pipe(odu_bridge_ctx->odu_teth_cons_hdl);
-	odu_bridge_ctx->odu_teth_cons_hdl = 0;
-fail_odu_teth_cons:
-	ipa_teardown_sys_pipe(odu_bridge_ctx->odu_prod_hdl);
-	odu_bridge_ctx->odu_prod_hdl = 0;
-fail_odu_prod:
-	ipa_rm_delete_dependency(IPA_RM_RESOURCE_Q6_PROD,
-				IPA_RM_RESOURCE_ODU_ADAPT_CONS);
-fail_add_dependency_2:
-	ipa_rm_delete_dependency(IPA_RM_RESOURCE_ODU_ADAPT_PROD,
-				IPA_RM_RESOURCE_Q6_CONS);
-fail_add_dependency_1:
-	return res;
-}
-
-static int odu_bridge_disconnect_router(void)
-{
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	res = ipa_teardown_sys_pipe(odu_bridge_ctx->odu_prod_hdl);
-	if (res)
-		ODU_BRIDGE_ERR("teardown ODU PROD failed\n");
-	odu_bridge_ctx->odu_prod_hdl = 0;
-
-	res = ipa_teardown_sys_pipe(odu_bridge_ctx->odu_emb_cons_hdl);
-	if (res)
-		ODU_BRIDGE_ERR("teardown ODU EMB CONS failed\n");
-	odu_bridge_ctx->odu_emb_cons_hdl = 0;
-
-	ODU_BRIDGE_FUNC_EXIT();
-
-	return 0;
-}
-
-static int odu_bridge_disconnect_bridge(void)
-{
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	res = ipa_teardown_sys_pipe(odu_bridge_ctx->odu_prod_hdl);
-	if (res)
-		ODU_BRIDGE_ERR("teardown ODU PROD failed\n");
-	odu_bridge_ctx->odu_prod_hdl = 0;
-
-	res = ipa_teardown_sys_pipe(odu_bridge_ctx->odu_teth_cons_hdl);
-	if (res)
-		ODU_BRIDGE_ERR("teardown ODU TETH CONS failed\n");
-	odu_bridge_ctx->odu_teth_cons_hdl = 0;
-
-	res = ipa_teardown_sys_pipe(odu_bridge_ctx->odu_emb_cons_hdl);
-	if (res)
-		ODU_BRIDGE_ERR("teardown ODU EMB CONS failed\n");
-	odu_bridge_ctx->odu_emb_cons_hdl = 0;
-
-	/* Delete IPA Resource manager dependency graph */
-	ODU_BRIDGE_DBG("deleting dependency graph\n");
-	res = ipa_rm_delete_dependency(IPA_RM_RESOURCE_ODU_ADAPT_PROD,
-		IPA_RM_RESOURCE_Q6_CONS);
-	if (res && res != -EINPROGRESS)
-		ODU_BRIDGE_ERR("ipa_rm_delete_dependency() failed\n");
-
-	res = ipa_rm_delete_dependency(IPA_RM_RESOURCE_Q6_PROD,
-		IPA_RM_RESOURCE_ODU_ADAPT_CONS);
-	if (res && res != -EINPROGRESS)
-		ODU_BRIDGE_ERR("ipa_rm_delete_dependency() failed\n");
-
-	return 0;
-}
-
-/**
- * odu_bridge_disconnect() - Disconnect odu bridge
- *
- * Disconnect all pipes and deletes IPA RM dependencies on bridge mode
- *
- * Return codes: 0- success, error otherwise
- */
-int odu_bridge_disconnect(void)
-{
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	if (!odu_bridge_ctx) {
-		ODU_BRIDGE_ERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	if (!odu_bridge_ctx->is_connected) {
-		ODU_BRIDGE_ERR("Not connected\n");
-		return -EFAULT;
-	}
-
-	mutex_lock(&odu_bridge_ctx->lock);
-	if (odu_bridge_ctx->mode == ODU_BRIDGE_MODE_ROUTER) {
-		res = odu_bridge_disconnect_router();
-		if (res) {
-			ODU_BRIDGE_ERR("disconnect_router failed %d\n", res);
-			goto out;
-		}
-	} else {
-		res = odu_bridge_disconnect_bridge();
-		if (res) {
-			ODU_BRIDGE_ERR("disconnect_bridge failed %d\n", res);
-			goto out;
-		}
-	}
-
-	odu_bridge_ctx->is_connected = false;
-	res = 0;
-out:
-	mutex_unlock(&odu_bridge_ctx->lock);
-	ODU_BRIDGE_FUNC_EXIT();
-	return res;
-}
-EXPORT_SYMBOL(odu_bridge_disconnect);
-
-/**
- * odu_bridge_connect() - Connect odu bridge.
- *
- * Call to the mode-specific connect function for connection IPA pipes
- * and adding IPA RM dependencies
-
- * Return codes: 0: success
- *		-EINVAL: invalid parameters
- *		-EPERM: Operation not permitted as the bridge is already
- *		connected
- */
-int odu_bridge_connect(void)
-{
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	if (!odu_bridge_ctx) {
-		ODU_BRIDGE_ERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	if (odu_bridge_ctx->is_connected) {
-		ODU_BRIDGE_ERR("already connected\n");
-		return -EFAULT;
-	}
-
-	mutex_lock(&odu_bridge_ctx->lock);
-	if (odu_bridge_ctx->mode == ODU_BRIDGE_MODE_ROUTER) {
-		res = odu_bridge_connect_router();
-		if (res) {
-			ODU_BRIDGE_ERR("connect_router failed\n");
-			goto bail;
-		}
-	} else {
-		res = odu_bridge_connect_bridge();
-		if (res) {
-			ODU_BRIDGE_ERR("connect_bridge failed\n");
-			goto bail;
-		}
-	}
-
-	odu_bridge_ctx->is_connected = true;
-	res = 0;
-bail:
-	mutex_unlock(&odu_bridge_ctx->lock);
-	ODU_BRIDGE_FUNC_EXIT();
-	return res;
-}
-EXPORT_SYMBOL(odu_bridge_connect);
-
-/**
- * odu_bridge_set_mode() - Set bridge mode to Router/Bridge
- * @mode: mode to be set
- */
-static int odu_bridge_set_mode(enum odu_bridge_mode mode)
-{
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	if (mode < 0 || mode >= ODU_BRIDGE_MODE_MAX) {
-		ODU_BRIDGE_ERR("Unsupported mode: %d\n", mode);
-		return -EFAULT;
-	}
-
-	ODU_BRIDGE_DBG("setting mode: %d\n", mode);
-	mutex_lock(&odu_bridge_ctx->lock);
-
-	if (odu_bridge_ctx->mode == mode) {
-		ODU_BRIDGE_DBG("same mode\n");
-		res = 0;
-		goto bail;
-	}
-
-	if (odu_bridge_ctx->is_connected) {
-		/* first disconnect the old configuration */
-		if (odu_bridge_ctx->mode == ODU_BRIDGE_MODE_ROUTER) {
-			res = odu_bridge_disconnect_router();
-			if (res) {
-				ODU_BRIDGE_ERR("disconnect_router failed\n");
-				goto bail;
-			}
-		} else {
-			res = odu_bridge_disconnect_bridge();
-			if (res) {
-				ODU_BRIDGE_ERR("disconnect_bridge failed\n");
-				goto bail;
-			}
-		}
-
-		/* connect the new configuration */
-		if (mode == ODU_BRIDGE_MODE_ROUTER) {
-			res = odu_bridge_connect_router();
-			if (res) {
-				ODU_BRIDGE_ERR("connect_router failed\n");
-				goto bail;
-			}
-		} else {
-			res = odu_bridge_connect_bridge();
-			if (res) {
-				ODU_BRIDGE_ERR("connect_bridge failed\n");
-				goto bail;
-			}
-		}
-	}
-	odu_bridge_ctx->mode = mode;
-	res = 0;
-bail:
-	mutex_unlock(&odu_bridge_ctx->lock);
-	ODU_BRIDGE_FUNC_EXIT();
-	return res;
-};
-
-/**
- * odu_bridge_set_llv6_addr() - Set link local ipv6 address
- * @llv6_addr: odu network interface link local address
- *
- * This function sets the link local ipv6 address provided by IOCTL
- */
-static int odu_bridge_set_llv6_addr(struct in6_addr *llv6_addr)
-{
-	struct in6_addr llv6_addr_host;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	llv6_addr_host.s6_addr32[0] = ntohl(llv6_addr->s6_addr32[0]);
-	llv6_addr_host.s6_addr32[1] = ntohl(llv6_addr->s6_addr32[1]);
-	llv6_addr_host.s6_addr32[2] = ntohl(llv6_addr->s6_addr32[2]);
-	llv6_addr_host.s6_addr32[3] = ntohl(llv6_addr->s6_addr32[3]);
-
-	memcpy(&odu_bridge_ctx->llv6_addr, &llv6_addr_host,
-				sizeof(odu_bridge_ctx->llv6_addr));
-	ODU_BRIDGE_DBG("LLV6 addr: %pI6c\n", &odu_bridge_ctx->llv6_addr);
-
-	ODU_BRIDGE_FUNC_EXIT();
-
-	return 0;
-};
-
-static long odu_bridge_ioctl(struct file *filp,
-			      unsigned int cmd,
-			      unsigned long arg)
-{
-	int res = 0;
-	struct in6_addr llv6_addr;
-
-	ODU_BRIDGE_DBG("cmd=%x nr=%d\n", cmd, _IOC_NR(cmd));
-
-	if ((_IOC_TYPE(cmd) != ODU_BRIDGE_IOC_MAGIC) ||
-	    (_IOC_NR(cmd) >= ODU_BRIDGE_IOCTL_MAX)) {
-		ODU_BRIDGE_ERR("Invalid ioctl\n");
-		return -ENOIOCTLCMD;
-	}
-
-	switch (cmd) {
-	case ODU_BRIDGE_IOC_SET_MODE:
-		ODU_BRIDGE_DBG("ODU_BRIDGE_IOC_SET_MODE ioctl called\n");
-		res = odu_bridge_set_mode(arg);
-		if (res) {
-			ODU_BRIDGE_ERR("Error, res = %d\n", res);
-			break;
-		}
-		break;
-
-	case ODU_BRIDGE_IOC_SET_LLV6_ADDR:
-		ODU_BRIDGE_DBG("ODU_BRIDGE_IOC_SET_LLV6_ADDR ioctl called\n");
-		res = copy_from_user(&llv6_addr,
-			(struct in6_addr *)arg,
-			sizeof(llv6_addr));
-		if (res) {
-			ODU_BRIDGE_ERR("Error, res = %d\n", res);
-			res = -EFAULT;
-			break;
-		}
-
-		res = odu_bridge_set_llv6_addr(&llv6_addr);
-		if (res) {
-			ODU_BRIDGE_ERR("Error, res = %d\n", res);
-			break;
-		}
-		break;
-
-	default:
-		ODU_BRIDGE_ERR("Unknown ioctl: %d\n", cmd);
-		WARN_ON(1);
-	}
-
-	return res;
-}
-
-#ifdef CONFIG_COMPAT
-static long compat_odu_bridge_ioctl(struct file *file,
-	unsigned int cmd, unsigned long arg)
-{
-	switch (cmd) {
-	case ODU_BRIDGE_IOC_SET_LLV6_ADDR32:
-		cmd = ODU_BRIDGE_IOC_SET_LLV6_ADDR;
-		break;
-	case ODU_BRIDGE_IOC_SET_MODE:
-		break;
-	default:
-		return -ENOIOCTLCMD;
-	}
-	return odu_bridge_ioctl(file, cmd, (unsigned long)compat_ptr(arg));
-}
-#endif
-
-#ifdef CONFIG_DEBUG_FS
-static struct dentry *dent;
-static struct dentry *dfile_stats;
-static struct dentry *dfile_mode;
-
-static ssize_t odu_debugfs_stats(struct file *file,
-				  char __user *ubuf,
-				  size_t count,
-				  loff_t *ppos)
-{
-	int nbytes = 0;
-
-	nbytes += scnprintf(&dbg_buff[nbytes],
-			    ODU_MAX_MSG_LEN - nbytes,
-			   "UL packets: %lld\n",
-			    odu_bridge_ctx->stats.num_ul_packets);
-	nbytes += scnprintf(&dbg_buff[nbytes],
-			    ODU_MAX_MSG_LEN - nbytes,
-			   "DL packets: %lld\n",
-			    odu_bridge_ctx->stats.num_dl_packets);
-	nbytes += scnprintf(&dbg_buff[nbytes],
-			    ODU_MAX_MSG_LEN - nbytes,
-			    "LAN packets: %lld\n",
-			    odu_bridge_ctx->stats.num_lan_packets);
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, nbytes);
-}
-
-static ssize_t odu_debugfs_hw_bridge_mode_write(struct file *file,
-					const char __user *ubuf,
-					size_t count,
-					loff_t *ppos)
-{
-	unsigned long missing;
-	enum odu_bridge_mode mode;
-
-	if (sizeof(dbg_buff) < count + 1)
-		return -EFAULT;
-
-	missing = copy_from_user(dbg_buff, ubuf, count);
-	if (missing)
-		return -EFAULT;
-
-	if (count > 0)
-		dbg_buff[count-1] = '\0';
-
-	if (strcmp(dbg_buff, "router") == 0) {
-		mode = ODU_BRIDGE_MODE_ROUTER;
-	} else if (strcmp(dbg_buff, "bridge") == 0) {
-		mode = ODU_BRIDGE_MODE_BRIDGE;
-	} else {
-		ODU_BRIDGE_ERR("Bad mode, got %s,\n"
-			 "Use <router> or <bridge>.\n", dbg_buff);
-		return count;
-	}
-
-	odu_bridge_set_mode(mode);
-	return count;
-}
-
-static ssize_t odu_debugfs_hw_bridge_mode_read(struct file *file,
-					     char __user *ubuf,
-					     size_t count,
-					     loff_t *ppos)
-{
-	int nbytes = 0;
-
-	switch (odu_bridge_ctx->mode) {
-	case ODU_BRIDGE_MODE_ROUTER:
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			ODU_MAX_MSG_LEN - nbytes,
-			"router\n");
-		break;
-	case ODU_BRIDGE_MODE_BRIDGE:
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			ODU_MAX_MSG_LEN - nbytes,
-			"bridge\n");
-		break;
-	default:
-		nbytes += scnprintf(&dbg_buff[nbytes],
-			ODU_MAX_MSG_LEN - nbytes,
-			"mode error\n");
-		break;
-
-	}
-
-	return simple_read_from_buffer(ubuf, count, ppos, dbg_buff, nbytes);
-}
-
-const struct file_operations odu_stats_ops = {
-	.read = odu_debugfs_stats,
-};
-
-const struct file_operations odu_hw_bridge_mode_ops = {
-	.read = odu_debugfs_hw_bridge_mode_read,
-	.write = odu_debugfs_hw_bridge_mode_write,
-};
-
-void odu_debugfs_init(void)
-{
-	const mode_t read_only_mode = S_IRUSR | S_IRGRP | S_IROTH;
-	const mode_t read_write_mode = S_IRUSR | S_IRGRP | S_IROTH |
-		S_IWUSR | S_IWGRP | S_IWOTH;
-
-	dent = debugfs_create_dir("odu_ipa_bridge", 0);
-	if (IS_ERR(dent)) {
-		ODU_BRIDGE_ERR("fail to create folder odu_ipa_bridge\n");
-		return;
-	}
-
-	dfile_stats =
-		debugfs_create_file("stats", read_only_mode, dent,
-				    0, &odu_stats_ops);
-	if (!dfile_stats || IS_ERR(dfile_stats)) {
-		ODU_BRIDGE_ERR("fail to create file stats\n");
-		goto fail;
-	}
-
-	dfile_mode =
-		debugfs_create_file("mode", read_write_mode,
-				    dent, 0, &odu_hw_bridge_mode_ops);
-	if (!dfile_mode ||
-	    IS_ERR(dfile_mode)) {
-		ODU_BRIDGE_ERR("fail to create file dfile_mode\n");
-		goto fail;
-	}
-
-	return;
-fail:
-	debugfs_remove_recursive(dent);
-}
-
-static void odu_debugfs_destroy(void)
-{
-	debugfs_remove_recursive(dent);
-}
-
-#else
-static void odu_debugfs_init(void) {}
-static void odu_debugfs_destroy(void) {}
-#endif /* CONFIG_DEBUG_FS */
-
-
-static const struct file_operations odu_bridge_drv_fops = {
-	.owner = THIS_MODULE,
-	.unlocked_ioctl = odu_bridge_ioctl,
-#ifdef CONFIG_COMPAT
-	.compat_ioctl = compat_odu_bridge_ioctl,
-#endif
-};
-
-/**
- * odu_bridge_tx_dp() - Send skb to ODU bridge
- * @skb: skb to send
- * @metadata: metadata on packet
- *
- * This function handles uplink packet.
- * In Router Mode:
- *	packet is sent directly to IPA.
- * In Router Mode:
- *	packet is classified if it should arrive to network stack.
- *	QMI IP packet should arrive to APPS network stack
- *	IPv6 Multicast packet should arrive to APPS network stack and Q6
- *
- * Return codes: 0- success, error otherwise
- */
-int odu_bridge_tx_dp(struct sk_buff *skb, struct ipa_tx_meta *metadata)
-{
-	struct sk_buff *skb_copied = NULL;
-	struct ipv6hdr *ipv6hdr;
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	switch (odu_bridge_ctx->mode) {
-	case ODU_BRIDGE_MODE_ROUTER:
-		/* Router mode - pass skb to IPA */
-		res = ipa_tx_dp(IPA_CLIENT_ODU_PROD, skb, metadata);
-		if (res) {
-			ODU_BRIDGE_DBG("tx dp failed %d\n", res);
-			goto out;
-		}
-		odu_bridge_ctx->stats.num_ul_packets++;
-		goto out;
-
-	case ODU_BRIDGE_MODE_BRIDGE:
-		ipv6hdr = (struct ipv6hdr *)(skb->data + ETH_HLEN);
-		if (ipv6hdr->version == 6 &&
-		    ODU_BRIDGE_IS_QMI_ADDR(ipv6hdr->daddr)) {
-			ODU_BRIDGE_DBG("QMI packet\n");
-			skb_copied = skb_clone(skb, GFP_KERNEL);
-			if (!skb_copied) {
-				ODU_BRIDGE_ERR("No memory\n");
-				return -ENOMEM;
-			}
-			odu_bridge_ctx->tx_dp_notify(odu_bridge_ctx->priv,
-						     IPA_RECEIVE,
-						     (unsigned long)skb_copied);
-			odu_bridge_ctx->tx_dp_notify(odu_bridge_ctx->priv,
-						     IPA_WRITE_DONE,
-						     (unsigned long)skb);
-			odu_bridge_ctx->stats.num_ul_packets++;
-			odu_bridge_ctx->stats.num_lan_packets++;
-			res = 0;
-			goto out;
-		}
-
-		if (ipv6hdr->version == 6 &&
-		    ipv6_addr_is_multicast(&ipv6hdr->daddr)) {
-			ODU_BRIDGE_DBG("Multicast pkt, send to APPS and IPA\n");
-			skb_copied = skb_clone(skb, GFP_KERNEL);
-			if (!skb_copied) {
-				ODU_BRIDGE_ERR("No memory\n");
-				return -ENOMEM;
-			}
-
-			res = ipa_tx_dp(IPA_CLIENT_ODU_PROD, skb, metadata);
-			if (res) {
-				ODU_BRIDGE_DBG("tx dp failed %d\n", res);
-				dev_kfree_skb(skb_copied);
-				goto out;
-			}
-
-			odu_bridge_ctx->tx_dp_notify(odu_bridge_ctx->priv,
-						     IPA_RECEIVE,
-						     (unsigned long)skb_copied);
-			odu_bridge_ctx->stats.num_ul_packets++;
-			odu_bridge_ctx->stats.num_lan_packets++;
-			goto out;
-		}
-
-		res = ipa_tx_dp(IPA_CLIENT_ODU_PROD, skb, metadata);
-		if (res) {
-			ODU_BRIDGE_DBG("tx dp failed %d\n", res);
-			goto out;
-		}
-		odu_bridge_ctx->stats.num_ul_packets++;
-		goto out;
-
-	default:
-		ODU_BRIDGE_ERR("Unsupported mode: %d\n", odu_bridge_ctx->mode);
-		WARN_ON(1);
-		res = -EFAULT;
-
-	}
-out:
-	ODU_BRIDGE_FUNC_EXIT();
-	return res;
-}
-EXPORT_SYMBOL(odu_bridge_tx_dp);
-
-static int odu_bridge_add_hdrs(void)
-{
-	struct ipa_ioc_add_hdr *hdrs;
-	struct ipa_hdr_add *ipv4_hdr;
-	struct ipa_hdr_add *ipv6_hdr;
-	struct ethhdr *eth_ipv4;
-	struct ethhdr *eth_ipv6;
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-	hdrs = kzalloc(sizeof(*hdrs) + sizeof(*ipv4_hdr) + sizeof(*ipv6_hdr),
-			GFP_KERNEL);
-	if (!hdrs) {
-		ODU_BRIDGE_ERR("no mem\n");
-		res = -ENOMEM;
-		goto out;
-	}
-	ipv4_hdr = &hdrs->hdr[0];
-	eth_ipv4 = (struct ethhdr *)(ipv4_hdr->hdr);
-	ipv6_hdr = &hdrs->hdr[1];
-	eth_ipv6 = (struct ethhdr *)(ipv6_hdr->hdr);
-	strlcpy(ipv4_hdr->name, ODU_BRIDGE_IPV4_HDR_NAME,
-		IPA_RESOURCE_NAME_MAX);
-	memcpy(eth_ipv4->h_source, odu_bridge_ctx->device_ethaddr, ETH_ALEN);
-	eth_ipv4->h_proto = htons(ETH_P_IP);
-	ipv4_hdr->hdr_len = ETH_HLEN;
-	ipv4_hdr->is_partial = 1;
-	ipv4_hdr->is_eth2_ofst_valid = 1;
-	ipv4_hdr->eth2_ofst = 0;
-	strlcpy(ipv6_hdr->name, ODU_BRIDGE_IPV6_HDR_NAME,
-		IPA_RESOURCE_NAME_MAX);
-	memcpy(eth_ipv6->h_source, odu_bridge_ctx->device_ethaddr, ETH_ALEN);
-	eth_ipv6->h_proto = htons(ETH_P_IPV6);
-	ipv6_hdr->hdr_len = ETH_HLEN;
-	ipv6_hdr->is_partial = 1;
-	ipv6_hdr->is_eth2_ofst_valid = 1;
-	ipv6_hdr->eth2_ofst = 0;
-	hdrs->commit = 1;
-	hdrs->num_hdrs = 2;
-	res = ipa_add_hdr(hdrs);
-	if (res) {
-		ODU_BRIDGE_ERR("Fail on Header-Insertion(%d)\n", res);
-		goto out_free_mem;
-	}
-	if (ipv4_hdr->status) {
-		ODU_BRIDGE_ERR("Fail on Header-Insertion ipv4(%d)\n",
-				ipv4_hdr->status);
-		res = ipv4_hdr->status;
-		goto out_free_mem;
-	}
-	if (ipv6_hdr->status) {
-		ODU_BRIDGE_ERR("Fail on Header-Insertion ipv6(%d)\n",
-				ipv6_hdr->status);
-		res = ipv6_hdr->status;
-		goto out_free_mem;
-	}
-	odu_bridge_ctx->odu_br_ipv4_hdr_hdl = ipv4_hdr->hdr_hdl;
-	odu_bridge_ctx->odu_br_ipv6_hdr_hdl = ipv6_hdr->hdr_hdl;
-
-	res = 0;
-out_free_mem:
-	kfree(hdrs);
-out:
-	ODU_BRIDGE_FUNC_EXIT();
-	return res;
-}
-
-static void odu_bridge_del_hdrs(void)
-{
-	struct ipa_ioc_del_hdr *del_hdr;
-	struct ipa_hdr_del *ipv4;
-	struct ipa_hdr_del *ipv6;
-	int result;
-	del_hdr = kzalloc(sizeof(*del_hdr) + sizeof(*ipv4) +
-			sizeof(*ipv6), GFP_KERNEL);
-	if (!del_hdr)
-		return;
-	del_hdr->commit = 1;
-	del_hdr->num_hdls = 2;
-	ipv4 = &del_hdr->hdl[0];
-	ipv4->hdl = odu_bridge_ctx->odu_br_ipv4_hdr_hdl;
-	ipv6 = &del_hdr->hdl[1];
-	ipv6->hdl = odu_bridge_ctx->odu_br_ipv6_hdr_hdl;
-	result = ipa_del_hdr(del_hdr);
-	if (result || ipv4->status || ipv6->status)
-		ODU_BRIDGE_ERR("ipa_del_hdr failed");
-	kfree(del_hdr);
-}
-
-/**
- * odu_bridge_register_properties() - set Tx/Rx properties for ipacm
- *
- * Register the network interface interface with Tx and Rx properties
- * Tx properties are for data flowing from IPA to adapter, they
- * have Header-Insertion properties both for Ipv4 and Ipv6 Ethernet framing.
- * Rx properties are for data flowing from adapter to IPA, they have
- * simple rule which always "hit".
- *
- */
-static int odu_bridge_register_properties(void)
-{
-	struct ipa_tx_intf tx_properties = {0};
-	struct ipa_ioc_tx_intf_prop properties[2] = { {0}, {0} };
-	struct ipa_ioc_tx_intf_prop *ipv4_property;
-	struct ipa_ioc_tx_intf_prop *ipv6_property;
-	struct ipa_ioc_rx_intf_prop rx_ioc_properties[2] = { {0}, {0} };
-	struct ipa_rx_intf rx_properties = {0};
-	struct ipa_ioc_rx_intf_prop *rx_ipv4_property;
-	struct ipa_ioc_rx_intf_prop *rx_ipv6_property;
-	int res = 0;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	tx_properties.prop = properties;
-	ipv4_property = &tx_properties.prop[0];
-	ipv4_property->ip = IPA_IP_v4;
-	ipv4_property->dst_pipe = IPA_CLIENT_ODU_EMB_CONS;
-	ipv4_property->hdr_l2_type = IPA_HDR_L2_ETHERNET_II;
-	strlcpy(ipv4_property->hdr_name, ODU_BRIDGE_IPV4_HDR_NAME,
-			IPA_RESOURCE_NAME_MAX);
-	ipv6_property = &tx_properties.prop[1];
-	ipv6_property->ip = IPA_IP_v6;
-	ipv6_property->dst_pipe = IPA_CLIENT_ODU_EMB_CONS;
-	ipv6_property->hdr_l2_type = IPA_HDR_L2_ETHERNET_II;
-	strlcpy(ipv6_property->hdr_name, ODU_BRIDGE_IPV6_HDR_NAME,
-			IPA_RESOURCE_NAME_MAX);
-	tx_properties.num_props = 2;
-
-	rx_properties.prop = rx_ioc_properties;
-	rx_ipv4_property = &rx_properties.prop[0];
-	rx_ipv4_property->ip = IPA_IP_v4;
-	rx_ipv4_property->attrib.attrib_mask = 0;
-	rx_ipv4_property->src_pipe = IPA_CLIENT_ODU_PROD;
-	rx_ipv4_property->hdr_l2_type = IPA_HDR_L2_ETHERNET_II;
-	rx_ipv6_property = &rx_properties.prop[1];
-	rx_ipv6_property->ip = IPA_IP_v6;
-	rx_ipv6_property->attrib.attrib_mask = 0;
-	rx_ipv6_property->src_pipe = IPA_CLIENT_ODU_PROD;
-	rx_ipv6_property->hdr_l2_type = IPA_HDR_L2_ETHERNET_II;
-	rx_properties.num_props = 2;
-
-	res = ipa_register_intf(odu_bridge_ctx->netdev_name, &tx_properties,
-		&rx_properties);
-	if (res) {
-		ODU_BRIDGE_ERR("fail on Tx/Rx properties registration %d\n",
-									res);
-	}
-
-	ODU_BRIDGE_FUNC_EXIT();
-
-	return res;
-}
-
-static void odu_bridge_deregister_properties(void)
-{
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-	res = ipa_deregister_intf(odu_bridge_ctx->netdev_name);
-	if (res)
-		ODU_BRIDGE_ERR("Fail on Tx prop deregister %d\n", res);
-	ODU_BRIDGE_FUNC_EXIT();
-	return;
-}
-
-/**
- * odu_bridge_init() - Initialize the ODU bridge driver
- * @params: initialization parameters
- *
- * This function initialize all bridge internal data and register odu bridge to
- * kernel for IOCTL and debugfs.
- * Header addition and properties are registered to IPA driver.
- *
- * Return codes: 0: success,
- *		-EINVAL - Bad parameter
- *		Other negative value - Failure
- */
-int odu_bridge_init(struct odu_bridge_params *params)
-{
-	int res;
-
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	NULL_CHECK(params);
-	NULL_CHECK(params->netdev_name);
-	NULL_CHECK(params->tx_dp_notify);
-	NULL_CHECK(params->send_dl_skb);
-
-	if (odu_bridge_ctx) {
-		ODU_BRIDGE_ERR("Already initialized\n");
-		return -EFAULT;
-	}
-
-	ODU_BRIDGE_DBG("device_ethaddr=%pM\n", params->device_ethaddr);
-
-	odu_bridge_ctx = kzalloc(sizeof(*odu_bridge_ctx), GFP_KERNEL);
-	if (!odu_bridge_ctx) {
-		ODU_BRIDGE_ERR("kzalloc err.\n");
-		return -ENOMEM;
-	}
-
-	odu_bridge_ctx->class = class_create(THIS_MODULE, ODU_BRIDGE_DRV_NAME);
-	if (!odu_bridge_ctx->class) {
-		ODU_BRIDGE_ERR("Class_create err.\n");
-		res = -ENODEV;
-		goto fail_class_create;
-	}
-
-	res = alloc_chrdev_region(&odu_bridge_ctx->dev_num, 0, 1,
-				  ODU_BRIDGE_DRV_NAME);
-	if (res) {
-		ODU_BRIDGE_ERR("alloc_chrdev_region err.\n");
-		res = -ENODEV;
-		goto fail_alloc_chrdev_region;
-	}
-
-	odu_bridge_ctx->dev = device_create(odu_bridge_ctx->class, NULL,
-		odu_bridge_ctx->dev_num, odu_bridge_ctx, ODU_BRIDGE_DRV_NAME);
-	if (IS_ERR(odu_bridge_ctx->dev)) {
-		ODU_BRIDGE_ERR(":device_create err.\n");
-		res = -ENODEV;
-		goto fail_device_create;
-	}
-
-	cdev_init(&odu_bridge_ctx->cdev, &odu_bridge_drv_fops);
-	odu_bridge_ctx->cdev.owner = THIS_MODULE;
-	odu_bridge_ctx->cdev.ops = &odu_bridge_drv_fops;
-
-	res = cdev_add(&odu_bridge_ctx->cdev, odu_bridge_ctx->dev_num, 1);
-	if (res) {
-		ODU_BRIDGE_ERR(":cdev_add err=%d\n", -res);
-		res = -ENODEV;
-		goto fail_cdev_add;
-	}
-
-	odu_debugfs_init();
-
-	strlcpy(odu_bridge_ctx->netdev_name, params->netdev_name,
-		IPA_RESOURCE_NAME_MAX);
-	odu_bridge_ctx->priv = params->priv;
-	odu_bridge_ctx->tx_dp_notify = params->tx_dp_notify;
-	odu_bridge_ctx->send_dl_skb = params->send_dl_skb;
-	memcpy(odu_bridge_ctx->device_ethaddr, params->device_ethaddr,
-		ETH_ALEN);
-	odu_bridge_ctx->ipa_sys_desc_size = params->ipa_desc_size;
-	odu_bridge_ctx->mode = ODU_BRIDGE_MODE_ROUTER;
-
-	mutex_init(&odu_bridge_ctx->lock);
-
-	res = odu_bridge_add_hdrs();
-	if (res) {
-		ODU_BRIDGE_ERR("fail on odu_bridge_add_hdr %d\n", res);
-		goto fail_add_hdrs;
-	}
-
-	res = odu_bridge_register_properties();
-	if (res) {
-		ODU_BRIDGE_ERR("fail on register properties %d\n", res);
-		goto fail_register_properties;
-	}
-
-	ODU_BRIDGE_FUNC_EXIT();
-	return 0;
-
-fail_register_properties:
-	odu_bridge_del_hdrs();
-fail_add_hdrs:
-	odu_debugfs_destroy();
-fail_cdev_add:
-	device_destroy(odu_bridge_ctx->class, odu_bridge_ctx->dev_num);
-fail_device_create:
-	unregister_chrdev_region(odu_bridge_ctx->dev_num, 1);
-fail_alloc_chrdev_region:
-	class_destroy(odu_bridge_ctx->class);
-fail_class_create:
-	kfree(odu_bridge_ctx);
-	odu_bridge_ctx = NULL;
-	return res;
-}
-EXPORT_SYMBOL(odu_bridge_init);
-
-/**
- * odu_bridge_cleanup() - De-Initialize the ODU bridge driver
- *
- * Return codes: 0: success,
- *		-EINVAL - Bad parameter
- *		Other negative value - Failure
- */
-int odu_bridge_cleanup(void)
-{
-	ODU_BRIDGE_FUNC_ENTRY();
-
-	if (!odu_bridge_ctx) {
-		ODU_BRIDGE_ERR("Not initialized\n");
-		return -EFAULT;
-	}
-
-	if (odu_bridge_ctx->is_connected) {
-		ODU_BRIDGE_ERR("cannot deinit while bridge is conncetd\n");
-		return -EFAULT;
-	}
-
-	odu_bridge_deregister_properties();
-	odu_bridge_del_hdrs();
-	odu_debugfs_destroy();
-	cdev_del(&odu_bridge_ctx->cdev);
-	device_destroy(odu_bridge_ctx->class, odu_bridge_ctx->dev_num);
-	unregister_chrdev_region(odu_bridge_ctx->dev_num, 1);
-	class_destroy(odu_bridge_ctx->class);
-	kfree(odu_bridge_ctx);
-	odu_bridge_ctx = NULL;
-
-	ODU_BRIDGE_FUNC_EXIT();
-	return 0;
-}
-EXPORT_SYMBOL(odu_bridge_cleanup);
-
-
-MODULE_LICENSE("GPL v2");
-MODULE_DESCRIPTION("ODU bridge driver");
diff --git a/drivers/platform/msm/ipa/rmnet_ipa.c b/drivers/platform/msm/ipa/rmnet_ipa.c
deleted file mode 100644
index 897a480..00000000
--- a/drivers/platform/msm/ipa/rmnet_ipa.c
+++ /dev/null
@@ -1,2810 +0,0 @@
-/* Copyright (c) 2014-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-/*
- * WWAN Transport Network Driver.
- */
-
-#include <linux/completion.h>
-#include <linux/errno.h>
-#include <linux/if_arp.h>
-#include <linux/interrupt.h>
-#include <linux/init.h>
-#include <linux/kernel.h>
-#include <linux/module.h>
-#include <linux/netdevice.h>
-#include <linux/of_device.h>
-#include <linux/string.h>
-#include <linux/skbuff.h>
-#include <linux/workqueue.h>
-#include <net/pkt_sched.h>
-#include <soc/qcom/subsystem_restart.h>
-#include <soc/qcom/subsystem_notif.h>
-#include "ipa_qmi_service.h"
-#include <linux/rmnet_ipa_fd_ioctl.h>
-#include <linux/ipa.h>
-
-#define WWAN_METADATA_SHFT 24
-#define WWAN_METADATA_MASK 0xFF000000
-#define WWAN_DATA_LEN 2000
-#define IPA_RM_INACTIVITY_TIMER 100 /* IPA_RM */
-#define HEADROOM_FOR_QMAP   8 /* for mux header */
-#define TAILROOM            0 /* for padding by mux layer */
-#define MAX_NUM_OF_MUX_CHANNEL  10 /* max mux channels */
-#define UL_FILTER_RULE_HANDLE_START 69
-#define DEFAULT_OUTSTANDING_HIGH 64
-#define DEFAULT_OUTSTANDING_LOW 32
-
-#define IPA_WWAN_DEV_NAME "rmnet_ipa%d"
-#define IPA_WWAN_DEVICE_COUNT (1)
-
-#define IPA_WWAN_RX_SOFTIRQ_THRESH 16
-
-#define INVALID_MUX_ID 0xFF
-#define IPA_QUOTA_REACH_ALERT_MAX_SIZE 64
-#define IPA_QUOTA_REACH_IF_NAME_MAX_SIZE 64
-#define IPA_UEVENT_NUM_EVNP 4 /* number of event pointers */
-
-static struct net_device *ipa_netdevs[IPA_WWAN_DEVICE_COUNT];
-static struct ipa_sys_connect_params apps_to_ipa_ep_cfg, ipa_to_apps_ep_cfg;
-static u32 qmap_hdr_hdl, dflt_v4_wan_rt_hdl, dflt_v6_wan_rt_hdl;
-static struct rmnet_mux_val mux_channel[MAX_NUM_OF_MUX_CHANNEL];
-static int num_q6_rule, old_num_q6_rule;
-static int rmnet_index;
-static bool egress_set, a7_ul_flt_set;
-static struct workqueue_struct *ipa_rm_q6_workqueue; /* IPA_RM workqueue*/
-static atomic_t is_initialized;
-static atomic_t is_ssr;
-static void *subsys_notify_handle;
-
-u32 apps_to_ipa_hdl, ipa_to_apps_hdl; /* get handler from ipa */
-static struct mutex ipa_to_apps_pipe_handle_guard;
-static struct mutex add_mux_channel_lock;
-static int wwan_add_ul_flt_rule_to_ipa(void);
-static int wwan_del_ul_flt_rule_to_ipa(void);
-static void ipa_wwan_msg_free_cb(void*, u32, u32);
-
-static void wake_tx_queue(struct work_struct *work);
-static DECLARE_WORK(ipa_tx_wakequeue_work, wake_tx_queue);
-
-static void tethering_stats_poll_queue(struct work_struct *work);
-static DECLARE_DELAYED_WORK(ipa_tether_stats_poll_wakequeue_work,
-			    tethering_stats_poll_queue);
-
-enum wwan_device_status {
-	WWAN_DEVICE_INACTIVE = 0,
-	WWAN_DEVICE_ACTIVE   = 1
-};
-
-struct ipa_rmnet_plat_drv_res {
-	bool ipa_rmnet_ssr;
-	bool ipa_loaduC;
-};
-
-/**
- * struct wwan_private - WWAN private data
- * @net: network interface struct implemented by this driver
- * @stats: iface statistics
- * @outstanding_pkts: number of packets sent to IPA without TX complete ACKed
- * @outstanding_high: number of outstanding packets allowed
- * @outstanding_low: number of outstanding packets which shall cause
- * @ch_id: channel id
- * @lock: spinlock for mutual exclusion
- * @device_status: holds device status
- *
- * WWAN private - holds all relevant info about WWAN driver
- */
-struct wwan_private {
-	struct net_device *net;
-	struct net_device_stats stats;
-	atomic_t outstanding_pkts;
-	int outstanding_high;
-	int outstanding_low;
-	uint32_t ch_id;
-	spinlock_t lock;
-	struct completion resource_granted_completion;
-	enum wwan_device_status device_status;
-};
-
-/**
-* ipa_setup_a7_qmap_hdr() - Setup default a7 qmap hdr
-*
-* Return codes:
-* 0: success
-* -ENOMEM: failed to allocate memory
-* -EPERM: failed to add the tables
-*/
-static int ipa_setup_a7_qmap_hdr(void)
-{
-	struct ipa_ioc_add_hdr *hdr;
-	struct ipa_hdr_add *hdr_entry;
-	u32 pyld_sz;
-	int ret;
-
-	/* install the basic exception header */
-	pyld_sz = sizeof(struct ipa_ioc_add_hdr) + 1 *
-		      sizeof(struct ipa_hdr_add);
-	hdr = kzalloc(pyld_sz, GFP_KERNEL);
-	if (!hdr) {
-		IPAWANERR("fail to alloc exception hdr\n");
-		return -ENOMEM;
-	}
-	hdr->num_hdrs = 1;
-	hdr->commit = 1;
-	hdr_entry = &hdr->hdr[0];
-
-	strlcpy(hdr_entry->name, IPA_A7_QMAP_HDR_NAME,
-				IPA_RESOURCE_NAME_MAX);
-	hdr_entry->hdr_len = IPA_QMAP_HEADER_LENGTH; /* 4 bytes */
-
-	if (ipa_add_hdr(hdr)) {
-		IPAWANERR("fail to add IPA_A7_QMAP hdr\n");
-		ret = -EPERM;
-		goto bail;
-	}
-
-	if (hdr_entry->status) {
-		IPAWANERR("fail to add IPA_A7_QMAP hdr\n");
-		ret = -EPERM;
-		goto bail;
-	}
-	qmap_hdr_hdl = hdr_entry->hdr_hdl;
-
-	ret = 0;
-bail:
-	kfree(hdr);
-	return ret;
-}
-
-static void ipa_del_a7_qmap_hdr(void)
-{
-	struct ipa_ioc_del_hdr *del_hdr;
-	struct ipa_hdr_del *hdl_entry;
-	u32 pyld_sz;
-	int ret;
-
-	pyld_sz = sizeof(struct ipa_ioc_del_hdr) + 1 *
-		      sizeof(struct ipa_hdr_del);
-	del_hdr = kzalloc(pyld_sz, GFP_KERNEL);
-	if (!del_hdr) {
-		IPAWANERR("fail to alloc exception hdr_del\n");
-		return;
-	}
-
-	del_hdr->commit = 1;
-	del_hdr->num_hdls = 1;
-	hdl_entry = &del_hdr->hdl[0];
-	hdl_entry->hdl = qmap_hdr_hdl;
-
-	ret = ipa_del_hdr(del_hdr);
-	if (ret || hdl_entry->status)
-		IPAWANERR("ipa_del_hdr failed\n");
-	else
-		IPAWANDBG("hdrs deletion done\n");
-
-	qmap_hdr_hdl = 0;
-	kfree(del_hdr);
-}
-
-static void ipa_del_qmap_hdr(uint32_t hdr_hdl)
-{
-	struct ipa_ioc_del_hdr *del_hdr;
-	struct ipa_hdr_del *hdl_entry;
-	u32 pyld_sz;
-	int ret;
-
-	if (hdr_hdl == 0) {
-		IPAWANERR("Invalid hdr_hdl provided\n");
-		return;
-	}
-
-	pyld_sz = sizeof(struct ipa_ioc_del_hdr) + 1 *
-		sizeof(struct ipa_hdr_del);
-	del_hdr = kzalloc(pyld_sz, GFP_KERNEL);
-	if (!del_hdr) {
-		IPAWANERR("fail to alloc exception hdr_del\n");
-		return;
-	}
-
-	del_hdr->commit = 1;
-	del_hdr->num_hdls = 1;
-	hdl_entry = &del_hdr->hdl[0];
-	hdl_entry->hdl = hdr_hdl;
-
-	ret = ipa_del_hdr(del_hdr);
-	if (ret || hdl_entry->status)
-		IPAWANERR("ipa_del_hdr failed\n");
-	else
-		IPAWANDBG("header deletion done\n");
-
-	qmap_hdr_hdl = 0;
-	kfree(del_hdr);
-}
-
-static void ipa_del_mux_qmap_hdrs(void)
-{
-	int index;
-
-	for (index = 0; index < rmnet_index; index++) {
-		ipa_del_qmap_hdr(mux_channel[index].hdr_hdl);
-		mux_channel[index].hdr_hdl = 0;
-	}
-}
-
-static int ipa_add_qmap_hdr(uint32_t mux_id, uint32_t *hdr_hdl)
-{
-	struct ipa_ioc_add_hdr *hdr;
-	struct ipa_hdr_add *hdr_entry;
-	char hdr_name[IPA_RESOURCE_NAME_MAX];
-	u32 pyld_sz;
-	int ret;
-
-	pyld_sz = sizeof(struct ipa_ioc_add_hdr) + 1 *
-		      sizeof(struct ipa_hdr_add);
-	hdr = kzalloc(pyld_sz, GFP_KERNEL);
-	if (!hdr) {
-		IPAWANERR("fail to alloc exception hdr\n");
-		return -ENOMEM;
-	}
-	hdr->num_hdrs = 1;
-	hdr->commit = 1;
-	hdr_entry = &hdr->hdr[0];
-
-	snprintf(hdr_name, IPA_RESOURCE_NAME_MAX, "%s%d",
-		 A2_MUX_HDR_NAME_V4_PREF,
-		 mux_id);
-	 strlcpy(hdr_entry->name, hdr_name,
-				IPA_RESOURCE_NAME_MAX);
-
-	hdr_entry->hdr_len = IPA_QMAP_HEADER_LENGTH; /* 4 bytes */
-	hdr_entry->hdr[1] = (uint8_t) mux_id;
-	IPAWANDBG("header (%s) with mux-id: (%d)\n",
-		hdr_name,
-		hdr_entry->hdr[1]);
-	if (ipa_add_hdr(hdr)) {
-		IPAWANERR("fail to add IPA_QMAP hdr\n");
-		ret = -EPERM;
-		goto bail;
-	}
-
-	if (hdr_entry->status) {
-		IPAWANERR("fail to add IPA_QMAP hdr\n");
-		ret = -EPERM;
-		goto bail;
-	}
-
-	ret = 0;
-	*hdr_hdl = hdr_entry->hdr_hdl;
-bail:
-	kfree(hdr);
-	return ret;
-}
-
-/**
-* ipa_setup_dflt_wan_rt_tables() - Setup default wan routing tables
-*
-* Return codes:
-* 0: success
-* -ENOMEM: failed to allocate memory
-* -EPERM: failed to add the tables
-*/
-static int ipa_setup_dflt_wan_rt_tables(void)
-{
-	struct ipa_ioc_add_rt_rule *rt_rule;
-	struct ipa_rt_rule_add *rt_rule_entry;
-
-	rt_rule =
-	   kzalloc(sizeof(struct ipa_ioc_add_rt_rule) + 1 *
-			   sizeof(struct ipa_rt_rule_add), GFP_KERNEL);
-	if (!rt_rule) {
-		IPAWANERR("fail to alloc mem\n");
-		return -ENOMEM;
-	}
-	/* setup a default v4 route to point to Apps */
-	rt_rule->num_rules = 1;
-	rt_rule->commit = 1;
-	rt_rule->ip = IPA_IP_v4;
-	strlcpy(rt_rule->rt_tbl_name, IPA_DFLT_WAN_RT_TBL_NAME,
-			IPA_RESOURCE_NAME_MAX);
-
-	rt_rule_entry = &rt_rule->rules[0];
-	rt_rule_entry->at_rear = 1;
-	rt_rule_entry->rule.dst = IPA_CLIENT_APPS_WAN_CONS;
-	rt_rule_entry->rule.hdr_hdl = qmap_hdr_hdl;
-
-	if (ipa_add_rt_rule(rt_rule)) {
-		IPAWANERR("fail to add dflt_wan v4 rule\n");
-		kfree(rt_rule);
-		return -EPERM;
-	}
-
-	IPAWANDBG("dflt v4 rt rule hdl=%x\n", rt_rule_entry->rt_rule_hdl);
-	dflt_v4_wan_rt_hdl = rt_rule_entry->rt_rule_hdl;
-
-	/* setup a default v6 route to point to A5 */
-	rt_rule->ip = IPA_IP_v6;
-	if (ipa_add_rt_rule(rt_rule)) {
-		IPAWANERR("fail to add dflt_wan v6 rule\n");
-		kfree(rt_rule);
-		return -EPERM;
-	}
-	IPAWANDBG("dflt v6 rt rule hdl=%x\n", rt_rule_entry->rt_rule_hdl);
-	dflt_v6_wan_rt_hdl = rt_rule_entry->rt_rule_hdl;
-
-	kfree(rt_rule);
-	return 0;
-}
-
-static void ipa_del_dflt_wan_rt_tables(void)
-{
-	struct ipa_ioc_del_rt_rule *rt_rule;
-	struct ipa_rt_rule_del *rt_rule_entry;
-	int len;
-
-	len = sizeof(struct ipa_ioc_del_rt_rule) + 1 *
-			   sizeof(struct ipa_rt_rule_del);
-	rt_rule = kzalloc(len, GFP_KERNEL);
-	if (!rt_rule) {
-		IPAWANERR("unable to allocate memory for del route rule\n");
-		return;
-	}
-
-	memset(rt_rule, 0, len);
-	rt_rule->commit = 1;
-	rt_rule->num_hdls = 1;
-	rt_rule->ip = IPA_IP_v4;
-
-	rt_rule_entry = &rt_rule->hdl[0];
-	rt_rule_entry->status = -1;
-	rt_rule_entry->hdl = dflt_v4_wan_rt_hdl;
-
-	IPAWANERR("Deleting Route hdl:(0x%x) with ip type: %d\n",
-		rt_rule_entry->hdl, IPA_IP_v4);
-	if (ipa_del_rt_rule(rt_rule) ||
-			(rt_rule_entry->status)) {
-		IPAWANERR("Routing rule deletion failed!\n");
-	}
-
-	rt_rule->ip = IPA_IP_v6;
-	rt_rule_entry->hdl = dflt_v6_wan_rt_hdl;
-	IPAWANERR("Deleting Route hdl:(0x%x) with ip type: %d\n",
-		rt_rule_entry->hdl, IPA_IP_v6);
-	if (ipa_del_rt_rule(rt_rule) ||
-			(rt_rule_entry->status)) {
-		IPAWANERR("Routing rule deletion failed!\n");
-	}
-
-	kfree(rt_rule);
-}
-
-int copy_ul_filter_rule_to_ipa(struct ipa_install_fltr_rule_req_msg_v01
-		*rule_req, uint32_t *rule_hdl)
-{
-	int i, j;
-
-	if (rule_req->filter_spec_list_valid == true) {
-		num_q6_rule = rule_req->filter_spec_list_len;
-		IPAWANDBG("Received (%d) install_flt_req\n", num_q6_rule);
-	} else {
-		num_q6_rule = 0;
-		IPAWANERR("got no UL rules from modem\n");
-		return -EINVAL;
-	}
-
-	/* copy UL filter rules from Modem*/
-	for (i = 0; i < num_q6_rule; i++) {
-		/* check if rules overside the cache*/
-		if (i == MAX_NUM_Q6_RULE) {
-			IPAWANERR("Reaching (%d) max cache ",
-				MAX_NUM_Q6_RULE);
-			IPAWANERR(" however total (%d)\n",
-				num_q6_rule);
-			goto failure;
-		}
-		/* construct UL_filter_rule handler QMI use-cas */
-		ipa_qmi_ctx->q6_ul_filter_rule[i].filter_hdl =
-			UL_FILTER_RULE_HANDLE_START + i;
-		rule_hdl[i] = ipa_qmi_ctx->q6_ul_filter_rule[i].filter_hdl;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].ip =
-			rule_req->filter_spec_list[i].ip_type;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].action =
-			rule_req->filter_spec_list[i].filter_action;
-		if (rule_req->filter_spec_list[i].is_routing_table_index_valid
-			== true)
-			ipa_qmi_ctx->q6_ul_filter_rule[i].rt_tbl_idx =
-			rule_req->filter_spec_list[i].route_table_index;
-		if (rule_req->filter_spec_list[i].is_mux_id_valid == true)
-			ipa_qmi_ctx->q6_ul_filter_rule[i].mux_id =
-			rule_req->filter_spec_list[i].mux_id;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.rule_eq_bitmap =
-			rule_req->filter_spec_list[i].filter_rule.
-			rule_eq_bitmap;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.tos_eq_present =
-			rule_req->filter_spec_list[i].filter_rule.
-			tos_eq_present;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.tos_eq =
-			rule_req->filter_spec_list[i].filter_rule.tos_eq;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			protocol_eq_present = rule_req->filter_spec_list[i].
-			filter_rule.protocol_eq_present;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.protocol_eq =
-			rule_req->filter_spec_list[i].filter_rule.
-			protocol_eq;
-
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			num_ihl_offset_range_16 = rule_req->filter_spec_list[i].
-			filter_rule.num_ihl_offset_range_16;
-		for (j = 0; j < ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			num_ihl_offset_range_16; j++) {
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			ihl_offset_range_16[j].offset = rule_req->
-			filter_spec_list[i].filter_rule.
-			ihl_offset_range_16[j].offset;
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			ihl_offset_range_16[j].range_low = rule_req->
-			filter_spec_list[i].filter_rule.
-			ihl_offset_range_16[j].range_low;
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			ihl_offset_range_16[j].range_high = rule_req->
-			filter_spec_list[i].filter_rule.
-			ihl_offset_range_16[j].range_high;
-		}
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.num_offset_meq_32 =
-			rule_req->filter_spec_list[i].filter_rule.
-			num_offset_meq_32;
-		for (j = 0; j < ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-				num_offset_meq_32; j++) {
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			offset_meq_32[j].offset = rule_req->filter_spec_list[i].
-			filter_rule.offset_meq_32[j].offset;
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			offset_meq_32[j].mask = rule_req->filter_spec_list[i].
-			filter_rule.offset_meq_32[j].mask;
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			offset_meq_32[j].value = rule_req->filter_spec_list[i].
-			filter_rule.offset_meq_32[j].value;
-		}
-
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.tc_eq_present =
-			rule_req->filter_spec_list[i].filter_rule.tc_eq_present;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.tc_eq =
-			rule_req->filter_spec_list[i].filter_rule.tc_eq;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.fl_eq_present =
-			rule_req->filter_spec_list[i].filter_rule.
-			flow_eq_present;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.fl_eq =
-			rule_req->filter_spec_list[i].filter_rule.flow_eq;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-		ihl_offset_eq_16_present = rule_req->filter_spec_list[i].
-		filter_rule.ihl_offset_eq_16_present;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-		ihl_offset_eq_16.offset = rule_req->filter_spec_list[i].
-		filter_rule.ihl_offset_eq_16.offset;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-		ihl_offset_eq_16.value = rule_req->filter_spec_list[i].
-		filter_rule.ihl_offset_eq_16.value;
-
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-		ihl_offset_eq_32_present = rule_req->filter_spec_list[i].
-		filter_rule.ihl_offset_eq_32_present;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-		ihl_offset_eq_32.offset = rule_req->filter_spec_list[i].
-		filter_rule.ihl_offset_eq_32.offset;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-		ihl_offset_eq_32.value = rule_req->filter_spec_list[i].
-		filter_rule.ihl_offset_eq_32.value;
-
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-		num_ihl_offset_meq_32 = rule_req->filter_spec_list[i].
-		filter_rule.num_ihl_offset_meq_32;
-		for (j = 0; j < ipa_qmi_ctx->q6_ul_filter_rule[i].
-			eq_attrib.num_ihl_offset_meq_32; j++) {
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-				ihl_offset_meq_32[j].offset = rule_req->
-				filter_spec_list[i].filter_rule.
-				ihl_offset_meq_32[j].offset;
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-				ihl_offset_meq_32[j].mask = rule_req->
-				filter_spec_list[i].filter_rule.
-				ihl_offset_meq_32[j].mask;
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-				ihl_offset_meq_32[j].value = rule_req->
-				filter_spec_list[i].filter_rule.
-				ihl_offset_meq_32[j].value;
-		}
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.num_offset_meq_128 =
-			rule_req->filter_spec_list[i].filter_rule.
-			num_offset_meq_128;
-		for (j = 0; j < ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			num_offset_meq_128; j++) {
-			ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-				offset_meq_128[j].offset = rule_req->
-				filter_spec_list[i].filter_rule.
-				offset_meq_128[j].offset;
-			memcpy(ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-					offset_meq_128[j].mask,
-					rule_req->filter_spec_list[i].
-					filter_rule.offset_meq_128[j].mask, 16);
-			memcpy(ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-					offset_meq_128[j].value, rule_req->
-					filter_spec_list[i].filter_rule.
-					offset_meq_128[j].value, 16);
-		}
-
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			metadata_meq32_present = rule_req->filter_spec_list[i].
-				filter_rule.metadata_meq32_present;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			metadata_meq32.offset = rule_req->filter_spec_list[i].
-			filter_rule.metadata_meq32.offset;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			metadata_meq32.mask = rule_req->filter_spec_list[i].
-			filter_rule.metadata_meq32.mask;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.metadata_meq32.
-			value = rule_req->filter_spec_list[i].filter_rule.
-			metadata_meq32.value;
-		ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib.
-			ipv4_frag_eq_present = rule_req->filter_spec_list[i].
-			filter_rule.ipv4_frag_eq_present;
-	}
-
-	if (rule_req->xlat_filter_indices_list_valid) {
-		if (rule_req->xlat_filter_indices_list_len > num_q6_rule) {
-			IPAWANERR("Number of xlat indices is not valid: %d\n",
-					rule_req->xlat_filter_indices_list_len);
-			goto failure;
-		}
-		IPAWANDBG("Receive %d XLAT indices: ",
-				rule_req->xlat_filter_indices_list_len);
-		for (i = 0; i < rule_req->xlat_filter_indices_list_len; i++)
-			IPAWANDBG("%d ", rule_req->xlat_filter_indices_list[i]);
-		IPAWANDBG("\n");
-
-		for (i = 0; i < rule_req->xlat_filter_indices_list_len; i++) {
-			if (rule_req->xlat_filter_indices_list[i]
-				>= num_q6_rule) {
-				IPAWANERR("Xlat rule idx is wrong: %d\n",
-					rule_req->xlat_filter_indices_list[i]);
-				goto failure;
-			} else {
-				ipa_qmi_ctx->q6_ul_filter_rule
-				[rule_req->xlat_filter_indices_list[i]]
-				.is_xlat_rule = 1;
-				IPAWANDBG("Rule %d is xlat rule\n",
-					rule_req->xlat_filter_indices_list[i]);
-			}
-		}
-	}
-	goto success;
-
-failure:
-	num_q6_rule = 0;
-	memset(ipa_qmi_ctx->q6_ul_filter_rule, 0,
-		sizeof(ipa_qmi_ctx->q6_ul_filter_rule));
-	return -EINVAL;
-
-success:
-	return 0;
-}
-
-static int wwan_add_ul_flt_rule_to_ipa(void)
-{
-	u32 pyld_sz;
-	int i, retval = 0;
-	int num_v4_rule = 0, num_v6_rule = 0;
-	struct ipa_ioc_add_flt_rule *param;
-	struct ipa_flt_rule_add flt_rule_entry;
-	struct ipa_fltr_installed_notif_req_msg_v01 *req;
-
-	pyld_sz = sizeof(struct ipa_ioc_add_flt_rule) +
-	   sizeof(struct ipa_flt_rule_add);
-	param = kzalloc(pyld_sz, GFP_KERNEL);
-	if (!param)
-		return -ENOMEM;
-
-	req = (struct ipa_fltr_installed_notif_req_msg_v01 *)
-		kzalloc(sizeof(struct ipa_fltr_installed_notif_req_msg_v01),
-			GFP_KERNEL);
-	if (!req) {
-		kfree(param);
-		return -ENOMEM;
-	}
-
-	param->commit = 1;
-	param->ep = IPA_CLIENT_APPS_LAN_WAN_PROD;
-	param->global = false;
-	param->num_rules = (uint8_t)1;
-
-	mutex_lock(&ipa_qmi_lock);
-	for (i = 0; i < num_q6_rule && (ipa_qmi_ctx != NULL); i++) {
-		param->ip = ipa_qmi_ctx->q6_ul_filter_rule[i].ip;
-		memset(&flt_rule_entry, 0, sizeof(struct ipa_flt_rule_add));
-		flt_rule_entry.at_rear = true;
-		flt_rule_entry.rule.action =
-			ipa_qmi_ctx->q6_ul_filter_rule[i].action;
-		flt_rule_entry.rule.rt_tbl_idx
-		= ipa_qmi_ctx->q6_ul_filter_rule[i].rt_tbl_idx;
-		flt_rule_entry.rule.retain_hdr = true;
-
-		/* debug rt-hdl*/
-		IPAWANDBG("install-IPA index(%d),rt-tbl:(%d)\n",
-			i, flt_rule_entry.rule.rt_tbl_idx);
-		flt_rule_entry.rule.eq_attrib_type = true;
-		memcpy(&(flt_rule_entry.rule.eq_attrib),
-			&ipa_qmi_ctx->q6_ul_filter_rule[i].eq_attrib,
-			sizeof(struct ipa_ipfltri_rule_eq));
-		memcpy(&(param->rules[0]), &flt_rule_entry,
-			sizeof(struct ipa_flt_rule_add));
-		if (ipa_add_flt_rule((struct ipa_ioc_add_flt_rule *)param)) {
-			retval = -EFAULT;
-			IPAWANERR("add A7 UL filter rule(%d) failed\n", i);
-		} else {
-			/* store the rule handler */
-			ipa_qmi_ctx->q6_ul_filter_rule_hdl[i] =
-				param->rules[0].flt_rule_hdl;
-		}
-	}
-	mutex_unlock(&ipa_qmi_lock);
-
-	/* send ipa_fltr_installed_notif_req_msg_v01 to Q6*/
-	req->source_pipe_index =
-		ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_WAN_PROD);
-	req->install_status = QMI_RESULT_SUCCESS_V01;
-	req->filter_index_list_len = num_q6_rule;
-	mutex_lock(&ipa_qmi_lock);
-	for (i = 0; i < num_q6_rule && (ipa_qmi_ctx != NULL); i++) {
-		if (ipa_qmi_ctx->q6_ul_filter_rule[i].ip == IPA_IP_v4) {
-			req->filter_index_list[i].filter_index = num_v4_rule;
-			num_v4_rule++;
-		} else {
-			req->filter_index_list[i].filter_index = num_v6_rule;
-			num_v6_rule++;
-		}
-		req->filter_index_list[i].filter_handle =
-			ipa_qmi_ctx->q6_ul_filter_rule[i].filter_hdl;
-	}
-	mutex_unlock(&ipa_qmi_lock);
-	if (qmi_filter_notify_send(req)) {
-		IPAWANDBG("add filter rule index on A7-RX failed\n");
-		retval = -EFAULT;
-	}
-	old_num_q6_rule = num_q6_rule;
-	IPAWANDBG("add (%d) filter rule index on A7-RX\n",
-			old_num_q6_rule);
-	kfree(param);
-	kfree(req);
-	return retval;
-}
-
-static int wwan_del_ul_flt_rule_to_ipa(void)
-{
-	u32 pyld_sz;
-	int i, retval = 0;
-	struct ipa_ioc_del_flt_rule *param;
-	struct ipa_flt_rule_del flt_rule_entry;
-
-	pyld_sz = sizeof(struct ipa_ioc_del_flt_rule) +
-	   sizeof(struct ipa_flt_rule_del);
-	param = kzalloc(pyld_sz, GFP_KERNEL);
-	if (!param) {
-		IPAWANERR("kzalloc failed\n");
-		return -ENOMEM;
-	}
-
-	param->commit = 1;
-	param->num_hdls = (uint8_t) 1;
-
-	for (i = 0; i < old_num_q6_rule; i++) {
-		param->ip = ipa_qmi_ctx->q6_ul_filter_rule[i].ip;
-		memset(&flt_rule_entry, 0, sizeof(struct ipa_flt_rule_del));
-		flt_rule_entry.hdl = ipa_qmi_ctx->q6_ul_filter_rule_hdl[i];
-		/* debug rt-hdl*/
-		IPAWANDBG("delete-IPA rule index(%d)\n", i);
-		memcpy(&(param->hdl[0]), &flt_rule_entry,
-			sizeof(struct ipa_flt_rule_del));
-		if (ipa_del_flt_rule((struct ipa_ioc_del_flt_rule *)param)) {
-			IPAWANERR("del A7 UL filter rule(%d) failed\n", i);
-			kfree(param);
-			return -EFAULT;
-		}
-	}
-
-	/* set UL filter-rule add-indication */
-	a7_ul_flt_set = false;
-	old_num_q6_rule = 0;
-
-	kfree(param);
-	return retval;
-}
-
-static int find_mux_channel_index(uint32_t mux_id)
-{
-	int i;
-
-	for (i = 0; i < MAX_NUM_OF_MUX_CHANNEL; i++) {
-		if (mux_id == mux_channel[i].mux_id)
-			return i;
-	}
-	return MAX_NUM_OF_MUX_CHANNEL;
-}
-
-static int find_vchannel_name_index(const char *vchannel_name)
-{
-	int i;
-
-	for (i = 0; i < MAX_NUM_OF_MUX_CHANNEL; i++) {
-		if (0 == strcmp(mux_channel[i].vchannel_name, vchannel_name))
-			return i;
-	}
-	return MAX_NUM_OF_MUX_CHANNEL;
-}
-
-static int wwan_register_to_ipa(int index)
-{
-	struct ipa_tx_intf tx_properties = {0};
-	struct ipa_ioc_tx_intf_prop tx_ioc_properties[2] = { {0}, {0} };
-	struct ipa_ioc_tx_intf_prop *tx_ipv4_property;
-	struct ipa_ioc_tx_intf_prop *tx_ipv6_property;
-	struct ipa_rx_intf rx_properties = {0};
-	struct ipa_ioc_rx_intf_prop rx_ioc_properties[2] = { {0}, {0} };
-	struct ipa_ioc_rx_intf_prop *rx_ipv4_property;
-	struct ipa_ioc_rx_intf_prop *rx_ipv6_property;
-	struct ipa_ext_intf ext_properties = {0};
-	struct ipa_ioc_ext_intf_prop *ext_ioc_properties;
-	u32 pyld_sz;
-	int ret = 0, i;
-
-	IPAWANDBG("index(%d) device[%s]:\n", index,
-		mux_channel[index].vchannel_name);
-	if (!mux_channel[index].mux_hdr_set) {
-		ret = ipa_add_qmap_hdr(mux_channel[index].mux_id,
-		      &mux_channel[index].hdr_hdl);
-		if (ret) {
-			IPAWANERR("ipa_add_mux_hdr failed (%d)\n", index);
-			return ret;
-		}
-		mux_channel[index].mux_hdr_set = true;
-	}
-	tx_properties.prop = tx_ioc_properties;
-	tx_ipv4_property = &tx_properties.prop[0];
-	tx_ipv4_property->ip = IPA_IP_v4;
-	tx_ipv4_property->dst_pipe = IPA_CLIENT_APPS_WAN_CONS;
-	snprintf(tx_ipv4_property->hdr_name, IPA_RESOURCE_NAME_MAX, "%s%d",
-		 A2_MUX_HDR_NAME_V4_PREF,
-		 mux_channel[index].mux_id);
-	tx_ipv6_property = &tx_properties.prop[1];
-	tx_ipv6_property->ip = IPA_IP_v6;
-	tx_ipv6_property->dst_pipe = IPA_CLIENT_APPS_WAN_CONS;
-	/* no need use A2_MUX_HDR_NAME_V6_PREF, same header */
-	snprintf(tx_ipv6_property->hdr_name, IPA_RESOURCE_NAME_MAX, "%s%d",
-		 A2_MUX_HDR_NAME_V4_PREF,
-		 mux_channel[index].mux_id);
-	tx_properties.num_props = 2;
-
-	rx_properties.prop = rx_ioc_properties;
-	rx_ipv4_property = &rx_properties.prop[0];
-	rx_ipv4_property->ip = IPA_IP_v4;
-	rx_ipv4_property->attrib.attrib_mask |= IPA_FLT_META_DATA;
-	rx_ipv4_property->attrib.meta_data =
-		mux_channel[index].mux_id << WWAN_METADATA_SHFT;
-	rx_ipv4_property->attrib.meta_data_mask = WWAN_METADATA_MASK;
-	rx_ipv4_property->src_pipe = IPA_CLIENT_APPS_LAN_WAN_PROD;
-	rx_ipv6_property = &rx_properties.prop[1];
-	rx_ipv6_property->ip = IPA_IP_v6;
-	rx_ipv6_property->attrib.attrib_mask |= IPA_FLT_META_DATA;
-	rx_ipv6_property->attrib.meta_data =
-		mux_channel[index].mux_id << WWAN_METADATA_SHFT;
-	rx_ipv6_property->attrib.meta_data_mask = WWAN_METADATA_MASK;
-	rx_ipv6_property->src_pipe = IPA_CLIENT_APPS_LAN_WAN_PROD;
-	rx_properties.num_props = 2;
-
-	pyld_sz = num_q6_rule *
-	   sizeof(struct ipa_ioc_ext_intf_prop);
-	ext_ioc_properties = kmalloc(pyld_sz, GFP_KERNEL);
-	if (!ext_ioc_properties) {
-		IPAWANERR("Error allocate memory\n");
-		return -ENOMEM;
-	}
-
-	ext_properties.prop = ext_ioc_properties;
-	ext_properties.excp_pipe_valid = true;
-	ext_properties.excp_pipe = IPA_CLIENT_APPS_WAN_CONS;
-	ext_properties.num_props = num_q6_rule;
-	for (i = 0; i < num_q6_rule; i++) {
-		memcpy(&(ext_properties.prop[i]),
-				 &(ipa_qmi_ctx->q6_ul_filter_rule[i]),
-				sizeof(struct ipa_ioc_ext_intf_prop));
-	ext_properties.prop[i].mux_id = mux_channel[index].mux_id;
-	IPAWANDBG("index %d ip: %d rt-tbl:%d\n", i,
-		ext_properties.prop[i].ip,
-		ext_properties.prop[i].rt_tbl_idx);
-	IPAWANDBG("action: %d mux:%d\n",
-		ext_properties.prop[i].action,
-		ext_properties.prop[i].mux_id);
-	}
-	ret = ipa_register_intf_ext(mux_channel[index].
-		vchannel_name, &tx_properties,
-		&rx_properties, &ext_properties);
-	if (ret) {
-		IPAWANERR("[%s]:ipa_register_intf failed %d\n",
-			mux_channel[index].vchannel_name, ret);
-		goto fail;
-	}
-	mux_channel[index].ul_flt_reg = true;
-fail:
-	kfree(ext_ioc_properties);
-	return ret;
-}
-
-static void ipa_cleanup_deregister_intf(void)
-{
-	int i;
-	int ret;
-
-	for (i = 0; i < rmnet_index; i++) {
-		if (mux_channel[i].ul_flt_reg) {
-			ret = ipa_deregister_intf(mux_channel[i].vchannel_name);
-			if (ret < 0) {
-				IPAWANERR("de-register device %s(%d) failed\n",
-					mux_channel[i].vchannel_name,
-					i);
-				return;
-			} else {
-				IPAWANDBG("de-register device %s(%d) success\n",
-					mux_channel[i].vchannel_name,
-					i);
-			}
-		}
-		mux_channel[i].ul_flt_reg = false;
-	}
-}
-
-int wwan_update_mux_channel_prop(void)
-{
-	int ret = 0, i;
-	/* install UL filter rules */
-	if (egress_set) {
-		if (ipa_qmi_ctx &&
-			ipa_qmi_ctx->modem_cfg_emb_pipe_flt == false) {
-			IPAWANDBG("setup UL filter rules\n");
-			if (a7_ul_flt_set) {
-				IPAWANDBG("del previous UL filter rules\n");
-				/* delete rule hdlers */
-				ret = wwan_del_ul_flt_rule_to_ipa();
-				if (ret) {
-					IPAWANERR("failed to del old rules\n");
-					return -EINVAL;
-				} else {
-					IPAWANDBG("deleted old UL rules\n");
-				}
-			}
-			ret = wwan_add_ul_flt_rule_to_ipa();
-		}
-		if (ret)
-			IPAWANERR("failed to install UL rules\n");
-		else
-			a7_ul_flt_set = true;
-	}
-	/* update Tx/Rx/Ext property */
-	IPAWANDBG("update Tx/Rx/Ext property in IPA\n");
-	if (rmnet_index == 0) {
-		IPAWANDBG("no Tx/Rx/Ext property registered in IPA\n");
-		return ret;
-	}
-
-	ipa_cleanup_deregister_intf();
-
-	for (i = 0; i < rmnet_index; i++) {
-		ret = wwan_register_to_ipa(i);
-		if (ret < 0) {
-			IPAWANERR("failed to re-regist %s, mux %d, index %d\n",
-				mux_channel[i].vchannel_name,
-				mux_channel[i].mux_id,
-				i);
-			return -ENODEV;
-		}
-		IPAWANERR("dev(%s) has registered to IPA\n",
-		mux_channel[i].vchannel_name);
-		mux_channel[i].ul_flt_reg = true;
-	}
-	return ret;
-}
-
-static int __ipa_wwan_open(struct net_device *dev)
-{
-	struct wwan_private *wwan_ptr = netdev_priv(dev);
-
-	IPAWANDBG("[%s] __wwan_open()\n", dev->name);
-	if (wwan_ptr->device_status != WWAN_DEVICE_ACTIVE)
-		INIT_COMPLETION(wwan_ptr->resource_granted_completion);
-	wwan_ptr->device_status = WWAN_DEVICE_ACTIVE;
-	return 0;
-}
-
-/**
- * wwan_open() - Opens the wwan network interface. Opens logical
- * channel on A2 MUX driver and starts the network stack queue
- *
- * @dev: network device
- *
- * Return codes:
- * 0: success
- * -ENODEV: Error while opening logical channel on A2 MUX driver
- */
-static int ipa_wwan_open(struct net_device *dev)
-{
-	int rc = 0;
-
-	IPAWANDBG("[%s] wwan_open()\n", dev->name);
-	rc = __ipa_wwan_open(dev);
-	if (rc == 0)
-		netif_start_queue(dev);
-	return rc;
-}
-
-static int __ipa_wwan_close(struct net_device *dev)
-{
-	struct wwan_private *wwan_ptr = netdev_priv(dev);
-	int rc = 0;
-
-	if (wwan_ptr->device_status == WWAN_DEVICE_ACTIVE) {
-		wwan_ptr->device_status = WWAN_DEVICE_INACTIVE;
-		/* do not close wwan port once up,  this causes
-			remote side to hang if tried to open again */
-		INIT_COMPLETION(wwan_ptr->resource_granted_completion);
-		rc = ipa_deregister_intf(dev->name);
-		if (rc) {
-			IPAWANERR("[%s]: ipa_deregister_intf failed %d\n",
-			       dev->name, rc);
-			return rc;
-		}
-		return rc;
-	} else {
-		return -EBADF;
-	}
-}
-
-/**
- * ipa_wwan_stop() - Stops the wwan network interface. Closes
- * logical channel on A2 MUX driver and stops the network stack
- * queue
- *
- * @dev: network device
- *
- * Return codes:
- * 0: success
- * -ENODEV: Error while opening logical channel on A2 MUX driver
- */
-static int ipa_wwan_stop(struct net_device *dev)
-{
-	IPAWANDBG("[%s] ipa_wwan_stop()\n", dev->name);
-	__ipa_wwan_close(dev);
-	netif_stop_queue(dev);
-	return 0;
-}
-
-static int ipa_wwan_change_mtu(struct net_device *dev, int new_mtu)
-{
-	if (0 > new_mtu || WWAN_DATA_LEN < new_mtu)
-		return -EINVAL;
-	IPAWANDBG("[%s] MTU change: old=%d new=%d\n",
-		dev->name, dev->mtu, new_mtu);
-	dev->mtu = new_mtu;
-	return 0;
-}
-
-/**
- * ipa_wwan_xmit() - Transmits an skb.
- *
- * @skb: skb to be transmitted
- * @dev: network device
- *
- * Return codes:
- * 0: success
- * NETDEV_TX_BUSY: Error while transmitting the skb. Try again
- * later
- * -EFAULT: Error while transmitting the skb
- */
-static int ipa_wwan_xmit(struct sk_buff *skb, struct net_device *dev)
-{
-	int ret = 0;
-	struct wwan_private *wwan_ptr = netdev_priv(dev);
-
-	if (netif_queue_stopped(dev)) {
-		IPAWANERR("[%s]fatal: ipa_wwan_xmit stopped\n", dev->name);
-	return 0;
-	}
-	/* IPA_RM checking start */
-	ret = ipa_rm_inactivity_timer_request_resource(
-		IPA_RM_RESOURCE_WWAN_0_PROD);
-	if (ret == -EINPROGRESS) {
-		netif_stop_queue(dev);
-		return NETDEV_TX_BUSY;
-	}
-	if (ret) {
-		pr_err("[%s] fatal: ipa rm timer request resource failed %d\n",
-		       dev->name, ret);
-		return -EFAULT;
-	}
-	/* IPA_RM checking end */
-	if (skb->protocol != htons(ETH_P_MAP)) {
-		IPAWANDBG
-		("SW filtering out none QMAP packet received from %s",
-		current->comm);
-		ret = NETDEV_TX_OK;
-		goto out;
-	}
-
-	/* checking High WM hit */
-	if (atomic_read(&wwan_ptr->outstanding_pkts) >=
-					wwan_ptr->outstanding_high) {
-		IPAWANDBG("Outstanding high (%d)- stopping\n",
-				wwan_ptr->outstanding_high);
-		netif_stop_queue(dev);
-		ret = NETDEV_TX_BUSY;
-		goto out;
-	}
-	ret = ipa_tx_dp(IPA_CLIENT_APPS_LAN_WAN_PROD, skb, NULL);
-	if (ret) {
-		ret = NETDEV_TX_BUSY;
-		dev->stats.tx_dropped++;
-		goto out;
-	}
-
-	atomic_inc(&wwan_ptr->outstanding_pkts);
-	dev->stats.tx_packets++;
-	dev->stats.tx_bytes += skb->len;
-	ret = NETDEV_TX_OK;
-
-out:
-	ipa_rm_inactivity_timer_release_resource(
-		IPA_RM_RESOURCE_WWAN_0_PROD);
-	return ret;
-}
-
-static void ipa_wwan_tx_timeout(struct net_device *dev)
-{
-	IPAWANERR("[%s] ipa_wwan_tx_timeout(), data stall in UL\n", dev->name);
-}
-
-/**
- * apps_ipa_tx_complete_notify() - Rx notify
- *
- * @priv: driver context
- * @evt: event type
- * @data: data provided with event
- *
- * Check that the packet is the one we sent and release it
- * This function will be called in defered context in IPA wq.
- */
-static void apps_ipa_tx_complete_notify(void *priv,
-		enum ipa_dp_evt_type evt,
-		unsigned long data)
-{
-	struct sk_buff *skb = (struct sk_buff *)data;
-	struct net_device *dev = (struct net_device *)priv;
-	struct wwan_private *wwan_ptr;
-
-	if (evt != IPA_WRITE_DONE) {
-		IPAWANDBG("unsupported event on Tx callback\n");
-		return;
-	}
-
-	if (dev != ipa_netdevs[0]) {
-		IPAWANDBG("Received pre-SSR packet completion\n");
-		dev_kfree_skb_any(skb);
-		return;
-	}
-
-	wwan_ptr = netdev_priv(dev);
-	atomic_dec(&wwan_ptr->outstanding_pkts);
-	__netif_tx_lock_bh(netdev_get_tx_queue(dev, 0));
-	if (!atomic_read(&is_ssr) &&
-		netif_queue_stopped(wwan_ptr->net) &&
-		atomic_read(&wwan_ptr->outstanding_pkts) <
-					(wwan_ptr->outstanding_low)) {
-		IPAWANDBG("Outstanding low (%d) - waking up queue\n",
-				wwan_ptr->outstanding_low);
-		netif_wake_queue(wwan_ptr->net);
-	}
-	__netif_tx_unlock_bh(netdev_get_tx_queue(dev, 0));
-	dev_kfree_skb_any(skb);
-	ipa_rm_inactivity_timer_release_resource(
-		IPA_RM_RESOURCE_WWAN_0_PROD);
-	return;
-}
-
-/**
- * apps_ipa_packet_receive_notify() - Rx notify
- *
- * @priv: driver context
- * @evt: event type
- * @data: data provided with event
- *
- * IPA will pass a packet to the Linux network stack with skb->data
- */
-static void apps_ipa_packet_receive_notify(void *priv,
-		enum ipa_dp_evt_type evt,
-		unsigned long data)
-{
-	struct sk_buff *skb = (struct sk_buff *)data;
-	struct net_device *dev = (struct net_device *)priv;
-	int result;
-	unsigned int packet_len = skb->len;
-
-	IPAWANDBG("Rx packet was received");
-	if (evt != IPA_RECEIVE) {
-		IPAWANERR("A none IPA_RECEIVE event in wan_ipa_receive\n");
-		return;
-	}
-
-	skb->dev = ipa_netdevs[0];
-	skb->protocol = htons(ETH_P_MAP);
-
-	if (dev->stats.rx_packets % IPA_WWAN_RX_SOFTIRQ_THRESH == 0)
-		result = netif_rx_ni(skb);
-	else
-		result = netif_rx(skb);
-
-	if (result)	{
-		pr_err_ratelimited(DEV_NAME " %s:%d fail on netif_rx_ni\n",
-				__func__, __LINE__);
-		dev->stats.rx_dropped++;
-	}
-	dev->stats.rx_packets++;
-	dev->stats.rx_bytes += packet_len;
-	return;
-}
-
-/**
- * ipa_wwan_ioctl() - I/O control for wwan network driver.
- *
- * @dev: network device
- * @ifr: ignored
- * @cmd: cmd to be excecuded. can be one of the following:
- * IPA_WWAN_IOCTL_OPEN - Open the network interface
- * IPA_WWAN_IOCTL_CLOSE - Close the network interface
- *
- * Return codes:
- * 0: success
- * NETDEV_TX_BUSY: Error while transmitting the skb. Try again
- * later
- * -EFAULT: Error while transmitting the skb
- */
-static int ipa_wwan_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
-{
-	int rc = 0;
-	int mru = 1000, epid = 1, mux_index, len;
-	struct ipa_msg_meta msg_meta;
-	struct ipa_wan_msg *wan_msg = NULL;
-	struct rmnet_ioctl_extended_s extend_ioctl_data;
-	struct rmnet_ioctl_data_s ioctl_data;
-
-	IPAWANDBG("rmnet_ipa got ioctl number 0x%08x", cmd);
-	switch (cmd) {
-	/*  Set Ethernet protocol  */
-	case RMNET_IOCTL_SET_LLP_ETHERNET:
-		break;
-	/*  Set RAWIP protocol  */
-	case RMNET_IOCTL_SET_LLP_IP:
-		break;
-	/*  Get link protocol  */
-	case RMNET_IOCTL_GET_LLP:
-		ioctl_data.u.operation_mode = RMNET_MODE_LLP_IP;
-		if (copy_to_user(ifr->ifr_ifru.ifru_data, &ioctl_data,
-			sizeof(struct rmnet_ioctl_data_s)))
-			rc = -EFAULT;
-		break;
-	/*  Set QoS header enabled  */
-	case RMNET_IOCTL_SET_QOS_ENABLE:
-		return -EINVAL;
-		break;
-	/*  Set QoS header disabled  */
-	case RMNET_IOCTL_SET_QOS_DISABLE:
-		break;
-	/*  Get QoS header state  */
-	case RMNET_IOCTL_GET_QOS:
-		ioctl_data.u.operation_mode = RMNET_MODE_NONE;
-		if (copy_to_user(ifr->ifr_ifru.ifru_data, &ioctl_data,
-			sizeof(struct rmnet_ioctl_data_s)))
-			rc = -EFAULT;
-		break;
-	/*  Get operation mode  */
-	case RMNET_IOCTL_GET_OPMODE:
-		ioctl_data.u.operation_mode = RMNET_MODE_LLP_IP;
-		if (copy_to_user(ifr->ifr_ifru.ifru_data, &ioctl_data,
-			sizeof(struct rmnet_ioctl_data_s)))
-			rc = -EFAULT;
-		break;
-	/*  Open transport port  */
-	case RMNET_IOCTL_OPEN:
-		break;
-	/*  Close transport port  */
-	case RMNET_IOCTL_CLOSE:
-		break;
-	/*  Flow enable  */
-	case RMNET_IOCTL_FLOW_ENABLE:
-		IPAWANDBG("Received flow enable\n");
-		if (copy_from_user(&ioctl_data, ifr->ifr_ifru.ifru_data,
-			sizeof(struct rmnet_ioctl_data_s))) {
-			rc = -EFAULT;
-			break;
-		}
-		ipa_flow_control(IPA_CLIENT_USB_PROD, true,
-			ioctl_data.u.tcm_handle);
-		break;
-	/*  Flow disable  */
-	case RMNET_IOCTL_FLOW_DISABLE:
-		IPAWANDBG("Received flow disable\n");
-		if (copy_from_user(&ioctl_data, ifr->ifr_ifru.ifru_data,
-			sizeof(struct rmnet_ioctl_data_s))) {
-			rc = -EFAULT;
-			break;
-		}
-		ipa_flow_control(IPA_CLIENT_USB_PROD, false,
-			ioctl_data.u.tcm_handle);
-		break;
-	/*  Set flow handle  */
-	case RMNET_IOCTL_FLOW_SET_HNDL:
-		break;
-
-	/*  Extended IOCTLs  */
-	case RMNET_IOCTL_EXTENDED:
-		IPAWANDBG("get ioctl: RMNET_IOCTL_EXTENDED\n");
-		if (copy_from_user(&extend_ioctl_data,
-			(u8 *)ifr->ifr_ifru.ifru_data,
-			sizeof(struct rmnet_ioctl_extended_s))) {
-			IPAWANERR("failed to copy extended ioctl data\n");
-			rc = -EFAULT;
-			break;
-		}
-		switch (extend_ioctl_data.extended_ioctl) {
-		/*  Get features  */
-		case RMNET_IOCTL_GET_SUPPORTED_FEATURES:
-			IPAWANDBG("get RMNET_IOCTL_GET_SUPPORTED_FEATURES\n");
-			extend_ioctl_data.u.data =
-				(RMNET_IOCTL_FEAT_NOTIFY_MUX_CHANNEL |
-				RMNET_IOCTL_FEAT_SET_EGRESS_DATA_FORMAT |
-				RMNET_IOCTL_FEAT_SET_INGRESS_DATA_FORMAT);
-			if (copy_to_user((u8 *)ifr->ifr_ifru.ifru_data,
-				&extend_ioctl_data,
-				sizeof(struct rmnet_ioctl_extended_s)))
-				rc = -EFAULT;
-			break;
-		/*  Set MRU  */
-		case RMNET_IOCTL_SET_MRU:
-			mru = extend_ioctl_data.u.data;
-			IPAWANDBG("get MRU size %d\n",
-				extend_ioctl_data.u.data);
-			break;
-		/*  Get MRU  */
-		case RMNET_IOCTL_GET_MRU:
-			extend_ioctl_data.u.data = mru;
-			if (copy_to_user((u8 *)ifr->ifr_ifru.ifru_data,
-				&extend_ioctl_data,
-				sizeof(struct rmnet_ioctl_extended_s)))
-				rc = -EFAULT;
-			break;
-		/*  Get endpoint ID  */
-		case RMNET_IOCTL_GET_EPID:
-			IPAWANDBG("get ioctl: RMNET_IOCTL_GET_EPID\n");
-			extend_ioctl_data.u.data = epid;
-			if (copy_to_user((u8 *)ifr->ifr_ifru.ifru_data,
-				&extend_ioctl_data,
-				sizeof(struct rmnet_ioctl_extended_s)))
-				rc = -EFAULT;
-			if (copy_from_user(&extend_ioctl_data,
-				(u8 *)ifr->ifr_ifru.ifru_data,
-				sizeof(struct rmnet_ioctl_extended_s))) {
-				IPAWANERR("copy extended ioctl data failed\n");
-				rc = -EFAULT;
-			break;
-			}
-			IPAWANDBG("RMNET_IOCTL_GET_EPID return %d\n",
-					extend_ioctl_data.u.data);
-			break;
-		/*  Endpoint pair  */
-		case RMNET_IOCTL_GET_EP_PAIR:
-			IPAWANDBG("get ioctl: RMNET_IOCTL_GET_EP_PAIR\n");
-			extend_ioctl_data.u.ipa_ep_pair.consumer_pipe_num =
-			ipa_get_ep_mapping(IPA_CLIENT_APPS_LAN_WAN_PROD);
-			extend_ioctl_data.u.ipa_ep_pair.producer_pipe_num =
-			ipa_get_ep_mapping(IPA_CLIENT_APPS_WAN_CONS);
-			if (copy_to_user((u8 *)ifr->ifr_ifru.ifru_data,
-				&extend_ioctl_data,
-				sizeof(struct rmnet_ioctl_extended_s)))
-				rc = -EFAULT;
-			if (copy_from_user(&extend_ioctl_data,
-				(u8 *)ifr->ifr_ifru.ifru_data,
-				sizeof(struct rmnet_ioctl_extended_s))) {
-				IPAWANERR("copy extended ioctl data failed\n");
-				rc = -EFAULT;
-			break;
-		}
-			IPAWANDBG("RMNET_IOCTL_GET_EP_PAIR c: %d p: %d\n",
-			extend_ioctl_data.u.ipa_ep_pair.consumer_pipe_num,
-			extend_ioctl_data.u.ipa_ep_pair.producer_pipe_num);
-			break;
-		/*  Get driver name  */
-		case RMNET_IOCTL_GET_DRIVER_NAME:
-			memcpy(&extend_ioctl_data.u.if_name,
-						ipa_netdevs[0]->name,
-							sizeof(IFNAMSIZ));
-			if (copy_to_user((u8 *)ifr->ifr_ifru.ifru_data,
-					&extend_ioctl_data,
-					sizeof(struct rmnet_ioctl_extended_s)))
-				rc = -EFAULT;
-			break;
-		/*  Add MUX ID  */
-		case RMNET_IOCTL_ADD_MUX_CHANNEL:
-			mux_index = find_mux_channel_index(
-				extend_ioctl_data.u.rmnet_mux_val.mux_id);
-			if (mux_index < MAX_NUM_OF_MUX_CHANNEL) {
-				IPAWANDBG("already setup mux(%d)\n",
-					extend_ioctl_data.u.
-					rmnet_mux_val.mux_id);
-				return rc;
-			}
-			mutex_lock(&add_mux_channel_lock);
-			if (rmnet_index >= MAX_NUM_OF_MUX_CHANNEL) {
-				IPAWANERR("Exceed mux_channel limit(%d)\n",
-				rmnet_index);
-				mutex_unlock(&add_mux_channel_lock);
-				return -EFAULT;
-			}
-			IPAWANDBG("ADD_MUX_CHANNEL(%d, name: %s)\n",
-			extend_ioctl_data.u.rmnet_mux_val.mux_id,
-			extend_ioctl_data.u.rmnet_mux_val.vchannel_name);
-			/* cache the mux name and id */
-			mux_channel[rmnet_index].mux_id =
-				extend_ioctl_data.u.rmnet_mux_val.mux_id;
-			memcpy(mux_channel[rmnet_index].vchannel_name,
-				extend_ioctl_data.u.rmnet_mux_val.vchannel_name,
-				sizeof(mux_channel[rmnet_index].vchannel_name));
-			mux_channel[rmnet_index].vchannel_name[
-				IFNAMSIZ - 1] = '\0';
-			IPAWANDBG("cashe device[%s:%d] in IPA_wan[%d]\n",
-				mux_channel[rmnet_index].vchannel_name,
-				mux_channel[rmnet_index].mux_id,
-				rmnet_index);
-			/* check if UL filter rules coming*/
-			if (num_q6_rule != 0) {
-				IPAWANERR("dev(%s) register to IPA\n",
-					extend_ioctl_data.u.rmnet_mux_val.
-					vchannel_name);
-				rc = wwan_register_to_ipa(rmnet_index);
-				if (rc < 0) {
-					IPAWANERR("device %s reg IPA failed\n",
-						extend_ioctl_data.u.
-						rmnet_mux_val.vchannel_name);
-					mutex_unlock(&add_mux_channel_lock);
-					return -ENODEV;
-				}
-				mux_channel[rmnet_index].mux_channel_set = true;
-				mux_channel[rmnet_index].ul_flt_reg = true;
-			} else {
-				IPAWANDBG("dev(%s) haven't registered to IPA\n",
-					extend_ioctl_data.u.
-					rmnet_mux_val.vchannel_name);
-				mux_channel[rmnet_index].mux_channel_set = true;
-				mux_channel[rmnet_index].ul_flt_reg = false;
-			}
-			rmnet_index++;
-			mutex_unlock(&add_mux_channel_lock);
-			break;
-		case RMNET_IOCTL_SET_EGRESS_DATA_FORMAT:
-			IPAWANDBG("get RMNET_IOCTL_SET_EGRESS_DATA_FORMAT\n");
-			if ((extend_ioctl_data.u.data) &
-					RMNET_IOCTL_EGRESS_FORMAT_CHECKSUM) {
-				apps_to_ipa_ep_cfg.ipa_ep_cfg.hdr.hdr_len = 8;
-				apps_to_ipa_ep_cfg.ipa_ep_cfg.cfg.
-					cs_offload_en = 1;
-				apps_to_ipa_ep_cfg.ipa_ep_cfg.cfg.
-					cs_metadata_hdr_offset = 1;
-			} else {
-				apps_to_ipa_ep_cfg.ipa_ep_cfg.hdr.hdr_len = 4;
-			}
-			if ((extend_ioctl_data.u.data) &
-					RMNET_IOCTL_EGRESS_FORMAT_AGGREGATION)
-				apps_to_ipa_ep_cfg.ipa_ep_cfg.aggr.aggr_en =
-					IPA_ENABLE_AGGR;
-			else
-				apps_to_ipa_ep_cfg.ipa_ep_cfg.aggr.aggr_en =
-					IPA_BYPASS_AGGR;
-			apps_to_ipa_ep_cfg.ipa_ep_cfg.hdr.
-				hdr_ofst_metadata_valid = 1;
-			/* modem want offset at 0! */
-			apps_to_ipa_ep_cfg.ipa_ep_cfg.hdr.hdr_ofst_metadata = 0;
-			apps_to_ipa_ep_cfg.ipa_ep_cfg.mode.dst =
-					IPA_CLIENT_APPS_LAN_WAN_PROD;
-			apps_to_ipa_ep_cfg.ipa_ep_cfg.mode.mode = IPA_BASIC;
-
-			apps_to_ipa_ep_cfg.client =
-				IPA_CLIENT_APPS_LAN_WAN_PROD;
-			apps_to_ipa_ep_cfg.notify =
-				apps_ipa_tx_complete_notify;
-			apps_to_ipa_ep_cfg.desc_fifo_sz =
-			IPA_SYS_TX_DATA_DESC_FIFO_SZ;
-			apps_to_ipa_ep_cfg.priv = dev;
-
-			rc = ipa_setup_sys_pipe(&apps_to_ipa_ep_cfg,
-				&apps_to_ipa_hdl);
-			if (rc)
-				IPAWANERR("failed to config egress endpoint\n");
-
-			if (num_q6_rule != 0) {
-				/* already got Q6 UL filter rules*/
-				if (ipa_qmi_ctx &&
-					ipa_qmi_ctx->modem_cfg_emb_pipe_flt
-					== false)
-					rc = wwan_add_ul_flt_rule_to_ipa();
-				else
-					rc = 0;
-				egress_set = true;
-				if (rc)
-					IPAWANERR("install UL rules failed\n");
-				else
-					a7_ul_flt_set = true;
-			} else {
-				/* wait Q6 UL filter rules*/
-				egress_set = true;
-				IPAWANDBG("no UL-rules, egress_set(%d)\n",
-					egress_set);
-			}
-			break;
-		case RMNET_IOCTL_SET_INGRESS_DATA_FORMAT:/*  Set IDF  */
-			IPAWANDBG("get RMNET_IOCTL_SET_INGRESS_DATA_FORMAT\n");
-			if ((extend_ioctl_data.u.data) &
-					RMNET_IOCTL_INGRESS_FORMAT_CHECKSUM)
-				ipa_to_apps_ep_cfg.ipa_ep_cfg.cfg.
-					cs_offload_en = 2;
-
-			if ((extend_ioctl_data.u.data) &
-					RMNET_IOCTL_INGRESS_FORMAT_AGG_DATA) {
-				IPAWANERR("get AGG size %d count %d\n",
-					extend_ioctl_data.u.
-					ingress_format.agg_size,
-					extend_ioctl_data.u.
-					ingress_format.agg_count);
-				if (!ipa_disable_apps_wan_cons_deaggr(
-					extend_ioctl_data.u.
-					ingress_format.agg_size,
-					extend_ioctl_data.
-					u.ingress_format.agg_count)) {
-					ipa_to_apps_ep_cfg.ipa_ep_cfg.aggr.
-					aggr_byte_limit = extend_ioctl_data.
-					u.ingress_format.agg_size;
-					ipa_to_apps_ep_cfg.ipa_ep_cfg.aggr.
-					aggr_pkt_limit = extend_ioctl_data.
-					u.ingress_format.agg_count;
-				}
-			}
-
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.hdr.hdr_len = 4;
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.hdr.
-				hdr_ofst_metadata_valid = 1;
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.
-				hdr.hdr_ofst_metadata = 1;
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.hdr.
-				hdr_ofst_pkt_size_valid = 1;
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.hdr.
-				hdr_ofst_pkt_size = 2;
-
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.hdr_ext.
-				hdr_total_len_or_pad_valid = true;
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.hdr_ext.
-				hdr_total_len_or_pad = 0;
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.hdr_ext.
-				hdr_payload_len_inc_padding = true;
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.hdr_ext.
-				hdr_total_len_or_pad_offset = 0;
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.hdr_ext.
-				hdr_little_endian = 0;
-			ipa_to_apps_ep_cfg.ipa_ep_cfg.metadata_mask.
-				metadata_mask = 0xFF000000;
-
-			ipa_to_apps_ep_cfg.client = IPA_CLIENT_APPS_WAN_CONS;
-			ipa_to_apps_ep_cfg.notify =
-				apps_ipa_packet_receive_notify;
-			ipa_to_apps_ep_cfg.desc_fifo_sz = IPA_SYS_DESC_FIFO_SZ;
-			ipa_to_apps_ep_cfg.priv = dev;
-
-			mutex_lock(&ipa_to_apps_pipe_handle_guard);
-			if (atomic_read(&is_ssr)) {
-				IPAWANDBG("In SSR sequence/recovery\n");
-				mutex_unlock(&ipa_to_apps_pipe_handle_guard);
-				rc = -EFAULT;
-				break;
-			}
-			rc = ipa_setup_sys_pipe(
-				&ipa_to_apps_ep_cfg, &ipa_to_apps_hdl);
-			mutex_unlock(&ipa_to_apps_pipe_handle_guard);
-			if (rc)
-				IPAWANERR("failed to configure ingress\n");
-			break;
-		case RMNET_IOCTL_SET_XLAT_DEV_INFO:
-			wan_msg = kzalloc(sizeof(struct ipa_wan_msg),
-						GFP_KERNEL);
-			if (!wan_msg) {
-				IPAWANERR("Failed to allocate memory.\n");
-				return -ENOMEM;
-			}
-			len = sizeof(wan_msg->upstream_ifname) >
-			sizeof(extend_ioctl_data.u.if_name) ?
-				sizeof(extend_ioctl_data.u.if_name) :
-				sizeof(wan_msg->upstream_ifname);
-			strlcpy(wan_msg->upstream_ifname,
-				extend_ioctl_data.u.if_name, len);
-			memset(&msg_meta, 0, sizeof(struct ipa_msg_meta));
-			msg_meta.msg_type = WAN_XLAT_CONNECT;
-			msg_meta.msg_len = sizeof(struct ipa_wan_msg);
-			rc = ipa_send_msg(&msg_meta, wan_msg,
-						ipa_wwan_msg_free_cb);
-			if (rc) {
-				IPAWANERR("Failed to send XLAT_CONNECT msg\n");
-				kfree(wan_msg);
-			}
-			break;
-		/*  Get agg count  */
-		case RMNET_IOCTL_GET_AGGREGATION_COUNT:
-			break;
-		/*  Set agg count  */
-		case RMNET_IOCTL_SET_AGGREGATION_COUNT:
-			break;
-		/*  Get agg size  */
-		case RMNET_IOCTL_GET_AGGREGATION_SIZE:
-			break;
-		/*  Set agg size  */
-		case RMNET_IOCTL_SET_AGGREGATION_SIZE:
-			break;
-		/*  Do flow control  */
-		case RMNET_IOCTL_FLOW_CONTROL:
-			break;
-		/*  For legacy use  */
-		case RMNET_IOCTL_GET_DFLT_CONTROL_CHANNEL:
-			break;
-		/*  Get HW/SW map  */
-		case RMNET_IOCTL_GET_HWSW_MAP:
-			break;
-		/*  Set RX Headroom  */
-		case RMNET_IOCTL_SET_RX_HEADROOM:
-			break;
-		default:
-			IPAWANERR("[%s] unsupported extended cmd[%d]",
-				dev->name,
-				extend_ioctl_data.extended_ioctl);
-			rc = -EINVAL;
-		}
-		break;
-	default:
-			IPAWANERR("[%s] unsupported cmd[%d]",
-				dev->name, cmd);
-			rc = -EINVAL;
-	}
-	return rc;
-}
-
-static const struct net_device_ops ipa_wwan_ops_ip = {
-	.ndo_open = ipa_wwan_open,
-	.ndo_stop = ipa_wwan_stop,
-	.ndo_start_xmit = ipa_wwan_xmit,
-	.ndo_tx_timeout = ipa_wwan_tx_timeout,
-	.ndo_do_ioctl = ipa_wwan_ioctl,
-	.ndo_change_mtu = ipa_wwan_change_mtu,
-	.ndo_set_mac_address = 0,
-	.ndo_validate_addr = 0,
-};
-
-/**
- * wwan_setup() - Setups the wwan network driver.
- *
- * @dev: network device
- *
- * Return codes:
- * None
- */
-
-static void ipa_wwan_setup(struct net_device *dev)
-{
-	dev->netdev_ops = &ipa_wwan_ops_ip;
-	ether_setup(dev);
-	/* set this after calling ether_setup */
-	dev->header_ops = 0;  /* No header */
-	dev->type = ARPHRD_RAWIP;
-	dev->hard_header_len = 0;
-	dev->mtu = WWAN_DATA_LEN;
-	dev->addr_len = 0;
-	dev->flags &= ~(IFF_BROADCAST | IFF_MULTICAST);
-	dev->needed_headroom = HEADROOM_FOR_QMAP;
-	dev->needed_tailroom = TAILROOM;
-	dev->watchdog_timeo = 1000;
-}
-
-/* IPA_RM related functions start*/
-static void q6_prod_rm_request_resource(struct work_struct *work);
-static DECLARE_DELAYED_WORK(q6_con_rm_request, q6_prod_rm_request_resource);
-static void q6_prod_rm_release_resource(struct work_struct *work);
-static DECLARE_DELAYED_WORK(q6_con_rm_release, q6_prod_rm_release_resource);
-
-static void q6_prod_rm_request_resource(struct work_struct *work)
-{
-	int ret = 0;
-
-	ret = ipa_rm_request_resource(IPA_RM_RESOURCE_Q6_PROD);
-	if (ret < 0 && ret != -EINPROGRESS) {
-		IPAWANERR("%s: ipa_rm_request_resource failed %d\n", __func__,
-		       ret);
-		return;
-	}
-}
-
-static int q6_rm_request_resource(void)
-{
-	queue_delayed_work(ipa_rm_q6_workqueue,
-	   &q6_con_rm_request, 0);
-	return 0;
-}
-
-static void q6_prod_rm_release_resource(struct work_struct *work)
-{
-	int ret = 0;
-	ret = ipa_rm_release_resource(IPA_RM_RESOURCE_Q6_PROD);
-	if (ret < 0 && ret != -EINPROGRESS) {
-		IPAWANERR("%s: ipa_rm_release_resource failed %d\n", __func__,
-		      ret);
-		return;
-	}
-}
-
-
-static int q6_rm_release_resource(void)
-{
-	queue_delayed_work(ipa_rm_q6_workqueue,
-	   &q6_con_rm_release, 0);
-	return 0;
-}
-
-
-static void q6_rm_notify_cb(void *user_data,
-		enum ipa_rm_event event,
-		unsigned long data)
-{
-	switch (event) {
-	case IPA_RM_RESOURCE_GRANTED:
-		IPAWANDBG("%s: Q6_PROD GRANTED CB\n", __func__);
-		break;
-	case IPA_RM_RESOURCE_RELEASED:
-		IPAWANDBG("%s: Q6_PROD RELEASED CB\n", __func__);
-		break;
-	default:
-		return;
-	}
-}
-static int q6_initialize_rm(void)
-{
-	struct ipa_rm_create_params create_params;
-	struct ipa_rm_perf_profile profile;
-	int result;
-
-	/* Initialize IPA_RM workqueue */
-	ipa_rm_q6_workqueue = create_singlethread_workqueue("clnt_req");
-	if (!ipa_rm_q6_workqueue)
-		return -ENOMEM;
-
-	memset(&create_params, 0, sizeof(create_params));
-	create_params.name = IPA_RM_RESOURCE_Q6_PROD;
-	create_params.reg_params.notify_cb = &q6_rm_notify_cb;
-	result = ipa_rm_create_resource(&create_params);
-	if (result)
-		goto create_rsrc_err1;
-	memset(&create_params, 0, sizeof(create_params));
-	create_params.name = IPA_RM_RESOURCE_Q6_CONS;
-	create_params.release_resource = &q6_rm_release_resource;
-	create_params.request_resource = &q6_rm_request_resource;
-	result = ipa_rm_create_resource(&create_params);
-	if (result)
-		goto create_rsrc_err2;
-	/* add dependency*/
-	result = ipa_rm_add_dependency(IPA_RM_RESOURCE_Q6_PROD,
-			IPA_RM_RESOURCE_APPS_CONS);
-	if (result)
-		goto add_dpnd_err;
-	/* setup Performance profile */
-	memset(&profile, 0, sizeof(profile));
-	profile.max_supported_bandwidth_mbps = 100;
-	result = ipa_rm_set_perf_profile(IPA_RM_RESOURCE_Q6_PROD,
-			&profile);
-	if (result)
-		goto set_perf_err;
-	result = ipa_rm_set_perf_profile(IPA_RM_RESOURCE_Q6_CONS,
-			&profile);
-	if (result)
-		goto set_perf_err;
-	return result;
-
-set_perf_err:
-	ipa_rm_delete_dependency(IPA_RM_RESOURCE_Q6_PROD,
-			IPA_RM_RESOURCE_APPS_CONS);
-add_dpnd_err:
-	result = ipa_rm_delete_resource(IPA_RM_RESOURCE_Q6_CONS);
-	if (result < 0)
-		IPAWANERR("Error deleting resource %d, ret=%d\n",
-			IPA_RM_RESOURCE_Q6_CONS, result);
-create_rsrc_err2:
-	result = ipa_rm_delete_resource(IPA_RM_RESOURCE_Q6_PROD);
-	if (result < 0)
-		IPAWANERR("Error deleting resource %d, ret=%d\n",
-			IPA_RM_RESOURCE_Q6_PROD, result);
-create_rsrc_err1:
-	destroy_workqueue(ipa_rm_q6_workqueue);
-	return result;
-}
-
-void q6_deinitialize_rm(void)
-{
-	int ret;
-
-	ret = ipa_rm_delete_dependency(IPA_RM_RESOURCE_Q6_PROD,
-			IPA_RM_RESOURCE_APPS_CONS);
-	if (ret < 0)
-		IPAWANERR("Error deleting dependency %d->%d, ret=%d\n",
-			IPA_RM_RESOURCE_Q6_PROD, IPA_RM_RESOURCE_APPS_CONS,
-			ret);
-	ret = ipa_rm_delete_resource(IPA_RM_RESOURCE_Q6_CONS);
-	if (ret < 0)
-		IPAWANERR("Error deleting resource %d, ret=%d\n",
-			IPA_RM_RESOURCE_Q6_CONS, ret);
-	ret = ipa_rm_delete_resource(IPA_RM_RESOURCE_Q6_PROD);
-	if (ret < 0)
-		IPAWANERR("Error deleting resource %d, ret=%d\n",
-			IPA_RM_RESOURCE_Q6_PROD, ret);
-	destroy_workqueue(ipa_rm_q6_workqueue);
-}
-
-static void wake_tx_queue(struct work_struct *work)
-{
-	if (ipa_netdevs[0]) {
-		__netif_tx_lock_bh(netdev_get_tx_queue(ipa_netdevs[0], 0));
-		netif_wake_queue(ipa_netdevs[0]);
-		__netif_tx_unlock_bh(netdev_get_tx_queue(ipa_netdevs[0], 0));
-	}
-}
-
-/**
- * ipa_rm_resource_granted() - Called upon
- * IPA_RM_RESOURCE_GRANTED event. Wakes up queue is was stopped.
- *
- * @work: work object supplied ny workqueue
- *
- * Return codes:
- * None
- */
-static void ipa_rm_resource_granted(void *dev)
-{
-	IPAWANDBG("Resource Granted - starting queue\n");
-	schedule_work(&ipa_tx_wakequeue_work);
-}
-
-/**
- * ipa_rm_notify() - Callback function for RM events. Handles
- * IPA_RM_RESOURCE_GRANTED and IPA_RM_RESOURCE_RELEASED events.
- * IPA_RM_RESOURCE_GRANTED is handled in the context of shared
- * workqueue.
- *
- * @dev: network device
- * @event: IPA RM event
- * @data: Additional data provided by IPA RM
- *
- * Return codes:
- * None
- */
-static void ipa_rm_notify(void *dev, enum ipa_rm_event event,
-			  unsigned long data)
-{
-	struct wwan_private *wwan_ptr = netdev_priv(dev);
-
-	pr_debug("%s: event %d\n", __func__, event);
-	switch (event) {
-	case IPA_RM_RESOURCE_GRANTED:
-		if (wwan_ptr->device_status == WWAN_DEVICE_INACTIVE) {
-			complete_all(&wwan_ptr->resource_granted_completion);
-			break;
-		}
-		ipa_rm_resource_granted(dev);
-		break;
-	case IPA_RM_RESOURCE_RELEASED:
-		break;
-	default:
-		pr_err("%s: unknown event %d\n", __func__, event);
-		break;
-	}
-}
-
-/* IPA_RM related functions end*/
-
-static int ssr_notifier_cb(struct notifier_block *this,
-			   unsigned long code,
-			   void *data);
-
-static struct notifier_block ssr_notifier = {
-	.notifier_call = ssr_notifier_cb,
-};
-
-static struct ipa_rmnet_plat_drv_res ipa_rmnet_res = {0, };
-
-static int get_ipa_rmnet_dts_configuration(struct platform_device *pdev,
-		struct ipa_rmnet_plat_drv_res *ipa_rmnet_drv_res)
-{
-	ipa_rmnet_drv_res->ipa_rmnet_ssr =
-			of_property_read_bool(pdev->dev.of_node,
-			"qcom,rmnet-ipa-ssr");
-	pr_info("IPA SSR support = %s\n",
-		ipa_rmnet_drv_res->ipa_rmnet_ssr ? "True" : "False");
-	ipa_rmnet_drv_res->ipa_loaduC =
-			of_property_read_bool(pdev->dev.of_node,
-			"qcom,ipa-loaduC");
-	pr_info("IPA ipa-loaduC = %s\n",
-		ipa_rmnet_drv_res->ipa_loaduC ? "True" : "False");
-	return 0;
-}
-
-struct ipa_rmnet_context ipa_rmnet_ctx;
-
-static int rmnet_ipa_panic_notifier(struct notifier_block *this,
-	unsigned long event, void *ptr)
-{
-	struct wwan_private *wwan_ptr = NULL;
-	char dbg_buff[128];
-
-	if (ipa_netdevs[0])
-		wwan_ptr = netdev_priv(ipa_netdevs[0]);
-	if (wwan_ptr) {
-		if (wwan_ptr->device_status)
-			scnprintf(dbg_buff, sizeof(dbg_buff),
-				  "RMNET-IPA is LOADED, Status: ACTIVE,");
-		else
-			scnprintf(dbg_buff, sizeof(dbg_buff),
-				  "RMNET-IPA is LOADED, Status: INACTIVE,");
-		pr_err("%s outstanding packts: %d\n",
-		       dbg_buff, atomic_read(&wwan_ptr->outstanding_pkts));
-	}
-
-	return NOTIFY_DONE;
-}
-
-static struct notifier_block rmnet_ipa_panic_blk = {
-	.notifier_call = rmnet_ipa_panic_notifier,
-};
-
-void rmnet_ipa_register_panic_func(void)
-{
-	atomic_notifier_chain_register(&panic_notifier_list,
-		&rmnet_ipa_panic_blk);
-}
-
-/**
- * ipa_wwan_probe() - Initialized the module and registers as a
- * network interface to the network stack
- *
- * Return codes:
- * 0: success
- * -ENOMEM: No memory available
- * -EFAULT: Internal error
- * -ENODEV: IPA driver not loaded
- */
-static int ipa_wwan_probe(struct platform_device *pdev)
-{
-	int ret, i;
-	struct net_device *dev;
-	struct wwan_private *wwan_ptr;
-	struct ipa_rm_create_params ipa_rm_params;	/* IPA_RM */
-	struct ipa_rm_perf_profile profile;			/* IPA_RM */
-
-	pr_info("rmnet_ipa started initialization\n");
-
-	if (!ipa_is_ready()) {
-		IPAWANERR("IPA driver not loaded\n");
-		return -ENODEV;
-	}
-
-	ret = get_ipa_rmnet_dts_configuration(pdev, &ipa_rmnet_res);
-	ipa_rmnet_ctx.ipa_rmnet_ssr = ipa_rmnet_res.ipa_rmnet_ssr;
-
-	ret = ipa_init_q6_smem();
-	if (ret) {
-		IPAWANERR("ipa_init_q6_smem failed!\n");
-		return ret;
-	}
-
-	/* initialize tx/rx enpoint setup */
-	memset(&apps_to_ipa_ep_cfg, 0, sizeof(struct ipa_sys_connect_params));
-	memset(&ipa_to_apps_ep_cfg, 0, sizeof(struct ipa_sys_connect_params));
-
-	/* initialize ex property setup */
-	num_q6_rule = 0;
-	old_num_q6_rule = 0;
-	rmnet_index = 0;
-	egress_set = false;
-	a7_ul_flt_set = false;
-	for (i = 0; i < MAX_NUM_OF_MUX_CHANNEL; i++)
-		memset(&mux_channel[i], 0, sizeof(struct rmnet_mux_val));
-
-	/* start A7 QMI service/client */
-	if (ipa_rmnet_res.ipa_loaduC)
-		/* Android platform loads uC */
-		ipa_qmi_service_init(QMI_IPA_PLATFORM_TYPE_MSM_ANDROID_V01);
-	else
-		/* LE platform not loads uC */
-		ipa_qmi_service_init(QMI_IPA_PLATFORM_TYPE_LE_V01);
-
-	/* construct default WAN RT tbl for IPACM */
-	ret = ipa_setup_a7_qmap_hdr();
-	if (ret)
-		goto setup_a7_qmap_hdr_err;
-	ret = ipa_setup_dflt_wan_rt_tables();
-	if (ret)
-		goto setup_dflt_wan_rt_tables_err;
-
-	if (!atomic_read(&is_ssr)) {
-		/* Start transport-driver fd ioctl for ipacm for first init */
-		ret = wan_ioctl_init();
-		if (ret)
-			goto wan_ioctl_init_err;
-	} else {
-		/* Enable sending QMI messages after SSR */
-		wan_ioctl_enable_qmi_messages();
-	}
-
-	/* initialize wan-driver netdev */
-	dev = alloc_netdev(sizeof(struct wwan_private),
-			   IPA_WWAN_DEV_NAME, ipa_wwan_setup);
-	if (!dev) {
-		IPAWANERR("no memory for netdev\n");
-		ret = -ENOMEM;
-		goto alloc_netdev_err;
-	}
-	ipa_netdevs[0] = dev;
-	wwan_ptr = netdev_priv(dev);
-	memset(wwan_ptr, 0, sizeof(*wwan_ptr));
-	IPAWANDBG("wwan_ptr (private) = %p", wwan_ptr);
-	wwan_ptr->net = dev;
-	wwan_ptr->outstanding_high = DEFAULT_OUTSTANDING_HIGH;
-	wwan_ptr->outstanding_low = DEFAULT_OUTSTANDING_LOW;
-	atomic_set(&wwan_ptr->outstanding_pkts, 0);
-	spin_lock_init(&wwan_ptr->lock);
-	init_completion(&wwan_ptr->resource_granted_completion);
-
-	if (!atomic_read(&is_ssr)) {
-		/* IPA_RM configuration starts */
-		ret = q6_initialize_rm();
-		if (ret) {
-			IPAWANERR("%s: q6_initialize_rm failed, ret: %d\n",
-				__func__, ret);
-			goto q6_init_err;
-		}
-	}
-
-	memset(&ipa_rm_params, 0, sizeof(struct ipa_rm_create_params));
-	ipa_rm_params.name = IPA_RM_RESOURCE_WWAN_0_PROD;
-	ipa_rm_params.reg_params.user_data = dev;
-	ipa_rm_params.reg_params.notify_cb = ipa_rm_notify;
-	ret = ipa_rm_create_resource(&ipa_rm_params);
-	if (ret) {
-		pr_err("%s: unable to create resourse %d in IPA RM\n",
-		       __func__, IPA_RM_RESOURCE_WWAN_0_PROD);
-		goto create_rsrc_err;
-	}
-	ret = ipa_rm_inactivity_timer_init(IPA_RM_RESOURCE_WWAN_0_PROD,
-					   IPA_RM_INACTIVITY_TIMER);
-	if (ret) {
-		pr_err("%s: ipa rm timer init failed %d on resourse %d\n",
-		       __func__, ret, IPA_RM_RESOURCE_WWAN_0_PROD);
-		goto timer_init_err;
-	}
-	/* add dependency */
-	ret = ipa_rm_add_dependency(IPA_RM_RESOURCE_WWAN_0_PROD,
-			IPA_RM_RESOURCE_Q6_CONS);
-	if (ret)
-		goto add_dpnd_err;
-	/* setup Performance profile */
-	memset(&profile, 0, sizeof(profile));
-	profile.max_supported_bandwidth_mbps = IPA_APPS_MAX_BW_IN_MBPS;
-	ret = ipa_rm_set_perf_profile(IPA_RM_RESOURCE_WWAN_0_PROD,
-			&profile);
-	if (ret)
-		goto set_perf_err;
-	/* IPA_RM configuration ends */
-
-	ret = register_netdev(dev);
-	if (ret) {
-		IPAWANERR("unable to register ipa_netdev %d rc=%d\n",
-			0, ret);
-		goto set_perf_err;
-	}
-
-	IPAWANDBG("IPA-WWAN devices (%s) initilization ok :>>>>\n",
-			ipa_netdevs[0]->name);
-	if (ret) {
-		IPAWANERR("default configuration failed rc=%d\n",
-				ret);
-		goto config_err;
-	}
-	atomic_set(&is_initialized, 1);
-	if (!atomic_read(&is_ssr)) {
-		/* offline charging mode */
-		ipa_proxy_clk_unvote();
-	}
-	atomic_set(&is_ssr, 0);
-
-	rmnet_ipa_register_panic_func();
-	pr_info("rmnet_ipa completed initialization\n");
-	return 0;
-config_err:
-	unregister_netdev(ipa_netdevs[0]);
-set_perf_err:
-	ret = ipa_rm_delete_dependency(IPA_RM_RESOURCE_WWAN_0_PROD,
-		IPA_RM_RESOURCE_Q6_CONS);
-	if (ret)
-		IPAWANERR("Error deleting dependency %d->%d, ret=%d\n",
-			IPA_RM_RESOURCE_WWAN_0_PROD, IPA_RM_RESOURCE_Q6_CONS,
-			ret);
-add_dpnd_err:
-	ret = ipa_rm_inactivity_timer_destroy(
-		IPA_RM_RESOURCE_WWAN_0_PROD); /* IPA_RM */
-	if (ret)
-		IPAWANERR("Error ipa_rm_inactivity_timer_destroy %d, ret=%d\n",
-		IPA_RM_RESOURCE_WWAN_0_PROD, ret);
-timer_init_err:
-	ret = ipa_rm_delete_resource(IPA_RM_RESOURCE_WWAN_0_PROD);
-	if (ret)
-		IPAWANERR("Error deleting resource %d, ret=%d\n",
-		IPA_RM_RESOURCE_WWAN_0_PROD, ret);
-create_rsrc_err:
-	q6_deinitialize_rm();
-q6_init_err:
-	free_netdev(ipa_netdevs[0]);
-	ipa_netdevs[0] = NULL;
-alloc_netdev_err:
-	wan_ioctl_deinit();
-wan_ioctl_init_err:
-	ipa_del_dflt_wan_rt_tables();
-setup_dflt_wan_rt_tables_err:
-	ipa_del_a7_qmap_hdr();
-setup_a7_qmap_hdr_err:
-	ipa_qmi_service_exit();
-	atomic_set(&is_ssr, 0);
-	return ret;
-}
-
-static int ipa_wwan_remove(struct platform_device *pdev)
-{
-	int ret;
-
-	pr_info("rmnet_ipa started deinitialization\n");
-	mutex_lock(&ipa_to_apps_pipe_handle_guard);
-	ret = ipa_teardown_sys_pipe(ipa_to_apps_hdl);
-	if (ret < 0)
-		IPAWANERR("Failed to teardown IPA->APPS pipe\n");
-	else
-		ipa_to_apps_hdl = -1;
-	mutex_unlock(&ipa_to_apps_pipe_handle_guard);
-	unregister_netdev(ipa_netdevs[0]);
-	ret = ipa_rm_delete_dependency(IPA_RM_RESOURCE_WWAN_0_PROD,
-		IPA_RM_RESOURCE_Q6_CONS);
-	if (ret < 0)
-		IPAWANERR("Error deleting dependency %d->%d, ret=%d\n",
-			IPA_RM_RESOURCE_WWAN_0_PROD, IPA_RM_RESOURCE_Q6_CONS,
-			ret);
-	ret = ipa_rm_inactivity_timer_destroy(IPA_RM_RESOURCE_WWAN_0_PROD);
-	if (ret < 0)
-		IPAWANERR(
-		"Error ipa_rm_inactivity_timer_destroy resource %d, ret=%d\n",
-		IPA_RM_RESOURCE_WWAN_0_PROD, ret);
-	ret = ipa_rm_delete_resource(IPA_RM_RESOURCE_WWAN_0_PROD);
-	if (ret < 0)
-		IPAWANERR("Error deleting resource %d, ret=%d\n",
-		IPA_RM_RESOURCE_WWAN_0_PROD, ret);
-	cancel_work_sync(&ipa_tx_wakequeue_work);
-	cancel_delayed_work(&ipa_tether_stats_poll_wakequeue_work);
-	free_netdev(ipa_netdevs[0]);
-	ipa_netdevs[0] = NULL;
-	/* No need to remove wwan_ioctl during SSR */
-	if (!atomic_read(&is_ssr))
-		wan_ioctl_deinit();
-	ipa_del_dflt_wan_rt_tables();
-	ipa_del_a7_qmap_hdr();
-	ipa_del_mux_qmap_hdrs();
-	if (ipa_qmi_ctx && ipa_qmi_ctx->modem_cfg_emb_pipe_flt == false)
-		wwan_del_ul_flt_rule_to_ipa();
-	ipa_cleanup_deregister_intf();
-	atomic_set(&is_initialized, 0);
-	pr_info("rmnet_ipa completed deinitialization\n");
-	return 0;
-}
-
-/**
-* rmnet_ipa_ap_suspend() - suspend callback for runtime_pm
-* @dev: pointer to device
-*
-* This callback will be invoked by the runtime_pm framework when an AP suspend
-* operation is invoked, usually by pressing a suspend button.
-*
-* Returns -EAGAIN to runtime_pm framework in case there are pending packets
-* in the Tx queue. This will postpone the suspend operation until all the
-* pending packets will be transmitted.
-*
-* In case there are no packets to send, releases the WWAN0_PROD entity.
-* As an outcome, the number of IPA active clients should be decremented
-* until IPA clocks can be gated.
-*/
-static int rmnet_ipa_ap_suspend(struct device *dev)
-{
-	struct net_device *netdev = ipa_netdevs[0];
-	struct wwan_private *wwan_ptr = netdev_priv(netdev);
-
-	IPAWANDBG("Enter...\n");
-	/* Do not allow A7 to suspend in case there are oustanding packets */
-	if (atomic_read(&wwan_ptr->outstanding_pkts) != 0) {
-		IPAWANDBG("Outstanding packets, postponing AP suspend.\n");
-		return -EAGAIN;
-	}
-
-	/* Make sure that there is no Tx operation ongoing */
-	netif_tx_lock_bh(netdev);
-	ipa_rm_release_resource(IPA_RM_RESOURCE_WWAN_0_PROD);
-	netif_tx_unlock_bh(netdev);
-	IPAWANDBG("Exit\n");
-
-	return 0;
-}
-
-/**
-* rmnet_ipa_ap_resume() - resume callback for runtime_pm
-* @dev: pointer to device
-*
-* This callback will be invoked by the runtime_pm framework when an AP resume
-* operation is invoked.
-*
-* Enables the network interface queue and returns success to the
-* runtime_pm framwork.
-*/
-static int rmnet_ipa_ap_resume(struct device *dev)
-{
-	struct net_device *netdev = ipa_netdevs[0];
-
-	IPAWANDBG("Enter...\n");
-	netif_wake_queue(netdev);
-	IPAWANDBG("Exit\n");
-
-	return 0;
-}
-
-static void ipa_stop_polling_stats(void)
-{
-	cancel_delayed_work(&ipa_tether_stats_poll_wakequeue_work);
-	ipa_rmnet_ctx.polling_interval = 0;
-}
-
-static const struct of_device_id rmnet_ipa_dt_match[] = {
-	{.compatible = "qcom,rmnet-ipa"},
-	{},
-};
-MODULE_DEVICE_TABLE(of, rmnet_ipa_dt_match);
-
-static const struct dev_pm_ops rmnet_ipa_pm_ops = {
-	.suspend_noirq = rmnet_ipa_ap_suspend,
-	.resume_noirq = rmnet_ipa_ap_resume,
-};
-
-static struct platform_driver rmnet_ipa_driver = {
-	.driver = {
-		.name = "rmnet_ipa",
-		.owner = THIS_MODULE,
-		.pm = &rmnet_ipa_pm_ops,
-		.of_match_table = rmnet_ipa_dt_match,
-	},
-	.probe = ipa_wwan_probe,
-	.remove = ipa_wwan_remove,
-};
-
-static int ssr_notifier_cb(struct notifier_block *this,
-			   unsigned long code,
-			   void *data)
-{
-	if (ipa_rmnet_ctx.ipa_rmnet_ssr) {
-		if (SUBSYS_BEFORE_SHUTDOWN == code) {
-			pr_info("IPA received MPSS BEFORE_SHUTDOWN\n");
-			atomic_set(&is_ssr, 1);
-			ipa_q6_pre_shutdown_cleanup();
-			if (ipa_netdevs[0])
-				netif_stop_queue(ipa_netdevs[0]);
-			ipa_qmi_stop_workqueues();
-			wan_ioctl_stop_qmi_messages();
-			ipa_stop_polling_stats();
-			if (atomic_read(&is_initialized))
-				platform_driver_unregister(&rmnet_ipa_driver);
-			pr_info("IPA BEFORE_SHUTDOWN handling is complete\n");
-			return NOTIFY_DONE;
-		}
-		if (SUBSYS_AFTER_SHUTDOWN == code) {
-			pr_info("IPA received MPSS AFTER_SHUTDOWN\n");
-			if (atomic_read(&is_ssr))
-				ipa_q6_post_shutdown_cleanup();
-			pr_info("IPA AFTER_SHUTDOWN handling is complete\n");
-			return NOTIFY_DONE;
-		}
-		if (SUBSYS_AFTER_POWERUP == code) {
-			pr_info("IPA received MPSS AFTER_POWERUP\n");
-			if (!atomic_read(&is_initialized)
-				&& atomic_read(&is_ssr))
-				platform_driver_register(&rmnet_ipa_driver);
-			pr_info("IPA AFTER_POWERUP handling is complete\n");
-			return NOTIFY_DONE;
-		}
-		if (SUBSYS_BEFORE_POWERUP == code) {
-			pr_info("IPA received MPSS BEFORE_POWERUP\n");
-			if (atomic_read(&is_ssr))
-				/* clean up cached QMI msg/handlers */
-				ipa_qmi_service_exit();
-			ipa_proxy_clk_vote();
-			pr_info("IPA BEFORE_POWERUP handling is complete\n");
-			return NOTIFY_DONE;
-		}
-	}
-	return NOTIFY_DONE;
-}
-
-/**
- * rmnet_ipa_free_msg() - Free the msg sent to user space via ipa_send_msg
- * @buff: pointer to buffer containing the message
- * @len: message len
- * @type: message type
- *
- * This function is invoked when ipa_send_msg is complete (Provided as a
- * free function pointer along with the message).
- */
-static void rmnet_ipa_free_msg(void *buff, u32 len, u32 type)
-{
-	if (!buff) {
-		IPAWANERR("Null buffer\n");
-		return;
-	}
-
-	if (type != IPA_TETHERING_STATS_UPDATE_STATS &&
-		type != IPA_TETHERING_STATS_UPDATE_NETWORK_STATS) {
-			IPAWANERR("Wrong type given. buff %p type %d\n",
-				  buff, type);
-	}
-	kfree(buff);
-}
-
-/**
- * rmnet_ipa_get_stats_and_update(bool reset) - Gets pipe stats from Modem
- *
- * This function queries the IPA Modem driver for the pipe stats
- * via QMI, and updates the user space IPA entity.
- */
-static void rmnet_ipa_get_stats_and_update(bool reset)
-{
-	struct ipa_get_data_stats_req_msg_v01 req;
-	struct ipa_get_data_stats_resp_msg_v01 *resp;
-	struct ipa_msg_meta msg_meta;
-	int rc;
-
-	resp = kzalloc(sizeof(struct ipa_get_data_stats_resp_msg_v01),
-		       GFP_KERNEL);
-	if (!resp) {
-		IPAWANERR("Can't allocate memory for stats message\n");
-		return;
-	}
-
-	memset(&req, 0, sizeof(struct ipa_get_data_stats_req_msg_v01));
-	memset(resp, 0, sizeof(struct ipa_get_data_stats_resp_msg_v01));
-
-	req.ipa_stats_type = QMI_IPA_STATS_TYPE_PIPE_V01;
-	if (reset == true) {
-		req.reset_stats_valid = true;
-		req.reset_stats = true;
-		IPAWANERR("Get the latest pipe-stats and reset it\n");
-	}
-
-	rc = ipa_qmi_get_data_stats(&req, resp);
-
-	if (!rc) {
-		memset(&msg_meta, 0, sizeof(struct ipa_msg_meta));
-		msg_meta.msg_type = IPA_TETHERING_STATS_UPDATE_STATS;
-		msg_meta.msg_len =
-			sizeof(struct ipa_get_data_stats_resp_msg_v01);
-		rc = ipa_send_msg(&msg_meta, resp, rmnet_ipa_free_msg);
-		if (rc) {
-			IPAWANERR("ipa_send_msg failed: %d\n", rc);
-			kfree(resp);
-			return;
-		}
-	}
-}
-
-/**
- * tethering_stats_poll_queue() - Stats polling function
- * @work - Work entry
- *
- * This function is scheduled periodically (per the interval) in
- * order to poll the IPA Modem driver for the pipe stats.
- */
-static void tethering_stats_poll_queue(struct work_struct *work)
-{
-	rmnet_ipa_get_stats_and_update(false);
-
-	/* Schedule again only if there's an active polling interval */
-	if (0 != ipa_rmnet_ctx.polling_interval)
-		schedule_delayed_work(&ipa_tether_stats_poll_wakequeue_work,
-			msecs_to_jiffies(ipa_rmnet_ctx.polling_interval*1000));
-}
-
-/**
- * rmnet_ipa_get_network_stats_and_update() - Get network stats from IPA Modem
- *
- * This function retrieves the data usage (used quota) from the IPA Modem driver
- * via QMI, and updates IPA user space entity.
- */
-static void rmnet_ipa_get_network_stats_and_update(void)
-{
-	struct ipa_get_apn_data_stats_req_msg_v01 req;
-	struct ipa_get_apn_data_stats_resp_msg_v01 *resp;
-	struct ipa_msg_meta msg_meta;
-	int rc;
-
-	resp = kzalloc(sizeof(struct ipa_get_apn_data_stats_resp_msg_v01),
-		       GFP_KERNEL);
-	if (!resp) {
-		IPAWANERR("Can't allocate memory for network stats message\n");
-		return;
-	}
-
-	memset(&req, 0, sizeof(struct ipa_get_apn_data_stats_req_msg_v01));
-	memset(resp, 0, sizeof(struct ipa_get_apn_data_stats_resp_msg_v01));
-
-	req.mux_id_list_valid = true;
-	req.mux_id_list_len = 1;
-	req.mux_id_list[0] = ipa_rmnet_ctx.metered_mux_id;
-
-	rc = ipa_qmi_get_network_stats(&req, resp);
-
-	if (!rc) {
-		memset(&msg_meta, 0, sizeof(struct ipa_msg_meta));
-		msg_meta.msg_type = IPA_TETHERING_STATS_UPDATE_NETWORK_STATS;
-		msg_meta.msg_len =
-			sizeof(struct ipa_get_apn_data_stats_resp_msg_v01);
-		rc = ipa_send_msg(&msg_meta, resp, rmnet_ipa_free_msg);
-		if (rc) {
-			IPAWANERR("ipa_send_msg failed: %d\n", rc);
-			kfree(resp);
-			return;
-		}
-	}
-}
-
-/**
- * rmnet_ipa_poll_tethering_stats() - Tethering stats polling IOCTL handler
- * @data - IOCTL data
- *
- * This function handles WAN_IOC_POLL_TETHERING_STATS.
- * In case polling interval received is 0, polling will stop
- * (If there's a polling in progress, it will allow it to finish), and then will
- * fetch network stats, and update the IPA user space.
- *
- * Return codes:
- * 0: Success
- */
-int rmnet_ipa_poll_tethering_stats(struct wan_ioctl_poll_tethering_stats *data)
-{
-	ipa_rmnet_ctx.polling_interval = data->polling_interval_secs;
-
-	cancel_delayed_work_sync(&ipa_tether_stats_poll_wakequeue_work);
-
-	if (0 == ipa_rmnet_ctx.polling_interval) {
-		ipa_qmi_stop_data_qouta();
-		rmnet_ipa_get_network_stats_and_update();
-		rmnet_ipa_get_stats_and_update(true);
-		return 0;
-	}
-
-	schedule_delayed_work(&ipa_tether_stats_poll_wakequeue_work, 0);
-	return 0;
-}
-
-/**
- * rmnet_ipa_set_data_quota() - Data quota setting IOCTL handler
- * @data - IOCTL data
- *
- * This function handles WAN_IOC_SET_DATA_QUOTA.
- * It translates the given inteface name to the Modem MUX ID and
- * sends the request of the quota to the IPA Modem driver via QMI.
- *
- * Return codes:
- * 0: Success
- * -EFAULT: Invalid interface name provided
- * other: See ipa_qmi_set_data_quota
- */
-int rmnet_ipa_set_data_quota(struct wan_ioctl_set_data_quota *data)
-{
-	u32 mux_id;
-	int index;
-	struct ipa_set_data_usage_quota_req_msg_v01 req;
-
-	/* prevent string buffer overflows */
-	data->interface_name[IFNAMSIZ-1] = '\0';
-
-	index = find_vchannel_name_index(data->interface_name);
-	IPAWANERR("iface name %s, quota %lu\n",
-			  data->interface_name,
-			  (unsigned long int) data->quota_mbytes);
-
-	if (index == MAX_NUM_OF_MUX_CHANNEL) {
-		IPAWANERR("%s is an invalid iface name\n",
-			  data->interface_name);
-		return -EFAULT;
-	}
-
-	mux_id = mux_channel[index].mux_id;
-
-	ipa_rmnet_ctx.metered_mux_id = mux_id;
-
-	memset(&req, 0, sizeof(struct ipa_set_data_usage_quota_req_msg_v01));
-	req.apn_quota_list_valid = true;
-	req.apn_quota_list_len = 1;
-	req.apn_quota_list[0].mux_id = mux_id;
-	req.apn_quota_list[0].num_Mbytes = data->quota_mbytes;
-
-	return ipa_qmi_set_data_quota(&req);
-}
-
- /* rmnet_ipa_set_tether_client_pipe() -
- * @data - IOCTL data
- *
- * This function handles WAN_IOC_SET_DATA_QUOTA.
- * It translates the given interface name to the Modem MUX ID and
- * sends the request of the quota to the IPA Modem driver via QMI.
- *
- * Return codes:
- * 0: Success
- * -EFAULT: Invalid src/dst pipes provided
- * other: See ipa_qmi_set_data_quota
- */
-int rmnet_ipa_set_tether_client_pipe(
-	struct wan_ioctl_set_tether_client_pipe *data)
-{
-	int number, i;
-
-	/* error checking if ul_src_pipe_len valid or not*/
-	if (data->ul_src_pipe_len > QMI_IPA_MAX_PIPES_V01 ||
-		data->ul_src_pipe_len < 0) {
-		IPAWANERR("UL src pipes %d exceeding max %d\n",
-			data->ul_src_pipe_len,
-			QMI_IPA_MAX_PIPES_V01);
-		return -EFAULT;
-	}
-	/* error checking if dl_dst_pipe_len valid or not*/
-	if (data->dl_dst_pipe_len > QMI_IPA_MAX_PIPES_V01 ||
-		data->dl_dst_pipe_len < 0) {
-		IPAWANERR("DL dst pipes %d exceeding max %d\n",
-			data->dl_dst_pipe_len,
-			QMI_IPA_MAX_PIPES_V01);
-		return -EFAULT;
-	}
-
-	IPAWANDBG("client %d, UL %d, DL %d, reset %d\n",
-	data->ipa_client,
-	data->ul_src_pipe_len,
-	data->dl_dst_pipe_len,
-	data->reset_client);
-	number = data->ul_src_pipe_len;
-	for (i = 0; i < number; i++) {
-		IPAWANDBG("UL index-%d pipe %d\n", i,
-			data->ul_src_pipe_list[i]);
-		if (data->reset_client)
-			ipa_set_client(data->ul_src_pipe_list[i],
-				0, false);
-		else
-			ipa_set_client(data->ul_src_pipe_list[i],
-				data->ipa_client, true);
-	}
-	number = data->dl_dst_pipe_len;
-	for (i = 0; i < number; i++) {
-		IPAWANDBG("DL index-%d pipe %d\n", i,
-			data->dl_dst_pipe_list[i]);
-		if (data->reset_client)
-			ipa_set_client(data->dl_dst_pipe_list[i],
-				0, false);
-		else
-			ipa_set_client(data->dl_dst_pipe_list[i],
-				data->ipa_client, false);
-	}
-	return 0;
-}
-
-int rmnet_ipa_query_tethering_stats(struct wan_ioctl_query_tether_stats *data,
-	bool reset)
-{
-	struct ipa_get_data_stats_req_msg_v01 *req;
-	struct ipa_get_data_stats_resp_msg_v01 *resp;
-	int pipe_len, rc;
-
-	req = kzalloc(sizeof(struct ipa_get_data_stats_req_msg_v01),
-			GFP_KERNEL);
-	if (!req) {
-		IPAWANERR("Can't allocate memory for stats message\n");
-		return rc;
-	}
-	resp = kzalloc(sizeof(struct ipa_get_data_stats_resp_msg_v01),
-			GFP_KERNEL);
-	if (!resp) {
-		IPAWANERR("Can't allocate memory for stats message\n");
-		kfree(req);
-		return rc;
-	}
-	memset(req, 0, sizeof(struct ipa_get_data_stats_req_msg_v01));
-	memset(resp, 0, sizeof(struct ipa_get_data_stats_resp_msg_v01));
-
-	req->ipa_stats_type = QMI_IPA_STATS_TYPE_PIPE_V01;
-	if (reset) {
-		req->reset_stats_valid = true;
-		req->reset_stats = true;
-		IPAWANERR("reset the pipe stats\n");
-	} else {
-		/* print tethered-client enum */
-		IPAWANDBG("Tethered-client enum(%d)\n", data->ipa_client);
-	}
-
-	rc = ipa_qmi_get_data_stats(req, resp);
-	if (rc) {
-		IPAWANERR("can't get ipa_qmi_get_data_stats\n");
-		kfree(req);
-		kfree(resp);
-		return rc;
-	} else if (reset) {
-		kfree(req);
-		kfree(resp);
-		return 0;
-	}
-
-	if (resp->dl_dst_pipe_stats_list_valid) {
-		for (pipe_len = 0; pipe_len < resp->dl_dst_pipe_stats_list_len;
-			pipe_len++) {
-			IPAWANDBG("Check entry(%d) dl_dst_pipe(%d)\n",
-				pipe_len, resp->dl_dst_pipe_stats_list
-					[pipe_len].pipe_index);
-			IPAWANDBG("dl_p_v4(%lu)v6(%lu) dl_b_v4(%lu)v6(%lu)\n",
-				(unsigned long int) resp->
-				dl_dst_pipe_stats_list[pipe_len].
-				num_ipv4_packets,
-				(unsigned long int) resp->
-				dl_dst_pipe_stats_list[pipe_len].
-				num_ipv6_packets,
-				(unsigned long int) resp->
-				dl_dst_pipe_stats_list[pipe_len].
-				num_ipv4_bytes,
-				(unsigned long int) resp->
-				dl_dst_pipe_stats_list[pipe_len].
-				num_ipv6_bytes);
-			if (ipa_get_client_uplink(resp->
-				dl_dst_pipe_stats_list[pipe_len].
-				pipe_index) == false) {
-				if (data->ipa_client == ipa_get_client(resp->
-					dl_dst_pipe_stats_list[pipe_len].
-					pipe_index)) {
-					/* update the DL stats */
-					data->ipv4_rx_packets += resp->
-					dl_dst_pipe_stats_list[pipe_len].
-					num_ipv4_packets;
-					data->ipv6_rx_packets += resp->
-					dl_dst_pipe_stats_list[pipe_len].
-					num_ipv6_packets;
-					data->ipv4_rx_bytes += resp->
-					dl_dst_pipe_stats_list[pipe_len].
-					num_ipv4_bytes;
-					data->ipv6_rx_bytes += resp->
-					dl_dst_pipe_stats_list[pipe_len].
-					num_ipv6_bytes;
-				}
-			}
-		}
-	}
-	IPAWANDBG("v4_rx_p(%lu) v6_rx_p(%lu) v4_rx_b(%lu) v6_rx_b(%lu)\n",
-		(unsigned long int) data->ipv4_rx_packets,
-		(unsigned long int) data->ipv6_rx_packets,
-		(unsigned long int) data->ipv4_rx_bytes,
-		(unsigned long int) data->ipv6_rx_bytes);
-
-	if (resp->ul_src_pipe_stats_list_valid) {
-		for (pipe_len = 0; pipe_len < resp->ul_src_pipe_stats_list_len;
-			pipe_len++) {
-			IPAWANDBG("Check entry(%d) ul_dst_pipe(%d)\n",
-				pipe_len,
-				resp->ul_src_pipe_stats_list[pipe_len].
-				pipe_index);
-			IPAWANDBG("ul_p_v4(%lu)v6(%lu)ul_b_v4(%lu)v6(%lu)\n",
-				(unsigned long int) resp->
-				ul_src_pipe_stats_list[pipe_len].
-				num_ipv4_packets,
-				(unsigned long int) resp->
-				ul_src_pipe_stats_list[pipe_len].
-				num_ipv6_packets,
-				(unsigned long int) resp->
-				ul_src_pipe_stats_list[pipe_len].
-				num_ipv4_bytes,
-				(unsigned long int) resp->
-				ul_src_pipe_stats_list[pipe_len].
-				num_ipv6_bytes);
-			if (ipa_get_client_uplink(resp->
-				ul_src_pipe_stats_list[pipe_len].
-				pipe_index) == true) {
-				if (data->ipa_client == ipa_get_client(resp->
-				ul_src_pipe_stats_list[pipe_len].
-				pipe_index)) {
-					/* update the DL stats */
-					data->ipv4_tx_packets += resp->
-					ul_src_pipe_stats_list[pipe_len].
-					num_ipv4_packets;
-					data->ipv6_tx_packets += resp->
-					ul_src_pipe_stats_list[pipe_len].
-					num_ipv6_packets;
-					data->ipv4_tx_bytes += resp->
-					ul_src_pipe_stats_list[pipe_len].
-					num_ipv4_bytes;
-					data->ipv6_tx_bytes += resp->
-					ul_src_pipe_stats_list[pipe_len].
-					num_ipv6_bytes;
-				}
-			}
-		}
-	}
-	IPAWANDBG("tx_p_v4(%lu)v6(%lu)tx_b_v4(%lu) v6(%lu)\n",
-		(unsigned long int) data->ipv4_tx_packets,
-		(unsigned long  int) data->ipv6_tx_packets,
-		(unsigned long int) data->ipv4_tx_bytes,
-		(unsigned long int) data->ipv6_tx_bytes);
-	kfree(req);
-	kfree(resp);
-	return 0;
-}
-
-/**
- * ipa_broadcast_quota_reach_ind() - Send Netlink broadcast on Quota
- * @mux_id - The MUX ID on which the quota has been reached
- *
- * This function broadcasts a Netlink event using the kobject of the
- * rmnet_ipa interface in order to alert the user space that the quota
- * on the specific interface which matches the mux_id has been reached.
- *
- */
-void ipa_broadcast_quota_reach_ind(u32 mux_id)
-{
-	char alert_msg[IPA_QUOTA_REACH_ALERT_MAX_SIZE];
-	char iface_name_l[IPA_QUOTA_REACH_IF_NAME_MAX_SIZE];
-	char iface_name_m[IPA_QUOTA_REACH_IF_NAME_MAX_SIZE];
-	char *envp[IPA_UEVENT_NUM_EVNP] = {
-		alert_msg, iface_name_l, iface_name_m, NULL };
-	int res;
-	int index;
-
-	index = find_mux_channel_index(mux_id);
-
-	if (index == MAX_NUM_OF_MUX_CHANNEL) {
-		IPAWANERR("%u is an mux ID\n", mux_id);
-		return;
-	}
-
-	res = snprintf(alert_msg, IPA_QUOTA_REACH_ALERT_MAX_SIZE,
-			"ALERT_NAME=%s", "quotaReachedAlert");
-	if (IPA_QUOTA_REACH_ALERT_MAX_SIZE <= res) {
-		IPAWANERR("message too long (%d)", res);
-		return;
-	}
-	/* posting msg for L-release for CNE */
-	res = snprintf(iface_name_l, IPA_QUOTA_REACH_IF_NAME_MAX_SIZE,
-		       "UPSTREAM=%s", mux_channel[index].vchannel_name);
-	if (IPA_QUOTA_REACH_IF_NAME_MAX_SIZE <= res) {
-		IPAWANERR("message too long (%d)", res);
-		return;
-	}
-	/* posting msg for M-release for CNE */
-	res = snprintf(iface_name_m, IPA_QUOTA_REACH_IF_NAME_MAX_SIZE,
-		       "INTERFACE=%s", mux_channel[index].vchannel_name);
-	if (IPA_QUOTA_REACH_IF_NAME_MAX_SIZE <= res) {
-		IPAWANERR("message too long (%d)", res);
-		return;
-	}
-
-	IPAWANERR("putting nlmsg: <%s> <%s> <%s>\n",
-		alert_msg, iface_name_l, iface_name_m);
-	kobject_uevent_env(&(ipa_netdevs[0]->dev.kobj), KOBJ_CHANGE, envp);
-}
-
-/**
- * ipa_q6_handshake_complete() - Perform operations once Q6 is up
- * @ssr_bootup - Indicates whether this is a cold boot-up or post-SSR.
- *
- * This function is invoked once the handshake between the IPA AP driver
- * and IPA Q6 driver is complete. At this point, it is possible to perform
- * operations which can't be performed until IPA Q6 driver is up.
- *
- */
-void ipa_q6_handshake_complete(bool ssr_bootup)
-{
-	if (ssr_bootup) {
-		/*
-		 * In case the uC is required to be loaded by the Modem,
-		 * the proxy vote will be removed only when uC loading is
-		 * complete and indication is received by the AP. After SSR,
-		 * uC is already loaded. Therefore, proxy vote can be removed
-		 * once Modem init is complete.
-		 */
-		ipa_proxy_clk_unvote();
-
-		/*
-		 * It is required to recover the network stats after
-		 * SSR recovery
-		 */
-		rmnet_ipa_get_network_stats_and_update();
-
-		/* Enable holb monitoring on Q6 pipes. */
-		ipa_q6_monitor_holb_mitigation(true);
-	}
-}
-
-static int __init ipa_wwan_init(void)
-{
-	atomic_set(&is_initialized, 0);
-	atomic_set(&is_ssr, 0);
-
-	mutex_init(&ipa_to_apps_pipe_handle_guard);
-	mutex_init(&add_mux_channel_lock);
-	ipa_to_apps_hdl = -1;
-
-	ipa_qmi_init();
-
-	/* Register for Modem SSR */
-	subsys_notify_handle = subsys_notif_register_notifier(SUBSYS_MODEM,
-						&ssr_notifier);
-	if (!IS_ERR(subsys_notify_handle))
-		return platform_driver_register(&rmnet_ipa_driver);
-	else
-		return (int)PTR_ERR(subsys_notify_handle);
-	}
-
-static void __exit ipa_wwan_cleanup(void)
-{
-	int ret;
-	ipa_qmi_cleanup();
-	mutex_destroy(&ipa_to_apps_pipe_handle_guard);
-	mutex_destroy(&add_mux_channel_lock);
-	ret = subsys_notif_unregister_notifier(subsys_notify_handle,
-					&ssr_notifier);
-	if (ret)
-		IPAWANERR(
-		"Error subsys_notif_unregister_notifier system %s, ret=%d\n",
-		SUBSYS_MODEM, ret);
-	platform_driver_unregister(&rmnet_ipa_driver);
-}
-
-static void ipa_wwan_msg_free_cb(void *buff, u32 len, u32 type)
-{
-	if (!buff)
-		IPAWANERR("Null buffer.\n");
-	kfree(buff);
-}
-
-late_initcall(ipa_wwan_init);
-module_exit(ipa_wwan_cleanup);
-MODULE_DESCRIPTION("WWAN Network Interface");
-MODULE_LICENSE("GPL v2");
diff --git a/drivers/platform/msm/ipa/rmnet_ipa_fd_ioctl.c b/drivers/platform/msm/ipa/rmnet_ipa_fd_ioctl.c
deleted file mode 100644
index 811dba4..00000000
--- a/drivers/platform/msm/ipa/rmnet_ipa_fd_ioctl.c
+++ /dev/null
@@ -1,391 +0,0 @@
-/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/init.h>
-#include <linux/module.h>
-#include <linux/fs.h>
-#include <linux/cdev.h>
-#include <linux/device.h>
-#include <linux/slab.h>
-#include <linux/uaccess.h>
-#include <linux/rmnet_ipa_fd_ioctl.h>
-#include "ipa_qmi_service.h"
-
-#define DRIVER_NAME "wwan_ioctl"
-
-#ifdef CONFIG_COMPAT
-#define WAN_IOC_ADD_FLT_RULE32 _IOWR(WAN_IOC_MAGIC, \
-		WAN_IOCTL_ADD_FLT_RULE, \
-		compat_uptr_t)
-#define WAN_IOC_ADD_FLT_RULE_INDEX32 _IOWR(WAN_IOC_MAGIC, \
-		WAN_IOCTL_ADD_FLT_INDEX, \
-		compat_uptr_t)
-#define WAN_IOC_POLL_TETHERING_STATS32 _IOWR(WAN_IOC_MAGIC, \
-		WAN_IOCTL_POLL_TETHERING_STATS, \
-		compat_uptr_t)
-#define WAN_IOC_SET_DATA_QUOTA32 _IOWR(WAN_IOC_MAGIC, \
-		WAN_IOCTL_SET_DATA_QUOTA, \
-		compat_uptr_t)
-#define WAN_IOC_SET_TETHER_CLIENT_PIPE32 _IOWR(WAN_IOC_MAGIC, \
-		WAN_IOCTL_SET_TETHER_CLIENT_PIPE, \
-		compat_uptr_t)
-#define WAN_IOC_QUERY_TETHER_STATS32 _IOWR(WAN_IOC_MAGIC, \
-		WAN_IOCTL_QUERY_TETHER_STATS, \
-		compat_uptr_t)
-#define WAN_IOC_RESET_TETHER_STATS32 _IOWR(WAN_IOC_MAGIC, \
-		WAN_IOCTL_RESET_TETHER_STATS, \
-		compat_uptr_t)
-#define WAN_IOC_QUERY_DL_FILTER_STATS32 _IOWR(WAN_IOC_MAGIC, \
-		WAN_IOCTL_QUERY_DL_FILTER_STATS, \
-		compat_uptr_t)
-#endif
-
-static unsigned int dev_num = 1;
-static struct cdev wan_ioctl_cdev;
-static unsigned int process_ioctl = 1;
-static struct class *class;
-static dev_t device;
-
-static long wan_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
-{
-	int retval = 0;
-	u32 pyld_sz;
-	u8 *param = NULL;
-
-	IPAWANDBG("device %s got ioctl events :>>>\n",
-		DRIVER_NAME);
-
-	if (!process_ioctl) {
-		IPAWANDBG("modem is in SSR, ignoring ioctl\n");
-		return -EAGAIN;
-	}
-
-	switch (cmd) {
-	case WAN_IOC_ADD_FLT_RULE:
-		IPAWANDBG("device %s got WAN_IOC_ADD_FLT_RULE :>>>\n",
-		DRIVER_NAME);
-		pyld_sz = sizeof(struct ipa_install_fltr_rule_req_msg_v01);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (qmi_filter_request_send(
-			(struct ipa_install_fltr_rule_req_msg_v01 *)param)) {
-			IPAWANDBG("IPACM->Q6 add filter rule failed\n");
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case WAN_IOC_ADD_FLT_RULE_INDEX:
-		IPAWANDBG("device %s got WAN_IOC_ADD_FLT_RULE_INDEX :>>>\n",
-		DRIVER_NAME);
-		pyld_sz = sizeof(struct ipa_fltr_installed_notif_req_msg_v01);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (qmi_filter_notify_send(
-		(struct ipa_fltr_installed_notif_req_msg_v01 *)param)) {
-			IPAWANDBG("IPACM->Q6 rule index fail\n");
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case WAN_IOC_VOTE_FOR_BW_MBPS:
-		IPAWANDBG("device %s got WAN_IOC_VOTE_FOR_BW_MBPS :>>>\n",
-		DRIVER_NAME);
-		pyld_sz = sizeof(uint32_t);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (vote_for_bus_bw((uint32_t *)param)) {
-			IPAWANERR("Failed to vote for bus BW\n");
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case WAN_IOC_POLL_TETHERING_STATS:
-		IPAWANDBG("device %s got WAN_IOCTL_POLL_TETHERING_STATS :>>>\n",
-			  DRIVER_NAME);
-		pyld_sz = sizeof(struct wan_ioctl_poll_tethering_stats);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (rmnet_ipa_poll_tethering_stats(
-		(struct wan_ioctl_poll_tethering_stats *)param)) {
-			IPAWANERR("WAN_IOCTL_POLL_TETHERING_STATS failed\n");
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case WAN_IOC_SET_DATA_QUOTA:
-		IPAWANDBG("device %s got WAN_IOCTL_SET_DATA_QUOTA :>>>\n",
-			  DRIVER_NAME);
-		pyld_sz = sizeof(struct wan_ioctl_set_data_quota);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (rmnet_ipa_set_data_quota(
-		(struct wan_ioctl_set_data_quota *)param)) {
-			IPAWANERR("WAN_IOC_SET_DATA_QUOTA failed\n");
-			retval = -EFAULT;
-			break;
-		}
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case WAN_IOC_SET_TETHER_CLIENT_PIPE:
-		IPAWANDBG("device %s got WAN_IOC_SET_TETHER_CLIENT_PIPE :>>>\n",
-				DRIVER_NAME);
-		pyld_sz = sizeof(struct wan_ioctl_set_tether_client_pipe);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		if (rmnet_ipa_set_tether_client_pipe(
-			(struct wan_ioctl_set_tether_client_pipe *)param)) {
-			IPAWANERR("WAN_IOC_SET_TETHER_CLIENT_PIPE failed\n");
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case WAN_IOC_QUERY_TETHER_STATS:
-		IPAWANDBG("device %s got WAN_IOC_QUERY_TETHER_STATS :>>>\n",
-				DRIVER_NAME);
-		pyld_sz = sizeof(struct wan_ioctl_query_tether_stats);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-
-		if (rmnet_ipa_query_tethering_stats(
-			(struct wan_ioctl_query_tether_stats *)param, false)) {
-			IPAWANERR("WAN_IOC_QUERY_TETHER_STATS failed\n");
-			retval = -EFAULT;
-			break;
-		}
-
-		if (copy_to_user((u8 *)arg, param, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	case WAN_IOC_RESET_TETHER_STATS:
-		IPAWANDBG("device %s got WAN_IOC_RESET_TETHER_STATS :>>>\n",
-				DRIVER_NAME);
-		pyld_sz = sizeof(struct wan_ioctl_reset_tether_stats);
-		param = kzalloc(pyld_sz, GFP_KERNEL);
-		if (!param) {
-			retval = -ENOMEM;
-			break;
-		}
-		if (copy_from_user(param, (u8 *)arg, pyld_sz)) {
-			retval = -EFAULT;
-			break;
-		}
-
-		if (rmnet_ipa_query_tethering_stats(NULL, true)) {
-			IPAWANERR("WAN_IOC_QUERY_TETHER_STATS failed\n");
-			retval = -EFAULT;
-			break;
-		}
-		break;
-
-	default:
-		retval = -ENOTTY;
-	}
-	kfree(param);
-	return retval;
-}
-
-#ifdef CONFIG_COMPAT
-long compat_wan_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
-{
-	switch (cmd) {
-	case WAN_IOC_ADD_FLT_RULE32:
-		cmd = WAN_IOC_ADD_FLT_RULE;
-		break;
-	case WAN_IOC_ADD_FLT_RULE_INDEX32:
-		cmd = WAN_IOC_ADD_FLT_RULE_INDEX;
-		break;
-	case WAN_IOC_POLL_TETHERING_STATS32:
-		cmd = WAN_IOC_POLL_TETHERING_STATS;
-		break;
-	case WAN_IOC_SET_DATA_QUOTA32:
-		cmd = WAN_IOC_SET_DATA_QUOTA;
-		break;
-	case WAN_IOC_SET_TETHER_CLIENT_PIPE32:
-		cmd = WAN_IOC_SET_TETHER_CLIENT_PIPE;
-		break;
-	case WAN_IOC_QUERY_TETHER_STATS32:
-		cmd = WAN_IOC_QUERY_TETHER_STATS;
-		break;
-	case WAN_IOC_RESET_TETHER_STATS32:
-		cmd = WAN_IOC_RESET_TETHER_STATS;
-		break;
-	case WAN_IOC_QUERY_DL_FILTER_STATS32:
-		cmd = WAN_IOC_QUERY_DL_FILTER_STATS;
-		break;
-	default:
-		return -ENOIOCTLCMD;
-	}
-	return wan_ioctl(file, cmd, (unsigned long) compat_ptr(arg));
-}
-#endif
-
-static int wan_ioctl_open(struct inode *inode, struct file *filp)
-{
-	IPAWANDBG("\n IPA A7 wan_ioctl open OK :>>>> ");
-	return 0;
-}
-
-const struct file_operations fops = {
-	.owner = THIS_MODULE,
-	.open = wan_ioctl_open,
-	.read = NULL,
-	.unlocked_ioctl = wan_ioctl,
-#ifdef CONFIG_COMPAT
-	.compat_ioctl = compat_wan_ioctl,
-#endif
-};
-
-int wan_ioctl_init(void)
-{
-	unsigned int wan_ioctl_major = 0;
-	int ret;
-	struct device *dev;
-
-	device = MKDEV(wan_ioctl_major, 0);
-
-	ret = alloc_chrdev_region(&device, 0, dev_num, DRIVER_NAME);
-	if (ret) {
-		IPAWANERR(":device_alloc err.\n");
-		goto dev_alloc_err;
-	}
-	wan_ioctl_major = MAJOR(device);
-
-	class = class_create(THIS_MODULE, DRIVER_NAME);
-	if (IS_ERR(class)) {
-		IPAWANERR(":class_create err.\n");
-		goto class_err;
-	}
-
-	dev = device_create(class, NULL, device,
-		NULL, DRIVER_NAME);
-	if (IS_ERR(dev)) {
-		IPAWANERR(":device_create err.\n");
-		goto device_err;
-	}
-
-	cdev_init(&wan_ioctl_cdev, &fops);
-	ret = cdev_add(&wan_ioctl_cdev, device, dev_num);
-	if (ret) {
-		IPAWANERR(":cdev_add err.\n");
-		goto cdev_add_err;
-	}
-
-	process_ioctl = 1;
-
-	IPAWANDBG("IPA %s major(%d) initial ok :>>>>\n",
-	DRIVER_NAME, wan_ioctl_major);
-	return 0;
-
-cdev_add_err:
-	device_destroy(class, device);
-device_err:
-	class_destroy(class);
-class_err:
-	unregister_chrdev_region(device, dev_num);
-dev_alloc_err:
-	return -ENODEV;
-}
-
-void wan_ioctl_stop_qmi_messages(void)
-{
-	process_ioctl = 0;
-}
-
-void wan_ioctl_enable_qmi_messages(void)
-{
-	process_ioctl = 1;
-}
-
-void wan_ioctl_deinit(void)
-{
-	cdev_del(&wan_ioctl_cdev);
-	device_destroy(class, device);
-	class_destroy(class);
-	unregister_chrdev_region(device, dev_num);
-}
diff --git a/drivers/platform/msm/ipa/teth_bridge.c b/drivers/platform/msm/ipa/teth_bridge.c
deleted file mode 100644
index 6424de5c..00000000
--- a/drivers/platform/msm/ipa/teth_bridge.c
+++ /dev/null
@@ -1,243 +0,0 @@
-/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include <linux/completion.h>
-#include <linux/debugfs.h>
-#include <linux/export.h>
-#include <linux/fs.h>
-#include <linux/if_ether.h>
-#include <linux/ioctl.h>
-#include <linux/kernel.h>
-#include <linux/module.h>
-#include <linux/msm_ipa.h>
-#include <linux/mutex.h>
-#include <linux/skbuff.h>
-#include <linux/types.h>
-#include <linux/ipa.h>
-#include <linux/netdevice.h>
-#include "ipa_i.h"
-
-#define TETH_BRIDGE_DRV_NAME "ipa_tethering_bridge"
-
-#define TETH_DBG(fmt, args...) \
-	pr_debug(TETH_BRIDGE_DRV_NAME " %s:%d " fmt, \
-		 __func__, __LINE__, ## args)
-#define TETH_DBG_FUNC_ENTRY() \
-	pr_debug(TETH_BRIDGE_DRV_NAME " %s:%d ENTRY\n", __func__, __LINE__)
-#define TETH_DBG_FUNC_EXIT() \
-	pr_debug(TETH_BRIDGE_DRV_NAME " %s:%d EXIT\n", __func__, __LINE__)
-#define TETH_ERR(fmt, args...) \
-	pr_err(TETH_BRIDGE_DRV_NAME " %s:%d " fmt, __func__, __LINE__, ## args)
-
-/**
- * struct teth_bridge_ctx - Tethering bridge driver context information
- * @class: kernel class pointer
- * @dev_num: kernel device number
- * @dev: kernel device struct pointer
- * @cdev: kernel character device struct
- */
-struct teth_bridge_ctx {
-	struct class *class;
-	dev_t dev_num;
-	struct device *dev;
-	struct cdev cdev;
-};
-static struct teth_bridge_ctx *teth_ctx;
-
-/**
-* teth_bridge_ipa_cb() - Callback to handle IPA data path events
-* @priv - private data
-* @evt - event type
-* @data - event specific data (usually skb)
-*
-* This callback is called by IPA driver for exception packets from USB.
-* All exception packets are handled by Q6 and should not reach this function.
-* Packets will arrive to AP exception pipe only in case where packets are
-* sent from USB before Q6 has setup the call.
-*/
-static void teth_bridge_ipa_cb(void *priv, enum ipa_dp_evt_type evt,
-	unsigned long data)
-{
-	struct sk_buff *skb = (struct sk_buff *)data;
-
-	TETH_DBG_FUNC_ENTRY();
-	if (evt != IPA_RECEIVE) {
-		TETH_ERR("unexpected event %d\n", evt);
-		WARN_ON(1);
-		return;
-	}
-
-	TETH_ERR("Unexpected exception packet from USB, dropping packet\n");
-	dev_kfree_skb_any(skb);
-	TETH_DBG_FUNC_EXIT();
-}
-
-/**
-* teth_bridge_init() - Initialize the Tethering bridge driver
-* @params - in/out params for USB initialization API (please look at struct
-*  definition for more info)
-*
-* USB driver gets a pointer to a callback function (usb_notify_cb) and an
-* associated data. USB driver installs this callback function in the call to
-* ipa_connect().
-*
-* Builds IPA resource manager dependency graph.
-*
-* Return codes: 0: success,
-*		-EINVAL - Bad parameter
-*		Other negative value - Failure
-*/
-int teth_bridge_init(struct teth_bridge_init_params *params)
-{
-	int res = 0;
-
-	TETH_DBG_FUNC_ENTRY();
-
-	if (!params) {
-		TETH_ERR("Bad parameter\n");
-		TETH_DBG_FUNC_EXIT();
-		return -EINVAL;
-	}
-
-	params->usb_notify_cb = teth_bridge_ipa_cb;
-	params->private_data = NULL;
-	params->skip_ep_cfg = true;
-
-	/* Build dependency graph */
-	res = ipa_rm_add_dependency(IPA_RM_RESOURCE_USB_PROD,
-				    IPA_RM_RESOURCE_Q6_CONS);
-	if (res < 0 && res != -EINPROGRESS) {
-		TETH_ERR("ipa_rm_add_dependency() failed.\n");
-		goto bail;
-	}
-	res = ipa_rm_add_dependency(IPA_RM_RESOURCE_Q6_PROD,
-				    IPA_RM_RESOURCE_USB_CONS);
-	if (res < 0 && res != -EINPROGRESS) {
-		ipa_rm_delete_dependency(IPA_RM_RESOURCE_USB_PROD,
-					IPA_RM_RESOURCE_Q6_CONS);
-		TETH_ERR("ipa_rm_add_dependency() failed.\n");
-		goto bail;
-	}
-
-	res = 0;
-	goto bail;
-
-bail:
-	TETH_DBG_FUNC_EXIT();
-	return res;
-}
-EXPORT_SYMBOL(teth_bridge_init);
-
-/**
-* teth_bridge_disconnect() - Disconnect tethering bridge module
-*/
-int teth_bridge_disconnect(enum ipa_client_type client)
-{
-	TETH_DBG_FUNC_ENTRY();
-	ipa_rm_delete_dependency(IPA_RM_RESOURCE_USB_PROD,
-				 IPA_RM_RESOURCE_Q6_CONS);
-	ipa_rm_delete_dependency(IPA_RM_RESOURCE_Q6_PROD,
-				 IPA_RM_RESOURCE_USB_CONS);
-	TETH_DBG_FUNC_EXIT();
-
-	return 0;
-}
-EXPORT_SYMBOL(teth_bridge_disconnect);
-
-/**
-* teth_bridge_connect() - Connect bridge for a tethered Rmnet / MBIM call
-* @connect_params:	Connection info
-*
-* Return codes: 0: success
-*		-EINVAL: invalid parameters
-*		-EPERM: Operation not permitted as the bridge is already
-*		connected
-*/
-int teth_bridge_connect(struct teth_bridge_connect_params *connect_params)
-{
-	return 0;
-}
-EXPORT_SYMBOL(teth_bridge_connect);
-
-static long teth_bridge_ioctl(struct file *filp,
-			      unsigned int cmd,
-			      unsigned long arg)
-{
-	IPAERR("No ioctls are supported for krypton !\n");
-	return -ENOIOCTLCMD;
-}
-
-static const struct file_operations teth_bridge_drv_fops = {
-	.owner = THIS_MODULE,
-	.unlocked_ioctl = teth_bridge_ioctl,
-};
-
-/**
-* teth_bridge_driver_init() - Initialize tethering bridge driver
-*
-*/
-int teth_bridge_driver_init(void)
-{
-	int res;
-
-	TETH_DBG("Tethering bridge driver init\n");
-	teth_ctx = kzalloc(sizeof(*teth_ctx), GFP_KERNEL);
-	if (!teth_ctx) {
-		TETH_ERR("kzalloc err.\n");
-		return -ENOMEM;
-	}
-
-	teth_ctx->class = class_create(THIS_MODULE, TETH_BRIDGE_DRV_NAME);
-
-	res = alloc_chrdev_region(&teth_ctx->dev_num, 0, 1,
-				  TETH_BRIDGE_DRV_NAME);
-	if (res) {
-		TETH_ERR("alloc_chrdev_region err.\n");
-		res = -ENODEV;
-		goto fail_alloc_chrdev_region;
-	}
-
-	teth_ctx->dev = device_create(teth_ctx->class, NULL, teth_ctx->dev_num,
-				      teth_ctx, TETH_BRIDGE_DRV_NAME);
-	if (IS_ERR(teth_ctx->dev)) {
-		TETH_ERR(":device_create err.\n");
-		res = -ENODEV;
-		goto fail_device_create;
-	}
-
-	cdev_init(&teth_ctx->cdev, &teth_bridge_drv_fops);
-	teth_ctx->cdev.owner = THIS_MODULE;
-	teth_ctx->cdev.ops = &teth_bridge_drv_fops;
-
-	res = cdev_add(&teth_ctx->cdev, teth_ctx->dev_num, 1);
-	if (res) {
-		TETH_ERR(":cdev_add err=%d\n", -res);
-		res = -ENODEV;
-		goto fail_cdev_add;
-	}
-	TETH_DBG("Tethering bridge driver init OK\n");
-
-	return 0;
-fail_cdev_add:
-	device_destroy(teth_ctx->class, teth_ctx->dev_num);
-fail_device_create:
-	unregister_chrdev_region(teth_ctx->dev_num, 1);
-fail_alloc_chrdev_region:
-	kfree(teth_ctx);
-	teth_ctx = NULL;
-
-	return res;
-}
-EXPORT_SYMBOL(teth_bridge_driver_init);
-
-MODULE_LICENSE("GPL v2");
-MODULE_DESCRIPTION("Tethering bridge driver");
-- 
cgit v1.1

